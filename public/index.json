[{"authors":["admin"],"categories":null,"content":"Hello there!\nMy name is Blas, and I am a spatial data scientist and engineer in AgTech, holding a PhD in computational ecology and an MSc in geographic information systems.\nMy expertise lies at the intersection of spatial and temporal modeling, soil and plant ecology, remote sensing, machine learning, and environmental dynamics and monitoring.\nMy work I\u0026rsquo;m deeply passionate about crafting automated data and modeling pipelines to tackle complex environmental challenges.\nCurrently, I lead the Environmental Data Team at Biome Makers Inc. In this role, I research and develop cutting-edge smart farming technologies, create essential R packages to enhance our Data Science Department\u0026rsquo;s capabilities, and oversee the design and maintenance of an environmental data infrastructure to further empower our flagship product, BeCrop®.\nMy Tech Stack My tech stack is built entirely on open-source tools: I rely on R and git+ GitHub for collaborative software development and version control. For pipeline design, I harness the power of targets, and to encapsulate code I employ renv and docker.\nFor GIS tasks, I turn to industry-standard tools like GRASS GIS, Quantum GIS, and PostGIS.\nMy data management and processing are handled by PostgreSQL, DuckDB, Apache Arrow, and Apache Spark.\nComputationally-intensive pipelines find their home in my tiny home-cluster managed by slurm.\nFor developing and deploying REST APIs, I turn to plumber, while interactive apps are crafted with Shiny. My interactive reports come to life using either Rmarkdown or Quarto.\nMy Academic Journey Before delving into AgTech, I honed my research and technical skills during a successful academic career in Computational Ecology. I worked in world-class labs in Spain ( IISTA and Maestre Lab), Denmark ( Jens-Christian Svenning Lab), and Norway ( EECRG).\nMy research primarily focused on unveiling the environmental drivers shaping the distribution of biological diversity in space and time. During this journey, I developed scientific R packages for various purposes, such as time-series comparison and analysis of lagged effects, spatial modeling with Random Forest, and ecological simulation.\nThroughout this journey, I collaborated with 210 esteemed coauthors from 22 countries to publish 49 research papers in reputable peer-reviewed journals. To date, our collective work has garnered over 1600 citations. Notably, three of these papers have received recognition as \u0026lsquo;most downloaded papers\u0026rsquo; in prestigious journals, and two have been honored as \u0026rsquo;editor\u0026rsquo;s picks\u0026rsquo;.\nBeyond Work In my leisure time, I cherish moments with my family, tinker on the piano with enthusiasm (regardless of the results!), embrace the serenity of the sea on my stand-up paddle board, and continue my passion for developing R packages.\nConnect with Me I\u0026rsquo;m always eager to connect with fellow data enthusiasts, researchers, and professionals. Feel free to connect with me on LinkedIn to explore potential collaborations and discussions within our shared field.\n","date":1709596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1709619263,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://blasbenito.com/author/blas-m.-benito/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/blas-m.-benito/","section":"authors","summary":"Hello there!\nMy name is Blas, and I am a spatial data scientist and engineer in AgTech, holding a PhD in computational ecology and an MSc in geographic information systems.","tags":null,"title":"Blas M. Benito","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\nOnline courses Project or software documentation Tutorials The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://blasbenito.com/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://blasbenito.com/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://blasbenito.com/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://blasbenito.com/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Blas M. Benito"],"categories":[],"content":"This is a live post listing links to Data Science related posts and videos I consider to be interesting, high-quality, or even essential to better understand particular topics within such a wide field.\nData Preprocessing Extending Target Encoding: post by Daniele Micci-Barreca explaining how he came up with the idea of target encoding, and its possible extensions.\nTarget encoding done the right way: post by Max Halford, Head of Data at Carbonfact, explaining in detail how to combine additive smoothing and target encoding.\nHandling and Management Apache Parquet A Deep Dive into Parquet: The Data Format Engineers Need to Know: This by Aditi Prakash, published in the Airbyte Blog offers a complete guide about the Apache Parquet file format.\nQuerying Parquet with Millisecond Latency this post from by Raphael Taylor-Davies and Andrew Lamb explains in deep the optimization methods used in Apache Parquet files. Warning, this is a very technical read!\nDuckDB Multi-Database Support in DuckDB This post by Mark Raasveldt published in the DuckDB blog explains how to query together data from different databases at once.\nAnalysis and Modeling Modeling Methods Mixed Models for Big Data: This post by Michael Clark (see entry below by the same author) reviews several mixed modelling approach for large data in R.\nGeneralized Additive Models: A good online book on Generalized Additive Models by Michael Clark, Senior Machine Learning Scientist at Strong Analytics.\nModel Explainability Model-Independent Score Explanation: Post by Daniele Micci-Barreca on model explainability. It also explains a very clever method to better understand any model just from it\u0026rsquo;s predictions.\nAI Explanations whitepaper: White paper of Google\u0026rsquo;s \u0026ldquo;AI Explanations\u0026rdquo; product with a pretty good overall view of the state of the art of model explainability.\nTowards A Rigorous Science of Interpretable Machine Learning: Pre-print by Finale Doshi-Velez and Been Kim offering a rigorous definition and evaluation of model interpretability.\nSpatial Analysis PostGEESE? Introducing The DuckDB Spatial Extension: In this post, the authors of DuckDB present the new PostGIS-like spatial extension for this popular in-process data base engine.\nGeocomputation with Python: A very nice book on geographic data analysis with Python.\nCoding General Concepts A Philosophy Of Software Design: This book by John Ousterhout is full of high-level concepts and tips to help tackle software complexity. It\u0026rsquo;s so good I had to buy a hard copy that now lives in my desk. This post by Gergely Orosz offers a balanced review of the book.\nWhy You Shouldn\u0026rsquo;t Nest Your Code: In this wonderful video, CodeAesthetic explains in detail (and beautiful graphics!) a couple of methods to reduce the level of nesting in our code to improve readability and maintainability. This video has truly changed how I code in R!\nR Beautiful Code, Because We’re Worth It!: This post by Maëlle Salmon (research software engineer), and Yanina Bellini Saibene (rOpenSci Community Manager) provides simple tips to help write more visually pleasant R code.\nColoring in R’s Blind Spot: This article published in The R Journal by Achim Zeileis (he has a great analytics blog too!) and Paul Murrel offers a great overview of the base R color functions, and offers specific advice on what color palettes work better in different scenarios.\nTaking R to its limits: 70+ tips: This pre-print (not peer-reviewed AFAIK) by Tsagris and Papadakis offers a long list of tips to speed-up computation with the R language. I think a few of these tips lack enough context or are poorly explained, but it\u0026rsquo;s still a good resource to help optimize our R code.\nCode Smell: Error Handling Eclipse : This post by Nick Tierney explains how to address these situations when error checking code totally eclipses the intent of the code.\nBuilding a team of internal R packages: This post by Emily Riederer delves into the particularities of building a team of R packages to do jobs helping a organization answer impactful questions.\nPython Deep Learning With Python: This book by François Chollet, Software Engineer at Google and creator of the Keras library, seems to me like the best resource out there for those wanting to understand and build deep learning models from scratch. I have a hard copy on my desk, and I am finding it pretty easy to follow. Also, the code examples are clearly explained, and they ramp up in a very consistent manner.\nPython Rgonomics: In this post, Emily Riederer offers a list of Python libraries with an \u0026ldquo;R feeling\u0026rdquo;.\nCoding Workflow How to use stacked PRs to unblock your entire team: This post in Graphite\u0026rsquo;s blog explains how to split large coding changes into small managed PRs (aka \u0026ldquo;stacked PRs\u0026rdquo;) to avoid blocks when PR reviews are hard to come by.\nOther Fancy Things What\u0026rsquo;s new with ML in production: This post by Vicki Boykis, machine learning engineer at Mozilla.ai, goes deep into the differences and similarities between classical Machine Learning approaches and Large Language Models. I learned a lot from this read!\nWhat is Retrieval-Augmented Generation (RAG)?: In this video, Marina Danilevsky, Senior Data Scientist at IBM, offers a pretty good explanation on how the Retrieval-Augmented Generation method can improve the credibility of large language models.\nA novel framework for spatio-temporal prediction of environmental data using deep learning: This paper by Federico Amato and collaborators describes an intriguing regression method combining a feedforward neural network with empirical orthogonal functions for spatio-temporal interpolation. Regrettably, the paper offers no code or data at all, but it\u0026rsquo;s still an interesting read.\nLarge Models for Time Series and Spatio-Temporal Data A Survey and Outlook: This pre-print by Weng and collaborators reviews the current state of the art in spatio-temporal modelling with Large Language Models and Pre-Trained Foundation Models.\nManagement and Leadership You are hurting your team without even noticing This post by Anton Zaides (Development Team Leader), and Eugene Shulga, (Software Engineer) offers insight on the harmful effects of a manager\u0026rsquo;s ego in their team dynamics.\nTeamwork Habits for Leaders: This post by Csaba Okrona focuses on how shifting from talker to listener in team meetings offers a good insight to better address the team\u0026rsquo;s needs.\n","date":1709596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709619263,"objectID":"b12882fd55a42d7db967e354c1b77d71","permalink":"https://blasbenito.com/post/my-reading-list-data-science/","publishdate":"2024-03-05T00:00:00Z","relpermalink":"/post/my-reading-list-data-science/","section":"post","summary":"Live post with a curated list of high-quality data science posts and videos I found enlightening.","tags":["Data Science","Data Preprocessing","Machine Learning","Model Explainability","Model Interpretability"],"title":"My Reading List: Data Science","type":"post"},{"authors":["Blas M. Benito"],"categories":[],"content":"Summary Categorical predictors are annoying stringy monsters that can turn any data analysis and modeling effort into a real annoyance. The post delves into the complexities of dealing with these types of predictors using methods such as one-hot encoding (please don\u0026rsquo;t) or target encoding, and provides insights into its mechanisms and quirks\nKey Highlights: Categorical Predictors are Kinda Annoying: This section discusses the common issues encountered with categorical predictors during data analysis.\nOne-Hot Encoding Pitfalls: While discussing one-hot encoding, the post focuses on its limitations, including dimensionality explosion, increased multicollinearity, and sparsity in tree-based models.\nIntro to Target Encoding: Introducing target encoding as an alternative, the post explains its concept, illustrating the basic form with mean encoding and subsequent enhancements with additive smoothing, leave-one-out encoding, and more.\nHandling Sparsity and Repetition: It emphasizes the potential pitfalls of target encoding, such as repeated values within categories and their impact on model performance, prompting the exploration of strategies like white noise addition and random encoding to mitigate these issues.\nTarget Encoding Lab: The post concludes with a detailed demonstration using the collinear::target_encoding_lab() function, offering a hands-on exploration of various target encoding methods, parameter combinations, and their visual representations.\nThe post intends to serve as a useful resource for data scientists exploring alternative encoding techniques for categorical predictors.\nResources Interactive notebook of this post. A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems Extending Target Encoding Target encoding done the right way. R packages This tutorial requires the development version (\u0026gt;= 1.0.3) of the newly released R package collinear, and a few more.\n#required install.packages(\u0026quot;remotes\u0026quot;) remotes::install_github( repo = \u0026quot;blasbenito/collinear\u0026quot;, ref = \u0026quot;development\u0026quot;, force = TRUE ) install.packages(\u0026quot;fastDummies\u0026quot;) install.packages(\u0026quot;rpart\u0026quot;) install.packages(\u0026quot;rpart.plot\u0026quot;) install.packages(\u0026quot;dplyr\u0026quot;) install.packages(\u0026quot;ggplot2\u0026quot;) library(rpart) library(rpart.plot) library(collinear) library(fastDummies) ## Thank you for using fastDummies! ## To acknowledge our work, please cite the package: ## Kaplan, J. \u0026amp; Schlegel, B. (2023). fastDummies: Fast Creation of Dummy (Binary) Columns and Rows from Categorical Variables. Version 1.7.1. URL: https://github.com/jacobkap/fastDummies, https://jacobkap.github.io/fastDummies/. library(dplyr) ## ## Attaching package: 'dplyr' ## The following objects are masked from 'package:stats': ## ## filter, lag ## The following objects are masked from 'package:base': ## ## intersect, setdiff, setequal, union library(ggplot2) Categorical Predictors are Kinda Annoying I mean, the title of this section says it already, and I bet you have experienced it during an Exploratory Data Analysis (EDA) or a feature selection for model training and the likes. You likely had a nice bunch of numerical variables to use as predictors, no issues there. But then, you discovered among your columns thingies like \u0026ldquo;sampling_location\u0026rdquo;, \u0026ldquo;region_name\u0026rdquo;, \u0026ldquo;favorite_color\u0026rdquo;, or any other type of predictor made of strings, lots of strings. And some of these made sense, and some of them didn\u0026rsquo;t, because who knows where they came from. And you had to branch your code to deal with numeric and categorical variables separately. Or maybe chose to ignore them, as I have done plenty of times.\nYeah, nobody likes them much at all, But sometimes, these stringy monsters are all you have to move on with your work. And you are not the only one. That\u0026rsquo;s why many efforts have been made to convert them to numeric and kill the problem at once, so now we all have two problems instead.\nLet me go ahead and illustrate the issue. There is a nice data frame in the collinear R package named vi, with one response variable named vi_mean, and several numeric and categorical predictors named in the vector vi_predictors.\ndata( vi, vi_predictors ) dplyr::glimpse(vi) ## Rows: 30,000 ## Columns: 68 ## $ longitude \u0026lt;dbl\u0026gt; -114.254306, 114.845693, -122.145972, 108.3… ## $ latitude \u0026lt;dbl\u0026gt; 45.0540272, 26.2706940, 56.3790272, 29.9456… ## $ vi_mean \u0026lt;dbl\u0026gt; 0.38, 0.53, 0.45, 0.69, 0.42, 0.68, 0.70, 0… ## $ vi_max \u0026lt;dbl\u0026gt; 0.57, 0.67, 0.65, 0.85, 0.64, 0.78, 0.77, 0… ## $ vi_min \u0026lt;dbl\u0026gt; 0.12, 0.41, 0.25, 0.50, 0.25, 0.48, 0.60, 0… ## $ vi_range \u0026lt;dbl\u0026gt; 0.45, 0.26, 0.40, 0.34, 0.39, 0.31, 0.17, 0… ## $ vi_binary \u0026lt;dbl\u0026gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0… ## $ koppen_zone \u0026lt;chr\u0026gt; \u0026quot;BSk\u0026quot;, \u0026quot;Cfa\u0026quot;, \u0026quot;Dfc\u0026quot;, \u0026quot;Cfb\u0026quot;, \u0026quot;Aw\u0026quot;, \u0026quot;Cfa\u0026quot;, \u0026quot;A… ## $ koppen_group \u0026lt;chr\u0026gt; \u0026quot;Arid\u0026quot;, \u0026quot;Temperate\u0026quot;, \u0026quot;Cold\u0026quot;, \u0026quot;Temperate\u0026quot;, \u0026quot;… ## $ koppen_description \u0026lt;chr\u0026gt; \u0026quot;steppe, cold\u0026quot;, \u0026quot;no dry season, hot summer\u0026quot;… ## $ soil_type \u0026lt;chr\u0026gt; \u0026quot;Cambisols\u0026quot;, \u0026quot;Acrisols\u0026quot;, \u0026quot;Luvisols\u0026quot;, \u0026quot;Aliso… ## $ topo_slope \u0026lt;int\u0026gt; 6, 2, 0, 10, 0, 10, 6, 0, 2, 0, 0, 1, 0, 1,… ## $ topo_diversity \u0026lt;int\u0026gt; 29, 24, 21, 25, 19, 30, 26, 20, 26, 22, 25,… ## $ topo_elevation \u0026lt;int\u0026gt; 1821, 143, 765, 1474, 378, 485, 604, 1159, … ## $ swi_mean \u0026lt;dbl\u0026gt; 27.5, 56.1, 41.4, 59.3, 37.4, 56.3, 52.3, 2… ## $ swi_max \u0026lt;dbl\u0026gt; 62.9, 74.4, 81.9, 81.1, 83.2, 73.8, 55.8, 3… ## $ swi_min \u0026lt;dbl\u0026gt; 24.5, 33.3, 42.2, 31.3, 8.3, 28.8, 25.3, 11… ## $ swi_range \u0026lt;dbl\u0026gt; 38.4, 41.2, 39.7, 49.8, 74.9, 45.0, 30.5, 2… ## $ soil_temperature_mean \u0026lt;dbl\u0026gt; 4.8, 19.9, 1.2, 13.0, 28.2, 18.1, 21.5, 23.… ## $ soil_temperature_max \u0026lt;dbl\u0026gt; 29.9, 32.6, 20.4, 24.6, 41.6, 29.1, 26.4, 4… ## $ soil_temperature_min \u0026lt;dbl\u0026gt; -12.4, 3.9, -16.0, -0.4, 16.8, 4.1, 17.3, 5… ## $ soil_temperature_range \u0026lt;dbl\u0026gt; 42.3, 28.8, 36.4, 25.0, 24.8, 24.9, 9.1, 38… ## $ soil_sand \u0026lt;int\u0026gt; 41, 39, 27, 29, 48, 33, 30, 78, 23, 64, 54,… ## $ soil_clay \u0026lt;int\u0026gt; 20, 24, 28, 31, 27, 29, 40, 15, 26, 22, 23,… ## $ soil_silt \u0026lt;int\u0026gt; 38, 35, 43, 38, 23, 36, 29, 6, 49, 13, 22, … ## $ soil_ph \u0026lt;dbl\u0026gt; 6.5, 5.9, 5.6, 5.5, 6.5, 5.8, 5.2, 7.1, 7.3… ## $ soil_soc \u0026lt;dbl\u0026gt; 43.1, 14.6, 36.4, 34.9, 8.1, 20.8, 44.5, 4.… ## $ soil_nitrogen \u0026lt;dbl\u0026gt; 2.8, 1.3, 2.9, 3.6, 1.2, 1.9, 2.8, 0.6, 3.1… ## $ solar_rad_mean \u0026lt;dbl\u0026gt; 17.634, 19.198, 13.257, 14.163, 24.512, 17.… ## $ solar_rad_max \u0026lt;dbl\u0026gt; 31.317, 24.498, 25.283, 17.237, 28.038, 22.… ## $ solar_rad_min \u0026lt;dbl\u0026gt; 5.209, 13.311, 1.587, 9.642, 19.102, 12.196… ## $ solar_rad_range \u0026lt;dbl\u0026gt; 26.108, 11.187, 23.696, 7.595, 8.936, 10.20… ## $ growing_season_length \u0026lt;dbl\u0026gt; 139, 365, 164, 333, 228, 365, 365, 60, 365,… ## $ growing_season_temperature \u0026lt;dbl\u0026gt; 12.65, 19.35, 11.55, 12.45, 26.45, 17.75, 2… ## $ growing_season_rainfall \u0026lt;dbl\u0026gt; 224.5, 1493.4, 345.4, 1765.5, 984.4, 1860.5… ## $ growing_degree_days \u0026lt;dbl\u0026gt; 2140.5, 7080.9, 2053.2, 4162.9, 10036.7, 64… ## $ temperature_mean \u0026lt;dbl\u0026gt; 3.65, 19.35, 1.45, 11.35, 27.55, 17.65, 22.… ## $ temperature_max \u0026lt;dbl\u0026gt; 24.65, 33.35, 21.15, 23.75, 38.35, 30.55, 2… ## $ temperature_min \u0026lt;dbl\u0026gt; -14.05, 3.05, -18.25, -3.55, 19.15, 2.45, 1… ## $ temperature_range \u0026lt;dbl\u0026gt; 38.7, 30.3, 39.4, 27.3, 19.2, 28.1, 7.0, 29… ## $ temperature_seasonality \u0026lt;dbl\u0026gt; 882.6, 786.6, 1070.9, 724.7, 219.3, 747.2, … ## $ rainfall_mean \u0026lt;int\u0026gt; 446, 1493, 560, 1794, 990, 1860, 3150, 356,… ## $ rainfall_min \u0026lt;int\u0026gt; 25, 37, 24, 29, 0, 60, 122, 1, 10, 12, 0, 0… ## $ rainfall_max \u0026lt;int\u0026gt; 62, 209, 87, 293, 226, 275, 425, 62, 256, 3… ## $ rainfall_range \u0026lt;int\u0026gt; 37, 172, 63, 264, 226, 215, 303, 61, 245, 2… ## $ evapotranspiration_mean \u0026lt;dbl\u0026gt; 78.32, 105.88, 50.03, 64.65, 156.60, 108.50… ## $ evapotranspiration_max \u0026lt;dbl\u0026gt; 164.70, 190.86, 117.53, 115.79, 187.71, 191… ## $ evapotranspiration_min \u0026lt;dbl\u0026gt; 13.67, 50.44, 3.53, 28.01, 128.59, 51.39, 8… ## $ evapotranspiration_range \u0026lt;dbl\u0026gt; 151.03, 140.42, 113.99, 87.79, 59.13, 139.9… ## $ cloud_cover_mean \u0026lt;int\u0026gt; 31, 48, 42, 64, 38, 52, 60, 13, 53, 20, 11,… ## $ cloud_cover_max \u0026lt;int\u0026gt; 39, 61, 49, 71, 58, 67, 77, 18, 60, 27, 23,… ## $ cloud_cover_min \u0026lt;int\u0026gt; 16, 34, 33, 54, 19, 39, 45, 6, 45, 14, 2, 1… ## $ cloud_cover_range \u0026lt;int\u0026gt; 23, 27, 15, 17, 38, 27, 32, 11, 15, 12, 21,… ## $ aridity_index \u0026lt;dbl\u0026gt; 0.54, 1.27, 0.90, 2.08, 0.55, 1.67, 2.88, 0… ## $ humidity_mean \u0026lt;dbl\u0026gt; 55.56, 62.14, 59.87, 69.32, 51.60, 62.76, 7… ## $ humidity_max \u0026lt;dbl\u0026gt; 63.98, 65.00, 68.19, 71.90, 67.07, 65.68, 7… ## $ humidity_min \u0026lt;dbl\u0026gt; 48.41, 58.97, 53.75, 67.21, 33.89, 59.92, 7… ## $ humidity_range \u0026lt;dbl\u0026gt; 15.57, 6.03, 14.44, 4.69, 33.18, 5.76, 3.99… ## $ biogeo_ecoregion \u0026lt;chr\u0026gt; \u0026quot;South Central Rockies forests\u0026quot;, \u0026quot;Jian Nan … ## $ biogeo_biome \u0026lt;chr\u0026gt; \u0026quot;Temperate Conifer Forests\u0026quot;, \u0026quot;Tropical \u0026amp; Su… ## $ biogeo_realm \u0026lt;chr\u0026gt; \u0026quot;Nearctic\u0026quot;, \u0026quot;Indomalayan\u0026quot;, \u0026quot;Nearctic\u0026quot;, \u0026quot;Pal… ## $ country_name \u0026lt;chr\u0026gt; \u0026quot;United States of America\u0026quot;, \u0026quot;China\u0026quot;, \u0026quot;Canad… ## $ country_population \u0026lt;dbl\u0026gt; 313973000, 1338612970, 33487208, 1338612970… ## $ country_gdp \u0026lt;dbl\u0026gt; 15094000, 7973000, 1300000, 7973000, 15860,… ## $ country_income \u0026lt;chr\u0026gt; \u0026quot;1. High income: OECD\u0026quot;, \u0026quot;3. Upper middle in… ## $ continent \u0026lt;chr\u0026gt; \u0026quot;North America\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;North America\u0026quot;, \u0026quot;… ## $ region \u0026lt;chr\u0026gt; \u0026quot;Americas\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Americas\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Af… ## $ subregion \u0026lt;chr\u0026gt; \u0026quot;Northern America\u0026quot;, \u0026quot;Eastern Asia\u0026quot;, \u0026quot;Northe… The categorical variables in this data frame are identified below:\nvi_categorical \u0026lt;- collinear::identify_non_numeric_predictors( df = vi, predictors = vi_predictors ) vi_categorical ## [1] \u0026quot;koppen_zone\u0026quot; \u0026quot;koppen_group\u0026quot; \u0026quot;koppen_description\u0026quot; ## [4] \u0026quot;soil_type\u0026quot; \u0026quot;biogeo_ecoregion\u0026quot; \u0026quot;biogeo_biome\u0026quot; ## [7] \u0026quot;biogeo_realm\u0026quot; \u0026quot;country_name\u0026quot; \u0026quot;country_income\u0026quot; ## [10] \u0026quot;continent\u0026quot; \u0026quot;region\u0026quot; \u0026quot;subregion\u0026quot; And finally, their number of categories:\ndata.frame( name = vi_categorical, categories = lapply( X = vi_categorical, FUN = function(x) length(unique(vi[[x]])) ) |\u0026gt; unlist() ) |\u0026gt; dplyr::arrange( dplyr::desc(categories) ) ## name categories ## 1 biogeo_ecoregion 604 ## 2 country_name 176 ## 3 soil_type 29 ## 4 koppen_zone 25 ## 5 subregion 21 ## 6 koppen_description 19 ## 7 biogeo_biome 13 ## 8 biogeo_realm 7 ## 9 continent 7 ## 10 country_income 6 ## 11 region 6 ## 12 koppen_group 5 A few, like country_name and biogeo_ecoregion, show a cardinality high enough to ruin our day, don\u0026rsquo;t they? But ok, let\u0026rsquo;s start with one with a moderate number of categories, like koppen_zone. This variable has 25 categories representing climate zones.\nsort(unique(vi$koppen_zone)) ## [1] \u0026quot;Af\u0026quot; \u0026quot;Am\u0026quot; \u0026quot;Aw\u0026quot; \u0026quot;BSh\u0026quot; \u0026quot;BSk\u0026quot; \u0026quot;BWh\u0026quot; \u0026quot;BWk\u0026quot; \u0026quot;Cfa\u0026quot; \u0026quot;Cfb\u0026quot; \u0026quot;Cfc\u0026quot; \u0026quot;Csa\u0026quot; \u0026quot;Csb\u0026quot; ## [13] \u0026quot;Cwa\u0026quot; \u0026quot;Cwb\u0026quot; \u0026quot;Dfa\u0026quot; \u0026quot;Dfb\u0026quot; \u0026quot;Dfc\u0026quot; \u0026quot;Dfd\u0026quot; \u0026quot;Dsa\u0026quot; \u0026quot;Dsb\u0026quot; \u0026quot;Dsc\u0026quot; \u0026quot;Dwa\u0026quot; \u0026quot;Dwb\u0026quot; \u0026quot;Dwc\u0026quot; ## [25] \u0026quot;ET\u0026quot; One-hot Encoding is here\u0026hellip; Let\u0026rsquo;s use it as predictor of vi_mean in a linear model and take a look at the summary.\nlm( formula = vi_mean ~ koppen_zone, data = vi ) |\u0026gt; summary() ## ## Call: ## lm(formula = vi_mean ~ koppen_zone, data = vi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.57090 -0.05592 -0.00305 0.05695 0.49212 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.670899 0.002054 326.651 \u0026lt; 2e-16 *** ## koppen_zoneAm -0.022807 0.003151 -7.239 4.64e-13 *** ## koppen_zoneAw -0.143375 0.002434 -58.903 \u0026lt; 2e-16 *** ## koppen_zoneBSh -0.347894 0.002787 -124.839 \u0026lt; 2e-16 *** ## koppen_zoneBSk -0.422162 0.002823 -149.523 \u0026lt; 2e-16 *** ## koppen_zoneBWh -0.537854 0.002392 -224.859 \u0026lt; 2e-16 *** ## koppen_zoneBWk -0.543022 0.002906 -186.883 \u0026lt; 2e-16 *** ## koppen_zoneCfa -0.104730 0.003087 -33.928 \u0026lt; 2e-16 *** ## koppen_zoneCfb -0.081909 0.003949 -20.744 \u0026lt; 2e-16 *** ## koppen_zoneCfc -0.120899 0.017419 -6.941 3.99e-12 *** ## koppen_zoneCsa -0.274720 0.005145 -53.399 \u0026lt; 2e-16 *** ## koppen_zoneCsb -0.136575 0.006142 -22.237 \u0026lt; 2e-16 *** ## koppen_zoneCwa -0.149006 0.003318 -44.910 \u0026lt; 2e-16 *** ## koppen_zoneCwb -0.177753 0.004579 -38.817 \u0026lt; 2e-16 *** ## koppen_zoneDfa -0.214981 0.004437 -48.453 \u0026lt; 2e-16 *** ## koppen_zoneDfb -0.179080 0.003347 -53.499 \u0026lt; 2e-16 *** ## koppen_zoneDfc -0.237050 0.003937 -60.207 \u0026lt; 2e-16 *** ## koppen_zoneDfd -0.395899 0.065900 -6.008 1.91e-09 *** ## koppen_zoneDsa -0.462494 0.011401 -40.567 \u0026lt; 2e-16 *** ## koppen_zoneDsb -0.330969 0.008056 -41.084 \u0026lt; 2e-16 *** ## koppen_zoneDsc -0.327097 0.011244 -29.090 \u0026lt; 2e-16 *** ## koppen_zoneDwa -0.282620 0.005248 -53.850 \u0026lt; 2e-16 *** ## koppen_zoneDwb -0.254027 0.005981 -42.473 \u0026lt; 2e-16 *** ## koppen_zoneDwc -0.306156 0.005660 -54.096 \u0026lt; 2e-16 *** ## koppen_zoneET -0.297869 0.011649 -25.571 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.09315 on 29975 degrees of freedom ## Multiple R-squared: 0.805,\tAdjusted R-squared: 0.8049 ## F-statistic: 5157 on 24 and 29975 DF, p-value: \u0026lt; 2.2e-16 Look at this monster! What the hell happened here? Linear models cannot deal with categorical predictors, so they create numeric dummy variables instead. The function stats::model.matrix() does exactly that:\ndummy_variables \u0026lt;- stats::model.matrix( ~ koppen_zone, data = vi ) ncol(dummy_variables) ## [1] 25 dummy_variables[1:10, 1:10] ## (Intercept) koppen_zoneAm koppen_zoneAw koppen_zoneBSh koppen_zoneBSk ## 1 1 0 0 0 1 ## 2 1 0 0 0 0 ## 3 1 0 0 0 0 ## 4 1 0 0 0 0 ## 5 1 0 1 0 0 ## 6 1 0 0 0 0 ## 7 1 0 0 0 0 ## 8 1 0 0 1 0 ## 9 1 0 0 0 0 ## 10 1 0 0 0 0 ## koppen_zoneBWh koppen_zoneBWk koppen_zoneCfa koppen_zoneCfb koppen_zoneCfc ## 1 0 0 0 0 0 ## 2 0 0 1 0 0 ## 3 0 0 0 0 0 ## 4 0 0 0 1 0 ## 5 0 0 0 0 0 ## 6 0 0 1 0 0 ## 7 0 0 0 0 0 ## 8 0 0 0 0 0 ## 9 0 0 0 0 0 ## 10 1 0 0 0 0 This function first creates an Intercept column with all ones. Then, for each original category except the first one (\u0026ldquo;Af\u0026rdquo;), a new column with value 1 in the cases where the given category was present and 0 otherwise is created. The category with no column (\u0026ldquo;Af\u0026rdquo;) is represented in these cases in the intercept where all other dummy columns are zero. This is, essentially, one-hot encoding with a little twist. You will find most people use the terms dummy variables and one-hot encoding interchangeably, and that\u0026rsquo;s ok. But in the end, the little twist of omitting the first category is what differentiates them. Most functions performing one-hot encoding, no matter their name, are creating as many columns as categories. That is for example the case of fastDummies::dummy_cols(), from the R package fastDummies:\ndf \u0026lt;- fastDummies::dummy_cols( .data = vi[, \u0026quot;koppen_zone\u0026quot;, drop = FALSE], select_columns = \u0026quot;koppen_zone\u0026quot;, remove_selected_columns = TRUE ) dplyr::glimpse(df) ## Rows: 30,000 ## Columns: 25 ## $ koppen_zone_Af \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Am \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Aw \u0026lt;int\u0026gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_BSh \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ koppen_zone_BSk \u0026lt;int\u0026gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_BWh \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, … ## $ koppen_zone_BWk \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, … ## $ koppen_zone_Cfa \u0026lt;int\u0026gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Cfb \u0026lt;int\u0026gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, … ## $ koppen_zone_Cfc \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Csa \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Csb \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Cwa \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Cwb \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Dfa \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, … ## $ koppen_zone_Dfb \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Dfc \u0026lt;int\u0026gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Dfd \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Dsa \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Dsb \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Dsc \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Dwa \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Dwb \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_Dwc \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ koppen_zone_ET \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … \u0026hellip;to mess-up your models As good as one-hot encoding is to fit linear models when predictors are categorical, it creates a couple of glaring issues that are hard to address when the number of encoded categories is high.\nThe first issue can easily be named the dimensionality explosion. If we created dummy variables for all categorical predictors in vi, then we\u0026rsquo;d go from the original 61 predictors to a total of 967 new columns to handle. This alone can degrade the computational performance of a model due to increased data size.\nThe second issue is increased multicollinearity. One-hot encoded features are highly collinear, which makes obtaining accurate estimates for the coefficients of the encoded categories very hard. Look at the Variance Inflation Factors of the encoded Koppen zones, they have incredibly high values!\ncollinear::vif_df( df = df ) ## variable vif ## 1 koppen_zone_Dfd 1.031226e+12 ## 2 koppen_zone_Cfc 1.493932e+13 ## 3 koppen_zone_ET 3.395786e+13 ## 4 koppen_zone_Dsa 3.549784e+13 ## 5 koppen_zone_Dsc 3.652432e+13 ## 6 koppen_zone_Dsb 7.338609e+13 ## 7 koppen_zone_Csb 1.323997e+14 ## 8 koppen_zone_Dwb 1.405032e+14 ## 9 koppen_zone_Dwc 1.592088e+14 ## 10 koppen_zone_Dwa 1.894423e+14 ## 11 koppen_zone_Csa 1.984881e+14 ## 12 koppen_zone_Cwb 2.624933e+14 ## 13 koppen_zone_Dfa 2.838687e+14 ## 14 koppen_zone_Cfb 3.834325e+14 ## 15 koppen_zone_Dfc 3.863684e+14 ## 16 koppen_zone_Dfb 6.139201e+14 ## 17 koppen_zone_Cwa 6.309241e+14 ## 18 koppen_zone_Am 7.440723e+14 ## 19 koppen_zone_Cfa 7.966760e+14 ## 20 koppen_zone_BWk 9.866240e+14 ## 21 koppen_zone_Af 9.879589e+14 ## 22 koppen_zone_BSk 1.100300e+15 ## 23 koppen_zone_BSh 1.158438e+15 ## 24 koppen_zone_Aw 2.177627e+15 ## 25 koppen_zone_BWh 2.403991e+15 On top of those issues, one-hot encoding also causes sparsity in tree-based models. Let me show you an example. Below I train a recursive partition tree using vi_mean as response, and the one-hot encoded version of koppen_zone we have in df.\n#add response variable to df df$vi_mean \u0026lt;- vi$vi_mean #fit model using all one-hot encoded variables koppen_zone_one_hot \u0026lt;- rpart::rpart( formula = vi_mean ~ ., data = df ) Now I do the same using the categorical version of koppen_zone in vi.\nkoppen_zone_categorical \u0026lt;- rpart::rpart( formula = vi_mean ~ koppen_zone, data = vi ) Finally, I am plotting the skeletons of these trees side by side (we don\u0026rsquo;t care about numbers here).\n#plot tree skeleton par(mfrow = c(1, 2)) plot(koppen_zone_one_hot, main = \u0026quot;One-hot encoding\u0026quot;) plot(koppen_zone_categorical, main = \u0026quot;Categorical\u0026quot;) Notice the stark differences in tree structure between both options. On the left, the tree trained on the one-hot encoded data only shows growth on one side! This is the sparsity I was talking about before. On the right side, however, the tree based on the categorical variable shows a balanced and healthy structure. One-hot encoded data can easily mess up a single univariate regression tree, so imagine what it can do to your fancy random forest model with hundreds of these trees.\nIn the end, the magic of one-hot encoding is in its inherent ability to create two or three problems for each one it promised to solve. We all know someone like that. Not so hot, if you ask me.\nTarget Encoding, Mean Encoding, and Dummy Variables (All The Same) On a bright summer day of 2001, Daniele Micci-Barreca finally got sick of the one-hot encoding wonders and decided to publish his ideas on a suitable alternative others later named mean encoding or target encoding. He told the story himself 20 years later, in a nice blog post titled Extending Target Encoding.\nBut what is target encoding? Let\u0026rsquo;s start with a continuous response variable y (a.k.a the target) and a categorical predictor x.\nMean Encoding In it\u0026rsquo;s simplest form, target encoding replaces each category in x with the mean of y across the category cases. This results in a new numeric version of x named x_encoded in the example below.\nyx |\u0026gt; dplyr::group_by(x) |\u0026gt; dplyr::mutate( x_encoded = mean(y) ) ## # A tibble: 7 × 3 ## # Groups: x [3] ## y x x_encoded ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 a 2 ## 2 2 a 2 ## 3 3 a 2 ## 4 4 b 5 ## 5 5 b 5 ## 6 6 b 5 ## 7 7 c 7 Simple is good, right? But sometimes it\u0026rsquo;s not. In our toy case, the category \u0026ldquo;c\u0026rdquo; has only one case that maps directly to an actual value of y.Imagine the worst case scenario of x having one different category per row, then x_encoded would be identical to y!\nMean Encoding With Additive Smoothing The issue can be solved by pushing the mean of y for each category in x towards the global mean of y by the weighted sample size of the category, as suggested by the expression\n$$x\\_encoded_i = \\frac{n_i \\times \\overline{y}_i + m \\times \\overline{y}}{n_i + m}$$\nwhere:\n\\(n_i\\) is the size of the category \\(i\\). \\(\\overline{y}_i\\) is the mean of the target over the category \\(i\\). \\(m\\) is the smoothing parameter. \\(\\overline{y}\\) is the global mean of the target. y_mean \u0026lt;- mean(yx$y) m \u0026lt;- 3 yx |\u0026gt; dplyr::group_by(x) |\u0026gt; dplyr::mutate( x_encoded = (dplyr::n() * mean(y) + m * y_mean) / (dplyr::n() + m) ) ## # A tibble: 7 × 3 ## # Groups: x [3] ## y x x_encoded ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 a 3 ## 2 2 a 3 ## 3 3 a 3 ## 4 4 b 4.5 ## 5 5 b 4.5 ## 6 6 b 4.5 ## 7 7 c 4.75 So far so good! But still, the simplest implementations of target encoding generate repeated values for all cases within a category. This can still mess-up tree-based models a bit, because splits may happen again and again in the same values of the predictor. However, there are several strategies to limit this issue as well.\nLeave-one-out Target Encoding In this version of target encoding, the encoded value of one case within a category is the mean of all other cases within the same category. This results in a robust encoding that avoids direct reference to the target value of the sample being encoded, and does not generate repeated values.\nThe code below implements the idea in a way so simple that it cannot even deal with one-case categories.\nyx |\u0026gt; dplyr::group_by(x) |\u0026gt; dplyr::mutate( x_encoded = (sum(y) - y) / (dplyr::n() - 1) ) ## # A tibble: 7 × 3 ## # Groups: x [3] ## y x x_encoded ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 a 2.5 ## 2 2 a 2 ## 3 3 a 1.5 ## 4 4 b 5.5 ## 5 5 b 5 ## 6 6 b 4.5 ## 7 7 c NaN Mean Encoding with White Noise Another way to avoid repeated values while keeping the encoding as simple as possible consists of just adding a white noise to the encoded values. The code below adds noise generated by stats::runif() to the mean-encoded values, but other options such as stats::rnorm() (noise from a normal distribution) can be useful here. Since white noise is random, we need to set the seed of the pseudo-random number generator (with set.seed()) to obtain constant results every time we run the code below.\nWhen using this method we have to be careful with the amount of noise we add. It should be a harmless fraction of target, small enough to not throw a model off the signal provided by the encoded variable. In our toy case y is between 1 and 7, so something like \u0026ldquo;one percent of the maximum\u0026rdquo; could work well here.\n#maximum noise to add max_noise \u0026lt;- max(yx$y)/100 #set seed for reproducibility set.seed(1) yx |\u0026gt; dplyr::group_by(x) |\u0026gt; dplyr::mutate( x_encoded = mean(y) + runif(n = dplyr::n(), max = max_noise) ) ## # A tibble: 7 × 3 ## # Groups: x [3] ## y x x_encoded ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 a 2.02 ## 2 2 a 2.03 ## 3 3 a 2.04 ## 4 4 b 5.06 ## 5 5 b 5.01 ## 6 6 b 5.06 ## 7 7 c 7.07 This method can deal with one-case categories without issues, and does not generate repeated values, but in exchange, we have to be mindful of the amount of noise we add, and we have to set a random seed to ensure reproducibility.\nRandom Encoding A more exotic non-deterministic method of encoding consists of computing the mean and the standard deviation of the target over the category, and then using these values to parameterize a normal distribution to extract randomized values from. This kind of encoding also requires to set the random seed to ensure reproducibility.\nset.seed(1) yx |\u0026gt; dplyr::group_by(x) |\u0026gt; dplyr::mutate( x_encoded = stats::rnorm( n = dplyr::n(), mean = mean(y), sd = ifelse( dplyr::n() == 1, stats::sd(yx$y), #use global sd for one-case groups stats::sd(y) #use local sd for n-cases groups ) ) ) ## # A tibble: 7 × 3 ## # Groups: x [3] ## y x x_encoded ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 a 1.37 ## 2 2 a 2.18 ## 3 3 a 1.16 ## 4 4 b 6.60 ## 5 5 b 5.33 ## 6 6 b 4.18 ## 7 7 c 8.05 Rank Encoding plus White Noise This is a little different from all the other methods, because it does not map the categories to values from the target, but to the rank/order of the target means per category. It basically converts the categorical variable into an ordinal one arranged along with the target, and then adds white noise on top to avoid value repetition.\n#maximum noise as function of the number of categories max_noise \u0026lt;- length(unique(yx$x))/100 yx |\u0026gt; dplyr::arrange(y) |\u0026gt; dplyr::group_by(x) |\u0026gt; dplyr::mutate( x_encoded = dplyr::cur_group_id() + runif(n = dplyr::n(), max = max_noise) ) ## # A tibble: 7 × 3 ## # Groups: x [3] ## y x x_encoded ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 a 1.02 ## 2 2 a 1.01 ## 3 3 a 1.02 ## 4 4 b 2.03 ## 5 5 b 2.01 ## 6 6 b 2.02 ## 7 7 c 3.03 The Target Encoding Lab The function collinear::target_encoding_lab() implements all these encoding methods, and allows defining different combinations of parameters. It was designed to help understand how they work, and maybe help make choices about what\u0026rsquo;s the right encoding for a given categorical predictor.\nIn the example below, the methods rank, mean, and leave-one-out are computed with white noise of 0 and 0.1 (that\u0026rsquo;s the width of the uniform distribution the noise is extracted from), the mean is also with and without smoothing, and the rnorm is computed using two different multipliers of the standard deviation of the normal distribution computed for each group in the predictor, just to help control the data spread.\nThe function also uses a random seed to generate the same noise across the encoded versions of the predictor to make them as comparable as possible. Every time you change the seed, results using white noise and the rnorm method should change as well.\nyx_encoded \u0026lt;- target_encoding_lab( df = yx, response = \u0026quot;y\u0026quot;, predictors = \u0026quot;x\u0026quot;, white_noise = c(0, 0.1), smoothing = c(0, 2), rnorm_sd_multiplier = c(0.25, 0.5), verbose = TRUE, seed = 1, #for reproducibility replace = FALSE #to replace or not the predictors with their encodings ) ## ## Encoding the predictor: x ## New encoded predictor: 'x__encoded_rank' ## New encoded predictor: 'x__encoded_mean' ## New encoded predictor: 'x__encoded_mean__smoothing_2' ## New encoded predictor: 'x__encoded_loo' ## New encoded predictor: 'x__encoded_rank__noise_0.1' ## New encoded predictor: 'x__encoded_mean__noise_0.1' ## New encoded predictor: 'x__encoded_mean__smoothing_2__noise_0.1' ## New encoded predictor: 'x__encoded_loo__noise_0.1' ## New encoded predictor: 'x__encoded_rnorm__sd_multiplier_0.25' ## New encoded predictor: 'x__encoded_rnorm__sd_multiplier_0.5' dplyr::glimpse(yx_encoded) ## Rows: 7 ## Columns: 12 ## $ y \u0026lt;int\u0026gt; 1, 2, 3, 4, 5, 6, 7 ## $ x \u0026lt;chr\u0026gt; \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;b\u0026quot;, … ## $ x__encoded_rank \u0026lt;int\u0026gt; 1, 1, 1, 2, 2, 2, 3 ## $ x__encoded_mean \u0026lt;dbl\u0026gt; 2, 2, 2, 5, 5, 5, 7 ## $ x__encoded_mean__smoothing_2 \u0026lt;dbl\u0026gt; 2.8, 2.8, 2.8, 4.6, 4.6, 4.6, … ## $ x__encoded_loo \u0026lt;dbl\u0026gt; 2.5, 2.0, 1.5, 5.5, 5.0, 4.5, … ## $ x__encoded_rank__noise_0.1 \u0026lt;dbl\u0026gt; 0.5030858, 0.6752789, 0.999474… ## $ x__encoded_mean__noise_0.1 \u0026lt;dbl\u0026gt; 1.503086, 1.675279, 1.999475, … ## $ x__encoded_mean__smoothing_2__noise_0.1 \u0026lt;dbl\u0026gt; 2.303086, 2.475279, 2.799475, … ## $ x__encoded_loo__noise_0.1 \u0026lt;dbl\u0026gt; 2.003086, 1.675279, 1.499475, … ## $ x__encoded_rnorm__sd_multiplier_0.25 \u0026lt;dbl\u0026gt; 1.843387, 2.045911, 1.791093, … ## $ x__encoded_rnorm__sd_multiplier_0.5 \u0026lt;dbl\u0026gt; 1.686773, 2.091822, 1.582186, … yx_encoded |\u0026gt; tidyr::pivot_longer( cols = dplyr::contains(\u0026quot;__encoded\u0026quot;), values_to = \u0026quot;x_encoded\u0026quot; ) |\u0026gt; ggplot() + facet_wrap(\u0026quot;name\u0026quot;) + aes( x = x_encoded, y = y, color = x ) + geom_point(size = 3) + theme_bw() The function also allows to replace a given predictor with their selected encoding. yx_encoded \u0026lt;- collinear::target_encoding_lab( df = yx, response = \u0026quot;y\u0026quot;, predictors = \u0026quot;x\u0026quot;, encoding_methods = \u0026quot;mean\u0026quot;, #selected encoding method smoothing = 2, verbose = TRUE, replace = TRUE ) ## Warning in validate_df(df = df, min_rows = 30): the number of rows in 'df' is ## lower than 30. A multicollinearity analysis may fail or yield meaningless ## results. dplyr::glimpse(yx_encoded) ## Rows: 7 ## Columns: 2 ## $ y \u0026lt;int\u0026gt; 1, 2, 3, 4, 5, 6, 7 ## $ x \u0026lt;dbl\u0026gt; 2.8, 2.8, 2.8, 4.6, 4.6, 4.6, 5.0 And that\u0026rsquo;s all about target encoding so far!\nI have a post in my TODO list with a little real experiment comparing target encoding with one-hot encoding in tree-based models. If you are interested, stay tuned!\nCheers,\nBlas\n","date":1700006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700028863,"objectID":"c5781c86093d66fb446e9375046afa41","permalink":"https://blasbenito.com/post/target-encoding/","publishdate":"2023-11-15T00:00:00Z","relpermalink":"/post/target-encoding/","section":"post","summary":"Target encoding is commonly used to map categorical variables to numeric with the objective of facilitating exploratory data analysis and machine learning modeling. This post covers the basics of this method, and explains how and when to use it.","tags":["R packages","Multicollinearity","Variable Selection"],"title":"Mapping Categorical Predictors to Numeric With Target Encoding","type":"post"},{"authors":["Blas M. Benito"],"categories":[],"content":"Resources Rmarkdown notebook used in this tutorial Multicollinearity Hinders Model Interpretability R package collinear Summary In this post, I dive deep into Variance Inflation Factors (VIF) and their crucial role in identifying multicollinearity within linear models.\nThe post covers the following main points:\nVIF meaning and interpretation: Through practical examples, I demonstrate how to compute VIF values and their significance in model design. Particularly, I try to shed light on their influence on coefficient estimates and their confidence intervals. The Impact of High VIF: I use a small simulation to show how having a model design with a high VIF hinders the identification of predictors with moderate effects, particularly in situations with limited data. Effective VIF Management: I introduce how to use the collinear package and its vif_select() function. to aid in the selection of predictors with low VIF, thereby enhancing model stability and interpretability. Ultimately, this post serves as a comprehensive resource for understanding, interpreting, and managing VIF in the context of linear modeling. It caters to those with a strong command of R and a keen interest in statistical modeling.\nR packages This tutorial requires the development version (\u0026gt;= 1.0.3) of the newly released R package collinear, and a few more.\n#required install.packages(\u0026quot;remotes\u0026quot;) remotes::install_github( repo = \u0026quot;blasbenito/collinear\u0026quot;, ref = \u0026quot;development\u0026quot; ) install.packages(\u0026quot;ranger\u0026quot;) install.packages(\u0026quot;dplyr\u0026quot;) install.packages(\u0026quot;ggplot2\u0026quot;) Example data This post uses the toy data set shipped with the version \u0026gt;= 1.0.3 of the R package collinear. It is a data frame of centered and scaled variables representing a model design of the form y ~ a + b + c + d, where the predictors show varying degrees of relatedness. Let\u0026rsquo;s load and check it.\nlibrary(dplyr) library(ggplot2) library(collinear) toy |\u0026gt; round(3) |\u0026gt; head() ## y a b c d ## 1 0.655 0.342 -0.158 0.254 0.502 ## 2 0.610 0.219 1.814 0.450 1.373 ## 3 0.316 1.078 -0.643 0.580 0.673 ## 4 0.202 0.956 -0.815 1.168 -0.147 ## 5 -0.509 -0.149 -0.356 -0.456 0.187 ## 6 0.675 0.465 1.292 -0.020 0.983 The columns in toy are related as follows:\ny: response generated from a and b using the expression y = a * 0.75 + b * 0.25 + noise. a: predictor of y uncorrelated with b. b: predictor of y uncorrelated with a. c: predictor generated as c = a + noise. d: predictor generated as d = (a + b)/2 + noise. The pairwise correlations between all predictors in toy are shown below.\ncollinear::cor_df( df = toy, predictors = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;) ) ## x y correlation ## 1 c a 0.96154984 ## 2 d b 0.63903887 ## 3 d a 0.63575882 ## 4 d c 0.61480312 ## 5 b a -0.04740881 ## 6 c b -0.04218308 Keep these pairwise correlations in mind for what comes next!\nThe Meaning of Variance Inflation Factors There are two general cases of multicollinearity in model designs:\nWhen there are pairs of predictors highly correlated. When there are predictors that are linear combinations of other predictors. The focus of this post is on the second one.\nWe can say a predictor is a linear combination of other predictors when it can be reasonably predicted from a multiple regression model against all other predictors.\nLet\u0026rsquo;s say we focus on a and fit the multiple regression model a ~ b + c + d. The higher the R-squared of this model, the more confident we are to say that a is a linear combination of b + c + d.\n#model of a against all other predictors abcd_model \u0026lt;- lm( formula = a ~ b + c + d, data = toy ) #r-squared of the a_model abcd_R2 \u0026lt;- summary(abcd_model)$r.squared abcd_R2 ## [1] 0.9381214 Since the R-squared of a against all other predictors is pretty high, it definitely seems that a is a linear combination of the other predictors, and we can conclude that there is multicollinearity in the model design.\nHowever, as informative as this R-squared is, it tells us nothing about the consequences of having multicollinearity in our model design. And this is where Variance Inflation Factors, or VIF for short, come into play.\nWhat are Variance Inflation Factors? The Variance Inflation Factor (VIF) of a predictor is computed as $1/(1 - R^2)$, where \\(R^²\\) is the R-squared of the multiple linear regression of the predictor against all other predictors.\nIn the case of a, we just have to apply the VIF expression to the R-squared of the regression model against all other predictors.\nabcd_vif \u0026lt;- 1/(1-abcd_R2) abcd_vif ## [1] 16.16067 This VIF score is relative to the other predictors in the model design. If we change the model design, so does the VIF of all predictors! For example, if we remove c and d from the model design, we are left with this VIF for a:\nab_model \u0026lt;- lm( formula = a ~ b, data = toy ) ab_vif \u0026lt;- 1/(1 - summary(ab_model)$r.squared) ab_vif ## [1] 1.002253 An almost perfect VIF score!\nWe can simplify the VIF computation using collinear::vif_df(), which returns the VIF of a and b at once.\ncollinear::vif_df( df = toy[, c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;)] ) ## predictor vif ## 1 a 1.0023 ## 2 b 1.0023 In plot below, the worst and best VIF scores of a are shown in the context of the relationship between R-squared and VIF, and three VIF thresholds commonly mentioned in the literature. These thresholds are represented as vertical dashed lines at VIF 2.5, 5, and 10, and are used as criteria to control multicollinearity in model designs. I will revisit this topic later in the post.\nWhen the R-squared of the linear regression model is 0, then the VIF expression becomes \\(1/(1 - 0) = 1\\) and returns the minimum possible VIF. On the other end, when R-squared is 1, then we get \\(1/(1 - 1) = Inf\\), the maximum VIF.\nSo far, we have learned that to assess whether the predictor a induces multicollinearity in the model design y ~ a + b + c + d we can compute it\u0026rsquo;s Variance Inflation Factor from the R-squared of the model a ~ b + c + d. We have also learned that if the model design changes, so does the VIF of a. We also know that there are some magic numbers (the VIF thresholds) we can use as reference.\nBut still, we have no indication of what these VIF values actually mean! I will try to fix that in the next section.\nBut really, what are Variance Inflation Factors? Variance Inflation Factors are inherently linked to these fundamental linear modeling concepts:\nCoefficient Estimate (\\(\\hat{\\beta}\\)): The estimated slope of the relationship between a predictor and the response in a linear model. Standard Error (\\(\\text{SE}\\)): Represents the uncertainty around the estimation of the coefficient due to data variability. Significance level (\\(1.96\\)): The acceptable level of error when determining the significance of the coefficient estimate. Here it is simplified to 1.96, the 97.5th percentile of a normal distribution, to approximate a significance level of 0.05. Confidence Interval (\\(CI\\)): The range of values containing the true value of the coefficient estimate withing a certain significance level. These terms are related by the expression to compute the confidence interval of the coefficient estimate:\n$$\\text{CI} = \\beta \\pm 1.96 \\cdot \\text{SE}$$ Let me convert this equation into a small function to compute confidence intervals of coefficient estimates named ci().\nci \u0026lt;- function(b, se){ x \u0026lt;- se * 1.96 as.numeric(c(b-x, b+x)) } #note: stats::confint() which uses t-critical values to compute more precise confidence intervals. Now we are going to look at the coefficient estimate and standard error of a in the model y ~ a + b. We know that a in this model has a vif of 1.0022527.\nyab_model \u0026lt;- lm( formula = y ~ a + b, data = toy ) |\u0026gt; summary() #coefficient estimate and standard error of a a_coef \u0026lt;- yab_model$coefficients[2, 1:2] a_coef ## Estimate Std. Error ## 0.747689326 0.006636511 Now we plug them into our little function to compute the confidence interval.\na_ci \u0026lt;- ci( b = a_coef[1], se = a_coef[2] ) a_ci ## [1] 0.7346818 0.7606969 And, finally, we compute the width of the confidence interval for a as the difference between the extremes of the confidence interval.\nold_width \u0026lt;- diff(a_ci) old_width ## [1] 0.02601512 Keep this number in mind, it\u0026rsquo;s important.\nNow, let me tell you something weird: The confidence interval of a predictor is widened by a factor equal to the square root of its Variance Inflation Factor.\nSo, if the VIF of a predictor is, let\u0026rsquo;s say, 16, then this means that, in a linear model, multicollinearity is inflating the width of its confidence interval by a factor of 4.\nIn case you don\u0026rsquo;t want to take my word for it, here goes a demonstration. Now we fit the model y ~ a + b + c + d, where a has a vif of 16.1606674. If we follow the definition above, we could now expect an inflation of the confidence interval for a of about 4.0200333. Let\u0026rsquo;s find out if that\u0026rsquo;s the case!\n#model y against all predictors and get summary yabcd_model \u0026lt;- lm( formula = y ~ a + b + c + d, data = toy ) |\u0026gt; summary() #compute confidence interval of a a_ci \u0026lt;- ci( b = yabcd_model$coefficients[\u0026quot;a\u0026quot;, \u0026quot;Estimate\u0026quot;], se = yabcd_model$coefficients[\u0026quot;a\u0026quot;, \u0026quot;Std. Error\u0026quot;] ) #compute width of confidence interval of a new_width \u0026lt;- diff(a_ci) new_width ## [1] 0.1044793 Now, to find out the inflation factor of this new confidence interval, we divide it by the width of the old one.\nnew_width/old_width ## [1] 4.016101 And the result is VERY CLOSE to the square root of the VIF of a (4.0200333) in this model. Notice that this works because in the model y ~ a + b, a has a perfect VIF of 1.0022527. This demonstration needs a model with a quasi-perfect VIF as reference..\nNow we can confirm our experiment about the meaning of VIF by repeating the exercise with b.\nFirst we compute the VIF of b against a alone, and against a, c, and d, and the expected level of inflation of the confidence interval as the square root of the second VIF.\n#vif of b vs a ba_vif \u0026lt;- collinear::vif_df( df = toy[, c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;)] ) |\u0026gt; dplyr::filter(predictor == \u0026quot;b\u0026quot;) #vif of b vs a c d bacd_vif \u0026lt;- collinear::vif_df( df = toy[, c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;)] ) |\u0026gt; dplyr::filter(predictor == \u0026quot;b\u0026quot;) #expeced inflation of the confidence interval sqrt(bacd_vif$vif) ## [1] 2.015515 Now, since b is already in the models y ~ a + b and y ~ a + b + c + d, we just need to extract its coefficients, compute their confidence intervals, and divide one by the other to obtain the\n#compute confidence interval of b in y ~ a + b b_ci_old \u0026lt;- ci( b = yab_model$coefficients[\u0026quot;b\u0026quot;, \u0026quot;Estimate\u0026quot;], se = yab_model$coefficients[\u0026quot;b\u0026quot;, \u0026quot;Std. Error\u0026quot;] ) #compute confidence interval of b in y ~ a + b + c + d b_ci_new \u0026lt;- ci( b = yabcd_model$coefficients[\u0026quot;b\u0026quot;, \u0026quot;Estimate\u0026quot;], se = yabcd_model$coefficients[\u0026quot;b\u0026quot;, \u0026quot;Std. Error\u0026quot;] ) #compute inflation diff(b_ci_new)/diff(b_ci_old) ## [1] 2.013543 Again, the square root of the VIF of b in y ~ a + b + c + d is a great indicator of how much the confidence interval of b is inflated by multicollinearity in the model.\nAnd that, folks, is the meaning of VIF.\nWhen the VIF Hurts In the previous sections we acquired an intuition of how Variance Inflation Factors measure the effect of multicollinearity in the precision of the coefficient estimates in a linear model. But there is more to that!\nA coefficient estimate divided by its standard error results in the T statistic. This number is named \u0026ldquo;t value\u0026rdquo; in the table of coefficients shown below, and represents the distance (in number of standard errors) between the estimate and zero.\nyabcd_model$coefficients[-1, ] |\u0026gt; round(4) ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## a 0.7184 0.0267 26.9552 0.0000 ## b 0.2596 0.0134 19.4253 0.0000 ## c 0.0273 0.0232 1.1757 0.2398 ## d 0.0039 0.0230 0.1693 0.8656 The p-value, named \u0026ldquo;Pr(\u0026gt;|t|)\u0026rdquo; above, is the probability of getting the T statistic when there is no effect of the predictor over the response. The part in italics is named the null hypothesis (H0), and happens when the confidence interval of the estimate intersects with zero, as in c and d.\nci( b = yabcd_model$coefficients[\u0026quot;c\u0026quot;, \u0026quot;Estimate\u0026quot;], se = yabcd_model$coefficients[\u0026quot;c\u0026quot;, \u0026quot;Std. Error\u0026quot;] ) ## [1] -0.01819994 0.07276692 ci( b = yabcd_model$coefficients[\u0026quot;d\u0026quot;, \u0026quot;Estimate\u0026quot;], se = yabcd_model$coefficients[\u0026quot;d\u0026quot;, \u0026quot;Std. Error\u0026quot;] ) ## [1] -0.04111146 0.04888457 The p-value of any predictor in the coefficients table above is computed as:\n#predictor predictor \u0026lt;- \u0026quot;d\u0026quot; #number of cases n \u0026lt;- nrow(toy) #number of model terms p \u0026lt;- nrow(yabcd_model$coefficients) #one-tailed p-value #q = absolute t-value #df = degrees of freedom p_value_one_tailed \u0026lt;- stats::pt( q = abs(yabcd_model$coefficients[predictor, \u0026quot;t value\u0026quot;]), df = n - p #degrees of freedom ) #two-tailed p-value 2 * (1 - p_value_one_tailed) ## [1] 0.8655869 This p-value is then compared to a significance level (for example, 0.05 for a 95% confidence), which is just the lowest p-value acceptable as strong evidence to make a claim:\np-value \u0026gt; significance: Evidence to claim that the predictor has no effect on the response. If the claim is wrong (we\u0026rsquo;ll see whey we could be wrong), we fall into a false negative (also Type II Error). p-value \u0026lt;= significance: Evidence to claim that the predictor has an effect on the response. If the claim is wrong, we fall into a false positive (also Type I Error). Now, how does all this matter when talking about the Variance Inflation Factor? Because a high VIF triggers a cascade of effects that increases p-values that can mess up your claims about the importance of the predictors!\n↑ VIF ► ↑ Std. Error ► ↓ T statistic ► ↑ p-value ► ↑ false negatives (Type II Error) This cascade becomes a problem when the predictor has a small effect on the response, and the number of cases is small.\nLet\u0026rsquo;s see how this works with b. This predictor has a solid effect on the response y (nonetheless, y was created as a * 0.75 + b * 0.25 + noise). It has a coefficient around 0.25, and a p-value of 0, so there is little to no risk of falling into a false negative when claiming that it is important to explain y, even when its confidence interval is inflated by a factor of two in the full model.\nBut let\u0026rsquo;s try a little experiment. We are going to create many small versions of toy, using only 30 cases selected by chance over a number of iterations, we are going to fit models in which b has a lower and a higher VIF, to monitor its p-values and estimates.\n#number of repetitions repetitions \u0026lt;- 1000 #number of cases to subset in toy sample_size \u0026lt;- 30 #vectors to store results lowvif_p_value \u0026lt;- highvif_p_value \u0026lt;- lowvif_estimate \u0026lt;- highvif_estimate \u0026lt;- vector(length = repetitions) #repetitions for(i in 1:repetitions){ #seed to make randomization reproducible set.seed(i) #toy subset toy.i \u0026lt;- toy[sample(x = 1:nrow(toy), size = sample_size), ] #high vif model highvif_model \u0026lt;- lm( formula = y ~ a + b + c + d, data = toy.i ) |\u0026gt; summary() #gather results of high vif model highvif_p_value[i] \u0026lt;- highvif_model$coefficients[\u0026quot;b\u0026quot;, \u0026quot;Pr(\u0026gt;|t|)\u0026quot;] highvif_estimate[i] \u0026lt;- highvif_model$coefficients[\u0026quot;b\u0026quot;, \u0026quot;Estimate\u0026quot;] #low_vif_model lowvif_model \u0026lt;- lm( formula = y ~ a + b, data = toy.i ) |\u0026gt; summary() #gather results of lowvif lowvif_p_value[i] \u0026lt;- lowvif_model$coefficients[\u0026quot;b\u0026quot;, \u0026quot;Pr(\u0026gt;|t|)\u0026quot;] lowvif_estimate[i] \u0026lt;- lowvif_model$coefficients[\u0026quot;b\u0026quot;, \u0026quot;Estimate\u0026quot;] } The plot below shows all p-values of the predictor b for the high and low VIF models across the experiment repetitions.\nAt a significance level of 0.05, the high VIF model rejects b as an important predictor of y on 53.5% of the model repetitions, while the low VIf model does the same on 2.2% of repetitions. This is a clear case of increase in Type II Error (false negatives) under multicollinearity.\nUnder multicollinearity, the probability of overlooking predictors with moderate effects increases dramatically!\nThe plot below shifts the focus towards the coefficient estimates for b across repetitions.\nThe gray vertical line represents the real value of the slope of b, and each dot represents a model repetition. The coefficients of the high VIF model are all over the place when compared to the low VIF one. Probably you have read somewhere that \u0026ldquo;multicollinearity induces model instability\u0026rdquo;, or something similar, and that is exactly what we are seeing here.\nFinding the true effect of a predictor with a moderate effect becomes harder under multicollinearity.\nManaging VIF in a Model Design The second most common form of modeling self-sabotage is having high VIF predictors in a model design, just right after throwing deep learning at tabular problems to see what sticks. I don\u0026rsquo;t have solutions for the deep learning issue, but I have some pointers for the VIFs one: letting things go!. And with things I mean predictors, not the pictures of your old love. There is no rule the more predictors the better rule written anywhere relevant, and letting your model shed some fat is the best way to go here.\nThe collinear package has something to help here. The function collinear::vif_select() is specifically designed to help reduce VIF in a model design. And it can do it in two ways: either using domain knowledge to guide the process, or applying quantitative criteria instead.\nLet\u0026rsquo;s follow the domain knowledge route first. Imagine you know a lot about y, you have read that a is very important to explain it, and you need to discuss this predictor in your results. But you are on the fence about the other predictors, so you don\u0026rsquo;t really care about what others are in the design. You can express such an idea using the argument preference_order, as shown below.\nselected_predictors \u0026lt;- collinear::vif_select( df = toy, predictors = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;), preference_order = \u0026quot;a\u0026quot;, max_vif = 2.5, quiet = TRUE ) selected_predictors ## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; ## attr(,\u0026quot;validated\u0026quot;) ## [1] TRUE Now you have it, your new model design with a VIF below 2.5 is now y ~ a + b!\nBut what if you get new information and it turns out that d is also a variable of interest? Then you should just modify preference_order to include this new information.\nselected_predictors \u0026lt;- collinear::vif_select( df = toy, predictors = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;), preference_order = c(\u0026quot;a\u0026quot;, \u0026quot;d\u0026quot;), max_vif = 2.5, quiet = TRUE ) selected_predictors ## [1] \u0026quot;a\u0026quot; \u0026quot;d\u0026quot; ## attr(,\u0026quot;validated\u0026quot;) ## [1] TRUE Notice that if your favorite variables are highly correlated, some of them are going to be removed anyway. For example, if a and c are your faves, since they are highly correlated, c is removed.\nselected_predictors \u0026lt;- collinear::vif_select( df = toy, predictors = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;), preference_order = c(\u0026quot;a\u0026quot;, \u0026quot;c\u0026quot;), max_vif = 2.5, quiet = TRUE ) selected_predictors ## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; ## attr(,\u0026quot;validated\u0026quot;) ## [1] TRUE In either case, you can now build your model while being sure that the coefficients of these predictors are going to be stable and precise.\nNow, what if y is totally new for you, and you have no idea about what to use? In this case, the function collinear::preference_order() helps you rank the predictors following a quantiative criteria, and after that, collinear::vif_select() can use it to reduce your VIFs.\nBy default, collinear::preference_order() calls collinear::f_rsquared() to compute the R-squared between each predictor and the response variable (that\u0026rsquo;s why the argument response is required here), to return a data frame with the variables ranked from \u0026ldquo;better\u0026rdquo; to \u0026ldquo;worse\u0026rdquo;.\npreference \u0026lt;- collinear::preference_order( df = toy, response = \u0026quot;y\u0026quot;, predictors = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;), f = collinear::f_r2_pearson, quiet = TRUE ) preference ## response predictor f preference ## 1 y a collinear::f_r2_pearson 0.77600503 ## 2 y c collinear::f_r2_pearson 0.72364944 ## 3 y d collinear::f_r2_pearson 0.59345954 ## 4 y b collinear::f_r2_pearson 0.07343563 Now you can use this data frame as input for the argument preference_order:\nselected_predictors \u0026lt;- collinear::vif_select( df = toy, predictors = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;), preference_order = preference, max_vif = 2.5, quiet = TRUE ) selected_predictors ## [1] \u0026quot;a\u0026quot; \u0026quot;d\u0026quot; ## attr(,\u0026quot;validated\u0026quot;) ## [1] TRUE Now at least you can be sure that the predictors in your model design have low VIF, and were selected taking their correlation with the response as criteria.\nWell, I think that\u0026rsquo;s enough for today. I hope you found this post helpful. Have a great time!\nBlas\n","date":1699142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699164863,"objectID":"8dcef99c3f9b26eeb681b7113acb5f05","permalink":"https://blasbenito.com/post/variance-inflation-factor/","publishdate":"2023-11-05T00:00:00Z","relpermalink":"/post/variance-inflation-factor/","section":"post","summary":"Deep explanation of what Variance Inflation Factors (VIF) are, how they work, what they really mean, and how they are used to manage multicollinearity in linear models.","tags":["Multicollinearity","Variable Selection","Variance Inflation Factors"],"title":"Everything You Don't Need to Know About Variance Inflation Factors","type":"post"},{"authors":["Blas M. Benito"],"categories":[],"content":" Summary The R package collinear combines four different methods to offer a comprehensive tool for multicollinearity management:\nPairwise correlation for numeric and categorical predictors: computed either via Pearson or Spearman methods for numeric predictors, and Cramer\u0026rsquo;s V for categorical predictors. Variance Inflation Factor analysis (VIF): to identify multicollinearity resulting from predictors being linear combinations of other predictors. Target encoding of categorical predictors: to convert them to numeric using a numeric variable as response (usually a response variable) and handle them as numerics during the multicollinearity filtering. Variable prioritization: method to prioritize predictors during variable selection either using expert knowledge or quantitative criteria. These methods are integrated in the collinear() function, which returns a vector of selected predictors with a controlled multicollinearity.\nselected_variables \u0026lt;- collinear( df, #your data frame response, #name of your response variable predictors, #names of your predictors, preference_order, #your predictors in order of interest max_cor, #maximum bivariate correlation max_vif, #maximum variance inflation factor encoding_method, #method to convert categorical predictors into numerics ) The package contains other functions that may be useful during multicollinearity management:\ncor_select(): like collinear(), but only using pairwise correlations. vif_select(): like collinear(), but only using variance inflation factors. preference_order(): to compute preference order based on univariate models. target_encoding_lab(): to convert categorical predictors into numeric using several methods. cor_df(): to generate a data frame with all pairwise correlation scores. cor_matrix(): to convert a correlation data frame into matrix, or obtain a correlation matrix. vif_df(): to obtain a data frame with all variance inflation factors. Citation If you found this package useful during your research work, please cite it as:\nBlas M. Benito (2023). collinear: R Package for Seamless Multicollinearity Management. Version 1.0.1. doi: 10.5281/zenodo.10039489\nInstall The package collinear can be installed from CRAN.\ninstall.packages(\u0026quot;collinear\u0026quot;) library(collinear) The development version can be installed from GitHub.\nremotes::install_github( repo = \u0026quot;blasbenito/collinear\u0026quot;, ref = \u0026quot;development\u0026quot; ) Multicollinearity management with the collinear package. This section shows the basic usage of the package and offers a brief explanation on the methods used within.\nRequired libraries and example data The libraries below are required to run the examples in this section.\nlibrary(collinear) library(dplyr) library(tictoc) The package collinear is shipped with a data frame named vi, with 30.000 rows and 67 columns with a mixture of numeric and categorical variables.\ndplyr::glimpse(vi) ## Rows: 30,000 ## Columns: 67 ## $ longitude \u0026lt;dbl\u0026gt; -114.254306, 114.845693, -122.145972, 108.3… ## $ latitude \u0026lt;dbl\u0026gt; 45.0540272, 26.2706940, 56.3790272, 29.9456… ## $ vi_mean \u0026lt;dbl\u0026gt; 0.38, 0.53, 0.45, 0.69, 0.42, 0.68, 0.70, 0… ## $ vi_max \u0026lt;dbl\u0026gt; 0.57, 0.67, 0.65, 0.85, 0.64, 0.78, 0.77, 0… ## $ vi_min \u0026lt;dbl\u0026gt; 0.12, 0.41, 0.25, 0.50, 0.25, 0.48, 0.60, 0… ## $ vi_range \u0026lt;dbl\u0026gt; 0.45, 0.26, 0.40, 0.34, 0.39, 0.31, 0.17, 0… ## $ koppen_zone \u0026lt;chr\u0026gt; \u0026quot;BSk\u0026quot;, \u0026quot;Cfa\u0026quot;, \u0026quot;Dfc\u0026quot;, \u0026quot;Cfb\u0026quot;, \u0026quot;Aw\u0026quot;, \u0026quot;Cfa\u0026quot;, \u0026quot;A… ## $ koppen_group \u0026lt;chr\u0026gt; \u0026quot;Arid\u0026quot;, \u0026quot;Temperate\u0026quot;, \u0026quot;Cold\u0026quot;, \u0026quot;Temperate\u0026quot;, \u0026quot;… ## $ koppen_description \u0026lt;chr\u0026gt; \u0026quot;steppe, cold\u0026quot;, \u0026quot;no dry season, hot summer\u0026quot;… ## $ soil_type \u0026lt;chr\u0026gt; \u0026quot;Cambisols\u0026quot;, \u0026quot;Acrisols\u0026quot;, \u0026quot;Luvisols\u0026quot;, \u0026quot;Aliso… ## $ topo_slope \u0026lt;int\u0026gt; 6, 2, 0, 10, 0, 10, 6, 0, 2, 0, 0, 1, 0, 1,… ## $ topo_diversity \u0026lt;int\u0026gt; 29, 24, 21, 25, 19, 30, 26, 20, 26, 22, 25,… ## $ topo_elevation \u0026lt;int\u0026gt; 1821, 143, 765, 1474, 378, 485, 604, 1159, … ## $ swi_mean \u0026lt;dbl\u0026gt; 27.5, 56.1, 41.4, 59.3, 37.4, 56.3, 52.3, 2… ## $ swi_max \u0026lt;dbl\u0026gt; 62.9, 74.4, 81.9, 81.1, 83.2, 73.8, 55.8, 3… ## $ swi_min \u0026lt;dbl\u0026gt; 24.5, 33.3, 42.2, 31.3, 8.3, 28.8, 25.3, 11… ## $ swi_range \u0026lt;dbl\u0026gt; 38.4, 41.2, 39.7, 49.8, 74.9, 45.0, 30.5, 2… ## $ soil_temperature_mean \u0026lt;dbl\u0026gt; 4.8, 19.9, 1.2, 13.0, 28.2, 18.1, 21.5, 23.… ## $ soil_temperature_max \u0026lt;dbl\u0026gt; 29.9, 32.6, 20.4, 24.6, 41.6, 29.1, 26.4, 4… ## $ soil_temperature_min \u0026lt;dbl\u0026gt; -12.4, 3.9, -16.0, -0.4, 16.8, 4.1, 17.3, 5… ## $ soil_temperature_range \u0026lt;dbl\u0026gt; 42.3, 28.8, 36.4, 25.0, 24.8, 24.9, 9.1, 38… ## $ soil_sand \u0026lt;int\u0026gt; 41, 39, 27, 29, 48, 33, 30, 78, 23, 64, 54,… ## $ soil_clay \u0026lt;int\u0026gt; 20, 24, 28, 31, 27, 29, 40, 15, 26, 22, 23,… ## $ soil_silt \u0026lt;int\u0026gt; 38, 35, 43, 38, 23, 36, 29, 6, 49, 13, 22, … ## $ soil_ph \u0026lt;dbl\u0026gt; 6.5, 5.9, 5.6, 5.5, 6.5, 5.8, 5.2, 7.1, 7.3… ## $ soil_soc \u0026lt;dbl\u0026gt; 43.1, 14.6, 36.4, 34.9, 8.1, 20.8, 44.5, 4.… ## $ soil_nitrogen \u0026lt;dbl\u0026gt; 2.8, 1.3, 2.9, 3.6, 1.2, 1.9, 2.8, 0.6, 3.1… ## $ solar_rad_mean \u0026lt;dbl\u0026gt; 17.634, 19.198, 13.257, 14.163, 24.512, 17.… ## $ solar_rad_max \u0026lt;dbl\u0026gt; 31.317, 24.498, 25.283, 17.237, 28.038, 22.… ## $ solar_rad_min \u0026lt;dbl\u0026gt; 5.209, 13.311, 1.587, 9.642, 19.102, 12.196… ## $ solar_rad_range \u0026lt;dbl\u0026gt; 26.108, 11.187, 23.696, 7.595, 8.936, 10.20… ## $ growing_season_length \u0026lt;dbl\u0026gt; 139, 365, 164, 333, 228, 365, 365, 60, 365,… ## $ growing_season_temperature \u0026lt;dbl\u0026gt; 12.65, 19.35, 11.55, 12.45, 26.45, 17.75, 2… ## $ growing_season_rainfall \u0026lt;dbl\u0026gt; 224.5, 1493.4, 345.4, 1765.5, 984.4, 1860.5… ## $ growing_degree_days \u0026lt;dbl\u0026gt; 2140.5, 7080.9, 2053.2, 4162.9, 10036.7, 64… ## $ temperature_mean \u0026lt;dbl\u0026gt; 3.65, 19.35, 1.45, 11.35, 27.55, 17.65, 22.… ## $ temperature_max \u0026lt;dbl\u0026gt; 24.65, 33.35, 21.15, 23.75, 38.35, 30.55, 2… ## $ temperature_min \u0026lt;dbl\u0026gt; -14.05, 3.05, -18.25, -3.55, 19.15, 2.45, 1… ## $ temperature_range \u0026lt;dbl\u0026gt; 38.7, 30.3, 39.4, 27.3, 19.2, 28.1, 7.0, 29… ## $ temperature_seasonality \u0026lt;dbl\u0026gt; 882.6, 786.6, 1070.9, 724.7, 219.3, 747.2, … ## $ rainfall_mean \u0026lt;int\u0026gt; 446, 1493, 560, 1794, 990, 1860, 3150, 356,… ## $ rainfall_min \u0026lt;int\u0026gt; 25, 37, 24, 29, 0, 60, 122, 1, 10, 12, 0, 0… ## $ rainfall_max \u0026lt;int\u0026gt; 62, 209, 87, 293, 226, 275, 425, 62, 256, 3… ## $ rainfall_range \u0026lt;int\u0026gt; 37, 172, 63, 264, 226, 215, 303, 61, 245, 2… ## $ evapotranspiration_mean \u0026lt;dbl\u0026gt; 78.32, 105.88, 50.03, 64.65, 156.60, 108.50… ## $ evapotranspiration_max \u0026lt;dbl\u0026gt; 164.70, 190.86, 117.53, 115.79, 187.71, 191… ## $ evapotranspiration_min \u0026lt;dbl\u0026gt; 13.67, 50.44, 3.53, 28.01, 128.59, 51.39, 8… ## $ evapotranspiration_range \u0026lt;dbl\u0026gt; 151.03, 140.42, 113.99, 87.79, 59.13, 139.9… ## $ cloud_cover_mean \u0026lt;int\u0026gt; 31, 48, 42, 64, 38, 52, 60, 13, 53, 20, 11,… ## $ cloud_cover_max \u0026lt;int\u0026gt; 39, 61, 49, 71, 58, 67, 77, 18, 60, 27, 23,… ## $ cloud_cover_min \u0026lt;int\u0026gt; 16, 34, 33, 54, 19, 39, 45, 6, 45, 14, 2, 1… ## $ cloud_cover_range \u0026lt;int\u0026gt; 23, 27, 15, 17, 38, 27, 32, 11, 15, 12, 21,… ## $ aridity_index \u0026lt;dbl\u0026gt; 0.54, 1.27, 0.90, 2.08, 0.55, 1.67, 2.88, 0… ## $ humidity_mean \u0026lt;dbl\u0026gt; 55.56, 62.14, 59.87, 69.32, 51.60, 62.76, 7… ## $ humidity_max \u0026lt;dbl\u0026gt; 63.98, 65.00, 68.19, 71.90, 67.07, 65.68, 7… ## $ humidity_min \u0026lt;dbl\u0026gt; 48.41, 58.97, 53.75, 67.21, 33.89, 59.92, 7… ## $ humidity_range \u0026lt;dbl\u0026gt; 15.57, 6.03, 14.44, 4.69, 33.18, 5.76, 3.99… ## $ biogeo_ecoregion \u0026lt;chr\u0026gt; \u0026quot;South Central Rockies forests\u0026quot;, \u0026quot;Jian Nan … ## $ biogeo_biome \u0026lt;chr\u0026gt; \u0026quot;Temperate Conifer Forests\u0026quot;, \u0026quot;Tropical \u0026amp; Su… ## $ biogeo_realm \u0026lt;chr\u0026gt; \u0026quot;Nearctic\u0026quot;, \u0026quot;Indomalayan\u0026quot;, \u0026quot;Nearctic\u0026quot;, \u0026quot;Pal… ## $ country_name \u0026lt;chr\u0026gt; \u0026quot;United States of America\u0026quot;, \u0026quot;China\u0026quot;, \u0026quot;Canad… ## $ country_population \u0026lt;dbl\u0026gt; 313973000, 1338612970, 33487208, 1338612970… ## $ country_gdp \u0026lt;dbl\u0026gt; 15094000, 7973000, 1300000, 7973000, 15860,… ## $ country_income \u0026lt;chr\u0026gt; \u0026quot;1. High income: OECD\u0026quot;, \u0026quot;3. Upper middle in… ## $ continent \u0026lt;chr\u0026gt; \u0026quot;North America\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;North America\u0026quot;, \u0026quot;… ## $ region \u0026lt;chr\u0026gt; \u0026quot;Americas\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Americas\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Af… ## $ subregion \u0026lt;chr\u0026gt; \u0026quot;Northern America\u0026quot;, \u0026quot;Eastern Asia\u0026quot;, \u0026quot;Northe… The response variables are \u0026ldquo;vi_mean\u0026rdquo;, \u0026ldquo;vi_max\u0026rdquo;, \u0026ldquo;vi_min\u0026rdquo;, and \u0026ldquo;vi_range\u0026rdquo;, with statistics of a vegetation index named NDVI. The predictors are stored in the character vector vi_predictors.\nvi_predictors ## [1] \u0026quot;koppen_zone\u0026quot; \u0026quot;koppen_group\u0026quot; ## [3] \u0026quot;koppen_description\u0026quot; \u0026quot;soil_type\u0026quot; ## [5] \u0026quot;topo_slope\u0026quot; \u0026quot;topo_diversity\u0026quot; ## [7] \u0026quot;topo_elevation\u0026quot; \u0026quot;swi_mean\u0026quot; ## [9] \u0026quot;swi_max\u0026quot; \u0026quot;swi_min\u0026quot; ## [11] \u0026quot;swi_range\u0026quot; \u0026quot;soil_temperature_mean\u0026quot; ## [13] \u0026quot;soil_temperature_max\u0026quot; \u0026quot;soil_temperature_min\u0026quot; ## [15] \u0026quot;soil_temperature_range\u0026quot; \u0026quot;soil_sand\u0026quot; ## [17] \u0026quot;soil_clay\u0026quot; \u0026quot;soil_silt\u0026quot; ## [19] \u0026quot;soil_ph\u0026quot; \u0026quot;soil_soc\u0026quot; ## [21] \u0026quot;soil_nitrogen\u0026quot; \u0026quot;solar_rad_mean\u0026quot; ## [23] \u0026quot;solar_rad_max\u0026quot; \u0026quot;solar_rad_min\u0026quot; ## [25] \u0026quot;solar_rad_range\u0026quot; \u0026quot;growing_season_length\u0026quot; ## [27] \u0026quot;growing_season_temperature\u0026quot; \u0026quot;growing_season_rainfall\u0026quot; ## [29] \u0026quot;growing_degree_days\u0026quot; \u0026quot;temperature_mean\u0026quot; ## [31] \u0026quot;temperature_max\u0026quot; \u0026quot;temperature_min\u0026quot; ## [33] \u0026quot;temperature_range\u0026quot; \u0026quot;temperature_seasonality\u0026quot; ## [35] \u0026quot;rainfall_mean\u0026quot; \u0026quot;rainfall_min\u0026quot; ## [37] \u0026quot;rainfall_max\u0026quot; \u0026quot;rainfall_range\u0026quot; ## [39] \u0026quot;evapotranspiration_mean\u0026quot; \u0026quot;evapotranspiration_max\u0026quot; ## [41] \u0026quot;evapotranspiration_min\u0026quot; \u0026quot;evapotranspiration_range\u0026quot; ## [43] \u0026quot;cloud_cover_mean\u0026quot; \u0026quot;cloud_cover_max\u0026quot; ## [45] \u0026quot;cloud_cover_min\u0026quot; \u0026quot;cloud_cover_range\u0026quot; ## [47] \u0026quot;aridity_index\u0026quot; \u0026quot;humidity_mean\u0026quot; ## [49] \u0026quot;humidity_max\u0026quot; \u0026quot;humidity_min\u0026quot; ## [51] \u0026quot;humidity_range\u0026quot; \u0026quot;biogeo_ecoregion\u0026quot; ## [53] \u0026quot;biogeo_biome\u0026quot; \u0026quot;biogeo_realm\u0026quot; ## [55] \u0026quot;country_name\u0026quot; \u0026quot;country_population\u0026quot; ## [57] \u0026quot;country_gdp\u0026quot; \u0026quot;country_income\u0026quot; ## [59] \u0026quot;continent\u0026quot; \u0026quot;region\u0026quot; ## [61] \u0026quot;subregion\u0026quot; collinear() The collinear() function applies a multicollinearity filtering to numeric and categorical variables via pairwise correlations (with cor_select()) and variance inflation factors (with vif_select()). Categorical variables are converted into numeric via target-encoding (with target_encoding_lab()) using a response variable as reference. If the response variable is not provided, categorical variables are ignored.\nInput arguments The function takes these inputs:\ndf: a data frame with predictors, and preferably, a response (more about this later). response: the name of the response variable, only relevant and highly recommended if there are categorical variables within the predictors. predictors: names of predictors involved in the multicollinearity analysis. preference_order: names of the predictors in the user\u0026rsquo;s order of preference. Does not need to name all predictors in predictors! cor_method: usually \u0026ldquo;pearson\u0026rdquo;, but also \u0026ldquo;spearman\u0026rdquo; is accepted. max_cor: maximum correlation allowed between two predictors. max_vif: maximum VIF allowed in a predictor. encoding_method: method used to convert categorical variables into numeric. Only relevant when a response is provided. By default, each group of the categorical variable is encoded with the mean of the response across the group. The code below shows a quick example. Notice that the argument preference_order was left as NULL, but will be explained later.\nselected_predictors \u0026lt;- collinear( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictors = vi_predictors, preference_order = NULL, max_cor = 0.75, max_vif = 5, encoding_method = \u0026quot;mean\u0026quot; ) selected_predictors ## [1] \u0026quot;country_income\u0026quot; \u0026quot;topo_diversity\u0026quot; ## [3] \u0026quot;topo_slope\u0026quot; \u0026quot;country_population\u0026quot; ## [5] \u0026quot;country_gdp\u0026quot; \u0026quot;humidity_range\u0026quot; ## [7] \u0026quot;soil_soc\u0026quot; \u0026quot;region\u0026quot; ## [9] \u0026quot;soil_clay\u0026quot; \u0026quot;soil_type\u0026quot; ## [11] \u0026quot;subregion\u0026quot; \u0026quot;biogeo_realm\u0026quot; ## [13] \u0026quot;soil_sand\u0026quot; \u0026quot;topo_elevation\u0026quot; ## [15] \u0026quot;soil_nitrogen\u0026quot; \u0026quot;swi_range\u0026quot; ## [17] \u0026quot;koppen_group\u0026quot; \u0026quot;swi_min\u0026quot; ## [19] \u0026quot;solar_rad_max\u0026quot; \u0026quot;rainfall_min\u0026quot; ## [21] \u0026quot;growing_season_temperature\u0026quot; \u0026quot;rainfall_range\u0026quot; ## [23] \u0026quot;solar_rad_min\u0026quot; \u0026quot;cloud_cover_range\u0026quot; The function has returned a list of predictors that should have a correlation lower than 0.75 with each other, and a VIF lower than 5. Let\u0026rsquo;s see if that\u0026rsquo;s true.\nThe function cor_df() returns a data frame with pairwise correlations, arranged by the absolute value of the correlation.\nselected_predictors_cor \u0026lt;- cor_df( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictors = selected_predictors ) head(selected_predictors_cor) ## # A tibble: 6 × 3 ## x y correlation ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 solar_rad_min growing_season_temperature 0.744 ## 2 koppen_group soil_type 0.732 ## 3 soil_nitrogen soil_soc 0.729 ## 4 swi_min soil_nitrogen 0.673 ## 5 soil_sand soil_clay -0.666 ## 6 koppen_group swi_range 0.659 The data frame above shows that the maximum correlation between two of the selected predictors is below 0.75, so here collinear() worked as expected.\nThe function vif_df() returns a data frame with the VIF scores of all predictors.\nselected_predictors_vif \u0026lt;- vif_df( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictors = selected_predictors ) selected_predictors_vif ## variable vif ## 1 country_income 1.215 ## 2 topo_diversity 1.662 ## 3 topo_slope 1.929 ## 4 humidity_range 2.043 ## 5 topo_elevation 2.101 ## 6 country_gdp 2.158 ## 7 country_population 2.171 ## 8 rainfall_min 2.269 ## 9 cloud_cover_range 2.418 ## 10 soil_soc 2.744 ## 11 region 2.849 ## 12 rainfall_range 2.876 ## 13 subregion 2.900 ## 14 soil_clay 2.966 ## 15 soil_type 2.991 ## 16 solar_rad_max 3.145 ## 17 biogeo_realm 3.150 ## 18 soil_sand 3.175 ## 19 soil_nitrogen 3.376 ## 20 swi_min 3.450 ## 21 swi_range 3.781 ## 22 koppen_group 4.151 ## 23 growing_season_temperature 4.314 ## 24 solar_rad_min 4.432 The output shows that the maximum VIF is 4.2, so here collinear() did its work as expected.\nArguments max_cor and max_vif The arguments max_cor and max_vif control the intensity of the multicollinearity filtering.\n#restrictive setup selected_predictors_restrictive \u0026lt;- collinear( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictors = vi_predictors, max_cor = 0.5, max_vif = 2.5 ) #permissive setup selected_predictors_permissive \u0026lt;- collinear( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictors = vi_predictors, max_cor = 0.9, max_vif = 10 ) These are the variables selected under a restrictive setup:\nselected_predictors_restrictive ## [1] \u0026quot;country_income\u0026quot; \u0026quot;soil_clay\u0026quot; ## [3] \u0026quot;country_population\u0026quot; \u0026quot;topo_slope\u0026quot; ## [5] \u0026quot;humidity_range\u0026quot; \u0026quot;topo_elevation\u0026quot; ## [7] \u0026quot;soil_soc\u0026quot; \u0026quot;soil_silt\u0026quot; ## [9] \u0026quot;cloud_cover_range\u0026quot; \u0026quot;region\u0026quot; ## [11] \u0026quot;solar_rad_max\u0026quot; \u0026quot;growing_season_temperature\u0026quot; ## [13] \u0026quot;biogeo_realm\u0026quot; These are the variables selected under a more permissive setup:\nselected_predictors_permissive ## [1] \u0026quot;country_income\u0026quot; \u0026quot;topo_diversity\u0026quot; ## [3] \u0026quot;topo_slope\u0026quot; \u0026quot;country_population\u0026quot; ## [5] \u0026quot;country_gdp\u0026quot; \u0026quot;soil_soc\u0026quot; ## [7] \u0026quot;region\u0026quot; \u0026quot;soil_type\u0026quot; ## [9] \u0026quot;soil_nitrogen\u0026quot; \u0026quot;subregion\u0026quot; ## [11] \u0026quot;biogeo_realm\u0026quot; \u0026quot;topo_elevation\u0026quot; ## [13] \u0026quot;koppen_group\u0026quot; \u0026quot;biogeo_biome\u0026quot; ## [15] \u0026quot;country_name\u0026quot; \u0026quot;soil_ph\u0026quot; ## [17] \u0026quot;aridity_index\u0026quot; \u0026quot;growing_season_temperature\u0026quot; ## [19] \u0026quot;rainfall_min\u0026quot; \u0026quot;rainfall_range\u0026quot; ## [21] \u0026quot;swi_mean\u0026quot; \u0026quot;soil_temperature_max\u0026quot; ## [23] \u0026quot;solar_rad_mean\u0026quot; \u0026quot;temperature_seasonality\u0026quot; ## [25] \u0026quot;soil_clay\u0026quot; \u0026quot;soil_silt\u0026quot; ## [27] \u0026quot;cloud_cover_min\u0026quot; \u0026quot;swi_range\u0026quot; ## [29] \u0026quot;humidity_range\u0026quot; As expected, the restrictive setup resulted in a smaller set of selected predictors. There are no hard rules for max_cor and max_vif, and their selection will depend on the objective of the analysis and the nature of the predictors.\nThe response argument The response argument is used to encode categorical variables as numeric. When omitted, the collinear() function ignores categorical variables. However, the function cor_select() can help when there is not a suitable response variable in a data frame. This option is discussed at the end of this section.\nselected_predictors_response \u0026lt;- collinear( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictors = vi_predictors ) selected_predictors_no_response \u0026lt;- collinear( df = vi, predictors = vi_predictors ) When the argument response is used, the output may contain categorical predictors (tagged with \u0026lt;chr\u0026gt;, from \u0026ldquo;character\u0026rdquo; below).\ndplyr::glimpse(vi[, selected_predictors_response]) ## Rows: 30,000 ## Columns: 24 ## $ country_income \u0026lt;chr\u0026gt; \u0026quot;1. High income: OECD\u0026quot;, \u0026quot;3. Upper middle in… ## $ topo_diversity \u0026lt;int\u0026gt; 29, 24, 21, 25, 19, 30, 26, 20, 26, 22, 25,… ## $ topo_slope \u0026lt;int\u0026gt; 6, 2, 0, 10, 0, 10, 6, 0, 2, 0, 0, 1, 0, 1,… ## $ country_population \u0026lt;dbl\u0026gt; 313973000, 1338612970, 33487208, 1338612970… ## $ country_gdp \u0026lt;dbl\u0026gt; 15094000, 7973000, 1300000, 7973000, 15860,… ## $ humidity_range \u0026lt;dbl\u0026gt; 15.57, 6.03, 14.44, 4.69, 33.18, 5.76, 3.99… ## $ soil_soc \u0026lt;dbl\u0026gt; 43.1, 14.6, 36.4, 34.9, 8.1, 20.8, 44.5, 4.… ## $ region \u0026lt;chr\u0026gt; \u0026quot;Americas\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Americas\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Af… ## $ soil_clay \u0026lt;int\u0026gt; 20, 24, 28, 31, 27, 29, 40, 15, 26, 22, 23,… ## $ soil_type \u0026lt;chr\u0026gt; \u0026quot;Cambisols\u0026quot;, \u0026quot;Acrisols\u0026quot;, \u0026quot;Luvisols\u0026quot;, \u0026quot;Aliso… ## $ subregion \u0026lt;chr\u0026gt; \u0026quot;Northern America\u0026quot;, \u0026quot;Eastern Asia\u0026quot;, \u0026quot;Northe… ## $ biogeo_realm \u0026lt;chr\u0026gt; \u0026quot;Nearctic\u0026quot;, \u0026quot;Indomalayan\u0026quot;, \u0026quot;Nearctic\u0026quot;, \u0026quot;Pal… ## $ soil_sand \u0026lt;int\u0026gt; 41, 39, 27, 29, 48, 33, 30, 78, 23, 64, 54,… ## $ topo_elevation \u0026lt;int\u0026gt; 1821, 143, 765, 1474, 378, 485, 604, 1159, … ## $ soil_nitrogen \u0026lt;dbl\u0026gt; 2.8, 1.3, 2.9, 3.6, 1.2, 1.9, 2.8, 0.6, 3.1… ## $ swi_range \u0026lt;dbl\u0026gt; 38.4, 41.2, 39.7, 49.8, 74.9, 45.0, 30.5, 2… ## $ koppen_group \u0026lt;chr\u0026gt; \u0026quot;Arid\u0026quot;, \u0026quot;Temperate\u0026quot;, \u0026quot;Cold\u0026quot;, \u0026quot;Temperate\u0026quot;, \u0026quot;… ## $ swi_min \u0026lt;dbl\u0026gt; 24.5, 33.3, 42.2, 31.3, 8.3, 28.8, 25.3, 11… ## $ solar_rad_max \u0026lt;dbl\u0026gt; 31.317, 24.498, 25.283, 17.237, 28.038, 22.… ## $ rainfall_min \u0026lt;int\u0026gt; 25, 37, 24, 29, 0, 60, 122, 1, 10, 12, 0, 0… ## $ growing_season_temperature \u0026lt;dbl\u0026gt; 12.65, 19.35, 11.55, 12.45, 26.45, 17.75, 2… ## $ rainfall_range \u0026lt;int\u0026gt; 37, 172, 63, 264, 226, 215, 303, 61, 245, 2… ## $ solar_rad_min \u0026lt;dbl\u0026gt; 5.209, 13.311, 1.587, 9.642, 19.102, 12.196… ## $ cloud_cover_range \u0026lt;int\u0026gt; 23, 27, 15, 17, 38, 27, 32, 11, 15, 12, 21,… However, when the argument response is ignored, all categorical predictors are ignored.\ndplyr::glimpse(vi[, selected_predictors_no_response]) ## Rows: 30,000 ## Columns: 18 ## $ topo_diversity \u0026lt;int\u0026gt; 29, 24, 21, 25, 19, 30, 26, 20, 26, 22, 25,… ## $ country_gdp \u0026lt;dbl\u0026gt; 15094000, 7973000, 1300000, 7973000, 15860,… ## $ country_population \u0026lt;dbl\u0026gt; 313973000, 1338612970, 33487208, 1338612970… ## $ topo_slope \u0026lt;int\u0026gt; 6, 2, 0, 10, 0, 10, 6, 0, 2, 0, 0, 1, 0, 1,… ## $ soil_soc \u0026lt;dbl\u0026gt; 43.1, 14.6, 36.4, 34.9, 8.1, 20.8, 44.5, 4.… ## $ soil_clay \u0026lt;int\u0026gt; 20, 24, 28, 31, 27, 29, 40, 15, 26, 22, 23,… ## $ soil_sand \u0026lt;int\u0026gt; 41, 39, 27, 29, 48, 33, 30, 78, 23, 64, 54,… ## $ soil_nitrogen \u0026lt;dbl\u0026gt; 2.8, 1.3, 2.9, 3.6, 1.2, 1.9, 2.8, 0.6, 3.1… ## $ humidity_range \u0026lt;dbl\u0026gt; 15.57, 6.03, 14.44, 4.69, 33.18, 5.76, 3.99… ## $ topo_elevation \u0026lt;int\u0026gt; 1821, 143, 765, 1474, 378, 485, 604, 1159, … ## $ cloud_cover_range \u0026lt;int\u0026gt; 23, 27, 15, 17, 38, 27, 32, 11, 15, 12, 21,… ## $ solar_rad_max \u0026lt;dbl\u0026gt; 31.317, 24.498, 25.283, 17.237, 28.038, 22.… ## $ rainfall_min \u0026lt;int\u0026gt; 25, 37, 24, 29, 0, 60, 122, 1, 10, 12, 0, 0… ## $ growing_season_temperature \u0026lt;dbl\u0026gt; 12.65, 19.35, 11.55, 12.45, 26.45, 17.75, 2… ## $ rainfall_range \u0026lt;int\u0026gt; 37, 172, 63, 264, 226, 215, 303, 61, 245, 2… ## $ swi_range \u0026lt;dbl\u0026gt; 38.4, 41.2, 39.7, 49.8, 74.9, 45.0, 30.5, 2… ## $ solar_rad_min \u0026lt;dbl\u0026gt; 5.209, 13.311, 1.587, 9.642, 19.102, 12.196… ## $ swi_min \u0026lt;dbl\u0026gt; 24.5, 33.3, 42.2, 31.3, 8.3, 28.8, 25.3, 11… If there are categorical variables in a data frame, but there is no suitable response variable, then the function cor_select() can handle the multicollinearity management via pairwise correlations, but at a MUCH higher computational cost, and with different results, as shown below.\ntictoc::tic() selected_predictors_response \u0026lt;- cor_select( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictors = vi_predictors ) tictoc::toc() ## 0.429 sec elapsed tictoc::tic() selected_predictors_no_response \u0026lt;- cor_select( df = vi, predictors = vi_predictors ) tictoc::toc() ## 34.945 sec elapsed selected_predictors_response ## [1] \u0026quot;country_population\u0026quot; \u0026quot;topo_elevation\u0026quot; ## [3] \u0026quot;country_income\u0026quot; \u0026quot;country_gdp\u0026quot; ## [5] \u0026quot;topo_slope\u0026quot; \u0026quot;humidity_range\u0026quot; ## [7] \u0026quot;soil_clay\u0026quot; \u0026quot;topo_diversity\u0026quot; ## [9] \u0026quot;soil_sand\u0026quot; \u0026quot;cloud_cover_range\u0026quot; ## [11] \u0026quot;region\u0026quot; \u0026quot;growing_season_temperature\u0026quot; ## [13] \u0026quot;solar_rad_min\u0026quot; \u0026quot;soil_soc\u0026quot; ## [15] \u0026quot;rainfall_min\u0026quot; \u0026quot;swi_range\u0026quot; ## [17] \u0026quot;soil_nitrogen\u0026quot; \u0026quot;rainfall_range\u0026quot; ## [19] \u0026quot;temperature_max\u0026quot; \u0026quot;swi_min\u0026quot; ## [21] \u0026quot;subregion\u0026quot; \u0026quot;temperature_seasonality\u0026quot; ## [23] \u0026quot;biogeo_realm\u0026quot; \u0026quot;cloud_cover_min\u0026quot; ## [25] \u0026quot;soil_type\u0026quot; \u0026quot;aridity_index\u0026quot; ## [27] \u0026quot;solar_rad_max\u0026quot; \u0026quot;koppen_group\u0026quot; ## [29] \u0026quot;cloud_cover_max\u0026quot; selected_predictors_no_response ## [1] \u0026quot;topo_elevation\u0026quot; \u0026quot;topo_slope\u0026quot; ## [3] \u0026quot;country_population\u0026quot; \u0026quot;topo_diversity\u0026quot; ## [5] \u0026quot;soil_clay\u0026quot; \u0026quot;humidity_range\u0026quot; ## [7] \u0026quot;soil_sand\u0026quot; \u0026quot;country_gdp\u0026quot; ## [9] \u0026quot;cloud_cover_range\u0026quot; \u0026quot;country_income\u0026quot; ## [11] \u0026quot;rainfall_min\u0026quot; \u0026quot;soil_soc\u0026quot; ## [13] \u0026quot;swi_range\u0026quot; \u0026quot;growing_season_temperature\u0026quot; ## [15] \u0026quot;rainfall_range\u0026quot; \u0026quot;soil_nitrogen\u0026quot; ## [17] \u0026quot;solar_rad_min\u0026quot; \u0026quot;aridity_index\u0026quot; ## [19] \u0026quot;cloud_cover_min\u0026quot; \u0026quot;temperature_max\u0026quot; ## [21] \u0026quot;region\u0026quot; \u0026quot;swi_min\u0026quot; ## [23] \u0026quot;solar_rad_max\u0026quot; \u0026quot;evapotranspiration_range\u0026quot; ## [25] \u0026quot;swi_mean\u0026quot; \u0026quot;humidity_max\u0026quot; ## [27] \u0026quot;soil_type\u0026quot; The variable selection results differ because the numeric representations of the categorical variables are rather different between the two options. When no response is provided, the function cor_select() compares categorical predictors against numeric ones by encoding each categorical after each numeric, and compares pairs of categoricals using Cramer\u0026rsquo;s V, implemented in the function cramer_v(). Additionally, Cramer\u0026rsquo;s V values are not directly comparable with Pearson or Spearman correlation scores, and having them together in the same analysis might induce bias during the variable selection. Not using the response argument should always be the last option.\nPreference order The argument preference_order gives the user some control on what predictors should be removed first and what predictors should be kept during the multicollinearity filtering. This argument accepts a vector of predictor names in the order of interest, or the result of the function preference_order(), which allows to define preference order following a quantitative criteria.\nManual preference order Let\u0026rsquo;s start with the former option. Below, the argument preference_order names several predictors that are of importance for a hypothetical analysis. The predictors not in preference_order are ranked by the absolute sum of their correlations with other predictors during the pairwise correlation filtering, and by their VIF during the VIF-based filtering.\nselected_predictors \u0026lt;- cor_select( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictors = vi_predictors, preference_order = c( \u0026quot;soil_temperature_mean\u0026quot;, \u0026quot;soil_temperature_max\u0026quot;, \u0026quot;soil_type\u0026quot; ) ) selected_predictors ## [1] \u0026quot;soil_temperature_mean\u0026quot; \u0026quot;soil_temperature_max\u0026quot; \u0026quot;soil_type\u0026quot; ## [4] \u0026quot;country_population\u0026quot; \u0026quot;topo_elevation\u0026quot; \u0026quot;country_income\u0026quot; ## [7] \u0026quot;country_gdp\u0026quot; \u0026quot;topo_slope\u0026quot; \u0026quot;humidity_range\u0026quot; ## [10] \u0026quot;soil_clay\u0026quot; \u0026quot;topo_diversity\u0026quot; \u0026quot;soil_sand\u0026quot; ## [13] \u0026quot;cloud_cover_range\u0026quot; \u0026quot;region\u0026quot; \u0026quot;soil_soc\u0026quot; ## [16] \u0026quot;rainfall_min\u0026quot; \u0026quot;solar_rad_range\u0026quot; \u0026quot;swi_range\u0026quot; ## [19] \u0026quot;soil_nitrogen\u0026quot; \u0026quot;rainfall_range\u0026quot; \u0026quot;subregion\u0026quot; ## [22] \u0026quot;biogeo_realm\u0026quot; \u0026quot;aridity_index\u0026quot; \u0026quot;solar_rad_max\u0026quot; ## [25] \u0026quot;koppen_group\u0026quot; \u0026quot;soil_temperature_range\u0026quot; \u0026quot;cloud_cover_max\u0026quot; Notice that in the output, two of the variables in preference_order are selected (\u0026ldquo;soil_temperature_mean\u0026rdquo; and \u0026ldquo;soil_type\u0026rdquo;), but one was removed (\u0026ldquo;soil_temperature_max\u0026rdquo;). This happens because at some point in the selection, the VIF of \u0026ldquo;soil_temperature_mean\u0026rdquo; and \u0026ldquo;soil_temperature_max\u0026rdquo; was higher than max_vif, and the one with lower preference was removed.\nQuantitative preference order The function preference_order() requires the response argument, and takes a function f that returns a value of association between the response and any predictor. This value is then located in the \u0026ldquo;preference\u0026rdquo; column of the function\u0026rsquo;s output.\npreference_rsquared \u0026lt;- preference_order( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictors = vi_predictors, f = f_rsquared, workers = 1 #requires package future and future.apply for more workers ) preference_rsquared ## predictor preference ## 1 biogeo_ecoregion 0.8971347093 ## 2 growing_season_length 0.8076216576 ## 3 koppen_zone 0.8050280970 ## 4 koppen_description 0.7903458680 ## 5 soil_ph 0.7664428862 ## 6 swi_mean 0.7286901614 ## 7 humidity_mean 0.7141389404 ## 8 koppen_group 0.6996959734 ## 9 biogeo_biome 0.6515724588 ## 10 country_name 0.6448346803 ## 11 cloud_cover_mean 0.6338773126 ## 12 soil_type 0.6318025761 ## 13 rainfall_mean 0.6005761078 ## 14 humidity_max 0.5876622545 ## 15 soil_temperature_max 0.5827628810 ## 16 swi_max 0.5813558512 ## 17 cloud_cover_max 0.5758002449 ## 18 humidity_min 0.5705720164 ## 19 growing_season_rainfall 0.5697006759 ## 20 soil_temperature_range 0.5523074848 ## 21 biogeo_realm 0.5031101984 ## 22 solar_rad_max 0.4905225950 ## 23 evapotranspiration_max 0.4814731607 ## 24 rainfall_max 0.4783927311 ## 25 aridity_index 0.4506424015 ## 26 subregion 0.4469207404 ## 27 swi_range 0.4217411381 ## 28 cloud_cover_min 0.4135724066 ## 29 evapotranspiration_range 0.4042241481 ## 30 temperature_range 0.3753489250 ## 31 rainfall_range 0.3545446680 ## 32 temperature_seasonality 0.2499469281 ## 33 rainfall_min 0.2484813976 ## 34 swi_min 0.2406964836 ## 35 solar_rad_mean 0.2140860965 ## 36 soil_nitrogen 0.1872886789 ## 37 continent 0.1818717607 ## 38 temperature_max 0.1589418736 ## 39 region 0.1505256024 ## 40 soil_soc 0.1493958026 ## 41 evapotranspiration_mean 0.1455828419 ## 42 solar_rad_range 0.1300751363 ## 43 temperature_min 0.1222051434 ## 44 cloud_cover_range 0.1216812855 ## 45 soil_temperature_min 0.1018471531 ## 46 topo_diversity 0.0925948262 ## 47 soil_clay 0.0769366113 ## 48 humidity_range 0.0575393339 ## 49 country_income 0.0489946403 ## 50 soil_sand 0.0427943817 ## 51 topo_elevation 0.0424759731 ## 52 growing_season_temperature 0.0239161476 ## 53 topo_slope 0.0203697134 ## 54 soil_temperature_mean 0.0170527033 ## 55 temperature_mean 0.0067479780 ## 56 soil_silt 0.0059316757 ## 57 growing_degree_days 0.0047849144 ## 58 evapotranspiration_min 0.0009965488 ## 59 country_gdp 0.0008850479 ## 60 solar_rad_min 0.0005751350 ## 61 country_population 0.0002513147 The result of preference_order() can be plugged right away into the preference_order argument of collinear.\nselected_predictors \u0026lt;- collinear( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictors = vi_predictors, preference_order = preference_rsquared ) selected_predictors ## [1] \u0026quot;biogeo_ecoregion\u0026quot; \u0026quot;biogeo_realm\u0026quot; ## [3] \u0026quot;solar_rad_max\u0026quot; \u0026quot;rainfall_max\u0026quot; ## [5] \u0026quot;subregion\u0026quot; \u0026quot;swi_range\u0026quot; ## [7] \u0026quot;rainfall_min\u0026quot; \u0026quot;soil_nitrogen\u0026quot; ## [9] \u0026quot;continent\u0026quot; \u0026quot;soil_soc\u0026quot; ## [11] \u0026quot;cloud_cover_range\u0026quot; \u0026quot;topo_diversity\u0026quot; ## [13] \u0026quot;soil_clay\u0026quot; \u0026quot;humidity_range\u0026quot; ## [15] \u0026quot;country_income\u0026quot; \u0026quot;soil_sand\u0026quot; ## [17] \u0026quot;topo_elevation\u0026quot; \u0026quot;growing_season_temperature\u0026quot; ## [19] \u0026quot;topo_slope\u0026quot; \u0026quot;country_gdp\u0026quot; ## [21] \u0026quot;country_population\u0026quot; This variable selection satisfies three conditions at once: maximum correlation between each predictor and the response, maximum pairwise correlation, and maximum VIF.\nThe f argument used by default is the function f_rsquared(), that returns the R-squared between the response and any predictor.\nf_rsquared( x = \u0026quot;growing_season_length\u0026quot;, y = \u0026quot;vi_mean\u0026quot;, df = vi ) ## [1] 0.8076217 There are two other f functions implemented:\nf_ga_deviance(): returns the explained deviance of a univariate GAM model between the response and each predictor, fitted with the function mgcv::gam(). Only if the R package mgcv is installed in the system. f_rf_deviance(): returns the explained deviance of a univariate Random Forest model between the response and each predictor, fitted with the function ranger::ranger(). Only if the R package ranger is installed in the system. Custom functions created by the user are also accepted as input, as long as they have the x, y, and df arguments, and they return a single numeric value.\nThese can be run in parallel across predictors by increasing the value of the workers argument if the R packages future and future.apply are installed in the system.\ncor_select() and vif_select() The functions cor_select() and vif_select(), called within collinear(), perform the pairwise correlation filtering, and the VIF-based filtering. The main difference between them is that cor_select() can handle categorical predictors even when the response is omitted, while vif_select() ignores them entirely in such case.\nselected_predictors_cor \u0026lt;- cor_select( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictors = vi_predictors, preference_order = preference_rsquared ) selected_predictors_vif \u0026lt;- vif_select( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictors = vi_predictors, preference_order = preference_rsquared ) selected_predictors_cor ## [1] \u0026quot;biogeo_ecoregion\u0026quot; \u0026quot;soil_temperature_max\u0026quot; ## [3] \u0026quot;soil_temperature_range\u0026quot; \u0026quot;biogeo_realm\u0026quot; ## [5] \u0026quot;solar_rad_max\u0026quot; \u0026quot;rainfall_max\u0026quot; ## [7] \u0026quot;aridity_index\u0026quot; \u0026quot;subregion\u0026quot; ## [9] \u0026quot;swi_range\u0026quot; \u0026quot;rainfall_min\u0026quot; ## [11] \u0026quot;solar_rad_mean\u0026quot; \u0026quot;soil_nitrogen\u0026quot; ## [13] \u0026quot;continent\u0026quot; \u0026quot;soil_soc\u0026quot; ## [15] \u0026quot;solar_rad_range\u0026quot; \u0026quot;cloud_cover_range\u0026quot; ## [17] \u0026quot;topo_diversity\u0026quot; \u0026quot;soil_clay\u0026quot; ## [19] \u0026quot;humidity_range\u0026quot; \u0026quot;country_income\u0026quot; ## [21] \u0026quot;soil_sand\u0026quot; \u0026quot;topo_elevation\u0026quot; ## [23] \u0026quot;growing_season_temperature\u0026quot; \u0026quot;topo_slope\u0026quot; ## [25] \u0026quot;country_gdp\u0026quot; \u0026quot;country_population\u0026quot; selected_predictors_vif ## [1] \u0026quot;biogeo_ecoregion\u0026quot; \u0026quot;soil_type\u0026quot; \u0026quot;rainfall_mean\u0026quot; ## [4] \u0026quot;biogeo_realm\u0026quot; \u0026quot;solar_rad_max\u0026quot; \u0026quot;subregion\u0026quot; ## [7] \u0026quot;soil_nitrogen\u0026quot; \u0026quot;continent\u0026quot; \u0026quot;soil_soc\u0026quot; ## [10] \u0026quot;topo_diversity\u0026quot; \u0026quot;soil_clay\u0026quot; \u0026quot;country_income\u0026quot; ## [13] \u0026quot;soil_sand\u0026quot; \u0026quot;topo_slope\u0026quot; \u0026quot;country_gdp\u0026quot; ## [16] \u0026quot;country_population\u0026quot; target_encoding_lab() The function target_encoding_lab() is used within all other functions in the package to encode categorical variables as numeric. It implements four target encoding methods:\n\u0026ldquo;mean\u0026rdquo; (in target_encoding_mean()): replaces each category with the mean of the response across the category. White noise can be added to this option to increase data variability. \u0026ldquo;rank\u0026rdquo; (in target_encoding_rank()): replaces each category with the rank of the mean of the response across the category. \u0026ldquo;rnorm\u0026rdquo; (in target_encoding_rnorm()): replaces each value in a category with a number generated by stats::rnorm() from a normal distribution with the mean and the standard deviation of the response over the category. \u0026ldquo;loo\u0026rdquo; (in target_encoding_loo()): replaces each value in a category with the mean of the response across all the other cases within the category. White noise can be added to this option to increase data variability. The method \u0026ldquo;mean\u0026rdquo; is used as default throughout all functions in the package, but can be changed via the argument encoding_method.\nBelow we use all methods to generate different numeric encodings for the categorical variable \u0026ldquo;koppen_zone\u0026rdquo;.\ndf \u0026lt;- target_encoding_lab( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictors = \u0026quot;koppen_zone\u0026quot;, encoding_methods = c( \u0026quot;mean\u0026quot;, \u0026quot;rank\u0026quot;, \u0026quot;rnorm\u0026quot;, \u0026quot;loo\u0026quot; ), seed = 1, rnorm_sd_multiplier = c(0, 0.01, 0.1), white_noise = c(0, 0.01, 0.1), verbose = TRUE ) ## Encoding the variables: ## koppen_zone ## New encoded predictor: 'koppen_zone__encoded_rank' ## New encoded predictor: 'koppen_zone__encoded_mean' ## New encoded predictor: 'koppen_zone__encoded_loo' ## New encoded predictor: 'koppen_zone__encoded_mean__white_noise_0.01' ## New encoded predictor: 'koppen_zone__encoded_loo__white_noise_0.01' ## New encoded predictor: 'koppen_zone__encoded_mean__white_noise_0.1' ## New encoded predictor: 'koppen_zone__encoded_loo__white_noise_0.1' ## New encoded predictor: 'koppen_zone__encoded_rnorm' ## New encoded predictor: 'koppen_zone__encoded_rnorm__sd_multiplier_0.01' ## New encoded predictor: 'koppen_zone__encoded_rnorm__sd_multiplier_0.1' The relationship between these encoded versions of \u0026ldquo;koppen_zone\u0026rdquo; and the response are shown below.\n#get names of encoded variables koppen_zone_encoded \u0026lt;- grep( pattern = \u0026quot;*__encoded*\u0026quot;, x = colnames(df), value = TRUE ) #record the user's graphical parameters user.par \u0026lt;- par(no.readonly = TRUE) #modify graphical parameters for the plot par(mfrow = c(4, 3)) #plot target encoding x \u0026lt;- lapply( X = koppen_zone_encoded, FUN = function(x) plot( x = df[[x]], y = df$vi_mean, xlab = x, ylab = \u0026quot;vi_mean\u0026quot;, cex = 0.5 ) ) #reset the user's graphical parameters par(user.par) The function implementing each method can be used directly as well. The example below shows the \u0026ldquo;mean\u0026rdquo; method with the option replace = FALSE, which replaces the categorical values with the numeric ones in the output data frame.\nhead(vi[, c(\u0026quot;vi_mean\u0026quot;, \u0026quot;koppen_zone\u0026quot;)], n = 10) ## vi_mean koppen_zone ## 1 0.38 BSk ## 2 0.53 Cfa ## 3 0.45 Dfc ## 4 0.69 Cfb ## 5 0.42 Aw ## 6 0.68 Cfa ## 7 0.70 Af ## 8 0.26 BSh ## 9 0.55 Cwa ## 10 0.16 BWh df \u0026lt;- target_encoding_mean( df = vi, response = \u0026quot;vi_mean\u0026quot;, predictor = \u0026quot;koppen_zone\u0026quot;, replace = TRUE ) head(df[, c(\u0026quot;vi_mean\u0026quot;, \u0026quot;koppen_zone\u0026quot;)], n = 10) ## vi_mean koppen_zone ## 1 0.38 0.2487370 ## 2 0.53 0.5661689 ## 3 0.45 0.4338492 ## 4 0.69 0.5889908 ## 5 0.42 0.5275241 ## 6 0.68 0.5661689 ## 7 0.70 0.6708994 ## 8 0.26 0.3230049 ## 9 0.55 0.5218936 ## 10 0.16 0.1330452 If you got here, thank you for your interest in the R package collinear, I hope it can serve you well.\nAnd that\u0026rsquo;s a wrap!\nBlas M. Benito, PhD\n","date":1698624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698646463,"objectID":"6759e253103d2f08c7543aa7a8565d10","permalink":"https://blasbenito.com/project/post-title/","publishdate":"2023-10-30T00:00:00Z","relpermalink":"/project/post-title/","section":"project","summary":"R package for multicollinearity management in data frames with numeric and categorical variables.","tags":["R packages","Machine Learning","Multicollinearity","Target Encoding","Statistics","Analytics","Data Science"],"title":"R package collinear","type":"project"},{"authors":["Blas M. Benito"],"categories":[],"content":" This post is written for beginner to intermediate R users wishing to learn what multicollinearity is and how it can turn model interpretation into a challenge. Summary In this post, I delve into the intricacies of model interpretation under the influence of multicollinearity, and use R and a toy data set to demonstrate how this phenomenon impacts both linear and machine learning models:\nThe section Multicollinearity Explained explains the origin of the word and the nature of the problem. The section Model Interpretation Challenges describes how to create the toy data set, and applies it to Linear Models and Random Forest to explain how multicollinearity can make model interpretation a challenge. The Appendix shows extra examples of linear and machine learning models affected by multicollinearity. I hope you\u0026rsquo;ll enjoy it!\nR packages This tutorial requires the newly released R package collinear, and a few more listed below. The optional ones are used only in the Appendix at the end of the post.\n#required install.packages(\u0026quot;collinear\u0026quot;) install.packages(\u0026quot;ranger\u0026quot;) install.packages(\u0026quot;dplyr\u0026quot;) #optional install.packages(\u0026quot;nlme\u0026quot;) install.packages(\u0026quot;glmnet\u0026quot;) install.packages(\u0026quot;xgboost\u0026quot;) Multicollinearity Explained This cute word comes from the amalgamation of these three Latin terms:\nmultus: adjective meaning many or multiple. con: preposition often converted to co- (as in co-worker) meaning together or mutually. linealis (later converted to linearis): from linea (line), adjective meaning \u0026ldquo;resembling a line\u0026rdquo; or \u0026ldquo;belonging to a line\u0026rdquo;, among others. After looking at these serious words, we can come up with a (VERY) liberal translation: \u0026ldquo;several things together in the same line\u0026rdquo;. From here, we just have to replace the word \u0026ldquo;things\u0026rdquo; with \u0026ldquo;predictors\u0026rdquo; (or \u0026ldquo;features\u0026rdquo;, or \u0026ldquo;independent variables\u0026rdquo;, whatever rocks your boat) to build an intuition of the whole meaning of the word in the context of statistical and machine learning modeling.\nIf I lost you there, we can move forward with this idea instead: multicollinearity happens when there are redundant predictors in a modeling dataset. A predictor can be redundant because it shows a high pairwise correlation with other predictors, or because it is a linear combination of other predictors. For example, in a data frame with the columns a, b, and c, if the correlation between a and b is high, we can say that a and b are mutually redundant and there is multicollinearity. But also, if c is the result of a linear operation between a and b, like c \u0026lt;- a + b, or c \u0026lt;- a * 1 + b * 0.5, then we can also say that there is multicollinearity between c, a, and b.\nMulticollinearity is a fact of life that lurks in most data sets. For example, in climate data, variables like temperature, humidity and air pressure are closely intertwined, leading to multicollinearity. That\u0026rsquo;s the case as well in medical research, where parameters like blood pressure, heart rate, and body mass index frequently display common patterns. Economic analysis is another good example, as variables such as Gross Domestic Product (GDP), unemployment rate, and consumer spending often exhibit multicollinearity.\nModel Interpretation Challenges Multicollinearity isn\u0026rsquo;t inherently problematic, but it can be a real buzz kill when the goal is interpreting predictor importance in explanatory models. In the presence of highly correlated predictors, most modelling methods, from the veteran linear models to the fancy gradient boosting, attribute a large part of the importance to only one of the predictors and not the others. In such cases, neglecting multicollinearity will certainly lead to underestimate the relevance of certain predictors.\nLet me go ahead and develop a toy data set to showcase this issue. But let\u0026rsquo;s load the required libraries first.\n#load the collinear package and its example data library(collinear) data(vi) #other required libraries library(ranger) library(dplyr) In the vi data frame shipped with the collinear package, the variables \u0026ldquo;soil_clay\u0026rdquo; and \u0026ldquo;humidity_range\u0026rdquo; are not correlated at all (Pearson correlation = -0.06).\nIn the code block below, the dplyr::transmute() command selects and renames them as a and b. After that, the two variables are scaled and centered, and dplyr::mutate() generates a few new columns:\ny: response variable resulting from a linear model where a has a slope of 0.75, b has a slope of 0.25, plus a bit of white noise generated with runif(). c: a new predictor highly correlated with a. d: a new predictor resulting from a linear combination of a and b. set.seed(1) df \u0026lt;- vi |\u0026gt; dplyr::slice_sample(n = 2000) |\u0026gt; dplyr::transmute( a = soil_clay, b = humidity_range ) |\u0026gt; scale() |\u0026gt; as.data.frame() |\u0026gt; dplyr::mutate( y = a * 0.75 + b * 0.25 + runif(n = dplyr::n(), min = -0.5, max = 0.5), c = a + runif(n = dplyr::n(), min = -0.5, max = 0.5), d = (a + b)/2 + runif(n = dplyr::n(), min = -0.5, max = 0.5) ) The Pearson correlation between all pairs of these predictors is shown below.\ncollinear::cor_df( df = df, predictors = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;) ) ## # A tibble: 6 × 3 ## x y correlation ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 c a 0.962 ## 2 d b 0.639 ## 3 d a 0.636 ## 4 d c 0.615 ## 5 b a -0.047 ## 6 c b -0.042 At this point, we have are two groups of predictors useful to understand how multicollinearity muddles model interpretation:\nPredictors with no multicollinearity: a and b. Predictors with multicollinearity: a, b, c, and d. In the next two sections and the Appendix, I show how and why model interpretation becomes challenging when multicollinearity is high. Let\u0026rsquo;s start with linear models.\nLinear Models The code below fits multiple linear regression models for both groups of predictors.\n#non-collinear predictors lm_ab \u0026lt;- lm( formula = y ~ a + b, data = df ) #collinear predictors lm_abcd \u0026lt;- lm( formula = y ~ a + b + c + d, data = df ) I would like you to pay attention to the estimates of the predictors a and b for both models. The estimates are the slopes in the linear model, a direct indication of the effect of a predictor over the response.\ncoefficients(lm_ab)[2:3] |\u0026gt; round(4) ## a b ## 0.7477 0.2616 coefficients(lm_abcd)[2:5] |\u0026gt; round(4) ## a b c d ## 0.7184 0.2596 0.0273 0.0039 On one hand, the model with no multicollinearity (lm_ab) achieved a pretty good solution for the coefficients of a and b. Remember that we created y as a * 0.75 + b * 0.25 plus some noise, and that\u0026rsquo;s exactly what the model is telling us here, so the interpretation is pretty straightforward.\nOn the other hand, the model with multicollinearity (lm_abcd) did well with b, but there are a few things in there that make the interpretation harder.\nThe coefficient of a (0.7165) is slightly smaller than the true one (0.75), which could lead us to downplay its relationship with y by a tiny bit. This is kinda OK though, as long as one is not using the model\u0026rsquo;s results to build nukes in the basement. The coefficient of c is so small that it could led us to believe that this predictor not important at all to explain y. But we know that a and c are almost identical copies, so model interpretation here is being definitely muddled by multicollinearity. The coefficient of d is tiny. Since d results from the sum of a and b, we could expect this predictor to be important in explaining y, but it got the shorter end of the stick in this case. It is not that the model it\u0026rsquo;s wrong though. This behavior of the linear model results from the QR decomposition (also QR factorization) applied by functions like lm(), glm(), glmnet::glmnet(), and nlme::gls() to improve numerical stability and computational efficiency, and to\u0026hellip; address multicollinearity in the model predictors.\nThe QR decomposition transforms the original predictors into a set of orthogonal predictors with no multicollinearity. This is the Q matrix, created in a fashion that resembles the way in which a Principal Components Analysis generates uncorrelated components from a set of correlated variables.\nThe code below applies QR decomposition to our multicollinear predictors, extracts the Q matrix, and shows the correlation between the new versions of a, b, c, and d.\n#predictors names predictors \u0026lt;- c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;) #QR decomposition of predictors df.qr \u0026lt;- qr(df[, predictors]) #extract Q matrix df.q \u0026lt;- qr.Q(df.qr) colnames(df.q) \u0026lt;- predictors #correlation between transformed predictors collinear::cor_df(df = df.q) ## # A tibble: 6 × 3 ## x y correlation ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 d c 0 ## 2 c b 0 ## 3 d b 0 ## 4 d a 0 ## 5 c a 0 ## 6 b a 0 The new set of predictors we are left with after the QR decomposition have exactly zero correlation! And now they are not our original predictors anymore, and have a different interpretation:\na is now \u0026ldquo;the part of a not in b, c, and d\u0026rdquo;. b is now \u0026ldquo;the part of b not in a, c, and d\u0026rdquo;. \u0026hellip;and so on\u0026hellip; The result of the QR decomposition can be plugged into the solve() function along with the response vector to estimate the coefficients of the linear model.\nsolve(a = df.qr, b = df$y) |\u0026gt; round(4) ## a b c d ## 0.7189 0.2595 0.0268 0.0040 These are almost exactly the ones we got for our model with multicollinearity. In the end, the coefficients resulting from a linear model are not those of the original predictors, but the ones of their uncorrelated versions generated by the QR decomposition.\nBut this is not the only issue of model interpretability under multicollinearity. Let\u0026rsquo;s take a look at the standard errors of the estimates. These are a measure of the coefficient estimation uncertainty, and are used to compute the p-values of the estimates. As such, they are directly linked with the \u0026ldquo;statistical significance\u0026rdquo; (whatever that means) of the predictors within the model.\nThe code below shows the standard errors of the model without and with multicollinearity.\nsummary(lm_ab)$coefficients[, \u0026quot;Std. Error\u0026quot;][2:3] |\u0026gt; round(4) ## a b ## 0.0066 0.0066 summary(lm_abcd)$coefficients[, \u0026quot;Std. Error\u0026quot;][2:5] |\u0026gt; round(4) ## a b c d ## 0.0267 0.0134 0.0232 0.0230 These standard errors of the model with multicollinearity are an order of magnitude higher than the ones of the model without multicollinearity.\nSince our toy dataset is relatively large (2000 cases) and the relationship between the response and a few of the predictors pretty robust, there are no real issues arising, as these differences in estimation precision are not enough to change the p-values of the estimates. However, in a small data set with high multicollinearity and a weaker relationship between the response and the predictors, standard errors of the estimate become wide, which increases p-values and reduces \u0026ldquo;significance\u0026rdquo;. Such a situation might lead us to believe that a predictor does not explain the response, when in fact it does. And this, again, is a model interpretability issue caused by multicollinearity.\nAt the end of this post there is an appendix with code examples of other types of linear models that use QR decomposition and become challenging to interpret in the presence of multicollinearity. Play with them as you please!\nNow, let\u0026rsquo;s take a look at how multicollinearity can also mess up the interpretation of a commonly used machine learning algorithm.\nRandom Forest It is not uncommon to hear something like \u0026ldquo;random forest is insensitive to multicollinearity\u0026rdquo;. Actually, I cannot confirm nor deny that I have said that before. Anyway, it is kind of true if one is focused on prediction problmes. However, when the aim is interpreting predictor importance scores, then one has to be mindful about multicollinearity as well.\nLet\u0026rsquo;s see an example. The code below fits two random forest models with our two sets of predictors.\n#non-collinear predictors rf_ab \u0026lt;- ranger::ranger( formula = y ~ a + b, data = df, importance = \u0026quot;permutation\u0026quot;, seed = 1 #for reproducibility ) #collinear predictors rf_abcd \u0026lt;- ranger::ranger( formula = y ~ a + b + c + d, data = df, importance = \u0026quot;permutation\u0026quot;, seed = 1 ) Let\u0026rsquo;s take a look at the prediction error the two models on the out-of-bag data. While building each regression tree, Random Forest leaves a random subset of the data out. Then, each case gets a prediction from all trees that had it in the out-of-bag data, and the prediction error is averaged across all cases to get the numbers below.\nrf_ab$prediction.error ## [1] 0.1026779 rf_abcd$prediction.error ## [1] 0.1035678 According to these numbers, these two models are basically equivalent in their ability to predict our response y.\nBut now, you noticed that I set the argument importance to \u0026ldquo;permutation\u0026rdquo;. Permutation importance quantifies how the out-of-bag error increases when a predictor is permuted across all trees where the predictor is used. It is pretty robust importance metric that bears no resemblance whatsoever with the coefficients of a linear model. Think of it as a very different way to answer the question \u0026ldquo;what variables are important in this model?\u0026rdquo;.\nThe permutation importance scores of the two random forest models are show below.\nrf_ab$variable.importance |\u0026gt; round(4) ## a b ## 1.0702 0.1322 rf_abcd$variable.importance |\u0026gt; round(4) ## a b c d ## 0.5019 0.0561 0.1662 0.0815 There is one interesting detail here. The predictor a has a permutation error three times higher than c in the second model, even though we could expect them to be similar due to their very high correlation. There are two reasons for this mismatch:\nRandom Forest is much more sensitive to the white noise in c than linear models, especially in the deep parts of the regression trees, due to local (within-split data) decoupling with the response y. In consequence, it does not get selected as often as a in these deeper areas of the trees, and has less overall importance. The predictor c competes with d, that has around 50% of the information in c (and a). If we remove d from the model, then the permutation importance of c doubles up. Then, with d in the model, we underestimate the real importance of c due to multicollinearity alone. rf_abc \u0026lt;- ranger::ranger( formula = y ~ a + b + c, data = df, importance = \u0026quot;permutation\u0026quot;, seed = 1 ) rf_abc$variable.importance |\u0026gt; round(4) ## a b c ## 0.5037 0.1234 0.3133 With all that in mind, we can conclude that interpreting importance scores in Random Forest models is challenging when multicollinearity is high. But Random Forest is not the only machine learning affected by this issue. In the Appendix below I have left an example with Extreme Gradient Boosting so you can play with it.\nAnd that\u0026rsquo;s all for now, folks, I hope you found this post useful!\nAppendix This section shows several extra examples of linear and machine learning models you can play with.\nOther linear models using QR decomposition As I commented above, many linear modeling functions use QR decomposition, and you will have to be careful interpreting model coefficients in the presence of strong multicollinearity in the predictors.\nHere I show several examples with glm() (Generalized Linear Models), nlme::gls() (Generalized Least Squares), and glmnet::cv.glmnet() (Elastic Net Regularization). In all them, no matter how fancy, the interpretation of coefficients becomes tricky when multicollinearity is high.\nGeneralized Linear Models with glm()\n#Generalized Linear Models #non-collinear predictors glm_ab \u0026lt;- glm( formula = y ~ a + b, data = df ) round(coefficients(glm_ab), 4)[2:3] ## a b ## 0.7477 0.2616 #collinear predictors glm_abcd \u0026lt;- glm( formula = y ~ a + b + c + d, data = df ) round(coefficients(glm_abcd), 4)[2:5] ## a b c d ## 0.7184 0.2596 0.0273 0.0039 Generalized Least Squares with nlme::gls()\nlibrary(nlme) ## ## Attaching package: 'nlme' ## The following object is masked from 'package:dplyr': ## ## collapse #Generalized Least Squares #non-collinear predictors gls_ab \u0026lt;- nlme::gls( model = y ~ a + b, data = df ) round(coefficients(gls_ab), 4)[2:3] ## a b ## 0.7477 0.2616 #collinear predictors gls_abcd \u0026lt;- nlme::gls( model = y ~ a + b + c + d, data = df ) round(coefficients(gls_abcd), 4)[2:5] ## a b c d ## 0.7184 0.2596 0.0273 0.0039 Elastic Net Regularization and Lasso penalty with glmnet::glmnet()\nlibrary(glmnet) ## Loading required package: Matrix ## Loaded glmnet 4.1-8 #Elastic net regularization with Lasso penalty #non-collinear predictors glmnet_ab \u0026lt;- glmnet::cv.glmnet( x = as.matrix(df[, c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;)]), y = df$y, alpha = 1 #lasso penalty ) round(coef(glmnet_ab$glmnet.fit, s = glmnet_ab$lambda.min), 4)[2:3] ## [1] 0.7438 0.2578 #collinear predictors glmnet_abcd \u0026lt;- glmnet::cv.glmnet( x = as.matrix(df[, c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;)]), y = df$y, alpha = 1 ) #notice that the lasso regularization nuked the coefficients of predictors b and c round(coef(glmnet_abcd$glmnet.fit, s = glmnet_abcd$lambda.min), 4)[2:5] ## [1] 0.7101 0.2507 0.0267 0.0149 Extreme Gradient Boosting under multicollinearity Gradient Boosting models trained with multicollinear predictors behave in a way similar to linear models with QR decomposition. When two variables are highly correlated, one of them is going to have an importance much higher than the other.\nlibrary(xgboost) ## ## Attaching package: 'xgboost' ## The following object is masked from 'package:dplyr': ## ## slice #without multicollinearity gb_ab \u0026lt;- xgboost::xgboost( data = as.matrix(df[, c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;)]), label = df$y, objective = \u0026quot;reg:squarederror\u0026quot;, nrounds = 100, verbose = FALSE ) #with multicollinearity gb_abcd \u0026lt;- xgboost::xgboost( data = as.matrix(df[, c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;)]), label = df$y, objective = \u0026quot;reg:squarederror\u0026quot;, nrounds = 100, verbose = FALSE ) xgb.importance(model = gb_ab)[, c(1:2)] ## Feature Gain ## 1: a 0.8463005 ## 2: b 0.1536995 xgb.importance(model = gb_abcd)[, c(1:2)] |\u0026gt; dplyr::arrange(Feature) ## Feature Gain ## 1: a 0.78129661 ## 2: b 0.07386393 ## 3: c 0.03595619 ## 4: d 0.10888327 But there is a twist too. When two variables are perfectly correlated, one of them is removed right away from the model!\n#replace c with perfect copy of a df$c \u0026lt;- df$a #with multicollinearity gb_abcd \u0026lt;- xgboost::xgboost( data = as.matrix(df[, c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;)]), label = df$y, objective = \u0026quot;reg:squarederror\u0026quot;, nrounds = 100, verbose = FALSE ) xgb.importance(model = gb_abcd)[, c(1:2)] |\u0026gt; dplyr::arrange(Feature) ## Feature Gain ## 1: a 0.79469959 ## 2: b 0.07857141 ## 3: d 0.12672900 ","date":1698537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698560063,"objectID":"bd8e1e7c6a466734ff79c9c15fc1508e","permalink":"https://blasbenito.com/post/multicollinearity-model-interpretability/","publishdate":"2023-10-29T00:00:00Z","relpermalink":"/post/multicollinearity-model-interpretability/","section":"post","summary":"In this post, I delve into the intricacies of model interpretation under the influence of multicollinearity, and use R and a toy data set to demonstrate how this phenomenon impacts both linear and machine learning models.","tags":["R packages","Multicollinearity","Explanatory Modeling","Variable Selection","Stats","Data Science"],"title":"Multicollinearity Hinders Model Interpretability","type":"post"},{"authors":["Emilio Guirado","Manuel Delgado-Baquerizo","Blas M. Benito","José Luis Molina Pardo","Miguel Berdugo","Jaime Martínez-Valderrama","Fernando T. Maestre"],"categories":null,"content":"","date":1696291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696291200,"objectID":"c5e31a2cdac8effe0b45c20cff2dbaf1","permalink":"https://blasbenito.com/publication/2023_guirado_pnas/","publishdate":"2023-10-03T00:00:00Z","relpermalink":"/publication/2023_guirado_pnas/","section":"publication","summary":"Fairy circles (FCs) are intriguing regular vegetation patterns that have only been described in Namibia and Australia so far. We conducted a global and systematic assessment of FC-like vegetation patterns and discovered hundreds of FC-like locations on three continents. We also characterized the range of environmental conditions that determine their presence, which is restricted to narrow and specific soil and climatic conditions. Areas showing FC-like vegetation patterns also had more stable productivity over time than surrounding areas having non-FC patterns. Our study provides insights into the ecology and biogeography of these fascinating vegetation patterns and the first atlas of their global distribution.","tags":["Biogeography","Random Forest","spatialRF","Plant Ecology","Dryland ecology"],"title":"The global biogeography and environmental drivers of fairy circles","type":"publication"},{"authors":["Blas M. Benito"],"categories":[],"content":" Introduction The package spatialRF facilitates fitting spatial regression models on regular or irregular data with Random Forest. It does so by generating spatial predictors that help the model \u0026ldquo;understand\u0026rdquo; the spatial structure of the training data with the end goal of minimizing the spatial autocorrelation of the model residuals and offering honest variable importance scores.\nTwo main methods to generate spatial predictors from the distance matrix of the data points are implemented in the package:\nMoran\u0026rsquo;s Eigenvector Maps (Dray, Legendre, and Peres-Neto 2006). Distance matrix columns as explanatory variables (Hengl et al. 2018). The package is designed to minimize the code required to fit a spatial model from a training dataset, the names of the response and the predictors, and a distance matrix, as shown below.\nspatial.model \u0026lt;- spatialRF::rf_spatial( data = your_dataframe, dependent.variable.name = \u0026quot;your_response_variable\u0026quot;, predictor.variable.names = c(\u0026quot;predictor1\u0026quot;, \u0026quot;predictor2\u0026quot;, ..., \u0026quot;predictorN\u0026quot;), distance.matrix = your_distance_matrix ) spatialRF uses the fast and efficient ranger package under the hood (Wright and Ziegler 2017), so please, cite the ranger package when using spatialRF!\nThis package also provides tools to identify potentially interesting variable interactions, tune random forest hyperparameters, assess model performance on spatially independent data folds, and examine the resulting models via importance plots, response curves, and response surfaces.\nDevelopment This package is reaching its final form, and big changes are not expected at this stage. However, it has many functions, and even though all them have been tested, only one dataset has been used for those tests. You will find bugs, and something will go wrong almost surely. If you have time to report bugs, please, do so in any of the following ways:\nOpen a new issue in the Issues GitHub page of the package. Send me an email explaining the issue and the error messages with enough detail at blasbenito at gmail dot com. Send a direct message to my twitter account explaining the issue. I will do my best to solve any issues ASAP!\nApplications The goal of spatialRF is to help fitting explanatory spatial regression, where the target is to understand how a set of predictors and the spatial structure of the data influences response variable. Therefore, the spatial analyses implemented in the package can be applied to any spatial dataset, regular or irregular, with a sample size between ~100 and ~5000 cases (the higher end will depend on the RAM memory available), a quantitative or binary (values 0 and 1) response variable, and a more or less large set of predictive variables.\nAll functions but rf_spatial() work with non-spatial data as well if the arguments distance.matrix and distance.thresholds are not provided In such case, the number of training cases is no longer limited by the size of the distance matrix, and models can be trained with hundreds of thousands of rows. In such case, the spatial autocorrelation of the model\u0026rsquo;s residuals is not assessed.\nHowever, when the focus is on fitting spatial models, and due to the nature of the spatial predictors used to represent the spatial structure of the training data, there are many things this package cannot do:\nPredict model results over raster data.\nPredict a model result over another region with a different spatial structure.\nWork with \u0026ldquo;big data\u0026rdquo;, whatever that means.\nImputation or extrapolation (it can be done, but models based on spatial predictors are hardly transferable).\nTake temporal autocorrelation into account (but this is something that might be implemented later on).\nIf after considering these limitations you are still interested, follow me, I will show you how it works.\nCitation There is a paper in the making about this package. In the meantime, if you find it useful for your academic work, please cite the ranger package as well, it is the true core of spatialRF!\nMarvin N. Wright, Andreas Ziegler (2017). ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software, 77(1), 1-17. doi:10.18637/jss.v077.i01\nBlas M. Benito (2021). spatialRF: Easy Spatial Regression with Random Forest. R package version 1.1.0. doi: 10.5281/zenodo.4745208. url: https://blasbenito.github.io/spatialRF/\nInstall The version 1.1.3 can be installed from CRAN:\ninstall.packages(\u0026quot;spatialRF\u0026quot;) The package can also be installed from GitHub as follows. There are several branches in the repository:\nmain: latest stable version (1.1.0 currently). development: development version, usually very broken. v.1.0.9 to v.1.1.2: archived versions. remotes::install_github( repo = \u0026quot;blasbenito/spatialRF\u0026quot;, ref = \u0026quot;main\u0026quot;, force = TRUE, quiet = TRUE ) There are a few other libraries that will be useful during this tutorial.\nlibrary(spatialRF) library(kableExtra) library(rnaturalearth) library(rnaturalearthdata) library(tidyverse) library(randomForestExplainer) library(pdp) Data requirements The data required to fit random forest models with spatialRF must fulfill several conditions:\nThe input format is data.frame. At the moment, tibbles are not fully supported. The number of rows must be somewhere between 100 and ~5000, at least if your target is fitting spatial models. This limitation comes from the fact that the distance matrix grows very fast with an increasing number of training records, so for large datasets, there might not be enough RAM in your machine. The number of predictors should be larger than 3. Fitting a Random Forest model is moot otherwise. Factors in the response or the predictors are not explicitly supported in the package. They may work, or they won\u0026rsquo;t, but in any case, I designed this package for quantitative data alone. However, binary responses with values 0 and 1 are partially supported. Must be free of NA. You can check if there are NA records with sum(apply(df, 2, is.na)). If the result is larger than 0, then just execute df \u0026lt;- na.omit(df) to remove rows with empty cells. Columns cannot have zero variance. This condition can be checked with apply(df, 2, var) == 0. Columns yielding TRUE should be removed. Columns must not yield NaN or Inf when scaled. You can check each condition with sum(apply(scale(df), 2, is.nan)) and sum(apply(scale(df), 2, is.infinite)). If higher than 0, you can find what columns are giving issues with sapply(as.data.frame(scale(df)), function(x)any(is.nan(x))) and sapply(as.data.frame(scale(df)), function(x)any(is.infinite(x))). Any column yielding TRUE will generate issues while trying to fit models with spatialRF. Example data The package includes an example dataset that fulfills the conditions mentioned above, named plant_richness_df. It is a data frame with plant species richness and predictors for 227 ecoregions in the Americas, and a distance matrix among the ecoregion edges named, well, distance_matrix.\nThe package follows a convention throughout functions:\nThe argument data requires a training data frame. The argument dependent.variable.name is the column name of the response variable. The argument predictor.variable.names contains the column names of the predictors. The argument xy takes a data frame or matrix with two columns named \u0026ldquo;x\u0026rdquo; and \u0026ldquo;y\u0026rdquo;, in that order, with the case coordinates. The argument distance.matrix requires a matrix of distances between the cases in data. The argument distance.thresholds is a numeric vector of distances at with spatial autocorrelation wants to be computed. It is convenient to define these arguments at the beginning of the workflow.\n#loading training data and distance matrix from the package data(plant_richness_df) data(distance_matrix) #names of the response variable and the predictors dependent.variable.name \u0026lt;- \u0026quot;richness_species_vascular\u0026quot; predictor.variable.names \u0026lt;- colnames(plant_richness_df)[5:21] #coordinates of the cases xy \u0026lt;- plant_richness_df[, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)] #distance matrix distance.matrix \u0026lt;- distance_matrix #distance thresholds (same units as distance_matrix) distance.thresholds \u0026lt;- c(0, 1000, 2000, 4000, 8000) #random seed for reproducibility random.seed \u0026lt;- 1 The response variable of plant_richness_df is \u0026ldquo;richness_species_vascular\u0026rdquo;, that represents the total count of vascular plant species found on each ecoregion. The figure below shows the centroids of each ecoregion along with their associated value of the response variable.\nworld \u0026lt;- rnaturalearth::ne_countries( scale = \u0026quot;medium\u0026quot;, returnclass = \u0026quot;sf\u0026quot; ) ggplot2::ggplot() + ggplot2::geom_sf( data = world, fill = \u0026quot;white\u0026quot; ) + ggplot2::geom_point( data = plant_richness_df, ggplot2::aes( x = x, y = y, color = richness_species_vascular ), size = 2.5 ) + ggplot2::scale_color_viridis_c( direction = -1, option = \u0026quot;F\u0026quot; ) + ggplot2::theme_bw() + ggplot2::labs(color = \u0026quot;Plant richness\u0026quot;) + ggplot2::scale_x_continuous(limits = c(-170, -30)) + ggplot2::scale_y_continuous(limits = c(-58, 80)) + ggplot2::ggtitle(\u0026quot;Plant richness of the American ecoregions\u0026quot;) + ggplot2::xlab(\u0026quot;Longitude\u0026quot;) + ggplot2::ylab(\u0026quot;Latitude\u0026quot;) The predictors (columns 5 to 21) represent diverse factors that may influence plant richness such as sampling bias, the area of the ecoregion, climatic variables, human presence and impact, topography, geographical fragmentation, and features of the neighbors of each ecoregion. The figure below shows the scatterplots of the response variable (y axis) against each predictor (x axis).\nNote: Every plotting function in the package now allows changing the colors of their main features via specific arguments such as point.color, line.color, or fill.color.\nspatialRF::plot_training_df( data = plant_richness_df, dependent.variable.name = dependent.variable.name, predictor.variable.names = predictor.variable.names, ncol = 3, point.color = viridis::viridis(100, option = \u0026quot;F\u0026quot;), line.color = \u0026quot;gray30\u0026quot; ) The function plot_training_df_moran() helps assessing the spatial autocorrelation of the response variable and the predictors across different distance thresholds. Low Moran\u0026rsquo;s I and p-values equal or larger than 0.05 indicate that there is no spatial autocorrelation for the given variable and distance threshold.\nspatialRF::plot_training_df_moran( data = plant_richness_df, dependent.variable.name = dependent.variable.name, predictor.variable.names = predictor.variable.names, distance.matrix = distance.matrix, distance.thresholds = distance.thresholds, fill.color = viridis::viridis( 100, option = \u0026quot;F\u0026quot;, direction = -1 ), point.color = \u0026quot;gray40\u0026quot; ) Reducing multicollinearity in the predictors The functions auto_cor() and auto_vif() help reduce redundancy in the predictors by using different criteria (bivariate R squared vs. variance inflation factor), while allowing the user to define an order of preference, which can be based either on domain expertise or on a quantitative assessment (e.g., order of preference based on variable importance scores or model coefficients). The preference order is defined as a character vector in the preference.order argument of both functions, and does not need to include the names of all predictors, but just the ones the user would like to keep in the analysis.\nNotice that I have set cor.threshold and vif.threshold to low values because the predictors in plant_richness_df already have little multicollinearity,. The default values (cor.threshold = 0.75 and vif.threshold = 5) should work well when combined together for any other set of predictors.\npreference.order \u0026lt;- c( \u0026quot;climate_bio1_average_X_bias_area_km2\u0026quot;, \u0026quot;climate_aridity_index_average\u0026quot;, \u0026quot;climate_hypervolume\u0026quot;, \u0026quot;climate_bio1_average\u0026quot;, \u0026quot;climate_bio15_minimum\u0026quot;, \u0026quot;bias_area_km2\u0026quot; ) predictor.variable.names \u0026lt;- spatialRF::auto_cor( x = plant_richness_df[, predictor.variable.names], cor.threshold = 0.6, preference.order = preference.order ) %\u0026gt;% spatialRF::auto_vif( vif.threshold = 2.5, preference.order = preference.order ) ## [auto_cor()]: Removed variables: human_footprint_average ## [auto_vif()]: Variables are not collinear. The output of auto_cor() or auto_vif() has the class \u0026ldquo;variable_selection\u0026rdquo;, which can be used as input in every function having the argument predictor.variable.names.\nnames(predictor.variable.names) ## [1] \u0026quot;vif\u0026quot; \u0026quot;selected.variables\u0026quot; \u0026quot;selected.variables.df\u0026quot; The slot selected.variables contains the names of the selected predictors.\npredictor.variable.names$selected.variables ## [1] \u0026quot;climate_aridity_index_average\u0026quot; \u0026quot;climate_hypervolume\u0026quot; ## [3] \u0026quot;climate_bio1_average\u0026quot; \u0026quot;climate_bio15_minimum\u0026quot; ## [5] \u0026quot;bias_area_km2\u0026quot; \u0026quot;bias_species_per_record\u0026quot; ## [7] \u0026quot;climate_velocity_lgm_average\u0026quot; \u0026quot;neighbors_count\u0026quot; ## [9] \u0026quot;neighbors_percent_shared_edge\u0026quot; \u0026quot;human_population_density\u0026quot; ## [11] \u0026quot;topography_elevation_average\u0026quot; \u0026quot;landcover_herbs_percent_average\u0026quot; ## [13] \u0026quot;fragmentation_cohesion\u0026quot; \u0026quot;fragmentation_division\u0026quot; ## [15] \u0026quot;neighbors_area\u0026quot; \u0026quot;human_population\u0026quot; Finding promising variable interactions Random Forests already takes into account variable interactions of the form \u0026ldquo;variable a becomes important when b is higher than x\u0026rdquo;. However, Random Forest can also take advantage of variable interactions of the form a * b, across the complete ranges of the predictors, as they are commonly defined in regression models, and \u0026ldquo;interactions\u0026rdquo; (not the proper name, but used here for simplicity) represented by the first component of a PCA on the predictors a and b.\nThe function the_feature_engineer() tests all possible interactions of both types among the most important predictors, and suggesting the ones not correlated among themselves and with the other predictors inducing an increase in the model\u0026rsquo;s R squared (or AUC when the response is binary) on independent data via spatial cross-validation (see rf_evaluate()).\ninteractions \u0026lt;- spatialRF::the_feature_engineer( data = plant_richness_df, dependent.variable.name = dependent.variable.name, predictor.variable.names = predictor.variable.names, xy = xy, importance.threshold = 0.50, #uses 50% best predictors cor.threshold = 0.60, #max corr between interactions and predictors seed = random.seed, repetitions = 100, verbose = TRUE ) ## Fitting and evaluating a model without interactions. ## Testing 28 candidate interactions. ## Interactions identified: 5 ## ┌──────────────────┬──────────────────┬──────────────────┬──────────────────┐ ## │ Interaction │ Importance (% of │ R-squared │ Max cor with │ ## │ │ max) │ improvement │ predictors │ ## ├──────────────────┼──────────────────┼──────────────────┼──────────────────┤ ## │ bias_area_km2..x │ 60.8 │ 0.096 │ 0.60 │ ## │ ..bias_species_p │ │ │ │ ## │ er_record │ │ │ │ ## ├──────────────────┼──────────────────┼──────────────────┼──────────────────┤ ## │ climate_bio1_ave │ 97.9 │ 0.067 │ 0.34 │ ## │ rage..pca..human │ │ │ │ ## │ _population_dens │ │ │ │ ## │ ity │ │ │ │ ## ├──────────────────┼──────────────────┼──────────────────┼──────────────────┤ ## │ climate_bio1_ave │ 98.7 │ 0.049 │ 0.24 │ ## │ rage..pca..neigh │ │ │ │ ## │ bors_count │ │ │ │ ## ├──────────────────┼──────────────────┼──────────────────┼──────────────────┤ ## │ human_population │ 66.5 │ 0.021 │ 0.55 │ ## │ ..x..bias_specie │ │ │ │ ## │ s_per_record │ │ │ │ ## ├──────────────────┼──────────────────┼──────────────────┼──────────────────┤ ## │ bias_area_km2..p │ 62.7 │ 0.029 │ 0.305 │ ## │ ca..neighbors_pe │ │ │ │ ## │ rcent_shared_edg │ │ │ │ ## │ e │ │ │ │ ## └──────────────────┴──────────────────┴──────────────────┴──────────────────┘ ## Comparing models with and without interactions via spatial cross-validation. The upper panel in the plot plot above shows the relationship between the interaction and the response variable. It also indicates the gain in R squared (+R2), the importance, in percentage, when used in a model along the other predictors (Imp. (%)), and the maximum Pearson correlation of the interaction with the predictors. The violin-plot shows a comparison of the model with and without the selected interaction made via spatial cross-validation using 100 repetitions (see rf_evaluate() and rf_compare() for further details).\nThe function also returns a data frame with all the interactions considered. The columns are:\ninteraction.name: Interactions computed via multiplication are named a..x..b, while interactions computed via PCA are named a..pca..b. interaction.importance: Importance of the interaction expressed as a percentage. If interaction.importance == 100, that means that the interaction is the most important predictor in the model fitted with the interaction and the predictors named in predictor.variable.names. interaction.metric.gain: Difference in R squared (or AUC for models fitting a binary response) between a model with and a model without the interaction. max.cor.with.predictors: The maximum Pearson correlation of the interaction with the predictors named in predictor.variable.names. Gives an idea of the amount of multicollinearity the interaction introduces in the model. variable.a.name and variable.b.name: Names of the predictors involved in the interaction. selected: TRUE if the interaction fulfills the selection criteria (importance higher than a threshold, positive gain in R squared or AUC, and Pearson correlation with other predictors lower than a threshold). The selected interactions have a correlation among themselves always lower than the value of the argument cor.threshold. kableExtra::kbl( head(interactions$screening, 10), format = \u0026quot;html\u0026quot; ) %\u0026gt;% kableExtra::kable_paper(\u0026quot;hover\u0026quot;, full_width = F) interaction.name interaction.importance interaction.metric.gain max.cor.with.predictors variable.a.name variable.b.name selected bias_area_km2..x..bias_species_per_record 60.779 0.096 0.5962899 bias_area_km2 bias_species_per_record TRUE climate_bio1_average..pca..human_population_density 97.944 0.067 0.3369664 climate_bio1_average human_population_density TRUE climate_bio1_average..pca..neighbors_count 98.742 0.049 0.2441858 climate_bio1_average neighbors_count TRUE climate_bio1_average..pca..neighbors_percent_shared_edge 84.196 0.066 0.2215337 climate_bio1_average neighbors_percent_shared_edge TRUE human_population..x..bias_species_per_record 66.512 0.021 0.5462406 human_population bias_species_per_record TRUE climate_bio1_average..pca..bias_species_per_record 70.082 0.018 0.3469834 climate_bio1_average bias_species_per_record TRUE bias_area_km2..pca..neighbors_percent_shared_edge 62.678 0.029 0.3046471 bias_area_km2 neighbors_percent_shared_edge TRUE climate_hypervolume..x..human_population_density 49.367 -0.006 0.5599486 climate_hypervolume human_population_density FALSE neighbors_count..pca..neighbors_percent_shared_edge 63.808 0.016 0.1584006 neighbors_count neighbors_percent_shared_edge TRUE bias_area_km2..pca..neighbors_count 58.662 0.012 0.2166191 bias_area_km2 neighbors_count TRUE The function returns a data frame with the response variables, the predictors, and the selected interactions that can be used right away as a training data frame. However, the function cannot say whether an interaction makes sense, and it is up to the user to choose wisely whether to select an interaction or not. In this particular case, and just for the sake of simplicity, we will be using the resulting data frame as training data.\n#adding interaction column to the training data plant_richness_df \u0026lt;- interactions$data #adding interaction name to predictor.variable.names predictor.variable.names \u0026lt;- interactions$predictor.variable.names Fitting a non-spatial Random Forest model with rf() The function rf() is a convenient wrapper for ranger::ranger() used in every modelling function of the spatialRF package. It takes the training data, the names of the response and the predictors, and optionally (to assess the spatial autocorrelation of the residuals), the distance matrix, and a vector of distance thresholds (in the same units as the distances in distance_matrix).\nThese distance thresholds are the neighborhoods at which the model will check the spatial autocorrelation of the residuals. Their values may depend on the spatial scale of the data, and the ecological system under study.\nNotice that here I plug the object predictor.variable.names, output of auto_cor() and auto_vif(), directly into the predictor.variable.names argument of the rf() function to fit a random forest model.\nmodel.non.spatial \u0026lt;- spatialRF::rf( data = plant_richness_df, dependent.variable.name = dependent.variable.name, predictor.variable.names = predictor.variable.names, distance.matrix = distance.matrix, distance.thresholds = distance.thresholds, xy = xy, #not needed by rf, but other functions read it from the model seed = random.seed, verbose = FALSE ) The output is a list with several slots containing the information required to interpret the model. The information available in these slots can be plotted (functions named plot_...()), printed to screen (print_...()) and captured for further analyses (get_...()).\nResiduals The slot residuals (model.non.spatial$residuals) stores the values of the residuals and the results of the normality and spatial autocorrelation tests, and its content can be plotted with plot_residuals_diagnostics().\nspatialRF::plot_residuals_diagnostics( model.non.spatial, verbose = FALSE ) The upper panels show the results of the normality test (interpretation in the title), the middle panel shows the relationship between the residuals and the fitted values, important to understand the behavior of the residuals, and the lower panel shows the Moran\u0026rsquo;s I of the residuals across distance thresholds and their respective p-values (positive for 0 and 1000 km).\nVariable importance Global variable importance The slot importance (model.non.spatial$variable.importance) contains the variable importance scores. These can be plotted with plot_importance(), printed with print_importance(), and the dataframe retrieved with get_importance()\nspatialRF::plot_importance( model.non.spatial, verbose = FALSE ) Variable importance represents the increase in mean error (computed on the out-of-bag data) across trees when a predictor is permuted. Values lower than zero would indicate that the variable performs worse than a random one.\nIf the argument scaled.importance = TRUE is used, the variable importance scores are computed from the scaled data, making the importance scores easier to compare across different models.\nThe package randomForestExplainer offers a couple of interesting options to deepen our understanding on variable importance scores. The first one is measure_importance(), which analyzes the forest to find out the average minimum tree depth at which each variable can be found (mean_min_depth), the number of nodes in which a variable was selected to make a split (no_of_nodes), the number of times the variable was selected as the first one to start a tree (times_a_root), and the probability of a variable to be in more nodes than what it would be expected by chance (p_value).\nimportance.df \u0026lt;- randomForestExplainer::measure_importance( model.non.spatial, measures = c(\u0026quot;mean_min_depth\u0026quot;, \u0026quot;no_of_nodes\u0026quot;, \u0026quot;times_a_root\u0026quot;, \u0026quot;p_value\u0026quot;) ) kableExtra::kbl( importance.df %\u0026gt;% dplyr::arrange(mean_min_depth) %\u0026gt;% dplyr::mutate(p_value = round(p_value, 4)), format = \u0026quot;html\u0026quot; ) %\u0026gt;% kableExtra::kable_paper(\u0026quot;hover\u0026quot;, full_width = F) variable mean_min_depth no_of_nodes times_a_root p_value climate_bio1_average..pca..neighbors_count 2.811087 2098 86 0.0000 human_population 2.989940 2222 51 0.0000 climate_bio1_average 3.117996 1979 57 0.0000 climate_hypervolume 3.200133 2072 44 0.0000 climate_bio1_average..pca..human_population_density 3.201070 1781 64 0.4909 bias_area_km2..x..bias_species_per_record 3.379787 1797 46 0.3406 human_population_density 3.454233 1900 23 0.0020 bias_species_per_record 3.564676 2321 2 0.0000 bias_area_km2..pca..neighbors_percent_shared_edge 3.959485 1712 33 0.9519 human_population..x..bias_species_per_record 3.977952 1780 32 0.5006 bias_area_km2 4.186849 1800 10 0.3144 neighbors_count 4.204326 1325 29 1.0000 topography_elevation_average 4.306636 1732 0 0.8795 neighbors_percent_shared_edge 4.409251 1700 5 0.9749 neighbors_area 4.643590 1621 2 1.0000 fragmentation_cohesion 4.770930 1518 8 1.0000 climate_velocity_lgm_average 4.845155 1658 3 0.9986 climate_aridity_index_average 4.858302 1682 3 0.9919 landcover_herbs_percent_average 4.984346 1656 0 0.9988 climate_bio15_minimum 5.057002 1552 2 1.0000 fragmentation_division 5.229187 1468 0 1.0000 Contribution of predictors to model transferability The new function rf_importance() offers a way to assess to what extent each predictor contributes to model transferability (predictive ability on independent spatial folds measured with rf_evaluate(), see below). It does so by comparing the performance of the full model with models fitted without each one of the predictors. The difference in performance between the full model and a model without a given predictor represents the contribution of such predictor to model transferability.\nmodel.non.spatial \u0026lt;- spatialRF::rf_importance( model = model.non.spatial ) The function results are added to the \u0026ldquo;importance\u0026rdquo; slot of the model.\nnames(model.non.spatial$importance) ## [1] \u0026quot;per.variable\u0026quot; \u0026quot;local\u0026quot; \u0026quot;oob.per.variable.plot\u0026quot; ## [4] \u0026quot;cv.per.variable.plot\u0026quot; The data frame \u0026ldquo;per.variable\u0026rdquo; contains the columns \u0026ldquo;importance.cv\u0026rdquo; (median importance), \u0026ldquo;importance.cv.mad\u0026rdquo; (median absolute deviation), \u0026ldquo;importance.cv.percent\u0026rdquo; (median importance in percentage), and \u0026ldquo;importance.cv.percent.mad\u0026rdquo; (median absolute deviation of the importance in percent). The ggplot object \u0026ldquo;cv.per.variable.plot\u0026rdquo; contains the importance plot with the median and the median absolute deviation shown above.\nThe importance computed by random forest on the out-of-bag data by permutating each predictor (as computed by rf()) and the contribution of each predictor to model transferability (as computed by rf_importance()) show a moderate correlation, indicating that both importance measures capture different aspects of the effect of the variables on the model results.\nmodel.non.spatial$importance$per.variable %\u0026gt;% ggplot2::ggplot() + ggplot2::aes( x = importance.oob, y = importance.cv ) + ggplot2::geom_point(size = 3) + ggplot2::theme_bw() + ggplot2::xlab(\u0026quot;Importance (out-of-bag)\u0026quot;) + ggplot2::ylab(\u0026quot;Contribution to transferability\u0026quot;) + ggplot2::geom_smooth(method = \u0026quot;lm\u0026quot;, formula = y ~ x, color = \u0026quot;red4\u0026quot;) Local variable importance Random forest also computes the average increase in error when a variable is permuted for each case. This is named \u0026ldquo;local importance\u0026rdquo;, is stored in model.non.spatial$importance$local as a data frame, and can be retrieved with get_importance_local().\nlocal.importance \u0026lt;- spatialRF::get_importance_local(model.non.spatial) The table below shows the first few records and columns. Larger values indicate larger average errors when estimating a case with the permuted version of the variable, so more important variables will show larger values.\nkableExtra::kbl( round(local.importance[1:10, 1:5], 0), format = \u0026quot;html\u0026quot; ) %\u0026gt;% kableExtra::kable_paper(\u0026quot;hover\u0026quot;, full_width = F) climate_aridity_index_average climate_hypervolume climate_bio1_average climate_bio15_minimum bias_area_km2 -120 705 214 231 -274 502 -400 -431 375 470 -182 -155 1152 46 -75 384 769 704 5 -538 -399 -706 -669 350 -711 248 1113 715 483 475 195 705 513 286 332 -247 -629 506 -332 -125 335 -519 1016 95 -246 275 1154 429 71 234 When case coordinates are joined with the local importance scores, it is possible to draw maps showing how variable importance changes over space.\n#adding coordinates local.importance \u0026lt;- cbind( xy, local.importance ) #colors color.low \u0026lt;- viridis::viridis( 3, option = \u0026quot;F\u0026quot; )[2] color.high \u0026lt;- viridis::viridis( 3, option = \u0026quot;F\u0026quot; )[1] #plot of climate_bio1_average p1 \u0026lt;- ggplot2::ggplot() + ggplot2::geom_sf( data = world, fill = \u0026quot;white\u0026quot; ) + ggplot2::geom_point( data = local.importance, ggplot2::aes( x = x, y = y, color = climate_bio1_average ) ) + ggplot2::scale_x_continuous(limits = c(-170, -30)) + ggplot2::scale_y_continuous(limits = c(-58, 80)) + ggplot2::scale_color_gradient2( low = color.low, high = color.high ) + ggplot2::theme_bw() + ggplot2::theme(legend.position = \u0026quot;bottom\u0026quot;) + ggplot2::ggtitle(\u0026quot;climate_bio1_average\u0026quot;) + ggplot2::theme( plot.title = ggplot2::element_text(hjust = 0.5), legend.key.width = ggplot2::unit(1,\u0026quot;cm\u0026quot;) ) + ggplot2::labs(color = \u0026quot;Importance\u0026quot;) + ggplot2::xlab(\u0026quot;Longitude\u0026quot;) + ggplot2::ylab(\u0026quot;Latitude\u0026quot;) p2 \u0026lt;- ggplot2::ggplot() + ggplot2::geom_sf( data = world, fill = \u0026quot;white\u0026quot; ) + ggplot2::geom_point( data = local.importance, ggplot2::aes( x = x, y = y, color = human_population ) ) + ggplot2::scale_x_continuous(limits = c(-170, -30)) + ggplot2::scale_y_continuous(limits = c(-58, 80)) + ggplot2::scale_color_gradient2( low = color.low, high = color.high ) + ggplot2::theme_bw() + ggplot2::theme(legend.position = \u0026quot;bottom\u0026quot;) + ggplot2::ggtitle(\u0026quot;human_population\u0026quot;) + ggplot2::theme( plot.title = ggplot2::element_text(hjust = 0.5), legend.key.width = ggplot2::unit(1,\u0026quot;cm\u0026quot;) ) + ggplot2::labs(color = \u0026quot;Importance\u0026quot;) + ggplot2::xlab(\u0026quot;Longitude\u0026quot;) + ggplot2::ylab(\u0026quot;Latitude\u0026quot;) p1 + p2 In these maps, values lower than 0 indicate that for a given record, the permuted version of the variable led to an accuracy score even higher than the one of the non-permuted variable, so again these negative values can be interpreted as \u0026ldquo;worse than chance\u0026rdquo;.\nResponse curves and surfaces The variable importance scores are also used by the function plot_response_curves() to plot partial dependence curves for the predictors (by default, only the ones with an importance score above the median). Building the partial dependency curve of a predictor requires setting the other predictors to their quantiles (0.1, 0.5, and 0.9 by default). This helps to understand how the response curve of a variable changes when all the other variables have low, centered, or high values. The function also allows to see the training data\nspatialRF::plot_response_curves( model.non.spatial, quantiles = c(0.1, 0.5, 0.9), line.color = viridis::viridis( 3, #same number of colors as quantiles option = \u0026quot;F\u0026quot;, end = 0.9 ), ncol = 3, show.data = TRUE ) Setting the argument quantiles to 0.5 and setting show.data to FALSE (default optioin) accentuates the shape of the response curves.\nspatialRF::plot_response_curves( model.non.spatial, quantiles = 0.5, ncol = 3 ) The package pdp provides a general way to plot partial dependence plots.\npdp::partial( model.non.spatial, train = plant_richness_df, pred.var = \u0026quot;climate_bio1_average\u0026quot;, plot = TRUE, grid.resolution = 200 ) If you need to do your own plots in a different way, the function get_response_curves() returns a data frame with the required data.\nreponse.curves.df \u0026lt;- spatialRF::get_response_curves(model.non.spatial) kableExtra::kbl( head(reponse.curves.df, n = 10), format = \u0026quot;html\u0026quot; ) %\u0026gt;% kableExtra::kable_paper(\u0026quot;hover\u0026quot;, full_width = F) response predictor quantile model predictor.name response.name 3081.941 -4.428994 0.1 1 climate_bio1_average..pca..human_population_density richness_species_vascular 3081.941 -4.393562 0.1 1 climate_bio1_average..pca..human_population_density richness_species_vascular 3081.941 -4.358129 0.1 1 climate_bio1_average..pca..human_population_density richness_species_vascular 3081.941 -4.322697 0.1 1 climate_bio1_average..pca..human_population_density richness_species_vascular 3081.941 -4.287265 0.1 1 climate_bio1_average..pca..human_population_density richness_species_vascular 3081.941 -4.251833 0.1 1 climate_bio1_average..pca..human_population_density richness_species_vascular 3081.941 -4.216400 0.1 1 climate_bio1_average..pca..human_population_density richness_species_vascular 3081.941 -4.180968 0.1 1 climate_bio1_average..pca..human_population_density richness_species_vascular 3081.941 -4.145536 0.1 1 climate_bio1_average..pca..human_population_density richness_species_vascular 3081.941 -4.110104 0.1 1 climate_bio1_average..pca..human_population_density richness_species_vascular Interactions between two variables can be plotted with plot_response_surface()\nspatialRF::plot_response_surface( model.non.spatial, a = \u0026quot;climate_bio1_average\u0026quot;, b = \u0026quot;neighbors_count\u0026quot; ) This can be done as well with the pdp package, that uses a slightly different algorithm to plot interaction surfaces.\npdp::partial( model.non.spatial, train = plant_richness_df, pred.var = c(\u0026quot;climate_bio1_average\u0026quot;, \u0026quot;neighbors_count\u0026quot;), plot = TRUE ) Model performance The performance slot (in model.non.spatial$performance) contains the values of several performance measures. It be printed via the function print_performance().\nspatialRF::print_performance(model.non.spatial) ## ## Model performance ## - R squared (oob): 0.6075314 ## - R squared (cor(obs, pred)^2): 0.9543769 ## - Pseudo R squared (cor(obs, pred)):0.9769221 ## - RMSE (oob): 2111.241 ## - RMSE: 902.1424 ## - Normalized RMSE: 0.2604337 R squared (oob) and RMSE (oob) are the R squared of the model and its root mean squared error when predicting the out-of-bag data (fraction of data not used to train individual trees). From all the values available in the performance slot, probably these the most honest ones, as it is the closer trying to get a performance estimate on independent data. However, out-of-bag data is not fully independent, and therefore will still be inflated, especially if the data is highly aggregated in space. R squared and pseudo R squared are computed from the observations and the predictions, and indicate to what extent model outcomes represent the input data. These values will usually be high the data is highly aggregated in space. The RMSE and its normalized version are computed via root_mean_squared_error(), and are linear with R squared and pseudo R squared. Spatial cross-validation The function rf_evaluate() overcomes the limitations of the performance scores explained above by providing honest performance based on spatial cross-validation. The function separates the data into a number of spatially independent training and testing folds. Then, it fits a model on each training fold, predicts over each testing fold, and computes statistics of performance measures across folds. Let\u0026rsquo;s see how it works.\nmodel.non.spatial \u0026lt;- spatialRF::rf_evaluate( model = model.non.spatial, xy = xy, #data coordinates repetitions = 30, #number of spatial folds training.fraction = 0.75, #training data fraction on each fold metrics = \u0026quot;r.squared\u0026quot;, seed = random.seed, verbose = FALSE ) The function generates a new slot in the model named evaluation (model.non.spatial$evaluation) with several objects that summarize the spatial cross-validation results.\nnames(model.non.spatial$evaluation) ## [1] \u0026quot;metrics\u0026quot; \u0026quot;training.fraction\u0026quot; \u0026quot;spatial.folds\u0026quot; ## [4] \u0026quot;per.fold\u0026quot; \u0026quot;per.fold.long\u0026quot; \u0026quot;per.model\u0026quot; ## [7] \u0026quot;aggregated\u0026quot; The slot \u0026ldquo;spatial.folds\u0026rdquo;, produced by make_spatial_folds(), contains the indices of the training and testing cases for each cross-validation repetition. The maps below show two sets of training and testing folds.\npr \u0026lt;- plant_richness_df[, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)] pr$group.2 \u0026lt;- pr$group.1 \u0026lt;- \u0026quot;Training\u0026quot; pr[model.non.spatial$evaluation$spatial.folds[[1]]$testing, \u0026quot;group.1\u0026quot;] \u0026lt;- \u0026quot;Testing\u0026quot; pr[model.non.spatial$evaluation$spatial.folds[[25]]$testing, \u0026quot;group.2\u0026quot;] \u0026lt;- \u0026quot;Testing\u0026quot; p1 \u0026lt;- ggplot2::ggplot() + ggplot2::geom_sf(data = world, fill = \u0026quot;white\u0026quot;) + ggplot2::geom_point(data = pr, ggplot2::aes( x = x, y = y, color = group.1 ), size = 2 ) + ggplot2::scale_color_viridis_d( direction = -1, end = 0.5, alpha = 0.8, option = \u0026quot;F\u0026quot; ) + ggplot2::theme_bw() + ggplot2::labs(color = \u0026quot;Group\u0026quot;) + ggplot2::scale_x_continuous(limits = c(-170, -30)) + ggplot2::scale_y_continuous(limits = c(-58, 80)) + ggplot2::ggtitle(\u0026quot;Spatial fold 1\u0026quot;) + ggplot2::theme( legend.position = \u0026quot;none\u0026quot;, plot.title = ggplot2::element_text(hjust = 0.5) ) + ggplot2::xlab(\u0026quot;Longitude\u0026quot;) + ggplot2::ylab(\u0026quot;Latitude\u0026quot;) p2 \u0026lt;- ggplot2::ggplot() + ggplot2::geom_sf(data = world, fill = \u0026quot;white\u0026quot;) + ggplot2::geom_point(data = pr, ggplot2::aes( x = x, y = y, color = group.2 ), size = 2 ) + ggplot2::scale_color_viridis_d( direction = -1, end = 0.5, alpha = 0.8, option = \u0026quot;F\u0026quot; ) + ggplot2::theme_bw() + ggplot2::labs(color = \u0026quot;Group\u0026quot;) + ggplot2::scale_x_continuous(limits = c(-170, -30)) + ggplot2::scale_y_continuous(limits = c(-58, 80)) + ggplot2::theme( plot.title = ggplot2::element_text(hjust = 0.5) ) + ggplot2::ggtitle(\u0026quot;Spatial fold 25\u0026quot;) + ggplot2::xlab(\u0026quot;Longitude\u0026quot;) + ggplot2::ylab(\u0026quot;\u0026quot;) p1 | p2 The information available in this new slot can be accessed with the functions print_evaluation(), plot_evaluation(), and get_evaluation().\nspatialRF::plot_evaluation(model.non.spatial) Full represents the R squared of the model trained on the full dataset. Training are the R-squared of the models fitted on the spatial folds (named Training in the maps above), and Testing are the R-squared of the same models on \u0026ldquo;unseen\u0026rdquo; data (data not used to train the model, named Testing in the maps above). The median, median absolute deviation (MAD), minimum, and maximum R-squared values on the testing folds can be printed with print_evaluation().\nspatialRF::print_evaluation(model.non.spatial) ## ## Spatial evaluation ## - Training fraction: 0.75 ## - Spatial folds: 29 ## ## Metric Median MAD Minimum Maximum ## r.squared 0.517 0.085 0.122 0.781 Other important things stored in the model The model predictions are stored in the slot predictions, the arguments used to fit the model in ranger.arguments, and the model itself, used to predict new values (see code chunk below), is in the forest slot.\npredicted \u0026lt;- stats::predict( object = model.non.spatial, data = plant_richness_df, type = \u0026quot;response\u0026quot; )$predictions Fitting a spatial model with rf_spatial() The spatial autocorrelation of the residuals of a model like model.non.spatial, measured with Moran\u0026rsquo;s I, can be plotted with plot_moran().\nspatialRF::plot_moran( model.non.spatial, verbose = FALSE ) According to the plot, the spatial autocorrelation of the residuals of `model.non.spatial` is highly positive for a neighborhood of 0 and 1000 km, while it becomes non-significant (p-value \\\u003e 0.05) at 2000, 4000, and 8000 km. To reduce the spatial autocorrelation of the residuals as much as possible, the non-spatial model can be transformed into a *spatial model* very easily with the function [`rf_spatial()`](https://blasbenito.github.io/spatialRF/reference/rf_spatial.html). This function is the true core of the package! model.spatial \u0026lt;- spatialRF::rf_spatial( model = model.non.spatial, method = \u0026quot;mem.moran.sequential\u0026quot;, #default method verbose = FALSE, seed = random.seed ) The plot below shows the Moran\u0026rsquo;s I of the residuals of the spatial model, and indicates that the residuals are not autocorrelated at any distance.\nspatialRF::plot_moran( model.spatial, verbose = FALSE ) If we compare the variable importance plots of both models, we can see that the spatial model has an additional set of dots under the name \u0026ldquo;spatial_predictors\u0026rdquo;, and that the maximum importance of a few of these spatial predictors matches the importance of the most relevant non-spatial predictors.\np1 \u0026lt;- spatialRF::plot_importance( model.non.spatial, verbose = FALSE) + ggplot2::ggtitle(\u0026quot;Non-spatial model\u0026quot;) p2 \u0026lt;- spatialRF::plot_importance( model.spatial, verbose = FALSE) + ggplot2::ggtitle(\u0026quot;Spatial model\u0026quot;) p1 | p2 If we look at the ten most important variables in model.spatial we will see that a few of them are spatial predictors. Spatial predictors are named spatial_predictor_X_Y, where X is the neighborhood distance at which the predictor has been generated, and Y is the index of the predictor.\nkableExtra::kbl( head(model.spatial$importance$per.variable, n = 10), format = \u0026quot;html\u0026quot; ) %\u0026gt;% kableExtra::kable_paper(\u0026quot;hover\u0026quot;, full_width = F) variable importance climate_bio1_average..pca..neighbors_count 1094.792 climate_hypervolume 1067.111 spatial_predictor_0_2 1035.365 human_population 997.965 climate_bio1_average 973.243 bias_area_km2..pca..neighbors_percent_shared_edge 902.209 climate_bio1_average..pca..human_population_density 882.910 spatial_predictor_0_1 815.656 bias_area_km2 750.278 bias_species_per_record 712.031 But what are spatial predictors? Spatial predictors, as shown below, are smooth surfaces representing neighborhood among records at different spatial scales. They are computed from the distance matrix in different ways. The ones below are the eigenvectors of the double-centered distance matrix of weights (a.k.a, Moran\u0026rsquo;s Eigenvector Maps). They represent the effect of spatial proximity among records, helping to represent biogeographic and spatial processes not considered by the non-spatial predictors.\nspatial.predictors \u0026lt;- spatialRF::get_spatial_predictors(model.spatial) pr \u0026lt;- data.frame(spatial.predictors, plant_richness_df[, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)]) p1 \u0026lt;- ggplot2::ggplot() + ggplot2::geom_sf(data = world, fill = \u0026quot;white\u0026quot;) + ggplot2::geom_point( data = pr, ggplot2::aes( x = x, y = y, color = spatial_predictor_0_2 ), size = 2.5 ) + ggplot2::scale_color_viridis_c(option = \u0026quot;F\u0026quot;) + ggplot2::theme_bw() + ggplot2::labs(color = \u0026quot;Eigenvalue\u0026quot;) + ggplot2::scale_x_continuous(limits = c(-170, -30)) + ggplot2::scale_y_continuous(limits = c(-58, 80)) + ggplot2::ggtitle(\u0026quot;Variable: spatial_predictor_0_2\u0026quot;) + ggplot2::theme(legend.position = \u0026quot;bottom\u0026quot;)+ ggplot2::xlab(\u0026quot;Longitude\u0026quot;) + ggplot2::ylab(\u0026quot;Latitude\u0026quot;) p2 \u0026lt;- ggplot2::ggplot() + ggplot2::geom_sf(data = world, fill = \u0026quot;white\u0026quot;) + ggplot2::geom_point( data = pr, ggplot2::aes( x = x, y = y, color = spatial_predictor_0_5, ), size = 2.5 ) + ggplot2::scale_color_viridis_c(option = \u0026quot;F\u0026quot;) + ggplot2::theme_bw() + ggplot2::labs(color = \u0026quot;Eigenvalue\u0026quot;) + ggplot2::scale_x_continuous(limits = c(-170, -30)) + ggplot2::scale_y_continuous(limits = c(-58, 80)) + ggplot2::ggtitle(\u0026quot;Variable: spatial_predictor_0_5\u0026quot;) + ggplot2::theme(legend.position = \u0026quot;bottom\u0026quot;) + ggplot2::xlab(\u0026quot;Longitude\u0026quot;) + ggplot2::ylab(\u0026quot;\u0026quot;) p1 | p2 The spatial predictors are included in the model one by one, in the order of their Moran\u0026rsquo;s I (spatial predictors with Moran\u0026rsquo;s I lower than 0 are removed). The selection procedure is performed by the function select_spatial_predictors_sequential(), which finds the smaller subset of spatial predictors maximizing the model\u0026rsquo;s R squared, and minimizing the Moran\u0026rsquo;s I of the residuals. This is shown in the optimization plot below (dots linked by lines represent the selected spatial predictors).\np \u0026lt;- spatialRF::plot_optimization(model.spatial) Tuning Random Forest hyperparameters The model fitted above was based on the default random forest hyperparameters of ranger(), and those might not be the most adequate ones for a given dataset. The function rf_tuning() helps the user to choose sensible values for three Random Forest hyperparameters that are critical to model performance:\nnum.trees: number of regression trees in the forest. mtry: number of variables to choose from on each tree split. min.node.size: minimum number of cases on a terminal node. These values can be modified in any model fitted with the package using the ranger.arguments argument. The example below shows how to fit a spatial model with a given set of hyperparameters.\nmodel.spatial \u0026lt;- spatialRF::rf_spatial( model = model.non.spatial, method = \u0026quot;mem.moran.sequential\u0026quot;, #default method ranger.arguments = list( mtry = 5, min.node.size = 20, num.trees = 1000 ), verbose = FALSE, seed = random.seed ) The usual method for model tuning relies on a grid search exploring the results of all the combinations of hyperparameters selected by the user. In spatialRF, model tuning is done via spatial cross-validation, to ensure that the selected combination of hyperparameters maximizes the ability of the model to predict over data not used to train it. Warning: model tuning consumes a lot of computational resources, using it on large datasets might freeze your computer.\nWARNING: model tunning is very RAM-hungry, but you can control RAM usage by defining a lower value for the argument n.cores.\nmodel.spatial \u0026lt;- rf_tuning( model = model.spatial, xy = xy, repetitions = 30, num.trees = c(500, 1000), mtry = seq( 2, length(model.spatial$ranger.arguments$predictor.variable.names), #number of predictors by = 9), min.node.size = c(5, 15), seed = random.seed, verbose = FALSE ) The function returns a tuned model only if the tuning finds a solution better than the original model. Otherwise the original model is returned. The results of the tuning are stored in the model under the name \u0026ldquo;tuning\u0026rdquo;.\nRepeating a model execution Random Forest is an stochastic algorithm that yields slightly different results on each run unless a random seed is set. This particularity has implications for the interpretation of variable importance scores and response curves. The function rf_repeat() repeats a model execution and yields the distribution of importance scores of the predictors across executions. NOTE: this function works better when used at the end of a workflow\nmodel.spatial.repeat \u0026lt;- spatialRF::rf_repeat( model = model.spatial, repetitions = 30, seed = random.seed, verbose = FALSE ) The importance scores of a model fitted with rf_repeat() are plotted as a violin plot, with the distribution of the importance scores of each predictor across repetitions.\nspatialRF::plot_importance( model.spatial.repeat, verbose = FALSE ) The response curves of models fitted with rf_repeat() can be plotted with plot_response_curves() as well. The median prediction is shown with a thicker line.\nspatialRF::plot_response_curves( model.spatial.repeat, quantiles = 0.5, ncol = 3 ) The function print_performance() generates a summary of the performance scores across model repetitions. As every other function of the package involving repetitions, the provided stats are the median, and the median absolute deviation (mad).\nspatialRF::print_performance(model.spatial.repeat) ## ## Model performance (median +/- mad) ## - R squared (oob): 0.593 +/- 0.0098 ## - R squared (cor(obs, pred)^2): 0.957 +/- 0.0016 ## - Pseudo R squared: 0.978 +/- 8e-04 ## - RMSE (oob): 2151.154 +/- 25.7818 ## - RMSE: 914.973 +/- 15.5809 ## - Normalized RMSE: 0.264 +/- 0.0045 Taking advantage of the %\u0026gt;% pipe The modeling functions of spatialRF are designed to facilitate using the pipe to combine them. The code below fits a spatial model, tunes its hyperparameters, evaluates it using spatial cross-validation, and repeats the execution several times, just by passing the model from one function to another. Replace eval = FALSE with eval = TRUE if you want to execute the code chunk.\nmodel.full \u0026lt;- rf_spatial( data = plant_richness_df, dependent.variable.name = dependent.variable.name, predictor.variable.names = predictor.variable.names, distance.matrix = distance_matrix, distance.thresholds = distance.thresholds, xy = xy ) %\u0026gt;% rf_tuning() %\u0026gt;% rf_evaluate() %\u0026gt;% rf_repeat() The code structure shown above can also be used to take advantage of a custom cluster, either defined in the local host, or a Beowulf cluster.\nWhen working with a single machine, a cluster can be defined and used as follows:\n#creating and registering the cluster local.cluster \u0026lt;- parallel::makeCluster( parallel::detectCores() - 1, type = \u0026quot;PSOCK\u0026quot; ) doParallel::registerDoParallel(cl = local.cluster) #fitting, tuning, evaluating, and repeating a model model.full \u0026lt;- rf_spatial( data = plant_richness_df, dependent.variable.name = dependent.variable.name, predictor.variable.names = predictor.variable.names, distance.matrix = distance_matrix, distance.thresholds = distance.thresholds, xy = xy, cluster = local.cluster #is passed via pipe to the other functions ) %\u0026gt;% rf_tuning() %\u0026gt;% rf_evaluate() %\u0026gt;% rf_repeat() #stopping the cluster parallel::stopCluster(cl = local.cluster) To facilitate working with Beowulf clusters ( just several computers connected via SSH), the package provides the function beowulf_cluster(), that generates the cluster definition from details such as the IPs of the machines, the number of cores to be used on each machine, the user name, and the connection port.\n#creating and registering the cluster beowulf.cluster \u0026lt;- beowulf_cluster( cluster.ips = c( \u0026quot;10.42.0.1\u0026quot;, \u0026quot;10.42.0.34\u0026quot;, \u0026quot;10.42.0.104\u0026quot; ), cluster.cores = c(7, 4, 4), cluster.user = \u0026quot;blas\u0026quot;, cluster.port = \u0026quot;11000\u0026quot; ) #fitting, tuning, evaluating, and repeating a model model.full \u0026lt;- rf_spatial( data = plant_richness_df, dependent.variable.name = dependent.variable.name, predictor.variable.names = predictor.variable.names, distance.matrix = distance_matrix, distance.thresholds = distance.thresholds, xy = xy, cluster = beowulf.cluster ) %\u0026gt;% rf_tuning() %\u0026gt;% rf_evaluate() %\u0026gt;% rf_repeat() doParallel::registerDoParallel(cl = beowulf.cluster) Comparing several models The function rf_compare() takes named list with as many models as the user needs to compare, and applies rf_evaluate() to each one of them to compare their predictive performances across spatial folds.\ncomparison \u0026lt;- spatialRF::rf_compare( models = list( `Non-spatial` = model.non.spatial, `Spatial` = model.spatial ), xy = xy, repetitions = 30, training.fraction = 0.8, metrics = \u0026quot;r.squared\u0026quot;, seed = random.seed ) x \u0026lt;- comparison$comparison.df %\u0026gt;% dplyr::group_by(model, metric) %\u0026gt;% dplyr::summarise(value = round(median(value), 3)) %\u0026gt;% dplyr::arrange(metric) %\u0026gt;% as.data.frame() colnames(x) \u0026lt;- c(\u0026quot;Model\u0026quot;, \u0026quot;Metric\u0026quot;, \u0026quot;Median\u0026quot;) kableExtra::kbl( x, format = \u0026quot;html\u0026quot; ) %\u0026gt;% kableExtra::kable_paper(\u0026quot;hover\u0026quot;, full_width = F) Model Metric Median Non-spatial r.squared 0.494 Spatial r.squared 0.305 Working with a binomial response This package can also perform binomial regression on response variables with zeros and ones. Let\u0026rsquo;s work on a quick example by turning the response variable of the previous models into a binomial one.\nplant_richness_df$response_binomial \u0026lt;- ifelse( plant_richness_df$richness_species_vascular \u0026gt; 5000, 1, 0 ) The new response variable, response_binomial, will have ones where richness_species_vascular \u0026gt; 5000, and zeros otherwise. This would be equivalent to having the classes \u0026ldquo;high richness\u0026rdquo; (represented by the ones) and \u0026ldquo;low richness\u0026rdquo;, represented by the zeros. The binomial regression model would then have as objective to compute the probability of each ecoregion to belong to the \u0026ldquo;high richness\u0026rdquo; class.\nThere is something important to notice before moving forward though. The number of zeros in the new response variable is larger than the number of ones.\ntable(plant_richness_df$response_binomial) ## ## 0 1 ## 165 62 This means that there is class imbalance, and under this scenario, any random forest model is going to get better at predicting the most abundant class, while in our case the \u0026ldquo;target\u0026rdquo; is the less abundant one. But the function rf() is ready to deal with this issue. Let\u0026rsquo;s fit a model to see what am I talking about.\nmodel.non.spatial \u0026lt;- spatialRF::rf( data = plant_richness_df, dependent.variable.name = \u0026quot;response_binomial\u0026quot;, predictor.variable.names = predictor.variable.names, distance.matrix = distance.matrix, distance.thresholds = distance.thresholds, seed = random.seed, verbose = FALSE ) The function detects that the response variable is binary (using the function is_binary()), and computes case weights for the ones and the zeros. These case weights are stored in the ranger.arguments slot of the model, and are used to give preference to the cases with larger weights during the selection of the out-of-bag data (check the case.weights argument in ranger::ranger()). As a result, each individual tree in the forest is trained with a similar proportion of zeros and ones, which helps mitigate the class imbalance issue. This method is named weighted Random Forest, and is very well explained in this white paper that includes the father of Random Forest, Leo Breiman, as coauthor.\nunique(model.non.spatial$ranger.arguments$case.weights) ## [1] 0.006060606 0.016129032 This model could be projected right away onto a raster stack with maps of the predictors, so, in fact, spatialRF can be used to fit Species Distribution Models, when it actually wasn\u0026rsquo;t really designed with such a purpose in mind. And as an additional advantage, the model can be evaluated with rf_evaluate(), which is way better than cross-validation via random data-splitting ( this blog post explains explains why).\nmodel.non.spatial \u0026lt;- spatialRF::rf_evaluate( model.non.spatial, xy = xy, metrics = \u0026quot;auc\u0026quot;, verbose = FALSE ) spatialRF::print_evaluation(model.non.spatial) ## ## Spatial evaluation ## - Training fraction: 0.75 ## - Spatial folds: 29 ## ## Metric Median MAD Minimum Maximum ## auc 0.932 0.024 0.83 0.977 The take away message here is that you can work with a binomial response with spatialRF, just as you would do with a continuous response, as long as it is represented with zeros and ones. Just remember that the class imbalance problem is tackled via case weights, and that predictive performance is also measured using the Area Under the ROC Curve (AUC).\nGenerating spatial predictors for other modelling methods You might not love Random Forest, but spatialRF loves you, and as such, it gives you tools to generate spatial predictors for other models anyway.\nThe first step requires generating Moran\u0026rsquo;s Eigenvector Maps (MEMs) from the distance matrix. Here there are two options, computing MEMs for a single neighborhood distance with mem(), and computing MEMs for several neighborhood distances at once with mem_multithreshold().\n#single distance (0km by default) mems \u0026lt;- spatialRF::mem(distance.matrix = distance_matrix) #several distances mems \u0026lt;- spatialRF::mem_multithreshold( distance.matrix = distance.matrix, distance.thresholds = distance.thresholds ) In either case the result is a data frame with Moran\u0026rsquo;s Eigenvector Maps (\u0026ldquo;just\u0026rdquo; the positive eigenvectors of the double-centered distance matrix).\nkableExtra::kbl( head(mems[, 1:4], n = 10), format = \u0026quot;html\u0026quot; ) %\u0026gt;% kableExtra::kable_paper(\u0026quot;hover\u0026quot;, full_width = F) spatial_predictor_0_1 spatial_predictor_0_2 spatial_predictor_0_3 spatial_predictor_0_4 -0.0259217 0.0052203 -0.0416969 -0.0363324 -0.0996679 0.0539713 -0.1324480 0.3826928 -0.0010477 -0.0143046 0.0443602 -0.0031386 -0.0165695 0.0047991 -0.0307457 0.0005170 -0.0225761 0.0019595 -0.0230368 -0.0524239 -0.0155252 0.0023742 -0.0197953 -0.0338956 -0.0229197 0.0039860 -0.0312561 -0.0416697 0.2436009 -0.1155295 -0.0791452 0.0189996 -0.0150725 -0.0158684 0.1010284 0.0095590 0.1187381 -0.0471879 -0.0359881 0.0065211 But not all MEMs are made equal, and you will need to rank them by their Moran\u0026rsquo;s I. The function rank_spatial_predictors() will help you do so.\nmem.rank \u0026lt;- spatialRF::rank_spatial_predictors( distance.matrix = distance_matrix, spatial.predictors.df = mems, ranking.method = \u0026quot;moran\u0026quot; ) The output of rank_spatial_predictors() is a list with three slots: \u0026ldquo;method\u0026rdquo;, a character string with the name of the ranking method; \u0026ldquo;criteria\u0026rdquo;, an ordered data frame with the criteria used to rank the spatial predictors; and \u0026ldquo;ranking\u0026rdquo;, a character vector with the names of the spatial predictors in the order of their ranking (it is just the first column of the \u0026ldquo;criteria\u0026rdquo; data frame). We can use this \u0026ldquo;ranking\u0026rdquo; object to reorder or mems data frame.\nmems \u0026lt;- mems[, mem.rank$ranking] #also: #mems \u0026lt;- mem.rank$spatial.predictors.df From here, spatial predictors can be included in any model one by one, in the order of the ranking, until the spatial autocorrelation of the residuals becomes neutral, if possible. A little example with a linear model follows.\n#model definition predictors \u0026lt;- c( \u0026quot;climate_aridity_index_average \u0026quot;, \u0026quot;climate_bio1_average\u0026quot;, \u0026quot;bias_species_per_record\u0026quot;, \u0026quot;human_population_density\u0026quot;, \u0026quot;topography_elevation_average\u0026quot;, \u0026quot;fragmentation_division\u0026quot; ) model.formula \u0026lt;- as.formula( paste( dependent.variable.name, \u0026quot; ~ \u0026quot;, paste( predictors, collapse = \u0026quot; + \u0026quot; ) ) ) #scaling the data model.data \u0026lt;- scale(plant_richness_df) %\u0026gt;% as.data.frame() #fitting the model m \u0026lt;- lm(model.formula, data = model.data) #Moran's I test of the residuals moran.test \u0026lt;- spatialRF::moran( x = residuals(m), distance.matrix = distance_matrix, verbose = FALSE ) moran.test$plot According to the Moran\u0026rsquo;s I test, the model residuals show spatial autocorrelation. Let\u0026rsquo;s introduce MEMs one by one until the problem is solved.\n#add mems to the data and applies scale() model.data \u0026lt;- data.frame( plant_richness_df, mems ) %\u0026gt;% scale() %\u0026gt;% as.data.frame() #initialize predictors.i predictors.i \u0026lt;- predictors #iterating through MEMs for(mem.i in colnames(mems)){ #add mem name to model definintion predictors.i \u0026lt;- c(predictors.i, mem.i) #generate model formula with the new spatial predictor model.formula.i \u0026lt;- as.formula( paste( dependent.variable.name, \u0026quot; ~ \u0026quot;, paste( predictors.i, collapse = \u0026quot; + \u0026quot; ) ) ) #fit model m.i \u0026lt;- lm(model.formula.i, data = model.data) #Moran's I test moran.test.i \u0026lt;- moran( x = residuals(m.i), distance.matrix = distance_matrix, verbose = FALSE ) #stop if no autocorrelation if(moran.test.i$test$interpretation == \u0026quot;No spatial correlation\u0026quot;){ break } }#end of loop #last moran test moran.test.i$plot Now we can compare the model without spatial predictors m and the model with spatial predictors m.i.\ncomparison.df \u0026lt;- data.frame( Model = c(\u0026quot;Non-spatial\u0026quot;, \u0026quot;Spatial\u0026quot;), Predictors = c(length(predictors), length(predictors.i)), R_squared = round(c(summary(m)$r.squared, summary(m.i)$r.squared), 2), AIC = round(c(AIC(m), AIC(m.i)), 0), BIC = round(c(BIC(m), BIC(m.i)), 0), `Moran I` = round(c(moran.test$test$moran.i, moran.test.i$test$moran.i), 2) ) kableExtra::kbl( comparison.df, format = \u0026quot;html\u0026quot; ) %\u0026gt;% kableExtra::kable_paper(\u0026quot;hover\u0026quot;, full_width = F) Model Predictors R_squared AIC BIC Moran.I Non-spatial 6 0.38 551 578 0.21 Spatial 22 0.50 533 615 0.06 According to the model comparison, it can be concluded that the addition of spatial predictors, in spite of the increase in complexity, has improved the model. In any case, this is just a simple demonstration of how spatial predictors generated with functions of the spatialRF package can still help you fit spatial models with other modeling methods.\n","date":1695686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695708863,"objectID":"9a938021864d1400cb7c9e337b52d3aa","permalink":"https://blasbenito.com/project/post-title/","publishdate":"2023-09-26T00:00:00Z","relpermalink":"/project/post-title/","section":"project","summary":"R package for spatial regression with Random Forest","tags":["R packages","Spatial Regression","Machine Learning","Random Forest"],"title":"R package spatialRF","type":"project"},{"authors":["Alice Monnier-Corbel","Alexandre Robert","Yves Hingrat","Blas M. Benito","Anne-Christine Monnet","Yves Hingrat"],"categories":null,"content":"","date":1687564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687564800,"objectID":"8975a0742cd533387a65c83f36f48fd6","permalink":"https://blasbenito.com/publication/2023_monnier_global_ecology_and_conservation/","publishdate":"2023-06-24T00:00:00Z","relpermalink":"/publication/2023_monnier_global_ecology_and_conservation/","section":"publication","summary":"Habitat Suitability Index (HSI) derived from Species Distribution Model (SDM) has been used to infer or predict local demographic properties such as abundance for many species. Across species studied, HSI has either been presented as a poor predictor of abundance or as a predictor of potential rather than realized abundance. The main explanation of the lack of relationship between HSI and abundance is that the local abundance of a species varies in time due to various ecological processes that are not integrated into correlative SDM. To better understand the HSI-abundance relationship, in addition to the study of the association between HSI and mean abundance, we explored its variation over time. We used data from 10-years monitoring of a Houbara bustard (Chlamydotis undulata undulata) population in Morocco. From various occurrence data we modelled the HSI. From (independent) count data we calculated four local abundance indices: mean abundance, maximum abundance, the temporal trend of abundance and the coefficient of variation of abundance over the study period. We explored the relationship between HSI and abundance indices using linear, polynomial and quantile regressions. We found a triangular relationship between local abundance (mean and maximum) and HSI, indicating that the upper limit of mean and maximum abundance increased with HSI. Our results also indicate that sites with the highest HSI were associated with least variation in local abundance, the highest variation being observed at intermediate HSI. Our results provide new empirical evidence supporting the generalization of the triangular relationship between HSI and abundance. Overall, our results support the hypothesis that HSI obtained from SDMs can reflect the local abundance potentialities of a species and emphasize the importance of investigating this relationship using temporal variation in abundance.","tags":["Population Dynamics","Quantile Regression","Habitat Suitability","Species Distribution Models","Random Forest","spatialRF"],"title":"Species Distribution Models predict abundance and its temporal variation in a steppe bird population.","type":"publication"},{"authors":["Laura Martín García","Nereida M. Rancel-Rodríguez","Carlos Sangil","Javier Reyes","Blas M. Benito","Sharay Orellana","Marta Sansón"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"8fb6bfd1a2bf4c1ca36096f8a900dfd9","permalink":"https://blasbenito.com/publication/2022_martin_marine_environmental_research/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/2022_martin_marine_environmental_research/","section":"publication","summary":"Large brown macroalgae are foundational threatened species in coastal ecosystems from the subtropical northeastern Atlantic, where they have exhibited a drastic decline in recent years. This study describes the potential habitat of Gongolaria abies-marina, its current distribution and conservation status, and the major drivers of population decline. The results show a strong reduction of more than 97% of G. abies-marina populations in the last thirty years and highlight the effects of drivers vary in terms of spatial heterogeneity. A decrease in the frequency of high waves and high human footprint are the principal factors accounting for the long-term decline in G. abies-marina populations. UV radiation and sea surface temperature have an important correlation only in certain locations. Both the methodology and the large amount of data analyzed in this study provide a valuable tool for the conservation and restoration of threatened macroalgae.","tags":["Conservation","Extinction","Population analysis","Species Distribution Models"],"title":"Environmental and human factors drive the subtropical marine forests of Gongolaria abies-marina to extinction.","type":"publication"},{"authors":["Xavier Benito","Blas M. Benito","Maria I. Vélez","Jorge Salgado","Tobias Schneider","Liviu Giosan","Majoi N. Nascimento"],"categories":null,"content":"","date":1655683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655683200,"objectID":"d9915d6854d321118a373f39685f4dc6","permalink":"https://blasbenito.com/publication/2022_benito_stoten/","publishdate":"2022-06-20T00:00:00Z","relpermalink":"/publication/2022_benito_stoten/","section":"publication","summary":"Anthropogenic climate change and landscape alteration are two of the most important threats to the terrestrial and aquatic ecosystems of the tropical Americas, thus jeopardizing water and soil resources for millions of people in the Andean nations. Understanding how aquatic ecosystems will respond to anthropogenic stressors and accelerated warming requires shifting from short-term and static to long-term, dynamic characterizations of human-terrestrial-aquatic relationships. Here we use sediment records from Lake Llaviucu, a tropical mountain Andean lake long accessed by Indigenous and post-European societies, and hypothesize that under natural historical conditions (i.e., low human pressure) vegetation and aquatic ecosystems' responses to change are coupled through indirect climate influences—that is, past climate-driven vegetation changes dictated limnological trajectories. We used a multi-proxy paleoecological approach including drivers of terrestrial vegetation change (pollen), soil erosion (Titanium), human activity (agropastoralism indicators), and aquatic responses (diatoms) to estimate assemblage-wide rates of change and model their synchronous and asynchronous (lagged) relationships using Generalized Additive Models. Assemblage-wide rate of change results showed that between ca. 3000 and 400 calibrated years before present (cal years BP) terrestrial vegetation, agropastoralism and diatoms fluctuated along their mean regimes of rate of change without consistent periods of synchronous rapid change. In contrast, positive lagged relationships (i.e., asynchrony) between climate-driven terrestrial pollen changes and diatom responses (i.e., asynchrony) were in operation until ca. 750 cal years BP. Thereafter, positive lagged relationships between agropastoralism and diatom rates of changes dictated the lake trajectory, reflecting the primary control of human practices over the aquatic ecosystem prior European occupation. We interpret that shifts in Indigenous practices (e.g., valley terracing) curtailed nutrient inputs into the lake decoupling the links between climate-driven vegetation changes and the aquatic community. Our results demonstrate how rates of change of anthropogenic and climatic influences can guide dynamic ecological baselines for managing water ecosystem services in the Andes.","tags":["Time-series analysis","Palaeoecology","Generalized Additive Models"],"title":"Human practices behind the aquatic and terrestrial ecological decoupling to climate change in the tropical Andes.","type":"publication"},{"authors":["Alice Monnier-Corbel","Anne-Christine Monnet","Léo Bacon","Blas M. Benito","Alexandre Robert","Yves Hingrat"],"categories":null,"content":"","date":1645660800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645660800,"objectID":"1d308c10d849cd36c160ef2d28d00467","permalink":"https://blasbenito.com/publication/2022_monnier_global_ecology_and_conservation/","publishdate":"2022-02-24T00:00:00Z","relpermalink":"/publication/2022_monnier_global_ecology_and_conservation/","section":"publication","summary":"Although density-dependent processes and their impacts on population dynamics are key issues in ecology and conservation biology, empirical evidence of density-dependence remains scarce for species or populations with low densities, scattered distributions, and especially for managed populations where densities may vary as a result of extrinsic factors (such as harvesting or releases). Here, we explore the presence of density-dependent processes in a reinforced population of North African Houbara bustard (Chlamydotis undulata undulata). We investigated the relationship between reproductive success and local density, and the possible variation of this relationship according to habitat suitability using three independent datasets. Based on eight years of nests monitoring (more than 7000 nests), we modeled the Daily Nest Survival Rate (DNSR) as a proxy of reproductive success. Our results indicate that DNSR was negatively impacted by local densities and that this relationship was approximately constant in space and time: (1) although DNSR strongly decreased over the breeding season, the negative relationship between DNSR and density remained constant over the breeding season; (2) this density-dependent relationship did not vary with the quality of the habitat associated with the nest location. Previous studies have shown that the demographic parameters and population dynamics of the reinforced North African Houbara bustard are strongly influenced by extrinsic environmental and management parameters. Our study further indicates the existence of density-dependent regulation in a low-density, managed population.","tags":["Population Ecology","Population Density","Random Forest","spatialRF","Biogeography","Conservation","Species Distribution Models"],"title":"Density-dependence of reproductive success in a Houbara bustard population.","type":"publication"},{"authors":["Joaquín Moreno","Sergio Asensio","Miguel Berdugo","Beatriz Gozalo","Victoria Ochoa","David S. Pescador","Blas M. Benito","Fernando T. Maestre"],"categories":null,"content":"","date":1642636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642636800,"objectID":"9333bad8b78eb5a131e7e8b91d1598c1","permalink":"https://blasbenito.com/publication/2022_moreno_scientific_data/","publishdate":"2022-01-20T00:00:00Z","relpermalink":"/publication/2022_moreno_scientific_data/","section":"publication","summary":"Drylands cover ~41% of the terrestrial surface. In these water-limited ecosystems, soil moisture contributes to multiple hydrological processes and is a crucial determinant of the activity and performance of above- and belowground organisms and of the ecosystem processes that rely on them. Thus, an accurate characterisation of the temporal dynamics of soil moisture is critical to improve our understanding of how dryland ecosystems function and are responding to ongoing climate change. Furthermore, it may help improve climatic forecasts and drought monitoring. Here we present the MOISCRUST dataset, a long-term (2006–2020) soil moisture dataset at a sub-daily resolution from five different microsites (vascular plants and biocrusts) in a Mediterranean semiarid dryland located in Central Spain. MOISCRUST is a unique dataset for improving our understanding on how both vascular plants and biocrusts determine soil water dynamics in drylands, and thus to better assess their hydrological impacts and responses to ongoing climate change.","tags":["Environmental Data","Soil Ecology","Soil Sensors","Time Series","Plant Ecology","Linear Models"],"title":" Fourteen years of continuous soil moisture records from plant and biocrust-dominated microsites.","type":"publication"},{"authors":["Fernando T. Maestre","Blas M. Benito","Miguel Berdugo","Laura Concostrina-Zubiri","Manuel Delgado-Baquerizo","David J. Eldridge","Emilio Guirado","Nicolas Gross","Sonia Kéfi","Yoann Le Bagousse-Pinguet","Raúl Ochoa-Hueso","Santiago Soliveres"],"categories":null,"content":"","date":1618617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618617600,"objectID":"4f98b73b5572de5d985bf1ec380a2c19","permalink":"https://blasbenito.com/publication/2021_maestre_new_phytologist/","publishdate":"2021-04-17T00:00:00Z","relpermalink":"/publication/2021_maestre_new_phytologist/","section":"publication","summary":"Here we synthesize the biogeography of key organisms (vascular and non‐vascular vegetation and soil microorganisms), attributes (functional traits, spatial patterns, plant‐plant and plant‐soil interactions) and processes (productivity and land cover) across global drylands. We finish our review discussing major research gaps, which include: i) studying regular vegetation spatial patterns, ii) establishing large‐scale plant and biocrust field surveys assessing individual‐level trait measurements, iii) knowing whether plant‐plant and plant‐soil interactions impacts on biodiversity are predictable and iv) assessing how elevated CO2 modulates future aridity conditions and plant productivity.","tags":["Biogeography","Plant Ecology","Dryland ecology"],"title":"Biogeography of global drylands","type":"publication"},{"authors":null,"categories":null,"content":" This is a tutorial written for R users needing to compute betadiversity indices from species lists rather than from presence-absence matrices, and for R beginners or intermediate users that want to start using their own functions. If you are an advanced R user, this post will likely waste your time. We ecologists like to measure all things in nature, and compositional changes in biological communities over time or space, a.k.a betadiversity, is one of these things. I am not going to explain what betadiversity is because others that know better than me have done it already. Good examples are this post published the blog of Methods in Ecology and Evolution by Andres Baselga, and this lecture by Tim Seipel.\nWhat I am actually going to do in this post is to explain how to write functions to compute betadiversity indices in R from species lists rather than from presence-absence matrices. For the latter there are a few packages such as vegan, BAT, MBI, or betapart, but for the former I was unable to find anything suitable. To make this post useful for R beginners, I will go step by step on the rationale behind the design of the functions to compute betadiversity indices, and by the end of the post I will explain how to organize them to achieve a clean R workflow.\nLet’s go!\nBetadiversity indices There are a few betadiversity indices out there, and I totally recommend you to start with Koleff et al. (2003) as a primer. They review the literature and analyze the properties of 24 different indices to provide guidance on how to use them.\nBetadiversity components a, b, and c Betadiversity indices are designed to compare the taxa pools of two sites at a time, and require the computation of three components:\na: number of common taxa of both sites. b: number of exclusive taxa of one site. c: number of exclusive taxa of the other site. Let’s see how can we use these diversity components to compute betadiversity indices.\nSørensen’s Beta Let’s start with the Sørensen’s Beta (\\(\\beta_{sor}\\) hereafter), as presented in Koleff et al. (2003).\n\\[\\beta_{sor} = \\frac{2a}{2a + b + c}\\]\n\\(\\beta_{sor}\\) is a similarity index in the range [0, 1] (the closer to one, the more similar the taxa pools of both sites are) that puts a lot of weight in the \\(a\\) component, and is therefore a measure of continuity, as it focuses the most in the common taxa among sites.\nSimpson’s Beta Another popular betadiversity index is the Simpson’s Beta (\\(\\beta_{sim}\\) hereafter).\n\\[\\beta_{sim} = \\frac{min(b, c)}{min(b, c) + a}\\] where \\(min()\\) is a function that takes the minimum value among the diversity components within the parenthesis. \\(\\beta_{sim}\\) is a dissimilarity measure that focuses on compositional turnover among sites because it focuses the most on the values of \\(b\\) and \\(c\\). It has its lower bound in zero, and an open upper value.\nTo bring these ideas into R, first we have to load a few R packages, and generate some fake data to help us develop the functions.\nlibrary(magrittr) library(foreach) library(doParallel) ## Loading required package: iterators ## Loading required package: parallel The code chunk below generates 15 fake taxa names, from taxon_1 to taxon_15.\ntaxa \u0026lt;- paste0(\u0026quot;taxon_\u0026quot;, 1:15) With these fake taxa we are going to generate taxa lists for four hypothetical sites named site1, site2, site3, and site4. Two of the sites will have identical taxa lists, two will have non-overlapping taxa lists, and two of them will have some overlap.\nsite1 \u0026lt;- site2 \u0026lt;- taxa[1:7] site3 \u0026lt;- taxa[8:12] site4 \u0026lt;- taxa[10:15] So now we have these taxa lists:\nsite1 #and site2 ## [1] \u0026quot;taxon_1\u0026quot; \u0026quot;taxon_2\u0026quot; \u0026quot;taxon_3\u0026quot; \u0026quot;taxon_4\u0026quot; \u0026quot;taxon_5\u0026quot; \u0026quot;taxon_6\u0026quot; \u0026quot;taxon_7\u0026quot; site3 ## [1] \u0026quot;taxon_8\u0026quot; \u0026quot;taxon_9\u0026quot; \u0026quot;taxon_10\u0026quot; \u0026quot;taxon_11\u0026quot; \u0026quot;taxon_12\u0026quot; site4 ## [1] \u0026quot;taxon_10\u0026quot; \u0026quot;taxon_11\u0026quot; \u0026quot;taxon_12\u0026quot; \u0026quot;taxon_13\u0026quot; \u0026quot;taxon_14\u0026quot; \u0026quot;taxon_15\u0026quot; Step-by-step computation of betadiversity indices with R For a given pair of sites, how can we compute the diversity components a, b, and c?\nLooking at it from an R perspective, each site is a character vector, so a can be found by counting the number of common elements between two vectors. These common elements can be found with the function intersect(), and the number of elements can be computed by applying length() on the result of intersect().\na \u0026lt;- length(intersect(site3, site4)) a ## [1] 3 To compute b and c we can use the function setdiff(), that finds the exclusive elements of one character vector when comparing it with another. In this case, b is computed for the first vector introduced in the function, site3 in this case…\nb \u0026lt;- length(setdiff(site3, site4)) b ## [1] 2 … so to compute the c component we only need to switch the sites.\nc \u0026lt;- length(setdiff(site4, site3)) c ## [1] 3 Now that we know a, b, and c, we can compute \\(\\beta_{sor}\\) and \\(\\beta_{sim}\\).\nBsor \u0026lt;- 2 * a / (2 * a + b + c) Bsor ## [1] 0.5454545 Bsim\u0026lt;- min(b, c) / (min(b, c) + a) Bsim ## [1] 0.4 Of course, if we have a long list of sites, computing betadiversity indices like this can get quite boring quite fast. Let’s put everything in a set of functions to make it easier to work with.\nWriting functions to compute betadiversity indices The basic structure of a function definition in R looks as follows:\nfunction_name \u0026lt;- function(x, y, ...){ output \u0026lt;- [body] output #also return(output) } Where:\nfunction_name is the name of your function. Ideally, a verb, or otherwise, something indicating somehow what the function will do with the input data and arguments. function() is a function to define functions, there isn’t much more to it… x is the first argument of the function, and ideally, represents the input data. If that is the case, you can later use pipes (%\u0026gt;%) to chain functions together. y (it could have any other name) is another function argument, an can be either another input dataset, or an argument defining how the function has to behave. ... refers to other arguments the function may require. body is the code that operates with the data and function arguments. This can be one line of code, or a thousand, it all comes down to the function’s objective. In any case, the body must return an object (or an error if something went wrong) that will be the function’s output. output is the object ultimately produced by the function. It can have any name, and can be any kind of structure, such a number, a vector, a data frame, a list, etc. R functions return one output object only. Since R functions return the last evaluated value, it is good practice to put the output object at the end of the function as an explicit way to state what the actual output of the function is. Let’s start writing a function to compute a, b, and c from a pair of sites.\n#x: taxa list of one site #y: taxa list of another site abc \u0026lt;- function(x, y){ #list to store output out \u0026lt;- list() #filling the list out$a \u0026lt;- length(intersect(x, y)) out$b \u0026lt;- length(setdiff(x, y)) out$c \u0026lt;- length(setdiff(y, x)) #returning the output out } Notice that to to return the three values I am wrapping them in a list. Let’s run a little test.\nx \u0026lt;- abc( x = site3, y = site4 ) x ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 So far so good! From here we build the functions sorensen_beta() and simpson_beta() making sure they can accept the output of abc(), and return it with an added slot.\nsorensen_beta \u0026lt;- function(x){ x$bsor \u0026lt;- round(2 * x$a / (2 * x$a + x$b + x$c), 3) x } simpson_beta \u0026lt;- function(x){ x$bsim \u0026lt;- round(min(x$b, x$c) / (min(x$b, x$c) + x$a), 3) x } Notice that both functions are returning the input x with an added slot named after the given betadiversity index. Let’s test them first, to later see why returning the input object gives these functions a lot of flexibility.\nsorensen_beta(x) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 ## ## $bsor ## [1] 0.545 simpson_beta(x) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 ## ## $bsim ## [1] 0.4 When I said that returning the input object with an added slot gave these functions a lot of flexibility I was talking about this:\nx \u0026lt;- abc( x = site3, y = site4 ) %\u0026gt;% sorensen_beta() %\u0026gt;% simpson_beta() x ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 ## ## $bsor ## [1] 0.545 ## ## $bsim ## [1] 0.4 Chaining the functions through the %\u0026gt;% pipe of the magrittr package now allows us to combine their results in a single output no matter whether we use sorensen_beta() or sorensen_beta() first, or whether we omit one of them. The only thing the pipe is doing here is moving the output of the first function into the next. There are a couple of very nice tutorials about the magrittr package and the %\u0026gt;% here and here.\nWe can put that idea right away into a function to compute both betadiversity indices at once from the taxa list of a pair of sites.\nbetadiversity \u0026lt;- function(x, y){ require(magrittr) abc(x, y) %\u0026gt;% sorensen_beta() %\u0026gt;% simpson_beta() } The function now works as follows.\nx \u0026lt;- betadiversity( x = site3, y = site4 ) x ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 ## ## $bsor ## [1] 0.545 ## ## $bsim ## [1] 0.4 So far we have four functions…\nabc() simpson_beta(), that requires abc(). sorensen_beta(), that requires abc(). betadiversity(), that requires abc(), simpson_beta(), and sorensen_beta(). … and one limitation: so far we can only return betadiversity indices for two sites at a time. So at the moment, to compute betadiversity indices for all combinations of sites we have to do a pretty ridiculous thing:\nx1 \u0026lt;- betadiversity(x = site1, y = site2) x2 \u0026lt;- betadiversity(x = site1, y = site3) x3 \u0026lt;- betadiversity(x = site1, y = site4) #... and so on If I see you doing this I’ll come to haunt you in your nightmares! Since a real analysis may involve hundreds of sites, the next step is to use the functions above to build a new one able to intake an arbitrary number of sites.\nWriting a function to compute betadiversity indices for an arbitrary number of sites. First we have to organize our sites in a data frame with a long format.\nsites \u0026lt;- data.frame( site = c( rep(\u0026quot;site1\u0026quot;, length(site1)), rep(\u0026quot;site2\u0026quot;, length(site2)), rep(\u0026quot;site3\u0026quot;, length(site3)), rep(\u0026quot;site4\u0026quot;, length(site4)) ), taxon = c( site1, site2, site3, site4 ) ) site taxon site1 taxon_1 site1 taxon_2 site1 taxon_3 site1 taxon_4 site1 taxon_5 site1 taxon_6 site1 taxon_7 site2 taxon_1 site2 taxon_2 site2 taxon_3 site2 taxon_4 site2 taxon_5 site2 taxon_6 site2 taxon_7 site3 taxon_8 site3 taxon_9 site3 taxon_10 site3 taxon_11 site3 taxon_12 site4 taxon_10 site4 taxon_11 site4 taxon_12 site4 taxon_13 site4 taxon_14 site4 taxon_15 Our new function will need to do several things:\nGenerate combinations of the unique values of the column site two by two without repetition. Iterate through these combinations of two sites to compute betadiversity components and indices. Return a dataframe with the results to facilitate further analyses. The combinations of site pairs are done with utils::combn() as follows:\nsite.combinations \u0026lt;- utils::combn( x = unique(sites$site), m = 2 ) site.combinations ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] \u0026quot;site1\u0026quot; \u0026quot;site1\u0026quot; \u0026quot;site1\u0026quot; \u0026quot;site2\u0026quot; \u0026quot;site2\u0026quot; \u0026quot;site3\u0026quot; ## [2,] \u0026quot;site2\u0026quot; \u0026quot;site3\u0026quot; \u0026quot;site4\u0026quot; \u0026quot;site3\u0026quot; \u0026quot;site4\u0026quot; \u0026quot;site4\u0026quot; The result is a matrix, and each pair of rows in a column contain a pair of sites. The idea now is to iterate over the matrix columns, obtain the set of taxa from each site from the taxon column of the sites data frame, and use these taxa lists to compute the betadiversity components and indices.\nTo easily generate the output data frame, I use the foreach::foreach() function to iterate through pairs instead of a more traditional for loop. You can read more about foreach() in a previous post.\nbetadiversity.df \u0026lt;- foreach::foreach( i = 1:ncol(site.combinations), #iterates through columns of site.combinations .combine = \u0026#39;rbind\u0026#39; #to produce a data frame ) %do% { #site names site.one \u0026lt;- site.combinations[1, i] #from column i, row 1 site.two \u0026lt;- site.combinations[2, i] #from column i, row 2 #getting taxa lists taxa.list.one \u0026lt;- sites[sites$site %in% site.one, \u0026quot;taxon\u0026quot;] taxa.list.two \u0026lt;- sites[sites$site %in% site.two, \u0026quot;taxon\u0026quot;] #betadiversity beta \u0026lt;- betadiversity( x = taxa.list.one, y = taxa.list.two ) #adding site names beta$site.one \u0026lt;- site.one beta$site.two \u0026lt;- site.two #returning output beta } a b c bsor bsim site.one site.two 7 0 0 1 0 site1 site2 0 7 5 0 1 site1 site3 0 7 6 0 1 site1 site4 0 7 5 0 1 site2 site3 0 7 6 0 1 site2 site4 3 2 3 0.545 0.4 site3 site4 Now that we know it works, we can put everything together in a function. Notice that to make the function more general, I have added arguments requesting the names of the columns with the site and the taxa names.\nbetadiversity_multisite \u0026lt;- function( x, site.column, #column with site names taxa.column #column with taxa names ){ #get site combinations site.combinations \u0026lt;- utils::combn( x = unique(x[, site.column]), m = 2 ) #iterating through site pairs betadiversity.df \u0026lt;- foreach::foreach( i = 1:ncol(site.combinations), .combine = \u0026#39;rbind\u0026#39; ) %do% { #site names site.one \u0026lt;- site.combinations[1, i] site.two \u0026lt;- site.combinations[2, i] #getting taxa lists taxa.list.one \u0026lt;- x[x[, site.column] %in% site.one, taxa.column] taxa.list.two \u0026lt;- x[x[, site.column] %in% site.two, taxa.column] #betadiversity beta \u0026lt;- betadiversity( x = taxa.list.one, y = taxa.list.two ) #adding site names beta$site.one \u0026lt;- site.one beta$site.two \u0026lt;- site.two #returning output beta } #remove bad rownames rownames(betadiversity.df) \u0026lt;- NULL #reordering columns betadiversity.df \u0026lt;- betadiversity.df[, c( \u0026quot;site.one\u0026quot;, \u0026quot;site.two\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;bsor\u0026quot;, \u0026quot;bsim\u0026quot; )] #returning output return(betadiversity.df) } And the test!\nsites.betadiversity \u0026lt;- betadiversity_multisite( x = sites, site.column = \u0026quot;site\u0026quot;, taxa.column = \u0026quot;taxon\u0026quot; ) site.one site.two a b c bsor bsim site1 site2 7 0 0 1 0 site1 site3 0 7 5 0 1 site1 site4 0 7 6 0 1 site2 site3 0 7 5 0 1 site2 site4 0 7 6 0 1 site3 site4 3 2 3 0.545 0.4 That went well!\nFinally, to have these functions available in my R session I always put them all in a single file in the same folder where my Rstudio project lives, name it something like functions_betadiversity.R, and source it at the beginning of my script or .Rmd file by running a line like the one below.\nsource(\u0026quot;functions_betadiversity.R\u0026quot;) I have placed the file functions_betadiversity.R in this GitHub Gist in case you want to give it a look. You can also source it right away to your R environment by executing the following line:\nsource(\u0026quot;https://gist.githubusercontent.com/BlasBenito/4c3740b056a0c9bb3602f33dfd35990c/raw/bbb40d868787fc5d10e391a2121045eb5d75f165/functions_betadiversity.R\u0026quot;) I hope this post helped you to better understand how to write and organize R functions!\n","date":1609891200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609891200,"objectID":"09127f8961ebfb017dfd29461698bb7c","permalink":"https://blasbenito.com/post/04_betadiversity/","publishdate":"2021-01-06T00:00:00Z","relpermalink":"/post/04_betadiversity/","section":"post","summary":"This is a tutorial written for R users needing to compute betadiversity indices from species lists rather than from presence-absence matrices, and for R beginners or intermediate users that want to start using their own functions.","tags":null,"title":"Designing R functions to compute betadiversity indices from species lists","type":"post"},{"authors":null,"categories":null,"content":" In the previous posts I have covered how to setup a home cluster, and how to run parallel processes with foreach in R. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.\nThis post (developed on Ubuntu and Ubuntu Server 20.04) assumes that the home cluster runs on Linux, and covers the following topics.\nBasics of the Network File System protocol (NFS). Setup of an NFS folder in a home cluster. Using an NFS folder in a parallelized loop. The Network File System protocol (NFS) The Network File System protocol offers the means for a host computer to allow other computers in the network (clients) to read and write the contents of a given folder. The clients access such folder like if it was a local one, while in fact it is a reference to the one in the host computer.\nThe image at the beginning of the post illustrates the concept. There is a host computer with a folder in the path /home/user/cluster_shared (were user is your user name) that is broadcasted to the network, and there are one or several clients that are mounting mounting (making accessible) the same folder in their local paths /home/user/cluster_shared.\nIf the host writes a file to the shared folder, it is available right away for the clients, and the other way around. At the end, the idea is to have a folder shared among all computers in the cluster, while having the same exact path on each one of them to write or read files from such shared folder.\nSetup of an NFS folder in a home cluster To setup the shared folder we’ll need to do some things in the host, and some things in the clients. Let’s start with the host.\nPreparing the host computer First we need to install the nfs-kernel-server.\nsudo apt update sudo apt install nfs-kernel-server Now we can create the shared folder. Remember to replace user with your user name, and cluster_shared with the actual folder name you want to use.\nmkdir /home/user/cluster_shared To broadcast it we need to open the file /etc/exports…\nsudo gedit /etc/exports … and add the following line\n/home/user/cluster_shared IP_CLIENT1(rw,no_subtree_check) IP_CLIENT2(rw,no_subtree_check) IP_CLIENT3(rw,no_subtree_check) where:\n/home/user/cluster_shared is the path of the shared folder. IP_CLIENTx are the IPs of each one of the clients. rw gives reading and writing permission on the shared folder to the given client. no_subtree_check prevents the host from checking the complete tree of shares before attending a request (read or write) by a client. For example, the last line of my /etc/exports file looks like this:\n/home/blas/cluster_shared 10.42.0.34(rw,async,no_subtree_check) 10.42.0.104(rw,async,no_subtree_check) Save the file, and to make the changes effective, execute:\nsudo exportfs -ra To end preparing the host we have to update the firewall rules to allow nfs connections from the clients. Notice that one rule per client needs to be defined, using the clients IPs to identify them.\nsudo ufw allow from IP_CLIENT1 to any port nfs sudo ufw allow from IP_CLIENT2 to any port nfs sudo ufw status Preparing the clients First we have to install the Linux package nfs-common on each client.\nsudo apt update sudp apt install nfs-common Now we can create a folder in the clients and use it to mount the NFS folder of the host.\nmkdir -p /home/user/cluster_shared sudo mount IP_HOST:/home/user/cluster_shared /home/user/cluster_shared The second line of code is mounting the folder /home/user/cluster_shared of the host in the folder /home/user/cluster_shared of the client.\nTo make the mount permanent, we have to open /etc/fstab with super-user privilege in the clients…\nsudo gedit /etc/fstab … and add the line\nIP_HOST:/home/user/cluster_shared /home/user/cluster_shared nfs defaults 0 0 Remember to replace IP_HOST and user with the right values!\nNow we can test that the shared folder works as intended by writing one file into it from each computer in the network using the code below in the command line.\ncd cluster_shared touch filename.txt Once the files are created, we can check they are visible from each computer using the ls command.\nls Using an NFS folder in a parallelized loop In a previous post I described how to run parallelized tasks with foreach in R. This section intends to complete that post by showing how a shared NFS folder can be used to store input and output files during the execution of a parallelized loop\nThe task In this hypothetical example we have a large number of data frames stored in /home/user/cluster_shared/input. Each data frame has the same predictors a, b, c, and d, and a different response variable, named y1 for the data frame y1, y2 for the data frame y2, and so on. In the example we will be using 100 data frames, but there could be thousands. This example would be valid as well for any other kind of object, such as raster files, time-series, etc etc.\nFirst we have to load the libraries we’ll be using.\n#automatic install of packages if they are not installed already list.of.packages \u0026lt;- c( \u0026quot;foreach\u0026quot;, \u0026quot;doParallel\u0026quot;, \u0026quot;ranger\u0026quot; ) new.packages \u0026lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\u0026quot;Package\u0026quot;])] if(length(new.packages) \u0026gt; 0){ install.packages(new.packages, dep=TRUE) } #loading packages for(package.i in list.of.packages){ suppressPackageStartupMessages( library( package.i, character.only = TRUE ) ) } The code chunk below generates the folder /home/user/cluster_shared/input and populates it with the dummy files.\n#creating the input folder input.folder \u0026lt;- \u0026quot;/home/blas/cluster_shared/input\u0026quot; dir.create(input.folder) #data frame names df.names \u0026lt;- paste0(\u0026quot;y\u0026quot;, 1:100) #filling it with files for(i in df.names){ #creating the df df.i \u0026lt;- data.frame( y = rnorm(1000), a = rnorm(1000), b = rnorm(1000), c = rnorm(1000), d = rnorm(1000) ) #changing name of the response variable colnames(df.i)[1] \u0026lt;- i #assign to a variable with name i assign(i, df.i) #saving the object save( list = i, file = paste0(input.folder, \u0026quot;/\u0026quot;, i, \u0026quot;.RData\u0026quot;) ) #removing the generated data frame form the environment rm(list = i, df.i, i) } Our target now will be to fit one ranger::ranger() model per data frame stored in /home/blas/cluster_shared/input, save the model result to a folder with the path /home/blas/cluster_shared/input, and write a small summary of the model to the output of foreach.\nSuch target is based on this rationale: When executing a foreach loop as in x \u0026lt;- foreach(...) %dopar% {...}, the variable x is going to grow in memory very fast, competing for RAM resources with the worker nodes. Furthermore, since x is being written on the fly, the results would be lost if the computer crashes. When the size of the input and the output of our parallelized operation is larger than memory, we can use an NFS folder to store inputs and outputs, while keeping the RAM memory free for computational tasks only, with the positive side effect of having our outputs already stored should our computer decide to crash.\nAlso, please notice that here I am focusing in a cluster setting, but using a folder to read and write data during a loop paralellized with foreach can indeed be done in a single computer without an NFS folder. Any folder in your system will do the trick as well!\nSo, from here, we are going to prepare the cluster, and execute a parallelized loop fitting one model per data frame that reads the inputs and writes the outputs to the shared folder.\nCluster setup We will also need the function I showed in the previous post to generate the cluster specification from a GitHub Gist.\nsource(\u0026quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R\u0026quot;) Below I use the function to create a cluster specification and initiate the cluster with parallel::makeCluster().\n#generate cluster specification spec \u0026lt;- cluster_spec( ips = c(\u0026#39;10.42.0.1\u0026#39;, \u0026#39;10.42.0.34\u0026#39;, \u0026#39;10.42.0.104\u0026#39;), cores = c(7, 4, 4), user = \u0026quot;blas\u0026quot; ) #define parallel port Sys.setenv(R_PARALLEL_PORT = 11000) Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;) #setting up cluster my.cluster \u0026lt;- parallel::makeCluster( master = \u0026#39;10.42.0.1\u0026#39;, spec = spec, port = Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;), outfile = \u0026quot;\u0026quot;, homogeneous = TRUE ) #check cluster definition (optional) print(my.cluster) #register cluster doParallel::registerDoParallel(cl = my.cluster) #check number of workers foreach::getDoParWorkers() Parallelized loop For everything to work as intended, we first need to create the output folder.\noutput.folder \u0026lt;- \u0026quot;/home/blas/cluster_shared/output\u0026quot; dir.create(output.folder) And now we are ready to execute the parallelized loop. Notice that I am using the output of list.files() to have a vector of file names the loop will iterate over. The steps inside of the loop go as follows:\n1. Remove the extension .RData from the file name. We’ll later use the result to use assign() on the fitted model to change its name to the same as the input file before saving it. 2. Read the input data frame and store in an object named df. 3. Fit the model with ranger, using the first column of df as respose variable. 4. Change the model name to the name of the input file without extension, resulting from the first step described above. 5. Save the model into the output folder with the extension .RData. 6. Return a data frame with one line with the name of the response variable, the r-squared, and the importance of each predictor. #list of input files as iterator input.files \u0026lt;- list.files( path = input.folder, full.names = FALSE ) modelling.summary \u0026lt;- foreach( input.file = input.files, .combine = \u0026#39;rbind\u0026#39;, .packages = \u0026quot;ranger\u0026quot; ) %dopar% { # 1. input file name without extension input.file.name \u0026lt;- tools::file_path_sans_ext(input.file) # 2. read input file df \u0026lt;- get(load(paste0(input.folder, \u0026quot;/\u0026quot;, input.file))) # 3. fit model m.i \u0026lt;- ranger::ranger( data = df, dependent.variable.name = colnames(df)[1], importance = \u0026quot;permutation\u0026quot; ) # 4. change name of the model to one of the response variable assign(input.file.name, m.i) # 5. save model save( list = input.file.name, file = paste0(output.folder, \u0026quot;/\u0026quot;, input.file) ) # 6. returning summary return( data.frame( response.variable = input.file.name, r.squared = m.i$r.squared, importance.a = m.i$variable.importance[\u0026quot;a\u0026quot;], importance.b = m.i$variable.importance[\u0026quot;b\u0026quot;], importance.c = m.i$variable.importance[\u0026quot;c\u0026quot;], importance.d = m.i$variable.importance[\u0026quot;d\u0026quot;] ) ) } Once this parallelized loop is executed, the folder /home/blas/cluster_shared/output should be filled with the results from the cluster workers, and the modelling.summary data frame contains the summary of each fitted model.\nNow that the work is done, we can stop the cluster.\nparallel::stopCluster(cl = my.cluster) Now you know how to work with data larger than memory in a parallelized loop!\n","date":1609632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609632000,"objectID":"bf28eeedb27403ed52da5fe9b61d9449","permalink":"https://blasbenito.com/post/03_shared_folder_in_cluster/","publishdate":"2021-01-03T00:00:00Z","relpermalink":"/post/03_shared_folder_in_cluster/","section":"post","summary":"In the previous posts I have covered how to setup a home cluster, and how to run parallel processes with foreach in R. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.","tags":null,"title":"Setup of a shared folder in a home cluster","type":"post"},{"authors":null,"categories":null,"content":" In the previous posts I have covered how to setup a home cluster, and how to run parallel processes with foreach in R. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.\nThis post (developed on Ubuntu and Ubuntu Server 20.04) assumes that the home cluster runs on Linux, and covers the following topics.\nBasics of the Network File System protocol (NFS). Setup of an NFS folder in a home cluster. Using an NFS folder in a parallelized loop. The Network File System protocol (NFS) The Network File System protocol offers the means for a host computer to allow other computers in the network (clients) to read and write the contents of a given folder. The clients access such folder like if it was a local one, while in fact it is a reference to the one in the host computer.\nThe image at the beginning of the post illustrates the concept. There is a host computer with a folder in the path /home/user/cluster_shared (were user is your user name) that is broadcasted to the network, and there are one or several clients that are mounting mounting (making accessible) the same folder in their local paths /home/user/cluster_shared.\nIf the host writes a file to the shared folder, it is available right away for the clients, and the other way around. At the end, the idea is to have a folder shared among all computers in the cluster, while having the same exact path on each one of them to write or read files from such shared folder.\nSetup of an NFS folder in a home cluster To setup the shared folder we’ll need to do some things in the host, and some things in the clients. Let’s start with the host.\nPreparing the host computer First we need to install the nfs-kernel-server.\nsudo apt update sudo apt install nfs-kernel-server Now we can create the shared folder. Remember to replace user with your user name, and cluster_shared with the actual folder name you want to use.\nmkdir /home/user/cluster_shared To broadcast it we need to open the file /etc/exports…\nsudo gedit /etc/exports … and add the following line\n/home/user/cluster_shared IP_CLIENT1(rw,no_subtree_check) IP_CLIENT2(rw,no_subtree_check) IP_CLIENT3(rw,no_subtree_check) where:\n/home/user/cluster_shared is the path of the shared folder. IP_CLIENTx are the IPs of each one of the clients. rw gives reading and writing permission on the shared folder to the given client. no_subtree_check prevents the host from checking the complete tree of shares before attending a request (read or write) by a client. For example, the last line of my /etc/exports file looks like this:\n/home/blas/cluster_shared 10.42.0.34(rw,async,no_subtree_check) 10.42.0.104(rw,async,no_subtree_check) Save the file, and to make the changes effective, execute:\nsudo exportfs -ra To end preparing the host we have to update the firewall rules to allow nfs connections from the clients. Notice that one rule per client needs to be defined, using the clients IPs to identify them.\nsudo ufw allow from IP_CLIENT1 to any port nfs sudo ufw allow from IP_CLIENT2 to any port nfs sudo ufw status Preparing the clients First we have to install the Linux package nfs-common on each client.\nsudo apt update sudp apt install nfs-common Now we can create a folder in the clients and use it to mount the NFS folder of the host.\nmkdir -p /home/user/cluster_shared sudo mount IP_HOST:/home/user/cluster_shared /home/user/cluster_shared The second line of code is mounting the folder /home/user/cluster_shared of the host in the folder /home/user/cluster_shared of the client.\nTo make the mount permanent, we have to open /etc/fstab with super-user privilege in the clients…\nsudo gedit /etc/fstab … and add the line\nIP_HOST:/home/user/cluster_shared /home/user/cluster_shared nfs defaults 0 0 Remember to replace IP_HOST and user with the right values!\nNow we can test that the shared folder works as intended by writing one file into it from each computer in the network using the code below in the command line.\ncd cluster_shared touch filename.txt Once the files are created, we can check they are visible from each computer using the ls command.\nls Using an NFS folder in a parallelized loop In a previous post I described how to run parallelized tasks with foreach in R. This section intends to complete that post by showing how a shared NFS folder can be used to store input and output files during the execution of a parallelized loop\nThe task In this hypothetical example we have a large number of data frames stored in /home/user/cluster_shared/input. Each data frame has the same predictors a, b, c, and d, and a different response variable, named y1 for the data frame y1, y2 for the data frame y2, and so on. In the example we will be using 100 data frames, but there could be thousands. This example would be valid as well for any other kind of object, such as raster files, time-series, etc etc.\nFirst we have to load the libraries we’ll be using.\n#automatic install of packages if they are not installed already list.of.packages \u0026lt;- c( \u0026quot;foreach\u0026quot;, \u0026quot;doParallel\u0026quot;, \u0026quot;ranger\u0026quot; ) new.packages \u0026lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\u0026quot;Package\u0026quot;])] if(length(new.packages) \u0026gt; 0){ install.packages(new.packages, dep=TRUE) } #loading packages for(package.i in list.of.packages){ suppressPackageStartupMessages( library( package.i, character.only = TRUE ) ) } The code chunk below generates the folder /home/user/cluster_shared/input and populates it with the dummy files.\n#creating the input folder input.folder \u0026lt;- \u0026quot;/home/blas/cluster_shared/input\u0026quot; dir.create(input.folder) #data frame names df.names \u0026lt;- paste0(\u0026quot;y\u0026quot;, 1:100) #filling it with files for(i in df.names){ #creating the df df.i \u0026lt;- data.frame( y = rnorm(1000), a = rnorm(1000), b = rnorm(1000), c = rnorm(1000), d = rnorm(1000) ) #changing name of the response variable colnames(df.i)[1] \u0026lt;- i #assign to a variable with name i assign(i, df.i) #saving the object save( list = i, file = paste0(input.folder, \u0026quot;/\u0026quot;, i, \u0026quot;.RData\u0026quot;) ) #removing the generated data frame form the environment rm(list = i, df.i, i) } Our target now will be to fit one ranger::ranger() model per data frame stored in /home/blas/cluster_shared/input, save the model result to a folder with the path /home/blas/cluster_shared/input, and write a small summary of the model to the output of foreach.\nSuch target is based on this rationale: When executing a foreach loop as in x \u0026lt;- foreach(...) %dopar% {...}, the variable x is going to grow in memory very fast, competing for RAM resources with the worker nodes. Furthermore, since x is being written on the fly, the results would be lost if the computer crashes. When the size of the input and the output of our parallelized operation is larger than memory, we can use an NFS folder to store inputs and outputs, while keeping the RAM memory free for computational tasks only, with the positive side effect of having our outputs already stored should our computer decide to crash.\nAlso, please notice that here I am focusing in a cluster setting, but using a folder to read and write data during a loop paralellized with foreach can indeed be done in a single computer without an NFS folder. Any folder in your system will do the trick as well!\nSo, from here, we are going to prepare the cluster, and execute a parallelized loop fitting one model per data frame that reads the inputs and writes the outputs to the shared folder.\nCluster setup We will also need the function I showed in the previous post to generate the cluster specification from a GitHub Gist.\nsource(\u0026quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R\u0026quot;) Below I use the function to create a cluster specification and initiate the cluster with parallel::makeCluster().\n#generate cluster specification spec \u0026lt;- cluster_spec( ips = c(\u0026#39;10.42.0.1\u0026#39;, \u0026#39;10.42.0.34\u0026#39;, \u0026#39;10.42.0.104\u0026#39;), cores = c(7, 4, 4), user = \u0026quot;blas\u0026quot; ) #define parallel port Sys.setenv(R_PARALLEL_PORT = 11000) Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;) #setting up cluster my.cluster \u0026lt;- parallel::makeCluster( master = \u0026#39;10.42.0.1\u0026#39;, spec = spec, port = Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;), outfile = \u0026quot;\u0026quot;, homogeneous = TRUE ) #check cluster definition (optional) print(my.cluster) #register cluster doParallel::registerDoParallel(cl = my.cluster) #check number of workers foreach::getDoParWorkers() Parallelized loop For everything to work as intended, we first need to create the output folder.\noutput.folder \u0026lt;- \u0026quot;/home/blas/cluster_shared/output\u0026quot; dir.create(output.folder) And now we are ready to execute the parallelized loop. Notice that I am using the output of list.files() to have a vector of file names the loop will iterate over. The steps inside of the loop go as follows:\n1. Remove the extension .RData from the file name. We’ll later use the result to use assign() on the fitted model to change its name to the same as the input file before saving it. 2. Read the input data frame and store in an object named df. 3. Fit the model with ranger, using the first column of df as respose variable. 4. Change the model name to the name of the input file without extension, resulting from the first step described above. 5. Save the model into the output folder with the extension .RData. 6. Return a data frame with one line with the name of the response variable, the r-squared, and the importance of each predictor. #list of input files as iterator input.files \u0026lt;- list.files( path = input.folder, full.names = FALSE ) modelling.summary \u0026lt;- foreach( input.file = input.files, .combine = \u0026#39;rbind\u0026#39;, .packages = \u0026quot;ranger\u0026quot; ) %dopar% { # 1. input file name without extension input.file.name \u0026lt;- tools::file_path_sans_ext(input.file) # 2. read input file df \u0026lt;- get(load(paste0(input.folder, \u0026quot;/\u0026quot;, input.file))) # 3. fit model m.i \u0026lt;- ranger::ranger( data = df, dependent.variable.name = colnames(df)[1], importance = \u0026quot;permutation\u0026quot; ) # 4. change name of the model to one of the response variable assign(input.file.name, m.i) # 5. save model save( list = input.file.name, file = paste0(output.folder, \u0026quot;/\u0026quot;, input.file) ) # 6. returning summary return( data.frame( response.variable = input.file.name, r.squared = m.i$r.squared, importance.a = m.i$variable.importance[\u0026quot;a\u0026quot;], importance.b = m.i$variable.importance[\u0026quot;b\u0026quot;], importance.c = m.i$variable.importance[\u0026quot;c\u0026quot;], importance.d = m.i$variable.importance[\u0026quot;d\u0026quot;] ) ) } Once this parallelized loop is executed, the folder /home/blas/cluster_shared/output should be filled with the results from the cluster workers, and the modelling.summary data frame contains the summary of each fitted model.\nNow that the work is done, we can stop the cluster.\nparallel::stopCluster(cl = my.cluster) Now you know how to work with data larger than memory in a parallelized loop!\n","date":1609632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609632000,"objectID":"4eb8b71dc831df7557610c90552ef52b","permalink":"https://blasbenito.com/post_projects/template/","publishdate":"2021-01-03T00:00:00Z","relpermalink":"/post_projects/template/","section":"post_projects","summary":"In the previous posts I have covered how to setup a home cluster, and how to run parallel processes with foreach in R. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.","tags":null,"title":"Setup of a shared folder in a home cluster","type":"post_projects"},{"authors":null,"categories":null,"content":"This is a spatio-temporal simulation of the effect of fire regimes on the population dynamics of five forest species (Pinus sylvestris, Pinus uncinata, Betula pendula, Corylus avellana, and Quercus petraea) during the Lateglacial-Holocene transition (15-7 cal Kyr BP) at El Portalet, a subalpine bog located in the central Pyrenees region (1802m asl, Spain), that has served for palaeoenvironmental studies (González-Smapériz et al. 2006; Gil-Romera et al., 2014). This model is described in the paper in prep. titled Forest - fire interactions in the Central Pyrenees: a data-model comparison for the Lateglacial-Holocene transition, and authored by Graciela Gil-Romera, Blas M. Benito, Juli G. Pausas, Penélope González-Sampériz, J. Julio. Camarero, Jens-Christian Svenning, and Blas Valero-Garcés.\nHOW DOES IT WORK Abiotic component The abiotic layer of the model is represented by three main environmental factors:\nTopography derived from a digital elevation model at 200 x 200 meters resolution. Slope (along temperature) is used to impose restrictions to species distributions. Northness (in the range [0, 1]) is used to restrict fire spread. Aspect is used to draw a shaded relief map (at the user\u0026rsquo;s request). Elevation is used to compute a lapse rate map (see below).\nTemperature (average of montly minimum temperatures) time series for the study area computed from palaeoclimatic data at annual resolution provided by the TraCe simulation, a transient model for the global climate evolution of the last 21K years with an annual resolution. The single temperature value of every year is converted into a temperature map (200 x 200 m resolution) using a lapse rate map based on the elevation map. Temperature, along with slope, is used to compute habitat suitability by using a logistic equation. Habitat suitability affects plant growth and survival.\nFire: The charcoal accumulation rate record (CHAR) from El Portalet palaeoenvironmental sequence (Gil-Romera et al., 2014) is used as input to simulate forest fires. A value of this time series is read each year, and a random number in the range [0, 1] is generated. If the random number is lower than the Fire-probability-per-year (FPY) parameter defined by the user, the value from the charcoal time series is multiplied by the parameter Number-ignitions-per-fire-event (NIF) (defined by the user) to compute the number of ignitions for the given year. As many adult tree as ignitions are selected to start spreading fire. Fire spreads to a neighbor patch if there is an adult tree in there, and a random number in the range [0, 1] is higher than the northness value of the patch.\nBiotic component The biotic layer of the model is composed by five tree species. We have introduced the following elements to represent their ecological dynamics:\nTopoclimatic niche, inferred from their present day distributions and high resolution temperature maps (presence data taken from GBIF, temperature maps taken from Worldclim and the Digital Climatic Atlas of the Iberian Peninsula). The ecological niche is represented by a logistic equation (see below). The results of this equation plus the dispersal dynamics of each species defines changes in distribution over time.\nPopulation dynamics, driven by species traits such as dispersal distance, longevity, fecundity, mortality, growth rate, post-fire response to fire, and heliophity (competition for light). The data is based on the literature and/or expert opinion from forest and fire ecologists, and it is used to simulate growth (using logistic equations), competition for light and space, decay due to senescence, and mortality due to climate, fire, or plagues.\nThe model doesn\u0026rsquo;t simulate the entire populations of the target species. Instead, on each 200 x 200 meters patch it simulates the dynamics of an small forest plot (around 10 x 10 meters) where a maximum of one individual per species can exist.\nModel dynamics Let\u0026rsquo;s burn it! Simulating fire-vegetation dynamics at millennial timescales in the central Pyrenees. from blas benito on Vimeo.\nThe life of an individual\nDuring the model setup seeds of every species are created on every patch. From there, every seed will go through the following steps every simulated year:\nIts age increases by one year, and its life-stage is changed to \u0026ldquo;seedling\u0026rdquo;.\nThe minimum average temperature of its patch is updated.\nThe individual computes its habitat suitability using the logistic equation 1 / ( 1 + exp( -(intercept + coefficient * patch-temperature))), where the intercept and the coefficient are user defined. These parameters are hardcoded to save space in the GUI, and have been computed beforehand by using current presence data and temperature maps.\nIf habitat suitability is higher than a random number in the range [0, 1], the habitat is considered suitable (NOTE: this random number is defined for the patch, and it changes every ~10 years following a random walk drawn from a normal distribution with the average set to the previous value, and a standard deviation of 0.001).\nIf it is lower, the habitat is considered unsuitable, and the number of years under unsuitable habitat is increased by 1.\nIf the number of years unders unsuitable habitat becomes higher than seedling-tolerance, the seedling dies, and another seed from the seed bank takes its place. Otherwise it stays alive. Mortality: If a random number in the range [0, 1] is lower than the seedling mortality of the species the plant dies, and it is replaced by a seed from the seed bank. Otherwise it stays alive.\nCompetition and growth:\nIf the patch total biomass of the individuals in the patch equals Max-biomass-per-patch, the individual loses an amount of biomass between 0 and the 20% of its current biomass. This number is randomly selected.\nIf Max-biomass-per-patch has not been reached yet:\nAn interaction term is computed as (1 - (biomass of other individuals in the patch / Max-biomass-per-patch)) * (1 - heliophilia)).\nThe interaction term is introduced in the growth equation max-biomass / (1 + max-biomass * exp(- growth-rate * interaction-term * habitat-suitability * age)) to compute the current biomass of the individual. The lower the interaction term and habitat suitability are, the lower the growth becomes.\nIf a fire reaches the patch and there are adult individuals of other species on it, the plant dies, and it is replaced by a seed (this seed inherites the traits of the parent).\nThese steps continue while the individual is still a seedling, but once it reaches its maturity some steps become slightly different:\nIf a random number in the range [0, 1] is lower than the adults mortality of the species, or the maximum age of the species is reached, the individual is marked for decay. The current biomass of decaying individuals is computed as previous-biomass - years-of-decay. To add the effect of climatic variability to this decreasing function, its result is multiplied by 1 - habitat-suitability x random[0, 10]. If the biomass is higher than zero, pollen productivity is computed as current-biomass x species-pollen-productivity. The individual dies and is replaced by a seed when the biomass is below 1.\nDispersal: If the individual is in suitable habitat, a seed from it is placed in one of the neighboring patches within a radius given by the dispersal distance of the species (which is measured in \u0026ldquo;number of patches\u0026rdquo; and hardcoded) with no individuals of the same species.\nIf the individual starts a fire, or if fire spreads in from neighboring patches, it is marked as \u0026ldquo;burned\u0026rdquo;, spreads fire to its neighbors, dies, and is replaced by a seed. If the individual belongs to an species with post-fire resprouting, the growth-rate of the seed is multiplied by 2 to boost growth after fire.\nSimulating pollen and charcoal deposition\nThe user defines the radius of a catchment area round the core location (10 km by default, that is 50 patches). All patches within this radius define the RSAP (relevant source area of pollen).\nAt the end of every simulated year the pollen productivity of every adult of each species within the RSAP is summed, and this value is used to compose the simulated pollen curves. The same is done with the biomass of the burned individuals to compose the virtual charcoal curve.\nOutput In GUI\nThe simulation GUI shows the following results in real time:\nPlots of the input values:\nMinimum Temperature of the coldest month. Real charcoal data. Simulated pollen curves for the target taxa.\nSimulated charcoal curve.\nMap showing the distribution of every species and the forest fires.\nWritten to disk\nThe simulated pollen counts and charcoal is exported to the path defined by the user as a table in csv format named output_table.csv. It contains one row per simulated year and the following columns:\nage: simulated year. temperature_minimum_average: average minimum winter temperature of the study area. pollen_Psylvestris: pollen sum for Pinus sylvestris. pollen_Puncinata pollen_Bpendula pollen_Cavellana pollen_Qpetraea real_charcoal: real charcoal values from El Portalet core. ignitions: number of fire ignitions. charcoal_sum: biomass sum of all burned individuals. charcoal_Psylvestris: sum of the biomass of burned individuals of Pinus sylvestris. charcoal_Puncinata charcoal_Bpendula charcoal_Cavellana charcoal_Qpetraea Snapshots of the simulation map taken at 1 or 10 years intervals are stored in the output folder is requested by the user. These snapshots are useful to compose a video of the simulation.\nHOW TO USE IT Input files Input files are stored in a folder named \u0026ldquo;data\u0026rdquo;. These are:\nage: text file with no extension and a single column with no header containing age values from -15000 to -5701 fire: text file with no extension and a single column with no header containing actual charcoal counts expresed in the range [0, 1]. There are as many rows as in the age file t_minimum_average: text file with same features as the ones above containing minimum winter temperatures for the study area extracted from the TraCe simualtion. correct_t_minimum_average.asc: Map at 200m resolution containing the minimum winter temperature difference (period 1970-2000) between the TraCe simulation and the Digital Climatic Atlas of the Iberian Peninsula. It is used to transform the values of t_minimum_average into a high resolution temperature map. elevation.asc: digital elevation model of the study area at 200m resolution, coordinate system with EPSG code 23030. slope.asc: topographic slope. topography.asc: shaded relief map. It is used for plotting purposes only. Input parameters General configuration of the simulation\nThe user can set-up the following parameters throught the GUI controls.\nOutput-path: Character. Path of the output folder. This parameter cannot be empty, and the output folder must exist. Snapshots?: Boolean. If on, creates snapshots of the GUI to make videos. Snapshots-frequency: Character. Defines the frequency of snapshots. Only two options: \u0026ldquo;every year\u0026rdquo; and \u0026ldquo;every 10 years\u0026rdquo;. Draw-topography?: Boolean. If on, plots a shaded relief map (stored in topography.asc). RSAP-radius: Numeric[5, 50]. Radius of the RSAP in number of patches. Each patch is 200 x 200 m, so an RSAP-radius of 10 equals 2 kilometres. Randommness-settings: Character. Allows to choose between \u0026ldquo;fixed seed\u0026rdquo; to obtain deterministic results, or \u0026ldquo;free seed\u0026rdquo; to obtain different results on each run. Max-biomass-per-patch: Numeric, integer. Maximum charge capacity of a patch. Fire?: Boolean. If on, fires are produced whenever the data fire triggers a fire event. If off, fires are not produced (control simulation). Fire-probability-per-year: Numeric [0, 1]. Whenever the fire file provides a number higher than 0, if a random number in the range [0, 1] is lower than Fire-probability-per-year, a number of ignitions is computed (see below) and fires are triggered. Fire-ignitions-amplification-factor: Numeric The fire file provides values in the range [0, 1], and this multiplication factor converts these values in an integer number of ignitions. If fire equals one, and Fire-ignitions-amplification-factor equals 10, the number of ignitions will be 10 for the given year. Mortality?: Boolean. If on, mortality due to predation, plagues and other unpredictable sources is active (see Xx-seedling-mortality and Xx-adult-mortality parameters below). Burn-in-iterations: Numeric, integer. Number of years to run the model at a constant temperature (the initial one in the t_minimum_average file) and no fires to allow the population model to reach an equilibrium before to start the actual simulation. P.sylvestris?, P.uncinata?, B.pendula?, Q.petraea?, and C.avellana?: Boolean. If off, the given species is removed from the simulation. Used for testing purposes. Species traits\nEach species has a set of traits to be filled by the user. Note that a particular species can be removed from the simulation by switching it to \u0026ldquo;off\u0026rdquo;.\nXx-max-age: Numeric, integer. Maximum longevity. Every individual reaching this age is marked for decay. Xx-maturity-age: Numeric, integer. Age of sexual maturity. Individuals reaching this age are considered adults. Xx-pollen-productivity: Numeric. Multiplier of biomass to obtain a relative measure of pollen productivity among species. Xx-growth-rate: Numeric. Growth rate of the given species. Xx-max-biomass: Numeric, integer. Maximum biomass reachable by the given species. Xx-heliophilia: Numeric, [0, 1]. Dependance of the species on solar light to grow. It is used to compute the effect of competence in plant growth. Xx-seedling-tolerance: Numeric, integer. Numer of years a seedling can tolerate unsuitable climate. Xx-adult-tolerance: Numeric, integer. Numer of years an adult can tolerate unsuitable climate. Xx-seedling-mortality: Numeric, [0, 1]. Proportion of seedlings dying due to predation. Xx-adult-mortality: Numeric, [0, 1]. Proportion of adults dying due to plagues or other mortality sources. Xx-resprout-after-fire: Boolean. If 0 the species doesn\u0026rsquo;t show a post-fire response. If 1, growth-rate is multiplied by two in the resprouted individual to increase growth rate. Xx-min-temperature: Numeric. Minimum temperature at which the species has been found using GBIF presence data. Xx-max-temperature: Numeric. Maximum temperature at which the species has been found using GBIF presence data. Xx-min-slope: Numeric. Minimum topographic slope at which the species has been found. Xx-max-slope: Numeric. Maximum topographic slope at which the species has been found. Xx-intercept: Numeric. Intercept of the logistic equation to compute habitat suitability fitted to presence data and minimum temperature maps. Xx-coefficient: Numeric. Coefficient of the logistic equation to compute habitat suitability. ","date":1609545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609545600,"objectID":"ccb2f0c0f3fc235a1a29cfdb9e5f5669","permalink":"https://blasbenito.com/project/palaeo_fire_modeling/","publishdate":"2021-01-02T00:00:00Z","relpermalink":"/project/palaeo_fire_modeling/","section":"project","summary":"This is a spatio-temporal simulation of the effect of fire regimes on the population dynamics of five forest species during the Lateglacial-Holocene transition (15-7 cal Kyr BP) at El Portalet, a subalpine bog located in the central Pyrenees region (1802m asl, Spain)","tags":["ABM","Netlogo","Fire dynamics","Mechanistic simulation","Palaeoecology","Agent-based models"],"title":"Palaeo fire modeling","type":"project"},{"authors":["Antonio J. Pérez-Luque","Blas M. Benito","Francisco J. Bonet","Regino Zamora"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"34e3f18f11e96f8c35049f582c26df75","permalink":"https://blasbenito.com/publication/2020_perez-luque_forests/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/2020_perez-luque_forests/","section":"publication","summary":"Understanding the ecology of populations located in the rear edge of their distribution is key to assessing the response of the species to changing environmental conditions. Here, we focus on rear-edge populations of Quercus pyrenaica in Sierra Nevada (southern Iberian Peninsula) to analyze their ecological and floristic diversity. We perform multivariate analyses using high-resolution environmental information and forest inventories to determine how environmental variables differ among oak populations, and to identify population groups based on environmental and floristic composition.","tags":["Biogeography","Plant Ecology"],"title":"Ecological Diversity within Rear-Edge: A Case Study from Mediterranean Quercus pyrenaica Willd.","type":"publication"},{"authors":null,"categories":null,"content":"This Netlogo model simulates the dispersal of Quercus pyrenaica populations in Sierra Nevada (Spain) at a yearly resolution until 2100 while considering different levels of model complexity, from random dispersal and seedling establishment, to realistic dispersal based on the dispersal behavior of the Eurasian Jay.\nThe data required to run the model can be downloaded from here. It must be decompressed in the same folder containing the netlogo code of the model.\nThe video below shows the model in action for one population of Quercus pyrenaica. On the left, it shows the effect of a random dispersal model, and on the right, a realistic dispersal model based on observations of the dispersal behavior of the Eurasian Jay.\n","date":1609372800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609372800,"objectID":"e58a80cc3e1a80acfc33794a87e4e941","permalink":"https://blasbenito.com/project/quercus_range_shift/","publishdate":"2020-12-31T00:00:00Z","relpermalink":"/project/quercus_range_shift/","section":"project","summary":"Agent-based model coded with Netlogo to simulate range shift of *Quercus pyrenaica* populations in Sierra Nevada (Spain) using a realistic dispersal model with different levels of complexity.","tags":["ABM","Netlogo","Range-shift simulation","Mechanistic simulation","Dispersal","Agent-based models"],"title":"Range-shift simulation","type":"project"},{"authors":null,"categories":null,"content":" Note: to better follow this tutorial you can download the .Rmd file from here.\nIn a previous post I explained how to set up a small home cluster. Many things can be done with a cluster, and parallelizing loops is one of them. But there is no need of a cluster to parallelize loops and improve the efficiency of your coding!\nI believe that coding parallelized loops is an important asset for anyone working with R. That’s why this post covers the following topics:\nBeyond for: building loops with foreach. What is a parallel backend? Setup of a parallel backend for a single computer. Setup for a Beowulf cluster. Practical examples. Tuning of random forest hyperparameters. Confidence intervals of the importance scores of the predictors in random forest models. for loops are fine, but… Many experienced R users frequently say that nobody should write loops with R because they are tacky or whatever. However, I find loops easy to write, read, and debug, and are therefore my workhorse whenever I need to repeat a task and I don’t feel like using apply() and the likes. However, regular for loops in R are highly inefficient, because they only use one of your computer cores to perform the iterations.\nFor example, the for loop below sorts vectors of random numbers a given number of times, and will only work on one of your computer cores for a few seconds, while the others are there, procrastinating with no shame.\nWot Cpu GIF from Wot GIFs (gif kindly suggested by Andreas Angourakis)\nfor(i in 1:10000){ sort(runif(10000)) } If every i could run in a different core, the operation would indeed run a bit faster, and we would get rid of lazy cores. This is were packages like foreach and doParallel come into play. Let’s start installing these packages and a few others that will be useful throughout this tutorial.\n#automatic install of packages if they are not installed already list.of.packages \u0026lt;- c( \u0026quot;foreach\u0026quot;, \u0026quot;doParallel\u0026quot;, \u0026quot;ranger\u0026quot;, \u0026quot;palmerpenguins\u0026quot;, \u0026quot;tidyverse\u0026quot;, \u0026quot;kableExtra\u0026quot; ) new.packages \u0026lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\u0026quot;Package\u0026quot;])] if(length(new.packages) \u0026gt; 0){ install.packages(new.packages, dep=TRUE) } #loading packages for(package.i in list.of.packages){ suppressPackageStartupMessages( library( package.i, character.only = TRUE ) ) } #loading example data data(\u0026quot;penguins\u0026quot;) Beyond for: building loops with foreach The foreach package (the vignette is here) provides a way to build loops that support parallel execution, and easily gather the results provided by each iteration in the loop.\nFor example, this classic for loop computes the square root of the numbers 1 to 5 with sqrt() (the function is vectorized, but let’s conveniently forget that for a moment). Notice that I have to create a vector x to gather the results before executing the loop.\nx \u0026lt;- vector() for(i in 1:10){ x[i] \u0026lt;- sqrt(i) } x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 The foreach version returns a list with the results automatically. Notice that %do% operator after the loop definition, I’ll talk more about it later.\nx \u0026lt;- foreach(i = 1:10) %do% { sqrt(i) } x ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 1.414214 ## ## [[3]] ## [1] 1.732051 ## ## [[4]] ## [1] 2 ## ## [[5]] ## [1] 2.236068 ## ## [[6]] ## [1] 2.44949 ## ## [[7]] ## [1] 2.645751 ## ## [[8]] ## [1] 2.828427 ## ## [[9]] ## [1] 3 ## ## [[10]] ## [1] 3.162278 We can use the .combine argument of foreach to arrange the list as a vector. Other options such as cbind, rbind, or even custom functions can be used as well, only depending on the structure of the output of each iteration.\nx \u0026lt;- foreach( i = 1:10, .combine = \u0026#39;c\u0026#39; ) %do% { sqrt(i) } x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 Another interesting capability of foreach is that it supports several iterators of the same length at once. Notice that the values of the iterators are not combined. When the first value of one iterator is being used, the first value of the other iterators will be used as well.\nx \u0026lt;- foreach( i = 1:3, j = 1:3, k = 1:3, .combine = \u0026#39;c\u0026#39; ) %do% { i + j + k } x ## [1] 3 6 9 Running foreach loops in parallel The foreach loops shown above use the operator %do%, that processes the tasks sequentially. To run tasks in parallel, foreach uses the operator %dopar%, that has to be supported by a parallel backend. If there is no parallel backend, %dopar% warns the user that it is being run sequentially, as shown below. But what the heck is a parallel backend?\nx \u0026lt;- foreach( i = 1:10, .combine = \u0026#39;c\u0026#39; ) %dopar% { sqrt(i) } ## Warning: executing %dopar% sequentially: no parallel backend registered x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 What is a parallel backend? When running tasks in parallel, there should be a director node that tells a group of workers what to do with a given set of data and functions. The workers execute the iterations, and the director manages execution and gathers the results provided by the workers. A parallel backend provides the means for the director and workers to communicate, while allocating and managing the required computing resources (processors, RAM memory, and network bandwidth among others).\nThere are two types of parallel backends that can be used with foreach, FORK and PSOCK.\nFORK FORK backends are only available on UNIX machines (Linux, Mac, and the likes), and do not work in clusters [sad face], so only single-machine environments are appropriate for this backend. In a FORK backend, the workers share the same environment (data, loaded packages, and functions) as the director. This setup is highly efficient because the main environment doesn’t have to be copied, and only worker outputs need to be sent back to the director.\nPSOCK PSOCK backends (Parallel Socket Cluster) are available for both UNIX and WINDOWS systems, and are the default option provided with foreach. As their main disadvantage, the environment of the director needs to be copied to the environment of each worker, which increases network overhead while decreasing the overall efficiency of the cluster. By default, all the functions available in base R are copied to each worker, and if a particular set of R packages are needed in the workers, they need to be copied to the respective environments of the workers as well.\nThis post compares both backends and concludes that FORK is about a 40% faster than PSOCK.\nSetup of a parallel backend Here I explain how to setup the parallel backend for a simple computer and for a Beowulf cluster as the one I described in a previous post.\nSetup for a single computer Setting up a cluster in a single computer requires first to find out how many cores we want to use from the ones we have available. It is recommended to leave one free core for other tasks.\nparallel::detectCores() ## [1] 8 n.cores \u0026lt;- parallel::detectCores() - 1 Now we need to define the cluster with parallel::makeCluster() and register it so it can be used by %dopar% with doParallel::registerDoParallel(my.cluster). The type argument of parallel::makeCluster() accepts the strings “PSOCK” and “FORK” to define the type of parallel backend to be used.\n#create the cluster my.cluster \u0026lt;- parallel::makeCluster( n.cores, type = \u0026quot;PSOCK\u0026quot; ) #check cluster definition (optional) print(my.cluster) ## socket cluster with 7 nodes on host \u0026#39;localhost\u0026#39; #register it to be used by %dopar% doParallel::registerDoParallel(cl = my.cluster) #check if it is registered (optional) foreach::getDoParRegistered() ## [1] TRUE #how many workers are available? (optional) foreach::getDoParWorkers() ## [1] 7 Now we can run a set of tasks in parallel!\nx \u0026lt;- foreach( i = 1:10, .combine = \u0026#39;c\u0026#39; ) %dopar% { sqrt(i) } x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 If everything went well, now %dopar% should not be throwing the warning executing %dopar% sequentially: no parallel backend registered, meaning that the parallel execution is working as it should. In this little example there is no gain in execution speed, because the operation being executed is extremely fast, but this will change when the operations running inside of the loop take longer times to run.\nFinally, it is always recommendable to stop the cluster when we are done working with it.\nparallel::stopCluster(cl = my.cluster) Setup for a Beowulf cluster This setup is a bit more complex, because it requires to open a port in every computer of the cluster. Ports are virtual communication channels, and are identified by a number.\nFirst, lets tell R what port we want to use:\n#define port Sys.setenv(R_PARALLEL_PORT = 11000) #check that it Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;) Now, we need to open the selected port in every computer of the network. In Linux we need to setup the firewall to allow connections from the network 10.42.1.0/24 (replace this with your network range if different!) to the port 11000 by splitting the window of the Terminator console in as many computers available in your network (the figure below shows three, one for my PC and two for my Intel NUCs), opening an ssh session on each remote machine, and setting Terminator with Grouping equal to Broadcast all so we only need to type the commands once.\nOpening port 11000 in three computers at once with Terminator\nNow we have to create an object defining the IPs of the computers in the network, the number of cores to use from each computer, the user name, and the identity of the director. This will be the spec argument required by parallel::makeCluster() to create the cluster throughtout the machines in the network. It is a list of lists, with as many lists as nodes are defined. Each sub-list has a slot named host with the IP of the computer where the given node is, and user, with the name of the user in each computer.\nThe code below shows how this would be done, step by step. Yes, this is CUMBERSOME.\n#main parameters director \u0026lt;- \u0026#39;10.42.0.1\u0026#39; nuc2 \u0026lt;- \u0026#39;10.42.0.34\u0026#39; nuc1 \u0026lt;- \u0026#39;10.42.0.104\u0026#39; user \u0026lt;- \u0026quot;blas\u0026quot; #list of machines, user names, and cores spec \u0026lt;- list( list( host = director, user = user, ncore = 7 ), list( host = nuc1, user = user, ncore = 4 ), list( host = nuc2, user = user, ncore = 4 ) ) #generating nodes from the list of machines spec \u0026lt;- lapply( spec, function(spec.i) rep( list( list( host = spec.i$host, user = spec.i$user) ), spec.i$ncore ) ) #formating into a list of lists spec \u0026lt;- unlist( spec, recursive = FALSE ) Generating the spec definition is a bit easier with the function below.\n#function to generate cluster specifications from a vector of IPs, a vector with the number of cores to use on each IP, and a user name cluster_spec \u0026lt;- function( ips, cores, user ){ #creating initial list spec \u0026lt;- list() for(i in 1:length(ips)){ spec[[i]] \u0026lt;- list() spec[[i]]$host \u0026lt;- ips[i] spec[[i]]$user \u0026lt;- user spec[[i]]$ncore \u0026lt;- cores[i] } #generating nodes from the list of machines spec \u0026lt;- lapply( spec, function(spec.i) rep( list( list( host = spec.i$host, user = spec.i$user) ), spec.i$ncore ) ) #formating into a list of lists spec \u0026lt;- unlist( spec, recursive = FALSE ) return(spec) } This function is also available in this GitHub Gist, so you can load it into your R environment by executing:\nsource(\u0026quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R\u0026quot;) Below I use it to generate the input to the spec argument to start the cluster with parallel::makeCluster(). Notice that I have added several arguments.\nThe argument outfile determines where the workers write a log. In this case it is set to nowhere with the double quotes, but the path to a text file in the director could be provided here. The argument homogeneous = TRUE indicates that all machines have the Rscript in the same location. In this case all three machines have it at “/usr/lib/R/bin/Rscript”. Otherwise, set it up to FALSE. #generate cluster specification spec \u0026lt;- cluster_spec( ips = c(\u0026#39;10.42.0.1\u0026#39;, \u0026#39;10.42.0.34\u0026#39;, \u0026#39;10.42.0.104\u0026#39;), cores = c(7, 4, 4), user = \u0026quot;blas\u0026quot; ) #setting up cluster my.cluster \u0026lt;- parallel::makeCluster( master = \u0026#39;10.42.0.1\u0026#39;, spec = spec, port = Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;), outfile = \u0026quot;\u0026quot;, homogeneous = TRUE ) #check cluster definition (optional) print(my.cluster) #register cluster doParallel::registerDoParallel(cl = my.cluster) #how many workers are available? (optional) foreach::getDoParWorkers() Now we can use the cluster to execute a dummy operation in parallel using all machines in the network.\nx \u0026lt;- foreach( i = 1:20, .combine = \u0026#39;c\u0026#39; ) %dopar% { sqrt(i) } x Once everything is done, remember to close the cluster.\nparallel::stopCluster(cl = my.cluster) Practical examples In this section I cover two examples on how to use parallelized loops to explore model outputs:\nTuning random forest hyperparameters to maximize classification accuracy. Obtain a confidence interval for the importance score of each predictor from a set random forest models fitted with ranger(). In the examples I use the penguins data from the palmerpenguins package to fit classification models with random forest using species as a response, and bill_length_mm, bill_depth_mm, flipper_length_mm, and body_mass_g as predictors.\n#removing NA and subsetting columns penguins \u0026lt;- as.data.frame( na.omit( penguins[, c( \u0026quot;species\u0026quot;, \u0026quot;bill_length_mm\u0026quot;, \u0026quot;bill_depth_mm\u0026quot;, \u0026quot;flipper_length_mm\u0026quot;, \u0026quot;body_mass_g\u0026quot; )] ) ) species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g Adelie 39.1 18.7 181 3750 Adelie 39.5 17.4 186 3800 Adelie 40.3 18.0 195 3250 Adelie 36.7 19.3 193 3450 Adelie 39.3 20.6 190 3650 Adelie 38.9 17.8 181 3625 Adelie 39.2 19.6 195 4675 Adelie 34.1 18.1 193 3475 Adelie 42.0 20.2 190 4250 Adelie 37.8 17.1 186 3300 Adelie 37.8 17.3 180 3700 Adelie 41.1 17.6 182 3200 Adelie 38.6 21.2 191 3800 Adelie 34.6 21.1 198 4400 Adelie 36.6 17.8 185 3700 Adelie 38.7 19.0 195 3450 Adelie 42.5 20.7 197 4500 Adelie 34.4 18.4 184 3325 Adelie 46.0 21.5 194 4200 Adelie 37.8 18.3 174 3400 We’ll fit random forest models with the ranger package, which works as follows:\n#fitting classification model m \u0026lt;- ranger::ranger( data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, importance = \u0026quot;permutation\u0026quot; ) #summary m ## Ranger result ## ## Call: ## ranger::ranger(data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, importance = \u0026quot;permutation\u0026quot;) ## ## Type: Classification ## Number of trees: 500 ## Sample size: 342 ## Number of independent variables: 4 ## Mtry: 2 ## Target node size: 1 ## Variable importance mode: permutation ## Splitrule: gini ## OOB prediction error: 2.34 % #variable importance m$variable.importance ## bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## 0.30464149 0.16554689 0.22329574 0.07775624 The output shows that the percentage of misclassified cases is 2.34, and that bill_length_mm is the variable that contributes the most to the accuracy of the classification.\nIf you are not familiar with random forest, this post and the video below do a pretty good job in explaining the basics:\nTuning random forest hyperparameters Random forest has several hyperparameters that influence model fit:\nnum.trees is the total number of trees to fit. The default value is 500. mtry is the number of variables selected by chance (from the total pool of variables) as candidates for a tree split. The minimum is 2, and the maximum is the total number of predictors. min.node.size is the minimum number of cases that shall go together in the terminal nodes of each tree. For classification models as the ones we are going to fit, 1 is the minimum. Here we are going to explore how combinations of these values increase or decrease the prediction error of the model (percentage of misclassified cases) on the out-of-bag data (not used to train each decision tree). This operation is usually named grid search for hyperparameter optimization.\nTo create these combinations of hyperparameters we use expand.grid().\nsensitivity.df \u0026lt;- expand.grid( num.trees = c(500, 1000, 1500), mtry = 2:4, min.node.size = c(1, 10, 20) ) num.trees mtry min.node.size 500 2 1 1000 2 1 1500 2 1 500 3 1 1000 3 1 1500 3 1 500 4 1 1000 4 1 1500 4 1 500 2 10 1000 2 10 1500 2 10 500 3 10 1000 3 10 1500 3 10 500 4 10 1000 4 10 1500 4 10 500 2 20 1000 2 20 1500 2 20 500 3 20 1000 3 20 1500 3 20 500 4 20 1000 4 20 1500 4 20 Each row in sensitivity.df corresponds to a combination of parameters to test, so there are 27 models to fit. The code below prepares the cluster, and uses the ability of foreach to work with several iterators at once to easily introduce the right set of hyperparameters to each fitted model.\nNotice how in the foreach definition I use the .packages argument to export the ranger package to the environments of the workers.\n#create and register cluster my.cluster \u0026lt;- parallel::makeCluster(n.cores) doParallel::registerDoParallel(cl = my.cluster) #fitting each rf model with different hyperparameters prediction.error \u0026lt;- foreach( num.trees = sensitivity.df$num.trees, mtry = sensitivity.df$mtry, min.node.size = sensitivity.df$min.node.size, .combine = \u0026#39;c\u0026#39;, .packages = \u0026quot;ranger\u0026quot; ) %dopar% { #fit model m.i \u0026lt;- ranger::ranger( data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, num.trees = num.trees, mtry = mtry, min.node.size = min.node.size ) #returning prediction error as percentage return(m.i$prediction.error * 100) } #adding the prediction error column sensitivity.df$prediction.error \u0026lt;- prediction.error To plot the results:\nggplot2::ggplot(data = sensitivity.df) + ggplot2::aes( x = mtry, y = as.factor(min.node.size), fill = prediction.error ) + ggplot2::facet_wrap(as.factor(num.trees)) + ggplot2::geom_tile() + ggplot2::scale_y_discrete(breaks = c(1, 10, 20)) + ggplot2::scale_fill_viridis_c() + ggplot2::ylab(\u0026quot;min.node.size\u0026quot;) The figure shows that combinations of lower values of min.node.size and mtry generally lead to models with a lower prediction error across different numbers of trees. Retrieving the first line of sensitivity.df ordered by ascending prediction.error will give us the values of the hyperparameters we need to use to reduce the prediction error as much as possible.\nbest.hyperparameters \u0026lt;- sensitivity.df %\u0026gt;% dplyr::arrange(prediction.error) %\u0026gt;% dplyr::slice(1) num.trees mtry min.node.size prediction.error 500 2 1 2.339181 Confidence intervals of variable importance scores Random forest has an important stochastic component during model fitting, and as consequence, the same model will return slightly different results in different runs (unless set.seed() or the seed argument of ranger are used). This variability also affects the importance scores of the predictors, and can be use to our advantage to assess whether the importance scores of different variables do really overlap or not.\nI have written a little function to transform the vector of importance scores returned by ranger into a data frame (of one row). It helps arranging the importance scores of different runs into a long format, which helps a lot to plot a boxplot with ggplot2 right away. This function could have been just some code thrown inside the foreach loop, but I want to illustrate how foreach automatically transfers functions available in the R environment into the environments of the workers when required, without the intervention of the user. The same will happen with the best.hyperparameters tiny data frame we created in the previous section.\nimportance_to_df \u0026lt;- function(model){ x \u0026lt;- as.data.frame(model$variable.importance) x$variable \u0026lt;- rownames(x) colnames(x)[1] \u0026lt;- \u0026quot;importance\u0026quot; rownames(x) \u0026lt;- NULL return(x) } The code chunk below setups the cluster and runs 1000 random forest models in parallel (using the best hyperparameters computed in the previous section) while using system.time() to assess running time.\n#we don\u0026#39;t need to create the cluster, it is still up print(my.cluster) ## socket cluster with 7 nodes on host \u0026#39;localhost\u0026#39; #assessing execution time system.time( #performing 1000 iterations in parallel importance.scores \u0026lt;- foreach( i = 1:1000, .combine = \u0026#39;rbind\u0026#39;, .packages = \u0026quot;ranger\u0026quot; ) %dopar% { #fit model m.i \u0026lt;- ranger::ranger( data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, importance = \u0026quot;permutation\u0026quot;, mtry = best.hyperparameters$mtry, num.trees = best.hyperparameters$num.trees, min.node.size = best.hyperparameters$min.node.size ) #format importance m.importance.i \u0026lt;- importance_to_df(model = m.i) #returning output return(m.importance.i) } ) ## user system elapsed ## 0.267 0.027 6.556 The output of system.time() goes as follows:\nuser: seconds the R session has been using the CPU. system: seconds the operating system has been using the CPU. elapsed: the total execution time experienced by the user. This will make sense in a minute. In the meantime, let’s plot our results!\nggplot2::ggplot(data = importance.scores) + ggplot2::aes( y = reorder(variable, importance), x = importance ) + ggplot2::geom_boxplot() + ggplot2::ylab(\u0026quot;\u0026quot;) The figure shows that the variable bill_length_mm is the most important in helping the model classifying penguin species, with no overlap with any other variable. In this particular case, since the distributions of the importance scores do not overlap, this analysis isn’t truly helpful, but now you know how to do it!\nI assessed the running time with system.time() because ranger() can run in parallel by itself just by setting the num.threads argument to the number of cores available in the machine. This capability cannot be used when executing ranger() inside a parallelized foreach loop though, and it is only useful inside classic for loops.\nWhat option is more efficient then? The code below executes a regular for loop running the function sequentially to evaluate whether it is more efficient to run ranger() in parallel using one core per model, as we did above, or sequentially while using several cores per model on each iteration.\n#list to save results importance.scores.list \u0026lt;- list() #performing 1000 iterations sequentially system.time( for(i in 1:1000){ #fit model m.i \u0026lt;- ranger::ranger( data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, importance = \u0026quot;permutation\u0026quot;, seed = i, num.threads = parallel::detectCores() - 1 ) #format importance importance.scores.list[[i]] \u0026lt;- importance_to_df(model = m.i) } ) ## user system elapsed ## 43.663 2.815 12.948 As you can see, ranger() takes longer to execute in a regular for loop using several cores at once than in a parallel foreach loop using one core at once. That’s a win for the parallelized loop!\nWe can stop our cluster now, we are done with it.\nparallel::stopCluster(cl = my.cluster) A few things to take in mind As I have shown in this post, using parallelized foreach loops can accelerate long computing processes, even when some functions have the ability to run in parallel on their own. However, there are things to take in mind, that might vary depending on whether we are executing the parallelized task on a single computer or on a small cluster.\nIn a single computer, the communication between workers and the director is usually pretty fast, so there are no obvious bottlenecks to take into account here. The only limitation that might arise comes from the availability of RAM memory. For example, if a computer has 8 cores and 8GB of RAM, less than 1GB of RAM will be available for each worker. So, if you need to repeat a process that consumes a significant amount of RAM, the ideal number of cores running in parallel might be lower than the total number of cores available in your system. Don’t be greedy, and try to understand the capabilities of your machine while designing a parallelized task.\nWhen running foreach loops as in x \u0026lt;- foreach(...){...}, the variable x is receiving whatever results the workers are producing. For example, if you are only returning the prediction error of a model, or its importance scores, x will have a very manageable size. But if you are returning heavy objects such as complete random forest models, the size of x is going to grow VERY FAST, and at the end it will be competing for RAM resources with the workers, which might even crash your R session. Again, don’t be greedy, and size your outputs carefully.\nClusters spanning several computers are a different beast, since the workers and the director communicate through a switch and network wires and interfaces. If the amount of data going to and coming from the workers is large, the network can get clogged easily, reducing the cluster’s efficiency drastically. In general, if the amount of data produced by a worker on each iteration takes longer to arrive to the director than the time it takes the worker to produce it, then a cluster is not going to be more efficient than a single machine. But this is not important if you don’t care about efficiency.\nOther issues you might come across while parallelizing tasks in R are thoroughly commented in this post, by Imre Gera.\nThat’s all for now folks, happy parallelization!\n","date":1608940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608940800,"objectID":"258c4372b0dc8ca13f2e28fc8d34faa5","permalink":"https://blasbenito.com/post/02_parallelizing_loops_with_r/","publishdate":"2020-12-26T00:00:00Z","relpermalink":"/post/02_parallelizing_loops_with_r/","section":"post","summary":"Note: to better follow this tutorial you can download the .Rmd file from here.\nIn a previous post I explained how to set up a small home cluster. Many things can be done with a cluster, and parallelizing loops is one of them.","tags":null,"title":"Parallelized loops with R","type":"post"},{"authors":null,"categories":null,"content":" The package distantia allows to measure the dissimilarity between multivariate time-series. The package assumes that the target sequences are ordered along a given dimension, being depth and time the most common ones, but others such as latitude or elevation are also possible. Furthermore, the target time-series can be regular or irregular, and have their samples aligned (same age/time/depth) or unaligned (different age/time/depth). The only requirement is that the sequences must have at least two (but ideally more) columns with the same name and units representing different variables relevant to the dynamics of a system of interest.\nThe GitHub page of the project contains a thorough explanation of the statistics behind the method. The paper published in the Ecography journal describes the method, the package, and a couple of practical examples. The code and data used to develop the examples can be found in GitHub and Zenodo.\nPlease, if you find this package useful, please cite it as:\nBenito, B.M. and Birks, H.J.B. (2020), distantia: an open‐source toolset to quantify dissimilarity between multivariate ecological time‐series. Ecography, 43: 660-667. https://doi.org/10.1111/ecog.04895\n","date":1608336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608336000,"objectID":"a64ca32d94825ca0e1d3efdede85f5d0","permalink":"https://blasbenito.com/project/distantia/","publishdate":"2020-12-19T00:00:00Z","relpermalink":"/project/distantia/","section":"project","summary":"R package to compare multivariate time-series.","tags":["R packages","Time Series Analysis"],"title":"R package distantia","type":"project"},{"authors":null,"categories":null,"content":" The goal of memoria is to provide the tools to quantify ecological memory in long time-series involving environmental drivers and biotic responses, including palaeoecological datasets.\nEcological memory has two main components: the endogenous component, which represents the effect of antecedent values of the response on itself, and endogenous component, which represents the effect of antecedent values of the driver or drivers on the current state of the biotic response. Additionally, the concurrent effect, which represents the synchronic effect of the environmental drivers over the response is measured. The functions in the package allow the user\nThe package memoria uses the fast implementation of Random Forest available in the ranger package to fit a model of the form shown in Equation 1:\nEquation 1 (simplified from the one in the paper): $$p_{t} = p_{t-1} +\u0026hellip;+ p_{t-n} + d_{t} + d_{t-1} +\u0026hellip;+ d_{t-n}$$\nWhere:\n$p$ is the response variable, Pollen counts were used in this particular case.. $d$ is an environmental Driver influencing the response variable. $t$ is the time of any given value of the response $p$. $t-1$ is the lag 1. $p_{t-1} +\u0026hellip;+ p_{t-n}$ represents the endogenous component of ecological memory. $d_{t-1} +\u0026hellip;+ d_{t-n}$ represents the exogenous component of ecological memory. $d_{t}$ represents the concurrent effect of the driver over the response. Random Forest returns an importance score for each model term, and the functions in memoria let the user to plot the importance scores across time lags for each ecological memory components, and to compute different features of each memory component (length, strength, and dominance).\nThe GitHub page of the package features complete examples on how to use the package. The paper published in the Ecography journal describes ecological memory concepts and the method based on Random Forest used to assess ecological memory components. The code used to generate the supplementary materials can be found in GitHub and Zenodo.\nIf you ever use the package, please, cite it as:\nBenito, B.M., Gil‐Romera, G. and Birks, H.J.B. (2020), Ecological memory at millennial time‐scales: the importance of data constraints, species longevity and niche features. Ecography, 43: 1-10. https://doi.org/10.1111/ecog.04772\n","date":1608336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608336000,"objectID":"b2480e8e20be7bd9954b85358f24acc6","permalink":"https://blasbenito.com/project/memoria/","publishdate":"2020-12-19T00:00:00Z","relpermalink":"/project/memoria/","section":"project","summary":"R package to assess ecological memory in multivariate time-series.","tags":["R packages","Ecological Memory","Time Series Analysis","Machine Learning","Random Forest"],"title":"R package memoria","type":"project"},{"authors":null,"categories":null,"content":" The goal of virtualPollen is to provide the tools to simulate pollen curves over millenial time-scales generated by virtual taxa with different life traits (life-span, fecundity, growth-rate) and niche features (niche position and breadth) as a response to virtual environmental drivers with a given temporal autocorrelation. It furthers allow to simulate specific data properties of fossil pollen datasets, such as sediment accumulation rate, and depth intervals between consecutive pollen samples. The simulation outcomes are useful to better understand the role of plant traits, niche properties, and climatic variability in defining the shape of pollen curves.\nThe GitHub page of the package offers a complete tutorial on how to use the package. The paper published in the Ecography journal describes the foundations of the model in brief.\nIf you ever use the package, please, cite it as:\nBenito, B.M., Gil‐Romera, G. and Birks, H.J.B. (2020), Ecological memory at millennial time‐scales: the importance of data constraints, species longevity and niche features. Ecography, 43: 1-10. https://doi.org/10.1111/ecog.04772\n","date":1608336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608336000,"objectID":"f3afdae9d91edee9e3abcdfb020c263f","permalink":"https://blasbenito.com/project/virtualpollen/","publishdate":"2020-12-19T00:00:00Z","relpermalink":"/project/virtualpollen/","section":"project","summary":"R package to simulate pollen production of mono-specific tree populations over millennia.","tags":["R packages","Time Series Analysis","Palaeoecology","Mechanistic simulation"],"title":"R package virtualPollen","type":"project"},{"authors":null,"categories":null,"content":"In this post I explain how to setup a small Beowulf cluster with a personal PC running Ubuntu 20.04 and a couple of Intel NUCs running Ubuntu Server 20.04, with the end-goal of parallelizing R tasks.\nThe topics I cover here are:\nRequired material Network setting Installing the secure shell protocol Installing Ubuntu server in the NUCs Installing R in the NUCs Managing the cluster\u0026rsquo;s network Preamble I have a little but nice HP ENVY model TE01-0008ns with 32 GB RAM, 8 CPUs, and 3TB of hard disk running Ubuntu 20.04 that I use to do all my computational work (and most of my tweeting). A few months ago I connected it with my two laptops (one of them deceased now, RIP my dear skynet) to create a little cluster to run parallel tasks in R.\nIt was just a draft cluster running on a wireless network, but it served me to think about getting a more permanent solution not requiring two additional laptops in my desk.\nThat\u0026rsquo;s were the nice INTEL NUCs (from Next Unit of Computing) come into play. NUCs are full-fledged computers fitted in small boxes usually sold without RAM memory sticks and no hard disk (hence the term barebone). Since they have a low energy consumption footprint, I thought these would be ideal units for my soon-to-be home cluster.\nMaterial I gifted myself with:\n2 Intel Barebone BOXNUC6CAYH, each with 4 cores, and a maximum RAM memory of 32GB (you might read they only accept 8GB, but that\u0026rsquo;s not the case anymore). Notice that these NUCs aren\u0026rsquo;t state-of-the-art now, they were released by the end of 2016. 2 Hard disks SSD 2.5\u0026quot; Western Digital WDS250G2B0A WD Blue (250GB) 4 Crucial CT102464BF186D DDR3 SODIMM (204 pins) RAM sticks with 8GB each. 1 ethernet switch Netgear GS308-300PES with 8 ports. 3 ethernet wires NanoCable 10.20.0400-BL of cat 6 quality. The whole set came to cost around 530€, but please notice that I had a clear goal in mind: \u0026ldquo;duplicating\u0026rdquo; my computing power with the minimum number of NUCs, while preserving a share of 4GB of RAM memory per CPU throughout the cluster (based on the features of my desk computer). A more basic setting with more modest NUCs and smaller RAM would cost half of that.\nThis instructive video by David Harry shows how to install the SSD and the RAM sticks in an Intel NUC. It really takes 5 minutes tops, one only has to be a bit careful with the RAM sticks, the pins need to go all the way in into their slots before securing the sticks in place.\nNetwork settings Before starting to install an operating system in the NUCS, the network setup goes as follows:\nMy desktop PC is connected to a router via WIFI and dynamic IP (DHCP). The PC and each NUC are connected to the switch with cat6 ethernet wires. To share my PC\u0026rsquo;s WIFI connection with the NUCs I have to prepare a new connection profile with the command line tool of Ubuntu\u0026rsquo;s NetworkManager, named nmcli, as follows.\nFirst, I need to find the name of my ethernet interface by checking the status of my network devices with the command line.\nnmcli device status DEVICE TYPE STATE CONNECTION wlp3s0 wifi connected my_wifi enp2s0 ethernet unavailable -- lo loopback unmanaged -- There I can see that my ethernet interface is named enp2s0.\nSecond, I have to configure the shared connection.\nnmcli connection add type ethernet ifname enp2s0 ipv4.method shared con-name cluster Were ifname enp2s0 is the name of the interface I want to use for the new connection, ipv4.method shared is the type of connection, and con-name cluster is the name I want the connection to have. This operation adds firewall rules to manage traffic within the cluster network, starts a DHCP server in the computer that serves IPs to the NUCS, and a DNS server that allows the NUCs to translate internet addresses.\nAfter turning on the switch, I can check the connection status again with\nnmcli device status DEVICE TYPE STATE CONNECTION enp2s0 ethernet connected cluster wlp3s0 wifi connected my_wifi lo loopback unmanaged -- When checking the IP of the device with bash ifconfig it should yield 10.42.0.1. Any other computer in the cluster network will have a dynamic IP in the range 10.42.0.1/24.\nFurther details about how to set a shared connection with NetworkManager can be found in this nice post by Beniamino Galvani.\nSSH setup My PC, as the director of the cluster, needs an SSH client running, while the NUCs need an SSH server. SSH (Secure Shell) is a remote authentication protocol that allows secure connections to remote servers that I will be using all the time to manage the cluster. To install, run, and check its status I just have to run these lines in the console:\nsudo apt install ssh sudo systemctl enable --now ssh sudo systemctl status ssh Now, a secure certificate of the identity of a given computer, named ssh-key, that grants access to remote ssh servers and services needs to be generated.\nssh-keygen \u0026quot;label\u0026quot; Here, substitute \u0026ldquo;label\u0026rdquo; by the name of the computer to be used as cluster\u0026rsquo;s \u0026ldquo;director\u0026rdquo;. The system will ask for a file name and a passphrase that will be used to encrypt the ssh-key.\nThe ssh-key needs to be added to the ssh-agent.\nssh-add ~/.ssh/id_rsa To copy the ssh-key to my GitHub account, I have to copy the contents of the file ~/.ssh/id_rsa.pub (can be done just opening it with gedit ~/.ssh/id_rsa.pub + Ctrl + a + Ctrl + c), and paste it on GitHub account \u0026gt; Settings \u0026gt; SSH and GPG keys \u0026gt; New SSH Key (green button in the upper right part of the window).\nNote: If you don\u0026rsquo;t use GitHub, you\u0026rsquo;ll need to copy your ssh-key to the NUCs once they are up and running with ssh-copy-id -i ~/.ssh/id_rsa.pub user_name@nuc_IP.\nInstalling and preparing ubuntu server in each NUC The NUCs don\u0026rsquo;t need to waste resources in a user graphical interface I won\u0026rsquo;t be using whatsoever. Since they will work in a headless configuration once the cluster is ready, a Linux distro without graphical user interface such as Ubuntu server is the way to go.\nInstalling Ubuntu server First it is important to connect a display, a keyboard, and a mouse to the NUC in preparation, and turn it on while pushing F2 to start the visual BIOS. These BIOS parameters need to be modified:\nAdvanced (upper right) \u0026gt; Boot \u0026gt; Boot Configuration \u0026gt; UEFI Boot \u0026gt; OS Selection: Linux Advanced \u0026gt; Boot \u0026gt; Boot Configuration \u0026gt; UEFI Boot \u0026gt; OS Selection: mark \u0026ldquo;Boot USB Devices First\u0026rdquo;. [optional] Advanced \u0026gt; Power \u0026gt; Secondary Power Settings \u0026gt; After Power Failure: \u0026ldquo;Power On\u0026rdquo;. I have the switch and nucs connected to an outlet plug extender with an interrupter. When I switch it on, the NUCs (and the switch) boot automatically after this option is enabled, so I only need to push one button to power up the cluster. F10 to save, and shutdown. To prepare the USB boot device with Ubuntu server 20.04 I first download the .iso from here, by choosing \u0026ldquo;Option 3\u0026rdquo;, which leads to the manual install. Once the .iso file is downloaded, I use Ubuntu\u0026rsquo;s Startup Disk Creator to prepare a bootable USB stick. Now I just have to plug the stick in the NUC and reboot it.\nThe Ubuntu server install is pretty straightforward, and only a few things need to be decided along the way:\nAs user name I choose the same I have in my personal computer. As name for the NUCs I choose \u0026ldquo;nuc1\u0026rdquo; and \u0026ldquo;nuc2\u0026rdquo;, but any other option will work well. As password, for comfort I use the same I have in my personal computer. During the network setup, choose DHCP. If the network is properly configured and the switch is powered on, after a few seconds the NUC will acquire an IP in the range 10.42.0.1/24, as any other machine within the cluster network. When asked, mark the option \u0026ldquo;Install in the whole disk\u0026rdquo;, unless you have other plans for your NUC. Mark \u0026ldquo;Install OpenSSH\u0026rdquo;. Provide it with your GitHub user name if you have your ssh-key there, and it will download it right away, facilitating a lot the ssh setup. Reboot once the install is completed. Now I keep configuring the NUC\u0026rsquo;s operating system from my PC through ssh.\nConfiguring a NUC First, to learn the IP of the NUC:\nsudo arp-scan 10.42.0.1/24 Other alternatives to this command are arp -a and sudo arp-scan -I enp2s0 --localnet. Once I learn the IP of the NUC, I add it to the file etc/hosts of my personal computer as follows.\nFirst I open the file as root.\nsudo gedit /etc/hosts Add a new line there: 10.42.0.XXX nuc1 and save the file.\nNow I access the NUC trough ssh to keep preparing it without a keyboard and a display. I do it from Tilix, that allows to open different command line tabs in the same window, which is quite handy to manage several NUCs at once.\nAnother great option to manage the NUCs through ssh is terminator, that allows to broadcast the same commands to several ssh sessions at once. I have been trying it, and it is much better for cluster management purposes than Tilix. Actually, using it would simplify this workflow a lot, because once Ubuntu server is installed on each NUC, the rest of the configuration commands can be broadcasted at once to both NUCs. It\u0026rsquo;s a bummer I discovered this possibility way too late!\nssh blas@10.42.0.XXX The NUC\u0026rsquo;s operating system probably has a bunch of pending software updates. To install these:\nsudo apt-get upgrade Now I have to install a set of software packages that will facilitate managing the cluster\u0026rsquo;s network and the NUC itself.\nsudo apt install net-tools arp-scan lm-sensors dirmngr gnupg apt-transport-https ca-certificates software-properties-common samba libopenmpi3 libopenmpi-dev openmpi-bin openmpi-common htop Setting the system time To set the system time of the NUC to the same you have in your computer, just repeat these steps in every computer in the cluster network.\n#list time zones: timedatectl list-timezones #set time zone sudo timedatectl set-timezone Europe/Madrid #enable timesyncd sudo timedatectl set-ntp on Setting the locale The operating systems of the NUCs and the PC need to have the same locale. It can be set by editing the file /etc/default/locale with either nano (in the NUCS) or gedit (in the PC) and adding these lines, just replacing en_US.UTF-8 with your preferred locale.\nLANG=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLANGUAGE=\u0026ldquo;en_US:en\u0026rdquo;\nLC_NUMERIC=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_TIME=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_MONETARY=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_PAPER=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_IDENTIFICATION=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_NAME=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_ADDRESS=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_TELEPHONE=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_MEASUREMENT=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nTemperature monitoring NUCs are prone to overheating when under heavy loads for prolonged times. Therefore, monitoring the temperature of the NUCs CPUs is kinda important. In a step before I installed lm-sensors in the NUC, which provides the tools to do so. To setup the sensors from an ssh session in the NUC:\nsudo sensors-detect The program will request permission to find sensors in the NUC. I answered \u0026ldquo;yes\u0026rdquo; to every request. Once all sensors are identified, to check them\nsensors iwlwifi_1-virtual-0 Adapter: Virtual device temp1: N/A acpitz-acpi-0 Adapter: ACPI interface temp1: +32.0°C (crit = +100.0°C) coretemp-isa-0000 Adapter: ISA adapter Package id 0: +30.0°C (high = +105.0°C, crit = +105.0°C) Core 0: +30.0°C (high = +105.0°C, crit = +105.0°C) Core 1: +30.0°C (high = +105.0°C, crit = +105.0°C) Core 2: +29.0°C (high = +105.0°C, crit = +105.0°C) Core 3: +30.0°C (high = +105.0°C, crit = +105.0°C) which gives the cpu temperatures at the moment the command was executed. The command watch sensors gives continuous temperature readings instead.\nTo control overheating in my NUCs I removed their top lids, and installed them into a custom LEGO \u0026ldquo;rack\u0026rdquo; with external USB fans with velocity control, as shown in the picture at the beginning of the post.\nInstalling R To install R in the NUCs I just proceed as I would when installing it in my personal computer. There is a thorough guide here.\nIn a step above I installed all the pre-required software packages. Now I only have to add the security key of the R repository, add the repository itself, update the information on the packages available in the new repository, and finally install R.\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 sudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/' sudo apt update sudo apt install r-base Note: If R has issues to recognize the system locale\nnano ~/.profile add the following lines, replacing en_US.UTF-8 with your preferred locale\nexport LANG=en_US.UTF-8 export LC_ALL=en_US.UTF-8\nsave, and execute the file to export the locale so R can read it.\n. ~/.profile Finalizing the network configuration Each NUC needs firewall rules to grant access from other computers withinn the cluster network. To activate the NUC\u0026rsquo;s firewall and check what ports are open:\nsudo ufw enable sudo ufw status To grant access from the PC to the NUC through ssh, and later through R for parallel computing, the ports 22 and 11000 must be open for the IP of the PC (10.42.0.1).\nsudo ufw allow ssh sudo ufw allow from 10.42.0.1 to any port 11000 sudo ufw allow from 10.42.0.1 to any port 22 Finally, the other members of the cluster network must be declared in the /etc/hosts file of each computer.\nIn each NUC edit the file through ssh with bash sudo nano /etc/hosts and add the lines\n10.42.0.1 pc_name\n10.42.0.XXX name_of_the_other_nuc\nIn the PC, add the lines\n10.42.0.XXX name_of_one_nuc\n10.42.0.XXX name_of_the_other_nuc\nAt this point, after rebooting every machine, the NUCs must be accessible through ssh by using their names (ssh username@nuc_name) instead of their IPs (ssh username@n10.42.0.XXX). Just take in mind that, since the cluster network works with dynamic IPs (and such setting cannot be changed in a shared connection), the IPs of the NUCs might change if a new device is added to the network. That\u0026rsquo;s something you need to check from the PC with sudo arp-scan 10.42.0.1/24, to update every /etc/hosts file accordingly.\nI think that\u0026rsquo;s all folks. Good luck setting your home cluster! Next time I will describe how to use it for parallel computing in R.\n","date":1607299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607299200,"objectID":"ead52a439ba25afd3dd271401b5f5734","permalink":"https://blasbenito.com/post/01_home_cluster/","publishdate":"2020-12-07T00:00:00Z","relpermalink":"/post/01_home_cluster/","section":"post","summary":"In this post I explain how to setup a small Beowulf cluster with a personal PC running Ubuntu 20.04 and a couple of Intel NUCs running Ubuntu Server 20.04, with the end-goal of parallelizing R tasks.","tags":null,"title":"Setting up a home cluster","type":"post"},{"authors":["Masahiro Ryo","Boyan Angelov","Stefano Mammola","Jamie M. Kass","Blas M. Benito","Florian Hartig"],"categories":null,"content":"","date":1605571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605571200,"objectID":"a92328360ad8f6c7395efe736f4f34e3","permalink":"https://blasbenito.com/publication/2020_ryo_ecography/","publishdate":"2020-11-17T00:00:00Z","relpermalink":"/publication/2020_ryo_ecography/","section":"publication","summary":"Here we draw attention to an emerging subdiscipline of artificial intelligence, explainable AI (xAI), as a toolbox for better interpreting SDMs. xAI aims at deciphering the behavior of complex statistical or machine learning models (e.g. neural networks, random forests, boosted regression trees), and can produce more transparent and understandable SDM predictions.","tags":["Species Distribution Models","Explainable Artificial Intelligence (xAI)"],"title":"Explainable artificial intelligence enhances the ecological interpretability of black‐box species distribution models","type":"publication"},{"authors":["Andrea Contina","Scott W. Yanco","Allison K. Pierce","Michelle DePrenger-Levin","Michael B. Wunder","Andreas M. Neophytou","C. Phoebe Lostroh","Richard J. Telford","Blas M. Benito","Joseph Chipperfield","Robert B. O'Hara","Colin J. Carlson"],"categories":null,"content":"","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600646400,"objectID":"70fd84519cc7f0520ad6720d1a1a28e9","permalink":"https://blasbenito.com/publication/2020_contina_ecological_modelling/","publishdate":"2020-09-21T00:00:00Z","relpermalink":"/publication/2020_contina_ecological_modelling/","section":"publication","summary":"In this letter we present comments on the article “A global-scale ecological niche model to predict SARS-CoV-2 coronavirus” by Coro published in 2020.","tags":["Irresponsible Covid19 modelling","Species Distribution Models"],"title":"Comment on “A global-scale ecological niche model to predict SARS-CoV-2 coronavirus infection rate”, author Coro","type":"publication"},{"authors":["Colin J. Carlson","Joseph D. Chipperfield","Blas M. Benito","Richard J. Telford","Robert B. O'Hara"],"categories":null,"content":"","date":1595980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595980800,"objectID":"0e2abe2a1f21b5de5d214beb5a8e28a3","permalink":"https://blasbenito.com/publication/2020_carlson_nature_ecology_and_evolution/","publishdate":"2020-07-29T00:00:00Z","relpermalink":"/publication/2020_carlson_nature_ecology_and_evolution/","section":"publication","summary":"Araújo et al. have published a response to our piece ‘Species distribution models are inappropriate for COVID-19’1 entitled ‘Ecological and epidemiological models are both useful for SARS-CoV-2’2, in which they defend the idea that ecological models are likely to identify the signature of climate drivers in the R0 of COVID-19 transmission.","tags":["Irresponsible Covid19 modelling","Species Distribution Models"],"title":"Don’t gamble the COVID-19 response on ecological hypotheses","type":"publication"},{"authors":["Quai.Yu Cui","Marie-José Gaillard","Boris Vannière","Daniele Colombaroli","Geoffrey Lemdahl","Fredrik Olsson","Blas M. Benito","Yan Zhao"],"categories":null,"content":"","date":1594684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594684800,"objectID":"b0ae2a004a1861145db6167d8ae1d7f1","permalink":"https://blasbenito.com/publication/2020_cui_the_holocene/","publishdate":"2020-07-14T00:00:00Z","relpermalink":"/publication/2020_cui_the_holocene/","section":"publication","summary":"In this study, we assess how representative a single charcoal record from a peat profile in small bogs (1.5–2 ha in area) is for the reconstruction of Holocene fire history.","tags":["Palaeoecology","Time Series Analysis"],"title":"Evaluating fossil charcoal representation in small peat bogs: Detailed Holocene fire records from southern Sweden","type":"publication"},{"authors":["Constantin M. Zohner","Lidong Mo","Susanne S. Renner","Jens-Christian Svenning","Yann Vitasse","Blas M. Benito","Alejandro Ordonez","Frederik Baumgarten","Jean-François Bastin","Veronica Sebald","Peter B. Reich","Jingjing Liang","Gert-Jan Nabuurs","Sergio de-Miguel","Giorgio Alberti","Clara Antón-Fernández","Radomir Balazy","Urs-Beat Brändli","Han Y. H. Chen","Chelsea Chisholm","Emil Cienciala","Selvadurai Dayanandan","Tom M. Fayle","Lorenzo Frizzera","Damiano Gianelle","Andrzej M. Jagodzinski","Bogdan Jaroszewicz","Tommaso Jucker","Sebastian Kepfer-Rojas","Mohammed Latif Khan","Hyun Seok Kim","Henn Korjus","Vivian Kvist Johannsen","Diana Laarmann","Mait Lang","Tomasz Zawila-Niedzwiecki","Pascal A. Niklaus","Alain Paquette","Hans Pretzsch","Purabi Saikia","Peter Schall","Vladimír Šebeň","Miroslav Svoboda","Elena Tikhonova","Helder Viana","Chunyu Zhang","Xiuhai Zhao","Thomas W. Crowther"],"categories":null,"content":"","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"a03c5f954b0fdb1d789e83f7b7821801","permalink":"https://blasbenito.com/publication/2020_zohner_pnas/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/publication/2020_zohner_pnas/","section":"publication","summary":"Frost in late spring causes severe ecosystem damage in temperate and boreal regions. We here analyze late-spring frost occurrences between 1959 and 2017 and woody species’ resistance strategies to forecast forest vulnerability under climate change. Leaf-out phenology and leaf-freezing resistance data come from up to 1,500 species cultivated in common gardens. The greatest increase in leaf-damaging spring frost has occurred in Europe and East Asia, where species are more vulnerable to spring frost than in North America. The data imply that 35 and 26% of Europe’s and Asia’s forests are increasingly threatened by frost damage, while this is only true for 10% of North America. Phenological strategies that helped trees tolerate past frost frequencies will thus be increasingly mismatched to future conditions.","tags":["Phenology","Biogeography","Climate Change","Plant Ecology"],"title":"Late-spring frost risk between 1959 and 2017 decreased in North America but increased in Europe and Asia","type":"publication"},{"authors":["Colin J. Carlson","Joseph D. Chipperfield","Blas M. Benito","Richard J. Telford","Robert B. O'Hara"],"categories":null,"content":"","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588723200,"objectID":"bb8b41d7db5a51c063f7a5e33c256d3f","permalink":"https://blasbenito.com/publication/2020_carlson_nature_ecology_and_evolution_b/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/publication/2020_carlson_nature_ecology_and_evolution_b/","section":"publication","summary":"Species distribution models are a powerful tool for ecological inference, but not every use is biologically justified. Applying these tools to the COVID-19 pandemic is unlikely to yield new insights, and could mislead policymakers at a critical moment.","tags":["Irresponsible Covid19 modelling","Species Distribution Models"],"title":"Species distribution models are inappropriate for COVID-19","type":"publication"},{"authors":["María Leunda","Graciela Gil-Romera","Anne-Laure Daniau","Blas M. Benito","Penélope González-Sampériz"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"48d1b269836f2685996b5ccae758bec5","permalink":"https://blasbenito.com/publication/2020_leunda_catena/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/2020_leunda_catena/","section":"publication","summary":"In this paper we aim to (1) reconstruct the Holocene fire history at high altitudes of the southern Central Pyrenees, (2) add evidence to the debate on fire origin, naturally or anthropogenically produced, (3) determine the importance of fire as a disturbance agent for sub-alpine and alpine vegetation, in comparison with the plant community internal dynamics.","tags":["Palaeoecology","Fire dynamics","Ecological Memory","Generalized Least Squares","Plant Ecology"],"title":"Holocene fire and vegetation dynamics in the Central Pyrenees (Spain)","type":"publication"},{"authors":["Felde, V. A.","...","Blas M. Benito","et al."],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"0945150d072317bd056f82234adf52f5","permalink":"https://blasbenito.com/publication/2020_felde_vegetation_history_and_archaeobotany/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/2020_felde_vegetation_history_and_archaeobotany/","section":"publication","summary":"The Eemian interglacial represents a natural experiment on how past vegetation with negligible human impact responded to amplified temperature changes compared to the Holocene. Here, we assemble 47 carefully selected Eemian pollen sequences from Europe to explore geographical patterns of (1) total compositional turnover and total variation for each sequence and (2) stratigraphical turnover between samples within each sequence using detrended canonical correspondence analysis, multivariate regression trees, and principal curves. Our synthesis shows that turnover and variation are highest in central Europe (47–55°N), low in southern Europe (south of 45°N), and lowest in the north (above 60°N). These results provide a basis for developing hypotheses about causes of vegetation change during the Eemian and their possible drivers.","tags":["Palaeoecology"],"title":"Compositional turnover and variation in Eemian pollen sequences in Europe","type":"publication"},{"authors":["Joseph D. Chipperfield","Robert B. O'Hara","Blas M. Benito","Richard J. Telford","Colin J. Carlson"],"categories":null,"content":"","date":1585353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585353600,"objectID":"33ce384276fda207d92a8cf37bd61e01","permalink":"https://blasbenito.com/publication/2020_chipperfield_ecoevorxiv/","publishdate":"2020-03-28T00:00:00Z","relpermalink":"/publication/2020_chipperfield_ecoevorxiv/","section":"publication","summary":"The ongoing pandemic of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is causing significant damage to public health and economic livelihoods, and is putting significant strains on healthcare services globally. This unfolding emergency has prompted the preparation and dissemination of the article “Spread of SARS-CoV-2 Coronavirus likely to be constrained by climate” by Araújo and Naimi (2020). The authors present the results of an ensemble forecast made from a suite of species distribution models (SDMs), where they attempt to predict the suitability of the climate for the spread of SARS-CoV-2 over the coming months. They argue that climate is likely to be a primary regulator for the spread of the infection and that people in warm-temperate and cold climates are more vulnerable than those in tropical and arid climates. A central finding of their study is that the possibility of a synchronous global pandemic of SARS-CoV-2 is unlikely. Whilst we understand that the motivations behind producing such work are grounded in trying to be helpful, we demonstrate here that there are clear conceptual and methodological deficiencies with their study that render their results and conclusions invalid.","tags":["Irresponsible Covid19 modelling","Species Distribution Models"],"title":"On the inadequacy of species distribution models for modelling the spread of SARS-CoV-2: response to Araújo and Naimi","type":"publication"},{"authors":["Blas M. Benito"],"categories":null,"content":"","date":1579737600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579737600,"objectID":"80c7b3f2edb958819795bfdb5c63cf9b","permalink":"https://blasbenito.com/publication/2020_benito_ecography_distantia/","publishdate":"2020-01-23T00:00:00Z","relpermalink":"/publication/2020_benito_ecography_distantia/","section":"publication","summary":"We introduce distantia (v1.0.1), an R package providing general toolset to quantify dissimilarity between ecological time‐series, independently of their regularity and number of samples. The functions in distantia provide the means to compute dissimilarity scores by time and by shape and assess their significance, evaluate the partial contribution of each variable to dissimilarity, and align or combine sequences by similarity.","tags":["Time Series Analysis","R packages"],"title":"distantia: an open‐source toolset to quantify dissimilarity between multivariate ecological time‐series","type":"publication"},{"authors":["Blas M. Benito","Graciela Gil-Romera"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"b54d3a6bfb2beb4f27f5c5b2a798cd55","permalink":"https://blasbenito.com/publication/2020_benito_ecography_memoria/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/2020_benito_ecography_memoria/","section":"publication","summary":"Paper published in the section \"Editor's Choice\" of the *Ecography* journal. It received [an award](https://www.dropbox.com/s/oacsy1xqx4omv1b/2019_BMB_Ecography_b_top_downloaded.png?dl=1) for the number of downloads during the 12 months after its publication.","tags":["Quantitative methods","R packages","Palaeoecology","Ecological Memory","Plant Ecology","Machine Learning","Random Forest"],"title":"Ecological memory at millennial time‐scales: the importance of data constraints, species longevity and niche features","type":"publication"},{"authors":["B.L Valero-Garcés","Penélope González-Sampériz","Graciela Gil-Romera","Blas M. Benito","A. Moreno","B. Oliva-Urcia","J. Aranbarri","E. García-Prieto","M. Frugone","M. Morellón","L.J. Arnold","M. Demuro","M. Hardiman","S.P.E. Blockey","C.S. Lane"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"a61e896d35882a74ae0ea224990d2ae3","permalink":"https://blasbenito.com/publication/2019_valero-garces_quaternary_geochronology/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/2019_valero-garces_quaternary_geochronology/","section":"publication","summary":"We present a multidisciplinary dating approach - including radiocarbon, Uranium/Thorium series (U/Th), paleomagnetism, single-grain optically stimulated luminescence (OSL), polymineral fine-grain infrared stimulated luminescence (IRSL) and tephrochronology - used for the development of an age model for the Cañizar de Villarquemado sequence (VIL) for the last ca. 135 ka.","tags":["Palaeoecology","Age-depth modelling","Bayesian models"],"title":"A multi-dating approach to age-modelling long continental records: The 135 ka El Cañizar de Villarquemado sequence (NE Spain)","type":"publication"},{"authors":["Graciela Gil-Romera","Carole Adolf","Blas M. Benito","Lucas Bittner","Maria U. Johansson","David A. Grady","Henry F. Lamb","Bruk Lemma","Mekbib Fekadu","Bruno Glaser","Betelhem Mekonnen","Miguel Sevilla-Callejo","Michael Zech","Wolfgang Zech","Georg Miaha"],"categories":null,"content":"","date":1563926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563926400,"objectID":"691382af0f3f50780ae78edc146f99e0","permalink":"https://blasbenito.com/publication/2019_gil_romera_biology_letters/","publishdate":"2019-07-24T00:00:00Z","relpermalink":"/publication/2019_gil_romera_biology_letters/","section":"publication","summary":"We hypothesize that fire has influenced Erica communities in the Bale Mountains at millennial time-scales. To test this, we (1) identify the fire history of the Bale Mountains through a pollen and charcoal record from Garba Guracha, a lake at 3950 m.a.s.l., and (2) describe the long-term bidirectional feedback between wildfire and Erica, which may control the ecosystem's resilience.","tags":["Palaeoecology","Fire dynamics","Ecological Memory","Time Series Analysis","Generalized Least Squares","Plant Ecology"],"title":"Long-term fire resilience of the Ericaceous Belt, Bale Mountains, Ethiopia","type":"publication"},{"authors":["Blas M. Benito"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png') print(\u0026quot;Welcome to Academic!\u0026quot;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata ( front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"1f184bf285911dee5df13e26218d60b2","permalink":"https://blasbenito.com/post_examples/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post_examples/jupyter/","section":"post_examples","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post_examples"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://blasbenito.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Albuquerque F.","Blas M. Benito","Rodríguez MÁM.","Gray C."],"categories":null,"content":"","date":1537315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537315200,"objectID":"0dd555719b21269ebb41f0603a2e3704","permalink":"https://blasbenito.com/publication/2018_albuquerque_peerj/","publishdate":"2018-09-19T00:00:00Z","relpermalink":"/publication/2018_albuquerque_peerj/","section":"publication","summary":"The goals of this study are to provide a map of actual habitat suitability (1), describe the relationships between abiotic predictors and the saguaro distribution at regional extents (2), and describe the potential effect of climate change on the spatial distribution of the saguaro (3).","tags":["Biogeography","Climate Change","Species Distribution Models","Machine Learning","Gradient Boosting","Plant Ecology"],"title":"Potential changes in the distribution of Carnegiea gigantea under future scenarios","type":"publication"},{"authors":["Radoslav Kozma","Mette Lillie","Blas M. Benito","Jens-Christian Svenning","Jacob Höglund"],"categories":null,"content":"","date":1527552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527552000,"objectID":"2a60bd7aa28feceb59407d600d738438","permalink":"https://blasbenito.com/publication/2018_kozma_ecology_and_evolution/","publishdate":"2018-05-29T00:00:00Z","relpermalink":"/publication/2018_kozma_ecology_and_evolution/","section":"publication","summary":"Here we investigated the demographic history of the willow grouse (Lagopus lagopus), rock ptarmigan (Lagopus muta), and black grouse (Tetrao tetrix) through the Late Pleistocene using two complementary methods and whole genome data. Species distribution modeling (SDM) allowed us to estimate the total range size during the Last Interglacial (LIG) and Last Glacial Maximum (LGM) as well as to indicate potential population subdivisions.","tags":["Biogeography","Climate Change","Species Distribution Models","Generalized Linear Models"],"title":"Past and potential future population dynamics of three grouse species using ecological and whole genome coalescent modeling","type":"publication"},{"authors":["Gang Feng","Ziyu Ma","Blas M. Benito","Signe Normand","Alejandro Ordoñez","Yi Jin","Lingfeng Mao","Jens-Christian Svenning"],"categories":null,"content":"","date":1500249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500249600,"objectID":"fc5bc772c0898cc6442de19eb3810539","permalink":"https://blasbenito.com/publication/2017_feng_global_ecology_and_biogeography/","publishdate":"2017-07-17T00:00:00Z","relpermalink":"/publication/2017_feng_global_ecology_and_biogeography/","section":"publication","summary":"Our results show that phylogenetically diverse assemblages with large phylogenetic age differences among species are associated with relatively high long‐term climate stability, with intra‐regional links between long‐term climate variability and phylogenetic composition especially strong in the more unstable regions. These findings point to future climate change as a key risk to the preservation of the phylogenetically diverse assemblages in regions characterized by relatively high paleoclimate stability, with China as a key example.","tags":["Biogeography","Plant Ecology"],"title":"Phylogenetic age differences in tree assemblages across the Northern Hemisphere increase with long-term climate stability in unstable regions","type":"publication"},{"authors":["Gang Feng","Lingfeng Mao","Blas M. Benito","Nathan G. Swenson","Jens-Christian Svenning"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"02c9b25a1d6d387966133e1a58983917","permalink":"https://blasbenito.com/publication/2017_feng_biological_conservation/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/2017_feng_biological_conservation/","section":"publication","summary":"In this study, for the first time, we linked the distribution of threatened species across China to current and historical changes in human population densities, cropland area, and pasture area since 1700 (at a 100 km × 100 km resolution). We find that variables describing historical changes in human impacts were consistently more strongly associated with proportions of threatened plants than variables describing current changes in human impacts. Notably, threatened plant species in China tend to be concentrated where historical anthropogenic impacts were relatively small, but anthropogenic activities have intensified relatively strongly since 1700.","tags":["Biogeography","Random Forest","Machine Learning","Biodiversity Conservation"],"title":"Historical anthropogenic footprints in the distribution of threatened plants in China","type":"publication"},{"authors":["Blas M. Benito","Jens-Christian Svenning","Trine Kellberg Nielsen","Felix Riede","Graciela Gil-Romera","Thomas Mailund","Brody Sandel"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"3fffb042779562c60fe874bc323131de","permalink":"https://blasbenito.com/publication/2017_benito_journal_of_biogeography/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/2017_benito_journal_of_biogeography/","section":"publication","summary":"This paper [was highlighted in the *Editor's Picks* section of the Science Journal](https://www.dropbox.com/s/6k308eczv7i6kbj/2017_BMB_Journal_of_Biogeography_editors_choice.pdf?dl=1), and was among the [top downloaded articles](https://www.dropbox.com/s/sowq1h4bdngmipy/2017_BMB_Journal_of_Biogeography.png?dl=1) from the *Journal of Biogeography* during the 12 months after its publication.","tags":["Biogeography","Species Distribution Models","Biogeography of Neanderthals","Generalized Linear Models","Ensemble models"],"title":"The ecological niche and distribution of Neanderthals during the Last Interglacial","type":"publication"},{"authors":["Trine Kellberg Nielsen","Blas M. Benito","Jens-Christian Svenning","Brody Sandel","Luseadra McKerracher","Felix Riede"],"categories":null,"content":"","date":1488240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488240000,"objectID":"70212c1a73a4ebfe55db225953cd1f5d","permalink":"https://blasbenito.com/publication/2017_kellberg-nielsen_quaternary_international/","publishdate":"2017-02-28T00:00:00Z","relpermalink":"/publication/2017_kellberg-nielsen_quaternary_international/","section":"publication","summary":"Our results are inconsistent with the claim that climatic constraint and/or a lack of suitable habitats can fully explain the absence of Neanderthals in Southern Scandinavia during the Eemian Interglacial and Early Weichselian Glaciation. We do, however, find evidence that a geographic barrier may have impeded northerly migrations during the Eemian.","tags":["Biogeography","Biogeography of Neanderthals","Species distribution models"],"title":"Investigating Neanderthal dispersal above 55°N in Europe during the Last Interglacial Complex","type":"publication"},{"authors":["Constantin M. Zohner","Blas M. Benito","Jason D. Fridley","Jens-Christian Svenning","Susanne S. Renner"],"categories":null,"content":"","date":1487030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487030400,"objectID":"1bf8e70c8f4c03b6d059e49a3fd0bf9b","permalink":"https://blasbenito.com/publication/2017_zohner_ecology_letters/","publishdate":"2017-02-14T00:00:00Z","relpermalink":"/publication/2017_zohner_ecology_letters/","section":"publication","summary":"Intuitively, interannual spring temperature variability (STV) should influence the leaf‐out strategies of temperate zone woody species, with high winter chilling requirements in species from regions where spring warming varies greatly among years. We tested this hypothesis using experiments in 215 species and leaf‐out monitoring in 1585 species from East Asia (EA), Europe (EU) and North America (NA). The results reveal that species from regions with high STV indeed have higher winter chilling requirements, and, when grown under the same conditions, leaf out later than related species from regions with lower STV. Since 1900, STV has been consistently higher in NA than in EU and EA, and under experimentally short winter conditions NA species required 84% more spring warming for bud break, EU ones 49% and EA ones only 1%. These previously unknown continental‐scale differences in phenological strategies underscore the need for considering regional climate histories in global change models.","tags":["Phenology","Biogeography","Plant Ecology"],"title":"Spring predictability explains different leaf‐out strategies in the woody floras of North America, Europe and East Asia","type":"publication"},{"authors":["Constantin M. Zohner","Blas M. Benito","Jens-Christian Svenning","Susanne S. Renner"],"categories":null,"content":"","date":1476662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476662400,"objectID":"8bb3d0dd4e4d13b48c7b50f18f1b7e44","permalink":"https://blasbenito.com/publication/2016_zohner_nature_climate_change/","publishdate":"2016-10-17T00:00:00Z","relpermalink":"/publication/2016_zohner_nature_climate_change/","section":"publication","summary":"Our results do not support previous ideas about phenological strategies in temperate woody species (the ‘high temperature variability’ hypothesis; the ‘oceanic climate’ hypothesis; the ‘high latitude’ hypothesis). In regions with long winters, trees appear to rely on cues other than day length, such as winter chilling and spring warming. By contrast, in regions with short winters, some species—mostly from lineages with a warm-temperate or subtropical background, for example, Fagus additionally rely on photoperiodism. Therefore, photoperiod may be expected to constrain climate-driven shifts in spring leaf unfolding only at lower latitudes.","tags":["Phenology","Biogeography","Plant Ecology"],"title":"Day length unlikely to constrain climate-driven shifts in leaf-out times of northern woody plants","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d337bc1d87a46134727f7fb46c0d4efc","permalink":"https://blasbenito.com/project_examples/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project_examples/external-project/","section":"project_examples","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project_examples"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"875a64698311258ca5954edf3adc2327","permalink":"https://blasbenito.com/project_examples/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project_examples/internal-project/","section":"project_examples","summary":"An example of using the in-built project page.","tags":[null],"title":"Internal Project","type":"project_examples"},{"authors":["Blas M. Benito","吳恩達"],"categories":["Demo","教程"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\nCheck out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n👉 Get Started 📚 View the documentation 💬 Ask a question on the forum 👥 Chat with the community 🐦 Twitter: @source_themes @GeorgeCushen #MadeWithAcademic 💡 Request a feature or report a bug ⬆️ Updating? View the Update Guide and Release Notes ❤️ Support development of Academic: ☕️ Donate a coffee 💵 Become a backer on Patreon 🖼️ Decorate your laptop or journal with an Academic sticker 👕 Wear the T-shirt 👩‍💻 Contribute Academic is mobile first with a responsive design to ensure that your site looks stunning on every device. Key features:\nPage builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic Install You can choose from one of the following four methods to install:\none-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio Then personalize and deploy your new site.\nUpdating View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"a25eb5ef2134a076531687948275d21e","permalink":"https://blasbenito.com/post_examples/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post_examples/getting-started/","section":"post_examples","summary":"Create a beautifully simple website in under 10 minutes.","tags":null,"title":"Academic: the website builder for Hugo","type":"post_examples"},{"authors":null,"categories":["R"],"content":" R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA ) Figure 1: A fancy pie chart. ","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"9554380237eec0879443a7c1a234a75f","permalink":"https://blasbenito.com/post_examples/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post_examples/2015-07-23-r-rmarkdown/","section":"post_examples","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post_examples"},{"authors":["Jacquelyn L. Gill","Jessica L. Blois","Blas M. Benito","Solomon Dobrowski","Malcolm L. Hunter Jr.","Jenny L. McGuire"],"categories":null,"content":"","date":1430179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430179200,"objectID":"aebebdc92cbf3072b7bf6d150d269b6c","permalink":"https://blasbenito.com/publication/2015_gill_conservation_biology/","publishdate":"2015-04-28T00:00:00Z","relpermalink":"/publication/2015_gill_conservation_biology/","section":"publication","summary":"Paleoecology provides a valuable perspective on coarse‐filter strategies by marshaling the natural experiments of the past to contextualize extinction risk due to the emerging impacts of climate change and anthropogenic threats. We reviewed examples from the paleoecological record that highlight the strengths, opportunities, and caveats of a CNS approach. We focused on the near‐time geological past of the Quaternary, during which species were subjected to widespread changes in climate and concomitant changes in the physical environment in general.","tags":["Palaeoecology","Biodiversity conservation"],"title":"A 2.5‐million‐year perspective on coarse‐filter strategies for conserving nature's stage","type":"publication"},{"authors":["Albuquerque F.","Blas M. Benito","Beier, P.","Assunção-Albuquerque, M.J.","Cayuela, L."],"categories":null,"content":"","date":1425340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1425340800,"objectID":"d1d254ad844854fa63cf4af98860d93d","permalink":"https://blasbenito.com/publication/2015_albuquerque_naturaleza_and_conservacao/","publishdate":"2015-03-03T00:00:00Z","relpermalink":"/publication/2015_albuquerque_naturaleza_and_conservacao/","section":"publication","summary":"We had three key findings. First, dry forest is the least protected biome in Mesoamerica (4.5% protected), indicating that further action to safeguard this biome is warranted. Secondly, the poor overlap between protected areas and high-value forest conservation areas found herein may provide evidence that the establishment of protected areas may not be fully accounting for tree priority rank map. Third, high percentages of forest cover and high-value forest conservation areas still need to be represented by the protected areas network. Because deforestation rates are still increasing in this region, Mesoamerica needs funding and coordinated action by policy makers, national and local governmental and non-governmental organizations, conservationists and other stakeholders.","tags":["Biodiversity Conservation","Forests","Species Distribution Models","Random Forest","Machine Learning"],"title":"Supporting underrepresented forests in Mesoamerica","type":"publication"},{"authors":["A.J. Mendoza-Fernández","F. Martínez-Hernández","F.J. Pérez-García","J.A. Garrido Becerra","Blas M. Benito","E. Salmerón Sánchez","J. Guirado","M.E. Merlo","J.F. Mota"],"categories":null,"content":"","date":1421107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1421107200,"objectID":"65c5f86bbc46066e9c0c0c783d5667b8","permalink":"https://blasbenito.com/publication/2015_mendoza-fernandez_plant_biosystems/","publishdate":"2015-01-13T00:00:00Z","relpermalink":"/publication/2015_mendoza-fernandez_plant_biosystems/","section":"publication","summary":"*Maytenus senegalensis* subsp. *europaea* communities are unique vegetal formations in Europe. In fact, they are considered Priority Habitat by Directive 92/43/EEC. These are ecologically valuable plant communities found in the southeast of Spain. By combining modeling methods of environmental variables, historical photo-interpretation, and fieldwork, a chronosequence of the evolution of their extent of occurrence (EOO) has been reconstructed in 1957 and 2011. Results showed a strong regression range of *Maytenus senegalensis* subsp. *europaea* populations. More than 26,000 ha of EOO for this species have been lost in the province of Almería. Considering the final number of polygons, this area has been fragmented 18 times since the 1950s. These results reinforce the idea that the alteration and fragmentation of habitat due to human activities is one of the most important drivers of biodiversity loss and global change. These activities are mostly intensive greenhouse agriculture and urbanization without sustainable land planning. Knowledge about the distribution of M. senegalensis subsp. europaea is of great interest for future habitat restoration. Therefore, this would be the key species to recover these damaged ecosystems.","tags":["Species distribution models","Biodiversity conservation","Ecoinformatics","Habitat loss"],"title":"Extreme habitat loss in a Mediterranean habitat: Maytenus senegalensis subsp. europaea","type":"publication"},{"authors":["José Miguel Barea-Azcón","Blas M. Benito (shared first coauthorship)","Francisco J. Olivares","Helena Ruiz","Javier Martín","Antonio L. García","Rogelio López"],"categories":null,"content":"","date":1392854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1392854400,"objectID":"24408a853074612aafa67c0a6df22901","permalink":"https://blasbenito.com/publication/2014_barea-azcon_and_benito_biodiversity_and_conservation/","publishdate":"2014-02-20T00:00:00Z","relpermalink":"/publication/2014_barea-azcon_and_benito_biodiversity_and_conservation/","section":"publication","summary":"Herein we investigate the distribution and conservation problems of a relict interaction in the Sierra Nevada mountains (southern Europe) between the butterfly *Agriades zullichi* —a rare and threatened butterfly— and its larval foodplant *Androsace vitaliana* subsp. *nevadensis*. We designed an intensive field survey to obtain a comprehensive presence dataset. This was used to calibrate species distribution models with absences taken at local and regional extents, analyze the potential distribution, evaluate the influence of environmental factors in different geographical contexts, and evaluate conservation threats for both organisms.","tags":["Biodiversity Conservation","Species Distribution Models","Ecoinformatics","Biogeography","Random Forest","Machine Learning"],"title":"Distribution and conservation of the relict interaction between the butterfly Agriades zullichi and its larval foodplant (Androsace vitaliana nevadensis)","type":"publication"},{"authors":["Francisco J. Bonet","Ramón Pérez-Pérez","Blas M. Benito","Fabio Suzart de Albuquerque","Regino Zamora"],"categories":null,"content":"","date":1391212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1391212800,"objectID":"bf6fe82e9413557dd6e0d799c6787ba1","permalink":"https://blasbenito.com/publication/2014_bonet_environmental_modelling_and_software/","publishdate":"2014-02-01T00:00:00Z","relpermalink":"/publication/2014_bonet_environmental_modelling_and_software/","section":"publication","summary":"Many of the best practices concerning the development of ecological models or analytic techniques published in the scientific literature are not fully available to modelers but rather are stored in scientists' digital or biological memories. We propose that it is time to address the problem of storing, documenting, and executing ecological models and analytical procedures. In this paper, we propose a conceptual framework to design and implement a web application that will help to meet this challenge. This tool will foster cooperation among scientists, enhancing the creation of relevant knowledge that could be transferred to environmental managers. We have implemented this conceptual framework in a tool called ModeleR. This is being used to document, share, and execute more than 200 models and analytical processes associated with a global change monitoring program that is being undertaken in the Sierra Nevada Mountains (south Spain). ModeleR uses the concept of scientific workflow to connect and execute different types of models and analytical processes. Finally, we have envisioned the creation of a federation of model repositories where models documented within a local repository could be linked and even executed by other researchers.","tags":["Ecoinformatics"],"title":"Documenting, storing, and executing models in Ecology: A conceptual framework and real implementation in a global change monitoring program","type":"publication"},{"authors":["Blas M. Benito","Juan Lorite","Ramón Pérez-Pérez","Lorena Gómez-Aparicio","Julio Peñas"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"c1e357c5f2a52ae592bcf91f774e7627","permalink":"https://blasbenito.com/publication/2014_benito_diversity_and_distributions/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/2014_benito_diversity_and_distributions/","section":"publication","summary":"The Mediterranean Basin is threatened by climate change, and there is an urgent need for studies to determine the risk of plant range shift and potential extinction. In this study, we simulate potential range shifts of 176 plant species to perform a detailed prognosis of critical range decline and extinction in a transformed mediterranean landscape. Particularly, we seek to answer two pivotal questions: (1) what are the general plant‐extinction patterns we should expect in mediterranean landscapes during the 21st century? and (2) does dispersal ability prevent extinction under climate change?.","tags":["Climate Change","Species Distribution Models","Ecoinformatics","Mechanistic Simulation","Random Forest","Ensemble models","Conditional Inference Trees","Plant Ecology"],"title":"Forecasting plant range collapse in a mediterranean hotspot: when dispersal uncertainties matter","type":"publication"},{"authors":["Elise S. Gornish","Jill A. Hamilton","Albert Barberán","Blas M. Benito","Amrei Binzer","Julie E. DeMeester","Robert Gruwez","Bruno Moreira","Shirin Taheri","Sara Tomiolo","Catarina Vinagre","Pauline Vurain","Jennifer Weaver"],"categories":null,"content":"","date":1366070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1366070400,"objectID":"5ee0e5600c367d4720aa53a935a5c256","permalink":"https://blasbenito.com/publication/2013_gornish_eos/","publishdate":"2013-04-16T00:00:00Z","relpermalink":"/publication/2013_gornish_eos/","section":"publication","summary":"Climate change research is an interdisciplinary field, and understanding its social, political, and environmental implications requires integration across fields of research where different tools may be used to address common concerns. One of the many advantages of interdisciplinary approaches is that they open communication between complementary fields, filling knowledge gaps and facilitating progression within both individual fields and the broader field of climate change research.","tags":["Climate Change"],"title":"Interdisciplinary Climate Change Collaborations Are Essential for Early‐Career Scientists","type":"publication"},{"authors":["Mireia Valle","Marieke M. van Katwijk","Dick J. de Jong","Tjeerd J. Bouma","Aafke M. Schipper","Guillem Chust","Blas M. Benito","Joxe M. Garmendia","Ángel Borja"],"categories":null,"content":"","date":1363305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1363305600,"objectID":"77359446d7cb7b0af132b5de61eb93b1","permalink":"https://blasbenito.com/publication/2013_valle_journal_of_sea_research/","publishdate":"2013-03-15T00:00:00Z","relpermalink":"/publication/2013_valle_journal_of_sea_research/","section":"publication","summary":"A time series of 14-year distribution data of Zostera marina in the Ems estuary (The Netherlands) was used to build different data subsets: (1) total presence area; (2) a conservative estimate of the total presence area, defined as the area which had been occupied during at least 4 years; (3) core area, defined as the area which had been occupied during at least 2/3 of the total period; and (4–6) three random selections of monitoring years. On average, colonized and disappeared areas of the species in the Ems estuary showed remarkably similar transition probabilities of 12.7% and 12.9%, respectively. SDMs based upon machine-learning methods (Boosted Regression Trees and Random Forest) outperformed regression-based methods. Current velocity and wave exposure were the most important variables predicting the species presence for widely distributed data. Depth and sea floor slope were relevant to predict conservative presence area and core area.","tags":["Biodiversity Conservation","Species Distribution Models","Machine learning","Random Forest","Gradient Boosting"],"title":"Comparing the performance of species distribution models of Zostera marina: Implications for conservation","type":"publication"},{"authors":["Blas M. Benito","Luis Cayuela","Fabio S. Albuquerque"],"categories":null,"content":"","date":1362528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1362528000,"objectID":"69d5141556741b6f7698c5bfd3796a74","permalink":"https://blasbenito.com/publication/2013_benito_methods_in_ecology_and_evolution/","publishdate":"2013-03-06T00:00:00Z","relpermalink":"/publication/2013_benito_methods_in_ecology_and_evolution/","section":"publication","summary":"We generated 380 S‐SDMs of 1224 tree species in Mesoamerica by combining 19 distribution modelling methods with 20 different thresholds using presence‐only data from the Global Biodiversity Information Facility. We compared the predicted richness and composition with inventory data obtained from the BIOTREE‐NET forest plot database. We designed two indicators of predictive performance that were based on the diversity factors used to measure species turnover: a (shared species between the observed and predicted compositions), b and c (the exclusive species of the predicted and observed compositions respectively) and compared them with the Sorensen and Beta‐Simpson turnover measures. Some modelling methods – especially machine learning and ensemble model forecasting methods performed significantly better than others in minimizing the error in predicted richness and composition. Our results also indicate that restrictive thresholds (with high omission errors) lead to more accurate S‐SDMs in terms of species richness and composition. Here, we demonstrate that particular combinations of modelling methods and thresholds provide results with higher predictive performance.","tags":["Species Distribution Models","Ecoinformatics","Random Forest","Ensemble models","Machine Learning"],"title":"The impact of modelling choices in the predictive performance of richness maps derived from species‐distribution models: guidelines to build better diversity models","type":"publication"},{"authors":["Albuquerque, F.","Assunção-Albuquerque, M.J.","Cayuela, L.","Zamora, R.","Blas M. Benito"],"categories":null,"content":"","date":1358726400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1358726400,"objectID":"bb2936f49fe5e9950d89f99bc84a0d90","permalink":"https://blasbenito.com/publication/2013_albuquerque_biological_conservation/","publishdate":"2013-01-21T00:00:00Z","relpermalink":"/publication/2013_albuquerque_biological_conservation/","section":"publication","summary":"Our assessments showed little association between bird richness patterns and the cover of protected areas (PAs) across EU countries. The congruence between high-value richness areas of all bird species and IBS with PAs cover was moderate, suggesting that different conservation planning targets should be taken into account to safeguard IBS, or the composition of bird species. Our results also showed that 16 (3.9%) threatened species were present in gaps of PAs. The poor relationship between PAs cover and bird richness pattern found herein may provide evidence that the establishment of SPAs across Europe may not be fully accounting for richness patterns to enhance the performance of the current network.","tags":["Biodiversity Conservation"],"title":"European Bird distribution is “well” represented by Special Protected Areas: Mission accomplished?","type":"publication"},{"authors":["Ramón Pérez-Pérez","Blas M. Benito","Francisco J. Bonet"],"categories":null,"content":"","date":1328313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1328313600,"objectID":"29fc5074a7e132bb1510f42cecbde2aa","permalink":"https://blasbenito.com/publication/2012_perez-perez_expert_systems_with_applications/","publishdate":"2012-02-04T00:00:00Z","relpermalink":"/publication/2012_perez-perez_expert_systems_with_applications/","section":"publication","summary":"In this paper, we present the development of ModeleR, a repository of models accessible from the web, which enables the user to design, document, manage, and execute environmental models.","tags":["Ecoinformatics"],"title":"ModeleR: An enviromental model repository as knowledge base for experts","type":"publication"},{"authors":["Julio Peñas","Blas M. Benito","Juan Lorite","Miguel Ballesteros","Eva María Cañadas","Montserrat Martínez-Ortega"],"categories":null,"content":"","date":1301097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1301097600,"objectID":"f7cb87fcde920214c11f820e82fdc695","permalink":"https://blasbenito.com/publication/2011_penas_environmental_management/","publishdate":"2011-03-26T00:00:00Z","relpermalink":"/publication/2011_penas_environmental_management/","section":"publication","summary":"The results indicate that greenhouses and construction activities (mainly for tourist purposes) exert a strong impact on the populations of this endangered species. The habitat depletion showed peaks that constitute the destruction of 85% of the initial area in only 20 years for some populations of L. nigricans. According to the forecast established by the model, a rapid extinction could take place and some populations may disappear as early as the year 2030. Fragmentation-cadence analysis can help identify population units of primary concern for its conservation, by means of the adoption of improved management and regulatory measures.","tags":["Habitat Fragmentation","Drylands","Endangered Plants","Biodiversity Conservation"],"title":"Habitat Fragmentation in Arid Zones: A Case Study of Linaria nigricans Under Land Use Changes (SE Spain)","type":"publication"},{"authors":["Blas M. Benito","Juan Lorite","Julio Peñas"],"categories":null,"content":"","date":1296604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1296604800,"objectID":"d94d778916672e3a2b7a6e74a2278246","permalink":"https://blasbenito.com/publication/2011_benito_climate_change/","publishdate":"2011-02-02T00:00:00Z","relpermalink":"/publication/2011_benito_climate_change/","section":"publication","summary":"According to the simulations, the suitable habitat for the key species inhabiting the summit area, where most of the endemic and/or rare species are located, may disappear before the middle of the century. The other key species considered show moderate to drastic suitable habitat loss depending on the considered scenario. Climate warming should provoke a strong substitution dynamics between species, increasing spatial competition between both of them. In this study, we introduce the application of differential suitability concept into the analysis of potential impact of climate change, forest management and environmental monitoring, and discuss the limitations and uncertainties of these simulations.","tags":["Climate Change","Species Distribution Models","Ecoinformatics","Plant Ecology"],"title":"Simulating potential effects of climatic warming on altitudinal patterns of key species in Mediterranean-alpine ecosystems","type":"publication"},{"authors":["Juan Lorite","Julio Peñas","Blas M. Benito","Eva Cañadas","Francisco Valle"],"categories":null,"content":"","date":1267401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1267401600,"objectID":"3d86a9f5c5dcbb080231f4e1d7755fe0","permalink":"https://blasbenito.com/publication/2010_lorite_annales_botanici_fennici/","publishdate":"2010-03-01T00:00:00Z","relpermalink":"/publication/2010_lorite_annales_botanici_fennici/","section":"publication","summary":"We studied the natural history as well as the conservation status of the first-known population of Polygala balansae in Europe (Granada, SE Spain). In the study area, we located only one population occupying a small patch of 1920 m2, between 120 and 160 m a.s.l., with 246 mature individuals. The species is classified as Critically Endangered (CR), under the following criteria: severely fragmented, inferred continuous decline, small population size, and continuing decline inferred from the number mature individuals.","tags":["Botany","Endangered Plants","Biogeography","Biodiversity Conservation"],"title":"Conservation Status of the First Known Population of Polygala balansae in Europe","type":"publication"},{"authors":["Francisca Alba-Sánchez","José A. López-Sáez","Blas M. Benito","Juan C. Linares","Diego Nieto-Lugilde","Lourdes López-Merino"],"categories":null,"content":"","date":1266710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1266710400,"objectID":"3bd0e1e66942974aaf0e65818e475037","permalink":"https://blasbenito.com/publication/2010_alba-sanchez_diversity_and_distributions/","publishdate":"2010-02-21T00:00:00Z","relpermalink":"/publication/2010_alba-sanchez_diversity_and_distributions/","section":"publication","summary":"Quaternary palaeopalynological records collected throughout the Iberian Peninsula and species distribution models (SDMs) were integrated to gain a better understanding of the historical biogeography of the Iberian Abies species (i.e. Abies pinsapo and Abies alba). We hypothesize that SDMs and Abies palaeorecords are closely correlated, assuming a certain stasis in climatic and topographic ecological niche dimensions. In addition, the modelling results were used to assign the fossil records to A. alba or A. pinsapo, to identify environmental variables affecting their distribution, and to evaluate the ecological segregation between the two taxa.","tags":["Palaeoecology","Species Distribution Models","Biogeography"],"title":"Past and present potential distribution of the Iberian Abies species: a phytogeographic approach using fossil pollen data and species distribution models ","type":"publication"},{"authors":["Blas M. Benito","Montserrat Martínez-Ortega","Luz M. Muñoz","Juan Lorite","Julio Peñas"],"categories":null,"content":"","date":1235779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1235779200,"objectID":"6039de1ce8c3c53184e8f0a191151510","permalink":"https://blasbenito.com/publication/2009_benito_biodiversity_and_conservation/","publishdate":"2009-02-28T00:00:00Z","relpermalink":"/publication/2009_benito_biodiversity_and_conservation/","section":"publication","summary":"In this paper, we propose the application of SDMs to assess the extinction-risk of plant species in relation to the spread of greenhouses in a Mediterranean landscape, where habitat depletion is one of the main causes of biodiversity loss.","tags":["Endangered Plants","Biodiversity Conservation","Species Distribution Models","Drylands"],"title":"Assessing extinction-risk of endangered plants using species distribution models: a case study of habitat depletion caused by the spread of greenhouses","type":"publication"},{"authors":["Blas M. Benito"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"5dbb5dd8f81f0274ffb682d6d27968b6","permalink":"https://blasbenito.com/publication/2008_penas_phyton/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/2008_penas_phyton/","section":"publication","summary":"We we develop a methodology predicting the expansion of greenhouses by combining a species distribution model (MaxEnt) and a simulator of land use change (Geomod).","tags":["Biodiversity Conservation","Species Distribution Models","Drylands"],"title":"Greenhouses, land use change, and predictive models: MaxEnt and Geomod working together","type":"publication"}]