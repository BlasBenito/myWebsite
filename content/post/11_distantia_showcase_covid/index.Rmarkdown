---
title: "Analyzing Epidemiological Time Series With The R Package {distantia}"
author: ""
date: '2025-01-25'
slug: distantia-showcase-covid
categories: []
tags:
- Rstats
- Dynamic Time Warping
- Data Science
- Time Series Analysis
- Tutorial
subtitle: ''
summary: "Tutorial on the applications of the R package {distantia} to the analysis of epidemiological time series."
authors: [admin]
lastmod: '2025-01-25T07:28:01+01:00'
featured: no
draft: true
image:
  caption: ''
  focal_point: Smart
  margin: auto
projects: []
toc: true
---

<style>
.alert-warning {
  background-color: #f4f4f4;
  color: #333333;
  border-color: #333333;
}
</style>

```{r, include = FALSE}
knitr::opts_chunk$set(
  fig.width = 8, 
  fig.height = 6
)
```


## Summary

In this post I show how the R package [`distantia`](https://blasbenito.github.io/distantia/) can help compare and analyze epidemiological time series. The the example data comprises weekly Covid-19 prevalence curves from 36 California counties between 2020-03-16 and 2023-12-18.

The post is structured as follows:

  - **Data Preparation**: Transform the raw data into a time series list, the format required by `distantia`.

  - **Exploration**: Overview of data features through visualization and descriptive statistics.

  - **Computing Dissimilarity Scores**: Calculate time series dissimilarity using lock-step and dynamic time warping.

  - **Analyzing Dissimilarity Scores**: Explore methods like hierarchical clustering, mapping, and dissimilarity modeling to analyze the results.

<div class="alert alert-warning">
  <strong>DISCLAIMER:</strong>  I am **not** an epidemiologist, so I will be focusing on the technical aspects of the applications of {distantia} to this kind of data, without any attempt to draw conclusions from the results.
</div>

## R packages

This tutorial requires the following packages:

  - [`distantia`](https://blasbenito.github.io/distantia/): time series dissimilarity analysis.
  - [`zoo`](https://cran.r-project.org/web/packages/zoo/index.html): management of regular and irregular time series.
  - [`dplyr`](https://dplyr.tidyverse.org/): data frame processing.
  - [`ggplot2`](https://ggplot2.tidyverse.org/): to make a few charts.
  - [`mapview`](https://r-spatial.github.io/mapview/): interactive spatial visualization.
  - [`reactable`](https://glin.github.io/reactable/): interactive tables
  - [`factoextra`](https://rpkgs.datanovia.com/factoextra/index.html): plotting results of hierarchical clustering.
  - [`visreg`](https://CRAN.R-project.org/package=visreg): response curves of statistical models.
  - [`ranger`](https://github.com/imbs-hl/ranger): random forest modelling.

```{r, message = FALSE, warning = FALSE}
library(distantia)
library(zoo)
library(dplyr)
library(ggplot2)
library(mapview)
library(reactable)
library(factoextra)
library(visreg)
library(ranger)
```

## Example Data

We will be working with two example data frames shipped with `distantia`: the simple features data frame `covid_counties`, and the data frame `covid_prevalence`.

### `covid_counties`

This [`simple features`](https://r-spatial.github.io/sf/) data frame comprises county polygons and several socioeconomic variables. It is connected to `covid_prevalence` by the column `name`.

```{r, echo = FALSE, cache=TRUE}
la_population <- mapview(
  covid_counties, 
  zcol = "population",
  label = "name"
  )

htmlwidgets::saveWidget(
  la_population@map, 
  file = "la_population.html", 
  selfcontained = TRUE
  )
```

<iframe src="la_population.html" name="LA_Population"
  width="600" height="600" scrolling="auto" frameborder="0">
   <p>California counties represented in the Covid-19 dataset.</p>
</iframe>

```{r, echo = FALSE}
rm(la_population)
```


The socioeconomic variables available in this dataset are:

  - `area_hectares`: county surface.
  - `population`: county population.
  - `poverty_percentage`: population percentage below the poverty line.
  - `median_income`: median county income in dollars.
  - `domestic_product`: yearly domestic product in **b**illions (not a typo) of dollars.
  - `daily_miles_traveled`: daily miles traveled by the average inhabitant.
  - `employed_percentage`: percentage of the county population under employment.
  

```{r, echo = FALSE}
covid_counties |> 
  sf::st_drop_geometry() |> 
  dplyr::transmute(
    name, 
    area_hectares = floor(area_hectares), 
    population, 
    poverty_percentage, 
    median_income, 
    domestic_product = floor(domestic_product), 
    daily_miles_traveled = round(daily_miles_traveled, 1), 
    employed_percentage = round(employed_percentage, 1)
  ) |> 
  reactable::reactable(
    pagination = TRUE,
    searchable = TRUE,
    sortable = TRUE,
    showSortable = TRUE,
    filterable = TRUE,
    resizable = TRUE,
    defaultPageSize = 10,
    showPageSizeOptions = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = TRUE
  )
```

<div class="alert alert-warning">
  <strong>Note:</strong> These variables were included in the dataset because they were easy to capture from on-line sources, not because of their importance for epidemiological analyses..
</div>

### `covid_prevalence`

Long format data frame with weekly Covid-19 prevalence in 36 California counties between 2020-03-16 and 2023-12-18. It has the columns `name`, `time`, and `prevalence`, with the county name, the date of the first day of the week each data point represents, and the maximum Covid-19 prevalence observed during the given week, expressed as proportion of positive tests. 
  
```{r, echo = FALSE}
covid_prevalence |> 
  reactable::reactable(
    pagination = TRUE,
    searchable = TRUE,
    sortable = TRUE,
    showSortable = TRUE,
    filterable = TRUE,
    resizable = TRUE,
    defaultPageSize = 10,
    showPageSizeOptions = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = FALSE
  )
```
<br>

## Data Preparation

Before starting with the analysis, we need to transform the data frame `covid_prevalence` into the format required by `distantia`.

A **time series list**, or `tsl` for short, is a list of [`zoo`](https://cran.r-project.org/web/packages/zoo/index.html) time series representing unique realizations of the same phenomena observed in different sites, times, or individuals. All [data manipulation](https://blasbenito.github.io/distantia/articles/time_series_lists.html) and analysis functions in `distantia` are designed to work with all time series stored in a `tsl` at once. 

The code below applies `tsl_initialize()` to `covid_prevalence` and returns the object `tsl`.

```{r tsl_init, cache=TRUE, cache.path="cache/"}
tsl <- distantia::tsl_initialize(
  x = covid_prevalence,
  name_column = "name",
  time_column = "time"
)
```

Each element in `tsl` is named after the county the data belongs to.

```{r}
names(tsl)
```

The `zoo` objects within `tsl` also have the attribute `name` to help track the data in case it is extracted from the time series list.

```{r}
attributes(tsl[[1]])$name
attributes(tsl[[2]])$name
```

Each individual `zoo` object comprises a time index, extracted with `zoo::index()`, and a data matrix accessed via `zoo::coredata()`.

```{r}
zoo::index(tsl[["Alameda"]]) |> 
  head()
```

```{r}
zoo::coredata(tsl[["Alameda"]]) |> 
  head()
```

## Exploration

In the package `distantia` there are several tools to help you visualize your time series and explore their basic properties.

### Visualization

The function `tsl_plot()` provides a static multipanel visualization of a large number of time series at once.

```{r, fig.height=12}
distantia::tsl_plot(
  tsl = tsl,
  columns = 3,
  guide = FALSE,
  text_cex = 1.2
)
```

If we combine `tsl_subset()` with `tsl_plot()`, we can focus on any particular sub-group of counties and zoom-in over specific time periods.

```{r, fig.height=3}
distantia::tsl_plot(
  tsl = tsl_subset(
    tsl = tsl,
    names = c("Los_Angeles", "Kings"),
    time = c("2021-09-01", "2022-01-31")
  ),
  guide = FALSE
)
```

### Descriptive Stats

Having a good understanding of the time in a time series is important, because details such as length, resolution, time units, or regularity usually define the design of an analysis. Here, the function `tsl_time()` will give us all relevant details about the time in a time series list. 

```{r}
df_time <- distantia::tsl_time(
  tsl = tsl
)
```


```{r, echo = FALSE}
df_time |> 
  dplyr::select(name, class, units, length, resolution, begin, end) |> 
  reactable::reactable(
    pagination = TRUE,
    searchable = TRUE,
    sortable = TRUE,
    showSortable = TRUE,
    filterable = TRUE,
    resizable = TRUE,
    defaultPageSize = 10,
    showPageSizeOptions = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = FALSE
  )
```
<br>

In our case all time series are regular and have the same time resolution and range, which makes the table above rightfully boring.

Once acquainted with time, having a summary of the values in each time series also helps make sense of the data, and that's where `tsl_stats()` comes into play.
  
```{r}
df_stats <- distantia::tsl_stats(
  tsl = tsl,
  lags = 1:6 #weeks
)
```

The first part of the output corresponds with common statistical descriptors, such as centrality and dispersion metrics, and higher moments.

```{r, echo=FALSE}
df_stats |> 
  dplyr::select(name, min, q1, median, q3, max, sd, range, iq_range, skewness, kurtosis) |> 
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~ round(.x, 2))) |> 
  reactable::reactable(
    pagination = TRUE,
    searchable = TRUE,
    sortable = TRUE,
    showSortable = TRUE,
    filterable = TRUE,
    resizable = TRUE,
    defaultPageSize = 10,
    showPageSizeOptions = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = FALSE
  )
```
<br>

The second part, produced by the `lags` argument, shows the temporal autocorrelation of the different time series. Given that the resolution of all time series is 7 days per data-point, each time lag represents a week. A value of 0.9 for the lag 1 indicates the Pearson correlation between prevalence values in consecutive weeks.

```{r, echo = FALSE}
df_lags <- df_stats |> 
  select(name, dplyr::contains("lag")) |> 
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~ round(.x, 2))) |> 
  dplyr::rename_with(~ gsub("ac_", "", .)) |> 
  dplyr::rename_with(~ gsub("_", "", .))

reactable::reactable(
  df_lags,
  pagination = TRUE,
  searchable = TRUE,
  sortable = TRUE,
  showSortable = TRUE,
  filterable = TRUE,
  resizable = TRUE,
  defaultPageSize = 10,
  showPageSizeOptions = TRUE,
  striped = TRUE,
  compact = TRUE,
  wrap = FALSE,
  fullWidth = TRUE
)
```
The autocorrelation values for each lag across all counties are summarized in the following boxplot.

```{r, message = FALSE, warning = FALSE, fig.width=5, fig.height=4, echo = FALSE}
df_lags |> 
  tidyr::pivot_longer(
    cols = starts_with("lag"),
    names_to = "week"
  ) |> 
  dplyr::mutate(
    week = as.numeric(gsub("lag", "", week))
  ) |> 
  ggplot() + 
  geom_smooth(
    aes(
      x = week,
      y = value,
      alpha = 0.5
    ),
    col = alpha("gray50", 0.5),
    se = FALSE
  ) +
  geom_boxplot(
    aes(
      x = week,
      y = value,
      group = week
    ),
    notch = TRUE, 
    fill = distantia::color_continuous(n = 6),
    alpha = 0.5
  ) + 
  guides(fill = FALSE, alpha = FALSE) +
  labs(
    x = "Lag (weeks)",
    y = "Autocorrelation (Pearson)"
  ) +
  scale_x_continuous(breaks = 1:6) +
  theme_bw()
```

According to the boxplot, our prevalence time series show a very strong short-term (1-2 weeks) memory, and a rapid diminishment of long-term memory (5-6 weeks), suggesting that the influence of past prevalence on future values dissipates quickly.

<br>

The stats produced by `tsl_stats()` can be joined to `covid_counties` using their common column `name`.

```{r}
sf_stats <- dplyr::inner_join(
  x = covid_counties[, "name"],
  y = df_stats,
  by = "name"
)
```

After the join, we can map any of these stats with `mapview()`. For example, the map below shows the maximum prevalence per county across the full period. 

```{r, eval = FALSE, warning = FALSE}
mapview::mapview(
  sf_stats, 
  zcol = "q3",
  layer.name = "Max prevalence",
  label = "name",
  col.regions = distantia::color_continuous()
  )
```

```{r, echo = FALSE, cache=TRUE, warning = FALSE}
max_prevalence <- mapview(
  sf_stats, 
  zcol = "max",
  label = "name",
  col.regions = distantia::color_continuous()
  )

htmlwidgets::saveWidget(
  max_prevalence@map, 
  file = "max_prevalence.html", 
  selfcontained = TRUE
  )

rm(max_prevalence)
```

<iframe src="max_prevalence.html" name="Max_prevalence"
  width="600" height="600" scrolling="auto" frameborder="0">
   <p>Maximum Covid-19 prevalence in California counties.</p>
</iframe>

If you wish to compute stats for a given period or group of counties, then you can combine `tsl_subset()` and `tsl_stats()`, as we did before.

```{r, eval = FALSE}
df_stats_2021 <- distantia::tsl_stats(
  tsl = tsl_subset(
    tsl = tsl,
    names = c("Napa", "San_Francisco"),
    time = c("2021-01-01", "2021-12-31")
  ),
  lags = 1:6
)
```

```{r, echo = FALSE}
rm(sf_stats, df_lags, df_stats)
```

## Computing Dissimilarity Scores

The package `distantia` was designed to streamline the comparison of time series using two methods: *Lock-Step (LS)* and and [*Dynamic time warping (DTW)*](https://www.blasbenito.com/post/dynamic-time-warping/). In this section I explain both methods in brief, demonstrate how they work, and show how to compute them to compare all our prevalence time series.

### Dynamic Time Warping and Lock-Step Comparison

**LS** directly compares values at the same time points in equal-length time series. It is efficient and interpretable but cannot adjust for differences in outbreak timing, potentially missing relevant time series similarities.

**DTW** aligns time series by warping the time axis to maximize shape similarity. It accounts for differences in outbreak timing, making it useful for uneven-length or misaligned time series. However, it is computationally expensive, harder to interpret, and sensitive to scaling.

Before moving forward, let me show you a small example focused on San Francisco, Napa, and Solano to illustrate how LS and DTW work.

```{r, fig.height=3.5, echo = FALSE}
tsl_smol <- distantia::tsl_subset(
  tsl = tsl,
  names = c("Napa", "Solano", "San_Francisco"),
  time = c("2021-01-01", "2023-01-01")
)

distantia::tsl_plot(
  tsl = tsl_smol, 
  guide = FALSE,
  text_cex = 1.3
  )
```

At first glance, Napa and Solano seem quite well synchronized, while San Francisco shows a timing difference of several months. The table below shows the **LS** comparison between these time series. The `psi` column indicates the dissimilarity between pairs of time series. The smaller the value, the more similar the time series are.

```{r}
tsl_smol |> 
  distantia::distantia_ls() |> 
  dplyr::mutate(
    psi = round(psi, 3)
  ) |> 
  reactable::reactable(
    resizable = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = FALSE
  )
```
<br>

As we could expect, Napa and Solano show the lower `psi` score, indicating that these are the most similar pair in this batch. LS seems to be capturing their synchronicity quite well! On the other hand, San Francisco shows a high dissimilarity with the other two time series, as expected due to its asynchronous pattern. So far so good!

Now, let's take a look at the **DTW** result:

```{r}
tsl_smol |> 
  distantia::distantia_dtw() |> 
  dplyr::mutate(
    psi = round(psi, 3)
  ) |> 
  reactable::reactable(
    resizable = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = FALSE
  )
```
<br>

Surprisingly, when adjusting for time shifts, San Francisco appears more similar to Solano than Napa!

This happens because DTW ignores absolute dates, and allows any sample in a time series to map to multiple samples from the other. That's where the *time warping* happens! The left side of the plot below shows San Francisco’s (dashed red line) first case aligning with a group of Napa samples with prevalence 0, effectively canceling their timing differences.

```{r, fig.height=3.5, echo = FALSE}
library(dtw)
xy_dtw <- dtw::dtw(
  x = tsl_smol$Solano$prevalence,
  y = tsl_smol$San_Francisco$prevalence,
  keep = TRUE
  )

dtw::dtwPlotTwoWay(
  xy_dtw, 
  offset = 0.5,
  xlab = "Sample index",
  ylab = "Prevalence",
  main = "DTW alignment: San Francisco vs. Napa"
  )
```

These results suggest that, in spite of their timing differences, San Francisco, Napa, and Solano have a similar dynamics. This is a conclusion LS alone would miss!

It is for this reason that a DTW analysis is preferable when outbreak timing varies significantly and shape similarity is more important than exact time alignment. On the other hand, LS is best applied when the question at hand is focused on time-series synchronization rather than shape similarity.

```{r, echo = FALSE}
rm(xy_dtw)
```


### Computing DTW and LS with `distantia`

The most straightforward way of computing DTW and LS in `distantia` involves the functions `distantia_ls()` and `distantia_dtw()`. Both are simplified versions of the function `distantia()`, which has a whole set of bells and whistles that are out of the scope of this post.

The function `distantia_ls()` computes lock-step dissimilarity using euclidean distances by default. It returns a data frame with the columns `x` and `y` with the time series names, and the `psi` score.

```{r}
df_psi_ls <- distantia::distantia_ls(
  tsl = tsl
  )

str(df_psi_ls)
```

The function `distantia_dtw()` returns an output with the same structure. You will notice that it runs a bit slower than `distantia_ls()`.

```{r}
df_psi_dtw <- distantia::distantia_dtw(
  tsl = tsl
  )

str(df_psi_dtw)
```

The table below combines both results to facilitate a direct comparison between both dissimilarity scores.

```{r, echo = FALSE}
df_table <- df_psi_dtw |> 
  dplyr::select(x, y, psi) |> 
  dplyr::inner_join(
    y = dplyr::select(df_psi_ls, x, y, psi),
    by = c("x", "y")
  ) |> 
  dplyr::transmute(
    x, y,
    `psi_dtw` = round(psi.x, 3),
    psi_ls = round(psi.y, 3)
  )

reactable::reactable(
  df_table,
  pagination = TRUE,
  searchable = TRUE,
  sortable = TRUE,
  showSortable = TRUE,
  filterable = TRUE,
  resizable = TRUE,
  defaultPageSize = 10,
  showPageSizeOptions = TRUE,
  striped = TRUE,
  compact = TRUE,
  wrap = FALSE,
  fullWidth = FALSE
)
```

Here goes a scatterplot between the columns `psi_dtw` and `psi_ls`, in case you are wondering how these two metrics are related in our experiment.

```{r, fig.width=5, fig.height=4.5, echo = FALSE}
correlation <- stats::cor(df_table$psi_dtw, df_table$psi_ls)
df_table$difference <- abs(df_table$psi_ls -  df_table$psi_dtw)

ggplot(df_table) + 
  aes(
    x = psi_dtw,
    y = psi_ls,
    color = difference
  ) + 
  geom_point(
    alpha = 0.75
  ) + 
  geom_smooth(
    method = mgcv::gam, 
    col = "gray20",
    formula = y ~ s(x)
    ) + 
  geom_abline(
    intercept = 0, 
    slope = 1,
    col = "gray50",
    lty = 2
  ) + 
  scale_color_gradientn(
    colors = distantia::color_continuous()
  ) +
  labs(
    title = paste("Pearson Correlation between DTW and LS:", round(correlation, 2)),
    x = "Psi score - Dynamic Time Warping",
    y = "Psi score - Lock-Step",
    color = "Difference\nLS - DTW"
  ) +
  theme_bw()
```
The chart shows that there is good correlation between the `psi` scores for DTW and LS, so in principle, using one or the other might not be *that* important, as they may lead to similar conclusions. However, notice all these cases with a warmer color in the upper-left corner. These represent pairs of time series with a large dissimilarity mismatch between DTW and LS. Remember San Francisco *versus* Napa? If these cases are important to your research question, then DTW is the method of choice. Otherwise, LS is a reasonable alternative.

In the end, the research question must guide the methodological choice, but it is up to us to understand the consequences of such choice.

<br>

## Analyzing Dissimilarity Scores

In this section I will show you the different kinds of analyses implemented in `distantia` that may help you better understand how your time series are related. For the sake of simplicity, I will focus on the DTW results alone. If you wish to use the LS results instead, just replace `df_psi_dtw` with `df_psi_ls` in the code below. 

```{r}
df_psi <- df_psi_dtw
```

### Summary Stats

The functions `distantia_stats()` and `distantia_boxplot()` provide alternative summarized representations of dissimilarity by aggregating `psi` scores across all time series.

```{r}
df_psi_stats <- distantia::distantia_stats(
  df = df_psi
)
```

```{r, echo = FALSE}
reactable::reactable(
  df_psi_stats |>  
    mutate_if(is.numeric, round, 3),
  pagination = FALSE,
  sortable = TRUE,
  showSortable = TRUE,
  filterable = TRUE,
  resizable = TRUE,
  defaultPageSize = nrow(df_psi_stats),
  showPageSizeOptions = TRUE,
  striped = TRUE,
  compact = TRUE,
  wrap = FALSE,
  fullWidth = TRUE
)
```
<br>

The boxplot generated by `distantia_boxplot()` serves as a quick visual summary of the table above.

```{r, fig.height=8, warning = FALSE, fig.width=5}
distantia::distantia_boxplot(
  df = df_psi,
  text_cex = 1.4
)
```

### Hierarchical Clustering

The functions `distantia_cluster_hclust()` and `distantia_cluster_kmeans()` are designed to apply hierarchical and K-means clustering to the data frames produced by `distantia_dtw()` and `distantia_ls()` right away. This section will be focusing on hierarchical clustering, which is more appropriate for the analysis of epidemiological time series.

By default, the function `distantia_cluster_hclust()` selects the clustering method and the number of groups in the clustering solution via silhouette-width maximization (see [`distantia::utils_cluster_silhouette()`](https://blasbenito.github.io/distantia/reference/utils_cluster_silhouette.html) and [`distantia::utils_cluster_hclust_optimizer()`](https://blasbenito.github.io/distantia/reference/utils_cluster_hclust_optimizer.html) for further details).

```{r}
psi_cluster <- distantia::distantia_cluster_hclust(
  df = df_psi
)

names(psi_cluster)
```

The output is a list with several components, but these are the key ones:

  - `df`: data frame with time series names, cluster group, and silhouette width. 
  - `cluster_object`: the output of `stats::hclust()`.

The object `df` has the column `silhouette_width`, in the range [-1, 1], which informs about how well each case fits within its assigned cluster:

  - `<0`: misclassified.
  - `≈0`: near decision boundary.
  - `≈1`: well-clustered.
  
```{r, echo = FALSE}
reactable::reactable(
  psi_cluster$df |>  
    mutate(silhouette_width = round(silhouette_width, 3)),
  pagination = FALSE,
  sortable = TRUE,
  showSortable = TRUE,
  filterable = TRUE,
  resizable = TRUE,
  defaultPageSize = nrow(psi_cluster$df),
  showPageSizeOptions = TRUE,
  striped = TRUE,
  compact = TRUE,
  wrap = FALSE,
  fullWidth = TRUE
)
```
<br>

Notice how the hirarchical clustering highlights "Santa Cruz", with a silhouette score of `r round(psi_cluster$df$silhouette_width[which(psi_cluster$df$name == "Santa_Cruz")], 3)`, as the county with the most dubious cluster membership.

The cluster object can be plotted as a dendrogram with `factoextra::fviz_dend()`.

```{r, fig.height=10, warning = FALSE}
#number of clusters
k <- psi_cluster$clusters

factoextra::fviz_dend(
  x = psi_cluster$cluster_object, 
  k = k, 
  k_colors = distantia::color_discrete(n = k),
  cex = 1,
  label_cols = "gray20",
  rect = TRUE,  
  horiz = TRUE,
  main = "",
  ylab = "Dissimilarity"
  )
```

Mapping these groups may be helpful for the ones like me not fully acquainted with the distribution of counties in California. To get there we first need to join `psi_cluster$df` to the spatial data frame `covid_counties`.

```{r}
sf_cluster <- dplyr::inner_join(
  x = covid_counties[, "name"],
  y = psi_cluster$df,
  by = "name"
)
```

Now we can use `mapview::mapview()` again to plot cluster membership, but with a twist: we are going to code the silhouette score of each case as polygon transparency, with higher transparency indicating a lower silhouette width. This will help identify these cases that do not fully belong to their assigned groups. 

Notice that to do so, the code below applies `distantia::f_rescale_local()` to rescale silhouette width between 0.1 and 1 to adjust it to what the argument `alpha.regions` expects.

```{r, eval = FALSE, warning = FALSE}
mapview::mapview(
  sf_cluster, 
  zcol = "cluster",
  layer.name = "Cluster",
  label = "name",
  col.regions = distantia::color_discrete(n = k),
  alpha.regions = distantia::f_rescale_local(
    x = sf_cluster$silhouette_width, 
    new_min = 0.1
    )
  )
```



```{r, echo = FALSE, cache=TRUE, warning = FALSE}
clustering <- mapview::mapview(
  sf_cluster, 
  zcol = "cluster",
  layer.name = "Cluster",
  label = "name",
  col.regions = distantia::color_discrete(n = k),
  alpha.regions = distantia::f_rescale_local(
    x = sf_cluster$silhouette_width, 
    new_min = 0.1
    ),
  legend.position = "upperleft"
  )

htmlwidgets::saveWidget(
  clustering@map, 
  file = "clustering.html", 
  selfcontained = TRUE
  )

rm(max_prevalence)
```

<iframe src="clustering.html" name="Covid_clustering"
  width="600" height="600" scrolling="auto" frameborder="0">
   <p>California counties grouped by similarity in their Covid-19 prevalence curves.</p>
</iframe>


### Dissimilarity Modelling

Dissimilarity scores reveal the degree of interrelation between time series but do not explain the reasons behind their similarity/dissimilarity. 

To fill this gap, this section shows how combining dissimilarity scores with geographic and socioeconomic variables to fit models may help unveil important drivers of dissimilarity between epidemiological time series. 

#### Building a Model Frame

The function [`distantia_model_frame()`](https://blasbenito.github.io/distantia/reference/distantia_model_frame.html) combines dissimilarity scores (stored in `df_psi`) with other county variables (stored in `covid_counties`) to build a model frame.

```{r}
df_model <- distantia::distantia_model_frame(
  response_df = df_psi,
  predictors_df = covid_counties,
  scale = FALSE
)
```


```{r, echo=FALSE}
df_model |> 
  dplyr::mutate(
    dplyr::across(
      dplyr::where(is.numeric), ~ round(.x, 2)
      )
    ) |> 
  reactable::reactable(
    pagination = TRUE,
    searchable = TRUE,
    sortable = TRUE,
    showSortable = TRUE,
    filterable = TRUE,
    resizable = TRUE,
    defaultPageSize = 10,
    showPageSizeOptions = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = TRUE
  )
```
<br>

In the data frame `df_model`, the response variable `psi` (from `df_psi`) represents time series dissimilarity. On the other hand, each predictor in `covid_counties` is transformed into absolute differences between each pair of counties. For example, the value of the column `area_hectares` for the counties Sacramento and Sonoma is `r df_model$area_hectares[which(df_model$x == "Sacramento" & df_model$y == "Sonoma")]` hectares. This is the absolute difference between the area of Sacramento (`r covid_counties[["area_hectares"]][which(covid_counties$name == "Sacramento")]` hectares) and the area of Sonoma (`r covid_counties[["area_hectares"]][which(covid_counties$name == "Sonoma")]` hectares).

Notice that the column `geographic_distance`, not in `covid_counties`, also showed up as a new column. This happens when the input of `predictors_df` is spatial data frame, like `covid_counties`.

The function `distantia_model_frame()` has a neat feature: it can combine different predictors into a new one! 

In the example below, the predictors `poverty_percentage`, `median_income`, `employed_percentage`, `daily_miles_traveled` and `domestic_product` are scaled, their multivariate distances between pairs of counties are computed and stored in the column `economy`. This new variable represents overall differences in economy between counties. 

Notice that the code below also scales the predictors, because in `distantia` version <=2.0.2 the argument `scale` of `distantia_model_frame()` does not work as it should.

```{r}
df_model <- distantia::distantia_model_frame(
  response_df = df_psi,
  predictors_df = covid_counties,
  composite_predictors = list(
    economy = c(
      "poverty_percentage",
      "median_income",
      "domestic_product",
      "employed_percentage",
      "daily_miles_traveled"
    )
  ),
  #scale = TRUE is broken in version <= 2.0.2
  scale = FALSE 
  ) |> 
  #selecting and renaming columns
  dplyr::transmute(
    psi, 
    economy, 
    population,
    area = area_hectares,
    distance = geographic_distance
  ) |> 
  #scale
  dplyr::mutate(
    dplyr::across(
      dplyr::all_of(c("economy", "distance", "population", "area")), 
      ~ as.numeric(scale(.x))
    )
  )
```

```{r, echo=FALSE}
df_model |> 
  dplyr::mutate(
    dplyr::across(
      dplyr::where(is.numeric), ~ round(.x, 2)
      )
    ) |> 
  reactable::reactable(
    pagination = TRUE,
    searchable = TRUE,
    sortable = TRUE,
    showSortable = TRUE,
    filterable = TRUE,
    resizable = TRUE,
    defaultPageSize = 10,
    showPageSizeOptions = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = TRUE
  )
```
<br>

With these variables at hand now we are ready to better understand how the similarity/dissimilarity between prevalence time series changes as a function of differences in distance, area, economy, and population between counties.

#### Fitting a Linear Model

The model frame generated above shows no multicollinearity between predictors (you can check yourself with `cor(df_model[, -1])`), so it can be used right away fit a multiple regression model.

```{r}
m <- stats::lm(
  formula = psi ~ distance + area + economy + population,
  data = df_model
)
```

<br>

The model has an R-squared of `r round(summary(m)$r.squared, 2)` and a standard error of the residuals (sigma) of `r round(summary(m)$sigma, 2)`. Model coefficients are shown below.

```{r, echo = FALSE}
m |> 
  summary() |> 
  broom::tidy() |> 
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~ round(.x, 4))) |> 
  dplyr::arrange(dplyr::desc(estimate)) |> 
  dplyr::filter(term != "(Intercept)") |> 
  reactable(
    striped = TRUE,
    fullWidth = TRUE,
    resizable = TRUE,
    sortable = TRUE,
    showSortable = TRUE
    )
```
<br>

The coefficients table shows that all effects are statistically different than zero. The predictor `distance` has the largest effect estimate, which is almost twice the estimate of `area` and `economy`, and more than four times the estimate of `population`.


```{r, fig.height=6, fig.width=6, echo = FALSE}
par(mfrow = c(2, 2), cex.axis = 1.5, cex.lab = 2)
line_params <- list(col = "red4", lwd = 1)
visreg::visreg(
  fit = m,
  xvar = "distance",
  line = line_params
)
visreg::visreg(
  fit = m,
  xvar = "area",
  line = line_params
)
visreg::visreg(
  fit = m,
  xvar = "economy",
  line = line_params
)
visreg::visreg(
  fit = m,
  xvar = "population",
  line = line_params
)
```

All predictors show a positive relationship with the response, meaning that increasing differences in distance, area, economy, and population leads to increased dissimilarity between the compared time series.

The dissimilarity model seems relatively strong, but, as the calibration plot below shows, it is also moderately biased. 

```{r, fig.width=3.5, fig.height=3.5}
df_model$psi_predicted <- stats::predict(object = m)

ggplot(df_model) + 
  aes(
    x = psi,
    y = psi_predicted
  ) + 
  geom_point(alpha = 0.5, col = "gray50") + 
  coord_fixed(
    xlim = range(c(df_model$psi, df_model$psi_predicted)),
    ylim = range(c(df_model$psi, df_model$psi_predicted))
  ) + 
  geom_smooth(
    method = "lm", 
    col = "red4",
    formula = y ~ x
    ) + 
  geom_abline(
    intercept = 0, 
    slope = 1,
    col = "black",
    lty = 2
  ) +
  labs(
    x = "Observed psi",
    y = "Predicted psi",
    title = "Calibration plot - Linear Model"
  ) +
  theme_bw()
```
The scatterplot of the observations (x axis) vs predictions shows that the model underpredicts high psi values and overpredict low psi values. This kind of bias indicates that there are relevant predictors, interactions, or transformations missing from the model.

#### Training a Random Forest Model

If the objective is obtaining a model with a stronger predictive ability, Random Forest always can help with that!

```{r}
m <- ranger::ranger(
  formula = psi ~ distance + area + economy + population,
  data = df_model,
  importance = "permutation"
)
```

The random forest model has an R-squared (on the out-of-bag data) of `r round(m$r.squared, 2)`, and as the permutation importance scores below show, it downplays the effects of `economy` and `population` when compared with the linear model.

```{r, echo = FALSE}
data.frame(
  variable = names(m$variable.importance),
  importance = m$variable.importance
) |> 
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~ round(.x, 4))) |> 
  reactable(
    striped = TRUE,
    fullWidth = FALSE,
    resizable = TRUE,
    sortable = TRUE,
    showSortable = TRUE,
    rownames = FALSE
    )
```

Still, as the calibration plot below shows, it has a very small amount of prediction bias.

```{r, fig.width=3.5, fig.height=3.5}
df_model$psi_predicted <- stats::predict(
  object = m, 
  data = df_model
  )$predictions

ggplot(df_model) + 
  aes(
    x = psi,
    y = psi_predicted
  ) + 
  geom_point(alpha = 0.5, col = "gray50") + 
  coord_fixed(
    xlim = range(c(df_model$psi, df_model$psi_predicted)),
    ylim = range(c(df_model$psi, df_model$psi_predicted))
  ) + 
  geom_smooth(
    method = "lm", 
    col = "red4",
    formula = y ~ x
    ) + 
  geom_abline(
    intercept = 0, 
    slope = 1,
    col = "black",
    lty = 2
  ) +
  labs(
    x = "Observed psi",
    y = "Predicted psi",
    title = "Calibration plot - Random Forest"
  ) +
  theme_bw()
```


## Closing Thoughts

TODO