<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blas M. Benito, PhD</title>
    <link>https://blasbenito.com/</link>
      <atom:link href="https://blasbenito.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Blas M. Benito, PhD</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2023 Blas M. Benito. All Rights Reserved.</copyright><lastBuildDate>Mon, 13 Jan 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://blasbenito.com/media/avatar.jpg</url>
      <title>Blas M. Benito, PhD</title>
      <link>https://blasbenito.com/</link>
    </image>
    
    <item>
      <title>Example Page 1</title>
      <link>https://blasbenito.com/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://blasbenito.com/courses/example/example1/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>https://blasbenito.com/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://blasbenito.com/courses/example/example2/</guid>
      <description>&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coding a Minimalistic Dynamic Time Warping Library with R</title>
      <link>https://blasbenito.com/post/dynamic-time-warping-from-scratch/</link>
      <pubDate>Mon, 13 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/dynamic-time-warping-from-scratch/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;This post walks you through the implementation of a minimalistic yet fully functional 
&lt;a href=&#34;https://www.blasbenito.com/post/dynamic-time-warping/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Time Warping&lt;/a&gt; (DTW) library in R, built entirely from scratch without dependencies or complex abstractions. While there are many 
&lt;a href=&#34;https://blasbenito.github.io/distantia/articles/dtw_applications.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;open-source DTW implementations&lt;/a&gt; readily available, understanding the inner workings of the algorithm can be invaluable. Whether youâre simply curious or need a deeper grasp of DTW for your projects, this step-by-step guide offers a hands-on approach to demystify the method.&lt;/p&gt;
&lt;h1 id=&#34;design&#34;&gt;Design&lt;/h1&gt;
&lt;h2 id=&#34;example-data&#34;&gt;Example Data&lt;/h2&gt;
&lt;p&gt;Having good example data at hand is a must when developing new code. For this tutorial we use three multivariate time series of temperature, rainfall, and normalized vegetation index. These time series are named &lt;code&gt;zoo_germany&lt;/code&gt;, &lt;code&gt;zoo_sweden&lt;/code&gt;, and &lt;code&gt;zoo_spain&lt;/code&gt;, and are stored as objects of the class 
&lt;a href=&#34;https://CRAN.R-project.org/package=zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;zoo&lt;/a&gt;, which is a very robust time series management library.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Each zoo object has a &lt;em&gt;core data&lt;/em&gt; of the class &lt;code&gt;matrix&lt;/code&gt; with one observation per row and one variable per column, and an &lt;em&gt;index&lt;/em&gt;, which is a vector of dates, one per row in the time series.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zoo::coredata(zoo_sweden)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         evi rainfall temperature
## 1092 0.1259     32.0        -4.4
## 1132 0.1901     55.6        -2.8
## 1142 0.2664     38.8         1.8
## 1152 0.2785     20.3         7.0
## 1162 0.7068     59.4        10.1
## 1172 0.7085     69.5        14.3
## 1182 0.6580     85.2        19.2
## 1192 0.5831    150.2        16.8
## 1202 0.5036     74.9        12.7
## 1103 0.3587     74.9         7.8
## 1113 0.2213    114.6         2.5
## 1122 0.1475     52.1        -4.7
## 1213 0.2140     49.5        -0.8
## attr(,&amp;quot;name&amp;quot;)
## [1] &amp;quot;zoo_sweden&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zoo::index(zoo_sweden)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;2010-01-01&amp;quot; &amp;quot;2010-02-01&amp;quot; &amp;quot;2010-03-01&amp;quot; &amp;quot;2010-04-01&amp;quot; &amp;quot;2010-05-01&amp;quot;
##  [6] &amp;quot;2010-06-01&amp;quot; &amp;quot;2010-07-01&amp;quot; &amp;quot;2010-08-01&amp;quot; &amp;quot;2010-09-01&amp;quot; &amp;quot;2010-10-01&amp;quot;
## [11] &amp;quot;2010-11-01&amp;quot; &amp;quot;2010-12-01&amp;quot; &amp;quot;2011-01-01&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;required-library-functions&#34;&gt;Required Library Functions&lt;/h2&gt;
&lt;p&gt;The section &lt;em&gt;DTW Step by Step&lt;/em&gt; from the previous article 
&lt;a href=&#34;https://www.blasbenito.com/post/dynamic-time-warping/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Gentle Intro to Dynamic Time Warping&lt;/a&gt; describes the computational steps required by the algorithm. Below, these steps are broken down into sub-steps that will correspond to specific functions in our library:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time Series Pre-processing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These steps help DTW work seamlessly with the input time series.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear detrending&lt;/strong&gt;: forces time series to be stationary by removing any upwards or downwards trends.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Z-score normalization&lt;/strong&gt;: equalizes the range of the time series to ensure that the different variables contribute evenly to the distance computation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Time Warping&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These steps perform dynamic time warping and evaluate the dissimilarity between the time series.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multivariate distance&lt;/strong&gt;: compute distances between pairs of samples from each time series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distance matrix&lt;/strong&gt;: organize the multivariate distances in a matrix in which each axis represents a time series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cost matrix&lt;/strong&gt;: this matrix accumulates the distances in the distance matrix across time and represents all possible alignments between two time series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Least-cost path&lt;/strong&gt;: path in the cost matrix that minimizes the overall distance between two time series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dissimilarity metric&lt;/strong&gt;: value to summarize the dissimilarity between time series.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Main function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once all the steps above are implemented in their respective functions, we will wrap all them in a single function to streamline the DTW analysis.&lt;/p&gt;
&lt;h1 id=&#34;implementation&#34;&gt;Implementation&lt;/h1&gt;
&lt;p&gt;In this section we will be developing the library concept by concept and function by function.&lt;/p&gt;
&lt;h2 id=&#34;time-series-pre-processing&#34;&gt;Time Series Pre-processing&lt;/h2&gt;
&lt;p&gt;In this section we develop the function &lt;code&gt;ts_preprocessing()&lt;/code&gt;, which will prepare the time series data for dynamic time warping. This function contains two functionalities: &lt;strong&gt;linear detrending&lt;/strong&gt;, and &lt;strong&gt;z-score normalization&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;linear-detrending&#34;&gt;Linear Detrending&lt;/h3&gt;
&lt;p&gt;Applying linear detrending to a multivariate time series involves computing a linear model of each variable against time, and subtracting the the model prediction to the original data. This can be performed in two steps:&lt;/p&gt;
&lt;p&gt;First, the function &lt;code&gt;stats::lm()&lt;/code&gt; can be applied to all variables in one of our time series at once.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model_sweden &amp;lt;- stats::lm(
  formula = zoo_sweden ~ stats::time(zoo_sweden)
  )

model_sweden
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## stats::lm(formula = zoo_sweden ~ stats::time(zoo_sweden))
## 
## Coefficients:
##                          evi         rainfall    temperature
## (Intercept)               8.965e-01  -1.710e+03  -5.706e+01 
## stats::time(zoo_sweden)  -3.480e-05   1.202e-01   4.271e-03
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, the residuals of the linear model, which represent the differences between observations and predictions, correspond exactly with the detrended time series. As a plus, these residuals are returned as a zoo object when the &lt;code&gt;zoo&lt;/code&gt; library is loaded!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;stats::residuals(model_sweden)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    evi     rainfall temperature
## 2010-01-01 -0.26214896 -13.62153187   -9.739028
## 2010-02-01 -0.19687011   6.25374419   -8.271433
## 2010-03-01 -0.11959566 -13.91052259   -3.791024
## 2010-04-01 -0.10641680 -36.13524652    1.276572
## 2010-05-01  0.32292725  -0.63981807    4.248439
## 2010-06-01  0.32570610   5.73545800    8.316034
## 2010-07-01  0.27625015  17.83088645   13.087901
## 2010-08-01  0.20242901  79.10616252   10.555496
## 2010-09-01  0.12400786   0.08143858    6.323092
## 2010-10-01 -0.01984809  -3.52313297    1.294959
## 2010-11-01 -0.15616924  32.45214310   -4.137446
## 2010-12-01 -0.22892518 -33.65242845  -11.465579
## 2011-01-01 -0.16134633 -39.97715238   -7.697983
## attr(,&amp;quot;name&amp;quot;)
## [1] zoo_sweden
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;
Then, the pre-processing function of our library could be something like this:
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Linear Detrending
#&#39; @param x (required, zoo object) time series to detrend.
#&#39; @return zoo object
ts_preprocessing &amp;lt;- function(x){
  m &amp;lt;- stats::lm(formula = x ~ stats::time(x))
  y &amp;lt;- stats::residuals(object = m)
  y
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function could be more concise, but it is written to facilitate line-by-line debugging instead. I have also added minimal roxygen documentation. Future me usually appreciates this kind of extra effort. Any kind of effort actually.&lt;/p&gt;
&lt;p&gt;This function should check that &lt;code&gt;x&lt;/code&gt; is really a zoo object, and any other condition that would make it fail, but to keep code simple, in this tutorial we won&amp;rsquo;t do any error catching.&lt;/p&gt;
&lt;p&gt;We can use a mock-up time series with an ascending trend to really test the effect of our detrending function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- zoo::zoo(0:10)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;If we apply &lt;code&gt;ts_preprocessing()&lt;/code&gt; to this time series, the result shows a horizontal line, which is a perfect linear detrending. Now we can be sure our implementation works!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x_detrended &amp;lt;- ts_preprocessing(x = x)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h3 id=&#34;z-score-normalization&#34;&gt;Z-score Normalization&lt;/h3&gt;
&lt;p&gt;Normalization (also named &lt;em&gt;standardization&lt;/em&gt;) consists of two operations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Centering&lt;/strong&gt;: performed by subtracting the column mean to each case, resulting in a column mean equal to zero.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scaling&lt;/strong&gt;: divides each case by the standard deviation of the column, resulting in a standard deviation equal to one.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The R base function &lt;code&gt;scale()&lt;/code&gt; implements z-score normalization, so there&amp;rsquo;s not much we have to do from scratch here. Also, when the library &lt;code&gt;zoo&lt;/code&gt; is loaded, the method &lt;code&gt;zoo:::scale.zoo&lt;/code&gt; (&lt;code&gt;:::&lt;/code&gt; denotes methods and functions that are not exported by a package) allows &lt;code&gt;scale()&lt;/code&gt; to work seamlessly with zoo objects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;scale(
  x = zoo_germany,
  center = TRUE,
  scale = TRUE
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   evi    rainfall temperature
## 2010-01-01 -2.1072111 -0.77369778 -1.37937171
## 2010-02-01 -0.5757809 -0.44531242 -0.97212864
## 2010-03-01 -0.4963522 -0.87526026 -0.40724308
## 2010-04-01 -0.1500661 -1.49817681  0.26273747
## 2010-05-01  1.2590783  1.21354145  0.39410620
## 2010-06-01  1.3482213 -0.06614582  1.20859236
## 2010-07-01  0.9973637  0.53307282  1.61583543
## 2010-08-01  0.9990780  1.72812469  1.19545548
## 2010-09-01  0.3750773 -0.07630207  0.66998055
## 2010-10-01  0.2670772 -0.94973941  0.05254749
## 2010-11-01 -0.3477806  0.33671869 -0.38096933
## 2010-12-01 -0.7843525  1.44713515 -1.36623484
## 2011-01-01 -0.7843525 -0.57395823 -0.89330739
## attr(,&amp;quot;name&amp;quot;)
## [1] zoo_germany
## attr(,&amp;quot;scaled:center&amp;quot;)
##         evi    rainfall temperature 
##   0.4376615  60.8538462   8.8000000 
## attr(,&amp;quot;scaled:scale&amp;quot;)
##         evi    rainfall temperature 
##   0.1749998  29.5384669   7.6121613
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to this seamless integration with &lt;code&gt;zoo&lt;/code&gt; time series, z-score normalization can be easily added to our pre-processing function!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Linear Detrending and Normalization
#&#39; @param x (required, zoo object) time series to detrend.
#&#39; @return zoo object
ts_preprocessing &amp;lt;- function(x){
  m &amp;lt;- stats::lm(formula = x ~ stats::time(x))
  y &amp;lt;- stats::residuals(object = m)
  z &amp;lt;- scale(y) #new
  z
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now test &lt;code&gt;ts_preprocessing()&lt;/code&gt; and move forward with our implementation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ts_preprocessing(x = zoo_germany)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    evi     rainfall temperature
## 2010-01-01 -1.91389650 -0.254652344 -1.35356479
## 2010-02-01 -0.40410400 -0.001606348 -0.95069824
## 2010-03-01 -0.35682407 -0.548358009 -0.38973731
## 2010-04-01 -0.04363355 -1.310450989  0.27590451
## 2010-05-01  1.34386644  1.489002490  0.40300011
## 2010-06-01  1.39742780  0.026066230  1.21316833
## 2010-07-01  1.00791041  0.571260913  1.61617795
## 2010-08-01  0.97319785  1.749131031  1.19130241
## 2010-09-01  0.30672100 -0.273757334  0.66131677
## 2010-10-01  0.16240893 -1.300041138  0.03950285
## 2010-11-01 -0.49483667 -0.024630977 -0.39851146
## 2010-12-01 -0.97089711  1.066065435 -1.38821075
## 2011-01-01 -1.00734053 -1.188028960 -0.91965038
## attr(,&amp;quot;name&amp;quot;)
## [1] zoo_germany
## attr(,&amp;quot;scaled:center&amp;quot;)
##          evi     rainfall  temperature 
## 2.135044e-18 8.113168e-16 5.465713e-16 
## attr(,&amp;quot;scaled:scale&amp;quot;)
##         evi    rainfall temperature 
##   0.1733241  27.6809389   7.6110663
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;dynamic-time-warping-functions&#34;&gt;Dynamic Time Warping Functions&lt;/h2&gt;
&lt;p&gt;This section describes the implementation of the DTW algorithm, which requires functions to compute a distance matrix, convert it to a cost matrix, find a least-cost path maximizing the alignment between the time series, and compute their dissimilarity.&lt;/p&gt;
&lt;h3 id=&#34;distance-matrix&#34;&gt;Distance Matrix&lt;/h3&gt;
&lt;p&gt;In DTW, a distance matrix represents the distances between all pairs of samples in two time series. Hence, each time series is represented in one axis of the matrix. But before getting there, we need a function to obtain the distance between arbitrary pairs of rows from two separate zoo objects.&lt;/p&gt;
&lt;h4 id=&#34;distance-function&#34;&gt;Distance Function&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s say we have two vectors, &lt;code&gt;x&lt;/code&gt; with one row of &lt;code&gt;zoo_germany&lt;/code&gt;, and &lt;code&gt;y&lt;/code&gt; with one row of &lt;code&gt;zoo_sweden&lt;/code&gt;. Then, the expression to compute the Euclidean distances between &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; becomes &lt;code&gt;sqrt(sum((x-y)^2))&lt;/code&gt;. From there, implementing a distance function is kinda trivial.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Euclidean Distance
#&#39; @param x (required, numeric) row of a zoo object.  
#&#39; @param y (required, numeric) row of a zoo object.
#&#39; @return numeric
distance_euclidean &amp;lt;- function(x, y){
  sqrt(sum((x - y)^2))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the function does not indicate the return explicitly. Since the function&amp;rsquo;s body is a one-liner, one cannot be really worried about the function returning something unexpected. Also, implementing such a simple expression in a function might seem like too much, but it may facilitate the addition of new distance metrics to the library in the future. For example, we could create something like &lt;code&gt;distance_manhattan()&lt;/code&gt; with the Manhattan distance, and later switch between one or another depending on the user&amp;rsquo;s needs.&lt;/p&gt;
&lt;p&gt;The code below tests the function by computing the euclidean distance between the row 1 from &lt;code&gt;zoo_sweden&lt;/code&gt; and the row 2 from &lt;code&gt;zoo_germany&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zoo_sweden[1, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               evi rainfall temperature
## 2010-01-01 0.1259       32        -4.4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zoo_germany[2, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               evi rainfall temperature
## 2010-02-01 0.3369     47.7         1.4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;distance_euclidean(
  x = zoo_sweden[1, ],
  y = zoo_germany[2, ]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sorry, what? That doesn&amp;rsquo;t seem right!&lt;/p&gt;
&lt;p&gt;For whatever reason, &lt;code&gt;zoo_sweden[1, ]&lt;/code&gt; and &lt;code&gt;zoo_germany[2, ]&lt;/code&gt; are not being interpreted as numeric vectors by &lt;code&gt;distance_euclidean()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try something different:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;distance_euclidean(
  x = as.numeric(zoo_sweden[1, ]),
  y = as.numeric(zoo_germany[2, ])
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16.73841
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, that makes more sense!&lt;/p&gt;
&lt;p&gt;Then, we just have to move these &lt;code&gt;as.numeric()&lt;/code&gt; commands inside &lt;code&gt;distance_euclidean()&lt;/code&gt; to simplify the usage of the function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Euclidean Distance
#&#39; @param x (required, numeric) row of a zoo object.  
#&#39; @param y (required, numeric) row of a zoo object.
#&#39; @return numeric
distance_euclidean &amp;lt;- function(x, y){
  x &amp;lt;- as.numeric(x) #new
  y &amp;lt;- as.numeric(y) #new
  z &amp;lt;- sqrt(sum((x - y)^2))
  z
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new function should have no issues returning the right distance between these rows now:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;distance_euclidean(
  x = zoo_sweden[1, ],
  y = zoo_germany[2, ]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16.73841
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can go compute the distance matrix.&lt;/p&gt;
&lt;h4 id=&#34;distance-matrix-1&#34;&gt;Distance Matrix&lt;/h4&gt;
&lt;p&gt;To generate the distance matrix, the function &lt;code&gt;distance_euclidean()&lt;/code&gt; must be applied to all pairs of rows in the two time series.&lt;/p&gt;
&lt;p&gt;A simple yet inefficient way to do this involves creating an empty matrix, and traversing it cell by cell to compute the euclidean distances between the corresponding pair of rows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#empty distance matrix
m_dist &amp;lt;- matrix(
  data = NA, 
  nrow = nrow(zoo_germany), 
  ncol = nrow(zoo_sweden)
)

#iterate over rows
for(row in 1:nrow(zoo_germany)){
  
  #iterate over columns
  for(col in 1:nrow(zoo_sweden)){
    
    #distance between time series rows
    m_dist[row, col] &amp;lt;- distance_euclidean(
      x = zoo_germany[col, ],
      y = zoo_sweden[row, ]
    )
    
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code generates a matrix with &lt;code&gt;zoo_germany&lt;/code&gt; in the rows, from top to bottom, and &lt;code&gt;zoo_sweden&lt;/code&gt; in the columns, from left to right. The first five rows and columns are shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_dist[1:5, 1:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]      [,3]      [,4]     [,5]
## [1,]  6.579761 16.738415 10.538528 21.639813 66.69942
## [2,] 17.634758  8.948271 22.285328 41.303861 43.61868
## [3,]  3.595693  8.909263  5.445835 23.955397 58.75852
## [4,] 19.723690 27.966469 14.757548  5.305437 76.55158
## [5,] 24.446000 14.584815 24.796103 42.806743 37.33875
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This matrix can be plotted with the function &lt;code&gt;graphics::image()&lt;/code&gt;, but please be aware that it rotates the distance matrix 90 degrees counter clock-wise, which can be pretty confusing at first.&lt;/p&gt;
&lt;p&gt;Remember this: &lt;strong&gt;in the matrix plot, the x axis represents the matrix rows&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;graphics::image(
  x = seq_len(nrow(m_dist)),
  y = seq_len(ncol(m_dist)),
  z = m_dist,
  xlab = &amp;quot;zoo_germany&amp;quot;,
  ylab = &amp;quot;zoo_sweden&amp;quot;,
  main = &amp;quot;Euclidean Distance&amp;quot;
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Darker values indicate larger distances between pairs of samples in each time series.&lt;/p&gt;
&lt;p&gt;We can now wrap the code above (without the plot) in a new function named &lt;code&gt;distance_matrix()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Distance Matrix Between Time Series
#&#39; @param a (required, zoo object) time series.
#&#39; @param b (required, zoo object) time series with same columns as `x`
#&#39; @return matrix
distance_matrix &amp;lt;- function(a, b){
  
  m &amp;lt;- matrix(
    data = NA, 
    nrow = nrow(b), 
    ncol = nrow(a)
  )
  
  for (row in 1:nrow(b)) {
    for (col in 1:nrow(a)) {
      m[row, col] &amp;lt;- distance_euclidean(
        x = a[col, ],
        y = b[row, ] 
      )
    }
  }
  
  m
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s run a little test before moving forward!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_dist &amp;lt;- distance_matrix(
  a = zoo_germany,
  b = zoo_sweden
)

m_dist[1:5, 1:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]      [,3]      [,4]     [,5]
## [1,]  6.579761 16.738415 10.538528 21.639813 66.69942
## [2,] 17.634758  8.948271 22.285328 41.303861 43.61868
## [3,]  3.595693  8.909263  5.445835 23.955397 58.75852
## [4,] 19.723690 27.966469 14.757548  5.305437 76.55158
## [5,] 24.446000 14.584815 24.796103 42.806743 37.33875
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are good to go! The next function will transform this distance matrix into a &lt;em&gt;cost matrix&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;cost-matrix&#34;&gt;Cost Matrix&lt;/h3&gt;
&lt;p&gt;Now we are getting into the important parts of the DTW algorithm!&lt;/p&gt;
&lt;p&gt;A cost matrix is like a valley&amp;rsquo;s landscape, with hills in regions where the time series are different, and ravines where they are more similar. Such landscape is built by accumulating the values of the distance matrix cell by cell, from &lt;code&gt;[1, 1]&lt;/code&gt; at the bottom of the valley (upper left corner of the matrix, but lower left in the plot), to &lt;code&gt;[m, n]&lt;/code&gt; at the top (lower right corner of the matrix, upper right in the plot).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see how that works!&lt;/p&gt;
&lt;p&gt;First, we use the dimensions of the distance matrix to create an empty cost matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_cost &amp;lt;- matrix(
  data = NA, 
  nrow = nrow(m_dist), 
  ncol = ncol(m_dist)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, to initialize the cost matrix we accumulate the values of the first row and column of the distance matrix using &lt;code&gt;cumsum()&lt;/code&gt;. This step is very important for the second part of the algorithm, as it provides the starting values of the cost matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_cost[1, ] &amp;lt;- cumsum(m_dist[1, ])
m_cost[, 1] &amp;lt;- cumsum(m_dist[, 1])
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Now, before going into the third step, let&amp;rsquo;s focus for a moment on the first cell of the cost matrix we need to fill, with coordinates &lt;code&gt;[2, 2]&lt;/code&gt; and value &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_cost[1:2, 1:2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]     [,2]
## [1,]  6.579761 23.31818
## [2,] 24.214519       NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new value of this cell results from the addition of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Its value in the distance matrix &lt;code&gt;m_dist&lt;/code&gt; (8.95).&lt;/li&gt;
&lt;li&gt;The minimum accumulated distance of its neighbors, which are:
&lt;ul&gt;
&lt;li&gt;Upper neighbor with coordinates &lt;code&gt;[1, 2]&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Left neighbor with coordinates &lt;code&gt;[2, 1]&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The general expression to find the value of the empty cell is shown below. It uses &lt;code&gt;min()&lt;/code&gt; to get the value of the &lt;em&gt;smallest&lt;/em&gt; neighbor, and then adds it to the vaue of the target cell in the distance matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_cost[2, 2] &amp;lt;- min(
  m_cost[1, 2], 
  m_cost[2, 1]
  ) + m_dist[2, 2]

m_cost[1:2, 1:2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]     [,2]
## [1,]  6.579761 23.31818
## [2,] 24.214519 32.26645
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But there are many cells to fill yet!&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;The expression we used to fill the cell &lt;code&gt;m_cost[2, 2]&lt;/code&gt; can be generalized to fill all remaining empty cells. We just have to wrap it in a nested loop that for each new empty cell identifies the smallest neighbor in the x and y axies, and adds its cumulative cost to the distance of the new cell.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#iterate over rows of the cost matrix
for(row in 2:nrow(m_dist)){
  
  #iterate over columns of the cost matrix
  for(col in 2:ncol(m_dist)){
    
    #get cost of neighbor with minimum accumulated cost
    min_cost &amp;lt;- min(
      m_cost[row - 1, col], 
      m_cost[row, col - 1]
      )
    
    #add it to the distance of the target cell
    new_value &amp;lt;- min_cost + m_dist[row, col]
    
    #fill the empty cell with the new value
    m_cost[row, col] &amp;lt;- new_value
    
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running the code above results in a nicely filled cost matrix!&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Now that we have all the pieces figured out, we can define our new function to compute the cost matrix. Notice that the code within the nested loops is slightly more concise than shown before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Cost Matrix from Distance Matrix
#&#39; @param distance_matrix (required, matrix) distance matrix.
#&#39; @return matrix
cost_matrix &amp;lt;- function(distance_matrix){
  
  m &amp;lt;- matrix(
    data = NA, 
    nrow = nrow(distance_matrix), 
    ncol = ncol(distance_matrix)
  )
  
  m[1, ] &amp;lt;- cumsum(distance_matrix[1, ])
  m[, 1] &amp;lt;- cumsum(distance_matrix[, 1])
  
  for(row in 2:nrow(distance_matrix)){
    for(col in 2:ncol(distance_matrix)){
      
      m[row, col] &amp;lt;- min(
        m[row - 1, col], 
        m[row, col - 1]
      ) + distance_matrix[row, col]
      
    }
  }
  
  m
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s test our new function using &lt;code&gt;m_dist&lt;/code&gt; as input:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_cost &amp;lt;- cost_matrix(distance_matrix = m_dist)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;So far so good! We can now dive into the generation of the least-cost path.&lt;/p&gt;
&lt;h3 id=&#34;least-cost-path&#34;&gt;Least-Cost Path&lt;/h3&gt;
&lt;p&gt;If we describe the cost matrix as a valley with its hills and ravines, then the least-cost path is the river following the line of maximum slope all the way to the bottom of the valley. Following the analogy, the least-cost path starts in the terminal cell of the cost matrix (&lt;code&gt;[13, 13]&lt;/code&gt;), and ends in the first cell.&lt;/p&gt;
&lt;p&gt;To find the least-cost path we first define a data frame with the coordinates of the terminal cell in the cost matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;path &amp;lt;- data.frame(
  row = ncol(m_cost),
  col = nrow(m_cost)
)

path
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   row col
## 1  13  13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the first step of the least cost path. From here, there are two candidate steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;one column left&lt;/em&gt;: &lt;code&gt;[row, col - 1]&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;one row up&lt;/em&gt;: &lt;code&gt;[row - 1, col]&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But before moving forward, notice that if we apply these steps indefinitely in our cost matrix, at some point a move up &lt;code&gt;row - 1&lt;/code&gt; or left &lt;code&gt;col - 1&lt;/code&gt; will go out of bounds and produce an error. That&amp;rsquo;s why it&amp;rsquo;s safer to define the next move as&amp;hellip;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;one column left&lt;/em&gt;: &lt;code&gt;[row, max(col - 1, 1)]&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;one row up&lt;/em&gt;: &lt;code&gt;[max(row - 1, 1), col]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;hellip;which confines all steps within the first row and column of the cost matrix.&lt;/p&gt;
&lt;p&gt;With that out of the way, now we have to select the move towards a cell with a lower cost. There are many ways to accomplish this task! Let&amp;rsquo;s look one of them.&lt;/p&gt;
&lt;p&gt;First, we define the candidate moves using the first row of the least-cost path as reference, and generate a list with the coordinates of the candidate steps.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;steps &amp;lt;- list(
  left = c(path$row, max(path$col - 1, 1)),
  up = c(max(path$row - 1, 1), path$col)
)

steps
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $left
## [1] 13 12
## 
## $up
## [1] 12 13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we extract the values of the cost matrix for the coordinates of these two steps.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;costs &amp;lt;- list(
  left = m_cost[steps$left[1], steps$left[2]],
  up = m_cost[steps$up[1], steps$up[2]]
)

costs
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $left
## [1] 495.481
## 
## $up
## [1] 457.7403
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we choose the candidate step with the lower cost using &lt;code&gt;which.min()&lt;/code&gt;, a function that returns the index of the smallest value in a vector or list. Notice that we use &lt;code&gt;[1]&lt;/code&gt; in &lt;code&gt;which.min(costs)[1]&lt;/code&gt; to resolve potential ties that may be returned by &lt;code&gt;which.min()&lt;/code&gt; if the two costs are the same (unlikely, but possible).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;steps[[which.min(costs)[1]]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12 13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Combining these pieces we can now build a function named &lt;code&gt;least_cost_step()&lt;/code&gt; that takes the cost matrix and the last row of a least-cost path, and returns a new row with the coordinates of the next step.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Identify Next Step of Least-Cost Path
#&#39; @param cost_matrix (required, matrix) cost matrix.
#&#39; @param last_step (required, data frame) one row data frame with columns &amp;quot;row&amp;quot; and &amp;quot;col&amp;quot; representing the last step of a least-cost path.
#&#39; @return one row data frame, new step in least-cost path
least_cost_step &amp;lt;- function(cost_matrix, last_step){
  
  #define candidate steps
  steps &amp;lt;- list(
    left = c(last_step$row, max(last_step$col - 1, 1)),
    up = c(max(last_step$row - 1, 1), last_step$col)
  )
  
  #obtain their costs
  costs &amp;lt;- list(
    left = cost_matrix[steps$left[1], steps$left[2]],
    up = cost_matrix[steps$up[1], steps$up[2]]
  )
  
  #select the one with a smaller cost
  coords &amp;lt;- steps[[which.min(costs)[1]]]
  
  #rewrite input with new values
  last_step[,] &amp;lt;- c(coords[1], coords[2])
  
  last_step
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the function overwrites the input data frame &lt;code&gt;step&lt;/code&gt; with the new values to avoid generating a new data frame, making the code a bit more concise.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s check how it works:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;least_cost_step(
  cost_matrix = m_cost, 
  last_step = path
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   row col
## 1  12  13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Good, it returned the move to the upper neighbor!&lt;/p&gt;
&lt;p&gt;Now, if you think about the function for a bit, you&amp;rsquo;ll see that it takes a step in the least-cost path, and returns a new one. From there, it seems we can feed it its own result again and again until it runs out of new steps to find.&lt;/p&gt;
&lt;p&gt;We can do that in a concise way using a &lt;code&gt;repeat{}&lt;/code&gt; loop. Notice that it will keep running until both coordinates in the last row of the path are equal to 1.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;repeat{
  
  #find next step
  new.step &amp;lt;- least_cost_step(
    cost_matrix = m_cost, 
    last_step = tail(path, n = 1)
    )
  
  #join the new step with path
  path &amp;lt;- rbind(
    path, new.step,
    make.row.names = FALSE
    )
  
  #stop when step coordinates are 1, 1
  if(all(tail(path, n = 1) == 1)){break}
  
}

path
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    row col
## 1   13  13
## 2   12  13
## 3   12  12
## 4   11  12
## 5   10  12
## 6   10  11
## 7    9  11
## 8    9  10
## 9    9   9
## 10   9   8
## 11   8   8
## 12   7   8
## 13   7   7
## 14   6   7
## 15   6   6
## 16   5   6
## 17   5   5
## 18   5   4
## 19   4   4
## 20   4   3
## 21   3   3
## 22   3   2
## 23   3   1
## 24   2   1
## 25   1   1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The resulting least-cost path can be plotted on top of the cost matrix. Please, remember that the data is not pre-processed, and the plot below does not represent the real alignment (yet) between our target time series.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;graphics::image(
    x = seq_len(nrow(m_cost)),
    y = seq_len(ncol(m_cost)),
    z = m_cost,
    xlab = &amp;quot;zoo_germany&amp;quot;,
    ylab = &amp;quot;zoo_sweden&amp;quot;,
    main = &amp;quot;Cost Matrix and Least-Cost Path&amp;quot;
    )

graphics::lines(
  x = path$row, 
  y = path$col,
  lwd = 2
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-46-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;At this point we have all the pieces required to write the function &lt;code&gt;least_cost_path()&lt;/code&gt;. Notice that the &lt;code&gt;repeat{}&lt;/code&gt; statement is slightly more concise than before, as &lt;code&gt;least_cost_step()&lt;/code&gt; is directly wrapped within &lt;code&gt;rbind()&lt;/code&gt;. However, using &lt;code&gt;rbind()&lt;/code&gt; in a loop to add rows to a data frame is not a computationally efficient operation, but it was used here anyway because it makes the code more concise.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Least-Cost Path from Cost Matrix
#&#39; @param cost_matrix (required, matrix) cost matrix.
#&#39; @return data frame with least-cost path coordinates
least_cost_path &amp;lt;- function(cost_matrix){
  
  #first step of the least cost path
  path &amp;lt;- data.frame(
    row = nrow(cost_matrix),
    col = ncol(cost_matrix)
  )
  
  #iterate until path is completed
  repeat{
    
    #merge path with result of least_cost_step()
    path &amp;lt;- rbind(
      path, 
      #find next step
      least_cost_step(
        cost_matrix = cost_matrix, 
        last_step = tail(path, n = 1)
      ),
      make.row.names = FALSE
    )
    
    #stop when coordinates are 1, 1
    if(all(tail(path, n = 1) == 1)){break}
    
  }
  
  path
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can give it a go now to see that it works as expected.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;least_cost_path(cost_matrix = m_cost)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    row col
## 1   13  13
## 2   12  13
## 3   12  12
## 4   11  12
## 5   10  12
## 6   10  11
## 7    9  11
## 8    9  10
## 9    9   9
## 10   9   8
## 11   8   8
## 12   7   8
## 13   7   7
## 14   6   7
## 15   6   6
## 16   5   6
## 17   5   5
## 18   5   4
## 19   4   4
## 20   4   3
## 21   3   3
## 22   3   2
## 23   3   1
## 24   2   1
## 25   1   1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice, it worked, and now we have a nice least-cost path.&lt;/p&gt;
&lt;p&gt;But before continuing, there is just a little detail to notice about the least-cost path. Every time the same row index (such as &lt;code&gt;9&lt;/code&gt;) is linked to different column indices (&lt;code&gt;8&lt;/code&gt; to &lt;code&gt;11&lt;/code&gt;), it means that the samples of the time series identified by these column indices are having their time &lt;em&gt;compressed&lt;/em&gt; (or &lt;em&gt;warped&lt;/em&gt;) to the time of the row index. Hence, according to the least-cost path, the samples &lt;code&gt;8&lt;/code&gt; to &lt;code&gt;11&lt;/code&gt; of &lt;code&gt;zoo_sweden&lt;/code&gt; would be aligned in time with the sample &lt;code&gt;9&lt;/code&gt; of &lt;code&gt;zoo_germany&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now it&amp;rsquo;s time to use the least-cost path to quantify the similarity between the time series.&lt;/p&gt;
&lt;h3 id=&#34;dissimilarity-metric&#34;&gt;Dissimilarity Metric&lt;/h3&gt;
&lt;p&gt;The objective of dynamic time warping is to compute a metric of &lt;em&gt;dissimilarity&lt;/em&gt; (or &lt;em&gt;similarity&lt;/em&gt;, it just depends on the side from where you are looking at the issue) between time series. This operation requires two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Obtain the accumulated distance of the least cost path.&lt;/li&gt;
&lt;li&gt;Normalize the sum of distances by some number to help make results comparable across pairs of time series of different lengths.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, the method designed to build the cost matrix accumulates the distance of the least-cost path in the terminal cell, so we just have to extract it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;distance &amp;lt;- m_cost[nrow(m_cost), ncol(m_cost)]
distance
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 464.0019
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we need to find a number to normalize this distance value to make it comparable across different time series. There are several options, such as dividing &lt;code&gt;distance&lt;/code&gt; by the sum of lengths of the two time series, or by the length of the least-cost path (&lt;code&gt;nrow(path)&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;distance/sum(dim(m_cost))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 17.84623
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;distance/nrow(path)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18.56008
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another elegant normalization option requires computing the sum of distances between consecutive samples on each time series. This operation, named &lt;em&gt;auto-sum&lt;/em&gt;, requires applying &lt;code&gt;distance_euclidean()&lt;/code&gt; between the samples 1 and 2 of the given time series, then between the samples 2 and the 3, and so on until all consecutive sample pairs are processed, to finally sum all computed distances.&lt;/p&gt;
&lt;p&gt;To apply this operation to a time series we iterate between pairs of consecutive samples and save their distance in a vector (named &lt;code&gt;autodistance&lt;/code&gt; in the code below). Once the loop is done, then the auto-sum of the time series is the sum of this vector.&lt;/p&gt;
&lt;p&gt;For example, for &lt;code&gt;zoo_germany&lt;/code&gt; we would have:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#vector to store auto-distances
autodistance &amp;lt;- vector(
  mode = &amp;quot;numeric&amp;quot;, 
  length = nrow(zoo_germany) - 1
  )

#compute of row-to-row distance
for (row in 2:nrow(zoo_germany)) {
  autodistance[row - 1] &amp;lt;- distance_euclidean(
    x = zoo_germany[row, ],
    y = zoo_germany[row - 1, ]
  )
}

#compute autosum
sum(autodistance)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 425.7877
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we&amp;rsquo;ll need to apply this operation to many time series, it&amp;rsquo;s better to formalize this logic as a function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Time Series Autosum
#&#39; @param x (required, zoo object) time series
#&#39; @return numeric
auto_sum &amp;lt;- function(x){
  
  autodistance &amp;lt;- vector(
    mode = &amp;quot;numeric&amp;quot;, 
    length = nrow(x) - 1
  )
  
  for (row in 2:nrow(x)) {
    autodistance[row - 1] &amp;lt;- distance_euclidean(
      x = x[row, ],
      y = x[row - 1, ]
    )
  }
  
  sum(autodistance)
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now apply it to our two time series:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zoo_germany_autosum &amp;lt;- auto_sum(
  x = zoo_germany
)

zoo_germany_autosum
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 425.7877
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zoo_sweden_autosum &amp;lt;- auto_sum(
  x = zoo_sweden
)

zoo_sweden_autosum
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 379.9118
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the auto-sum of both time series, we just have to add them together to obtain our normalization value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;normalizer &amp;lt;- zoo_germany_autosum + zoo_sweden_autosum
normalizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 805.6995
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we obtained &lt;code&gt;distance&lt;/code&gt; from the cost matrix and the &lt;code&gt;normalizer&lt;/code&gt; from the auto-sum of the two time series, we can compute our dissimilarity score, which follows the expression below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;((2 * distance) / normalizer) - 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1517989
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why this particular formula? Because it returns zero when comparing a time series with itself!&lt;/p&gt;
&lt;p&gt;When the two time series are the same, &lt;code&gt;2 * distance&lt;/code&gt; equals &lt;code&gt;normalizer&lt;/code&gt; because DTW and auto-sum are equivalent (sample-to-sample distances!), and dividing them returns one. We then subtract one to it, and get zero, which represents a perfect similarity score.&lt;/p&gt;
&lt;p&gt;The same affect cannot be achieved when using other normalization values, such as the sum of lengths of the time series, or the length of the least cost path.&lt;/p&gt;
&lt;p&gt;We can integrate these pieces into the function &lt;code&gt;dissimilarity_score()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Similarity Metric from Least Cost Path and Cost Matrix
#&#39; @param a (required, zoo object) time series.
#&#39; @param b (required, zoo object) time series with same columns as `x`
#&#39; @param cost_path (required, data frame) least cost path with the columns &amp;quot;row&amp;quot; and &amp;quot;col&amp;quot;.
#&#39; @return numeric, similarity metric
dissimilarity_score &amp;lt;- function(a, b, cost_matrix){
  
  #distance of the least cost path
  distance &amp;lt;- cost_matrix[nrow(cost_matrix), ncol(cost_matrix)]
  
  #compute normalization factor from autosum
  autosum_a &amp;lt;- auto_sum(x = a)
    
  autosum_b &amp;lt;- auto_sum(x = b)
  
  normalizer &amp;lt;- autosum_a + autosum_b
  
  #compute dissimilarity
  psi &amp;lt;- ((2 * distance) / normalizer) - 1
  
  psi
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can give it a test run now:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dissimilarity_score(
  a = zoo_germany,
  b = zoo_sweden,
  cost_matrix = m_cost
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1517989
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the &lt;code&gt;dissimilarity_score()&lt;/code&gt; function ready, it is time to go write our main function!&lt;/p&gt;
&lt;h2 id=&#34;main-function&#34;&gt;Main Function&lt;/h2&gt;
&lt;p&gt;To recapitulate before moving forward, we have the following functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ts_preprocessing()&lt;/code&gt; applies linear detrending and z-score normalization to a time series.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;distance_matrix()&lt;/code&gt; and &lt;code&gt;distance_euclidean()&lt;/code&gt; work together to compute a distance matrix.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cost_matrix()&lt;/code&gt; transforms the distance matrix into a cost matrix.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;least_cost_path()&lt;/code&gt; applies &lt;code&gt;least_cost_step()&lt;/code&gt; recursively to build a least-cost path.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dissimilarity_score()&lt;/code&gt;, which calls &lt;code&gt;auto_sum()&lt;/code&gt; and quantifies time series dissimilarity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can wrap together all these functions into a new one with the unimaginative name &lt;code&gt;dynamic_time_warping()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Similarity Between Time Series
#&#39; @param a (required, zoo object) time series.
#&#39; @param b (required, zoo object) time series with same columns as `x`
#&#39; @param plot (optional, logical) if `TRUE`, a dynamic time warping plot is produced.
#&#39; @return psi score
dynamic_time_warping &amp;lt;- function(a, b, plot = FALSE){
  
  #linear detrending and z-score normalization
  a_ &amp;lt;- ts_preprocessing(x = a)
  b_ &amp;lt;- ts_preprocessing(x = b)
  
  #distance matrix
  m_dist &amp;lt;- distance_matrix(
    a = a_,
    b = b_
  )
  
  #cost matrix
  m_cost &amp;lt;- cost_matrix(
    distance_matrix = m_dist
  )
  
  #least-cost path
  cost_path &amp;lt;- least_cost_path(
    cost_matrix = m_cost
  )
  
  #similarity metric
  score &amp;lt;- dissimilarity_score(
    a = a_,
    b = b_,
    cost_matrix = m_cost
  )
  
  #plot
  if(plot == TRUE){
    
    graphics::image(
      x = seq_len(nrow(m_cost)),
      y = seq_len(ncol(m_cost)),
      z = m_cost,
      xlab = &amp;quot;a&amp;quot;,
      ylab = &amp;quot;b&amp;quot;,
      main = paste0(&amp;quot;Similarity score = &amp;quot;, round(score, 3))
    )
    
    graphics::lines(
      x = cost_path$row, 
      y = cost_path$col,
      lwd = 2
    )
    
  }
  
  score
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before I said that one advantage of the presented dissimilarity formula is that it returns 0 when comparing a time series with itself. Let&amp;rsquo;s see if that&amp;rsquo;s true by comparing &lt;code&gt;zoo_germany&lt;/code&gt; with itself:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dynamic_time_warping(
  a = zoo_germany,
  b = zoo_germany
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That&amp;rsquo;s good, right? Perfect similarity happens at 0, which is a lovely number to start with. If we now compare &lt;code&gt;zoo_germany&lt;/code&gt; and &lt;code&gt;zoo_sweden&lt;/code&gt;, we should expect a larger number:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dynamic_time_warping(
  a = zoo_germany,
  b = zoo_sweden
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2366642
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, when comparing &lt;code&gt;zoo_sweden&lt;/code&gt; with &lt;code&gt;zoo_spain&lt;/code&gt; we should expect an even higher dissimilarity score, given that they are quite far apart. As a bonus, we let the function plot their alignment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dynamic_time_warping(
  a = zoo_sweden,
  b = zoo_spain,
  plot = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-63-1.png&#34; width=&#34;672&#34; /&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.509285
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, that was freaking long, but these are such nice results, aren&amp;rsquo;t they? I hope it was worth it! One minute more, and we are done.&lt;/p&gt;
&lt;h2 id=&#34;library-source&#34;&gt;Library Source&lt;/h2&gt;
&lt;p&gt;Once we have all our DTW functions written and tested, the easiest way to make them usable without any extra hassle is to write them to a source file. Having them all in a single file allows loading them at once via the &lt;code&gt;source()&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;For example, if our library file is 
&lt;a href=&#34;https://www.dropbox.com/scl/fi/z2n9hnenxwuxwol5i0zcn/dtw.R?rlkey=bhsyew12r1glnqihtnocxwri6&amp;amp;dl=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;dtw.R&lt;/code&gt;&lt;/a&gt;, running &lt;code&gt;source(&amp;quot;dtw.R&amp;quot;)&lt;/code&gt; will load all functions into your R environment and they will be readily available for your DTW analysis.&lt;/p&gt;
&lt;h2 id=&#34;closing-thoughts&#34;&gt;Closing Thoughts&lt;/h2&gt;
&lt;p&gt;I hope you found this tutorial useful in one way or another. Writing a methodological library from scratch is hard work. There are many moving parts to consider, many concepts that need to be mapped and then translated into code, that making mistakes becomes exceedingly easy. Never worry about that and take your time until things start clicking.&lt;/p&gt;
&lt;p&gt;But above everything, enjoy the learning journey!&lt;/p&gt;
&lt;h2 id=&#34;coming-next&#34;&gt;Coming Next&lt;/h2&gt;
&lt;p&gt;In my TODO list there is a post focused on identifying computational bottlenecks in the DTW library we just wrote, and optimize the parts worth optimizing. There&amp;rsquo;s no timeline yet though, so stay tuned!&lt;/p&gt;
&lt;p&gt;Blas&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R package collinear</title>
      <link>https://blasbenito.com/project/collinear/</link>
      <pubDate>Sun, 12 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/project/collinear/</guid>
      <description>&lt;!-- badges: start --&gt;
&lt;p&gt;
&lt;a href=&#34;https://doi.org/10.5281/zenodo.10039489&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.10039489.svg&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://cran.r-project.org/package=collinear&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version/collinear&#34; alt=&#34;CRAN status&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://CRAN.R-project.org/package=collinear&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://cranlogs.r-pkg.org/badges/grand-total/collinear&#34; alt=&#34;CRAN\_Download\_Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;!-- badges: end --&gt;
&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.blasbenito.com/post/multicollinearity-model-interpretability/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multicollinearity hinders the interpretability&lt;/a&gt; of linear and machine learning models.&lt;/p&gt;
&lt;p&gt;The R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;, 
&lt;a href=&#34;https://CRAN.R-project.org/package=collinear&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available on CRAN&lt;/a&gt;, combines four methods for easy management of multicollinearity in modelling data frames with numeric and categorical variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Target Encoding&lt;/strong&gt;: Transforms categorical predictors to numeric using a numeric response as reference.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Preference Order&lt;/strong&gt;: Ranks predictors by their association with a response variable to preserve important ones in multicollinearity filtering.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pairwise Correlation Filtering&lt;/strong&gt;: Automated multicollinearity filtering of numeric and categorical predictors based&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;main-improvements-in-version-200&#34;&gt;Main Improvements in Version 2.0.0&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Expanded Functionality&lt;/strong&gt;: Functions &lt;code&gt;collinear()&lt;/code&gt; and &lt;code&gt;preference_order()&lt;/code&gt; support both categorical and numeric responses and predictors, and can handle several responses at once.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Robust Selection Algorithms&lt;/strong&gt;: Enhanced selection in &lt;code&gt;vif_select()&lt;/code&gt; and &lt;code&gt;cor_select()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enhanced Functionality to Rank Predictors&lt;/strong&gt;: New functions to compute association between response and predictors covering most use-cases, and automated function selection depending on data features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simplified Target Encoding&lt;/strong&gt;: Streamlined and parallelized for better efficiency, and new default is &amp;ldquo;loo&amp;rdquo; (leave-one-out).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallelization and Progress Bars&lt;/strong&gt;: Utilizes &lt;code&gt;future&lt;/code&gt; and &lt;code&gt;progressr&lt;/code&gt; for enhanced performance and user experience.on pairwise correlations.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Variance Inflation Factor Filtering&lt;/strong&gt;: Automated multicollinearity filtering of numeric predictors based on Variance Inflation Factors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The article 
&lt;a href=&#34;https://blasbenito.github.io/collinear/articles/how_it_works.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How It Works&lt;/a&gt; explains how the package works in detail.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you find this package useful, please cite it as:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Blas M. Benito (2024). collinear: R Package for Seamless Multicollinearity Management. Version 2.0.0. doi: 10.5281/zenodo.10039489&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R package distantia</title>
      <link>https://blasbenito.com/project/distantia/</link>
      <pubDate>Sun, 12 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/project/distantia/</guid>
      <description>&lt;!-- badges: start --&gt;
&lt;p&gt;
&lt;a href=&#34;https://zenodo.org/badge/latestdoi/187805264&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/187805264.svg&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://CRAN.R-project.org/package=distantia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version-ago/distantia&#34; alt=&#34;CRAN\_Release\_Badge&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://CRAN.R-project.org/package=distantia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://cranlogs.r-pkg.org/badges/distantia&#34; alt=&#34;CRAN\_Download\_Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;!-- badges: end --&gt;
&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;The R package 
&lt;a href=&#34;https://blasbenito.github.io/distantia/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;&lt;code&gt;distantia&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;, 
&lt;a href=&#34;https://CRAN.R-project.org/package=distantia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available on CRAN&lt;/a&gt;, offers an efficient, feature-rich toolkit for managing, comparing, and analyzing time series data. It is designed to handle a wide range of scenarios, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multivariate and univariate time series.&lt;/li&gt;
&lt;li&gt;Regular and irregular sampling.&lt;/li&gt;
&lt;li&gt;Time series of different lengths.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;h3 id=&#34;comprehensive-analytical-tools&#34;&gt;Comprehensive Analytical Tools&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;10 distance metrics: see &lt;code&gt;distantia::distances&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The normalized dissimilarity metric &lt;code&gt;psi&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Free and Restricted Dynamic Time Warping (DTW) for shape-based comparison.&lt;/li&gt;
&lt;li&gt;A Lock-Step method for sample-to-sample comparison&lt;/li&gt;
&lt;li&gt;Restricted permutation tests for robust inferential support.&lt;/li&gt;
&lt;li&gt;Analysis of contribution to dissimilarity of individual variables in multivariate time series.&lt;/li&gt;
&lt;li&gt;Hierarchical and K-means clustering of time series based on dissimilarity matrices.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;computational-efficiency&#34;&gt;Computational Efficiency&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;C++ back-end&lt;/strong&gt; powered by 
&lt;a href=&#34;https://www.rcpp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rcpp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel processing&lt;/strong&gt; managed through the 
&lt;a href=&#34;https://future.futureverse.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;future&lt;/a&gt; package.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficient data handling&lt;/strong&gt; via 
&lt;a href=&#34;https://CRAN.R-project.org/package=zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;zoo&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;time-series-management-tools&#34;&gt;Time Series Management Tools&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Introduces &lt;strong&gt;time series lists&lt;/strong&gt; (TSL), a versatile format for handling collections of time series stored as lists of &lt;code&gt;zoo&lt;/code&gt; objects.&lt;/li&gt;
&lt;li&gt;Includes a suite of &lt;code&gt;tsl_...()&lt;/code&gt; functions for generating, resampling, transforming, analyzing, and visualizing univariate and multivariate time series.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you find this package useful, please cite it as:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Blas M. Benito, H. John B. Birks (2020). distantia: an open-source toolset to quantify dissimilarity between multivariate ecological time-series. Ecography, 43(5), 660-667. doi: 
&lt;a href=&#34;https://nsojournals.onlinelibrary.wiley.com/doi/10.1111/ecog.04895&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.1111/ecog.04895&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Blas M. Benito (2024). distantia: A Toolset for Time Series Dissimilarity Analysis. R package version 2.0.0. url:  
&lt;a href=&#34;https://blasbenito.github.io/distantia/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://blasbenito.github.io/distantia/&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Gentle Intro to Dynamic Time Warping</title>
      <link>https://blasbenito.com/post/dynamic-time-warping/</link>
      <pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/dynamic-time-warping/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;This post provides a gentle conceptual introduction to &lt;em&gt;Dynamic Time Warping&lt;/em&gt; (DTW), a method to compare time series of different lengths that has found its way into our daily lives. It starts with a very general introduction to the comparison of time series, follows with a bit of history about its development and a step-by-step breakdown, to finalize with a summary of its real-world applications.&lt;/p&gt;
&lt;h1 id=&#34;comparing-time-series&#34;&gt;Comparing Time Series&lt;/h1&gt;
&lt;p&gt;Time series comparison is a critical task in many fields, such as environmental monitoring, finance, and healthcare. The goal is often to quantify similarities or differences between pairs of time series to gain insights into how the data is structured and identify meaningful patterns.&lt;/p&gt;
&lt;p&gt;For example, the data below shows time series representing the same phenomenon in three different places and time ranges: &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; have 30 synchronized observations, while &lt;code&gt;c&lt;/code&gt; has 20 observations from a different year.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;750&#34; /&gt;
&lt;p&gt;There are several options to compare &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; directly, such as assessing their correlation (0.955), or computing the sum of Euclidean distances between their respective samples (2.021).&lt;/p&gt;
&lt;p&gt;This comparison approach is named &lt;em&gt;lock-step&lt;/em&gt; (also known as &lt;em&gt;inelastic comparison&lt;/em&gt;), and works best when the time series represent phenomena with relatively similar shapes and are aligned in time and frequency, as it is the case with &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Comparing &lt;code&gt;c&lt;/code&gt; with &lt;code&gt;a&lt;/code&gt; and/or &lt;code&gt;b&lt;/code&gt; is a completely different task though, exactly the one 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Dynamic_time_warping&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Time Warping&lt;/a&gt; was designed to address.&lt;/p&gt;
&lt;p&gt;Now it would make sense to explain right away what dynamic time warping is and how it works, but there&amp;rsquo;s a bit of history to explore first.&lt;/p&gt;
&lt;h1 id=&#34;a-bit-of-history&#34;&gt;A Bit of History&lt;/h1&gt;
&lt;p&gt;Dynamic Time Warping (DTW) might sound like a modern high-tech buzzword, but its roots go way backâolder than me (gen X guy here!). This powerful method was first developed in the pioneering days of speech recognition. The earliest reference I uncovered is from Shearme and Leachâs 1968 paper, 
&lt;a href=&#34;https://doi.org/10.1109/TAU.1968.1161985&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Some experiments with a simple word recognition system&lt;/em&gt;&lt;/a&gt;, published by the Joint Speech Research Unit in the UK.&lt;/p&gt;
&lt;p&gt;These foundational ideas were later expanded upon by Sakoe and Chiba in their seminal 1971 paper, 
&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:107516844&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;A Dynamic Programming Approach to Continuous Speech Recognition&lt;/em&gt;&lt;/a&gt;, often regarded as the definitive starting point for modern DTW applications.&lt;/p&gt;
&lt;p&gt;From there, DTW has found applications in diverse fields relying on time-dependent data, such as 
&lt;a href=&#34;https://doi.org/10.1016/j.bspc.2024.106677&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;medical sciences&lt;/a&gt;, 
&lt;a href=&#34;https://doi.org/10.1371/journal.pone.0272848&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sports analytics&lt;/a&gt;, 
&lt;a href=&#34;https://iopscience.iop.org/article/10.3847/1538-4357/ac4af6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;astronomy&lt;/a&gt;, 
&lt;a href=&#34;https://doi.org/10.1016/j.eneco.2020.105036&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;econometrics&lt;/a&gt;, 
&lt;a href=&#34;https://www.mdpi.com/2079-9292/8/11/1306&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;robotics&lt;/a&gt;, 
&lt;a href=&#34;https://doi.org/10.1111/exsy.13237&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;epidemiology&lt;/a&gt;, and many others.&lt;/p&gt;
&lt;p&gt;Ok, let&amp;rsquo;s stop wandering in time, and go back to the meat in this post.&lt;/p&gt;
&lt;h1 id=&#34;what-is-dynamic-time-warping&#34;&gt;What is &lt;em&gt;Dynamic Time Warping&lt;/em&gt;?&lt;/h1&gt;
&lt;p&gt;Dynamic Time Warping is a method to compare univariate or multivariate time series of different length, timing, and/or shape. To do so, DTW stretches or compresses parts of the time series (hence &lt;em&gt;warping&lt;/em&gt;) until it finds the alignment that minimizes their overall differences. Think of it as a way to match the rhythm of two songs even if one plays faster than the other.&lt;/p&gt;
&lt;p&gt;The figure below represents a dynamic time warping solution for the time series &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;a&lt;/code&gt;. Notice how each sample in one time series matches one or several samples from the other. These matches are optimized to minimize the sum of distances between the samples they connect (3.285 in this case). Any other combination of matches would result in a higher sum of distances.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;750&#34; /&gt;
&lt;p&gt;In dynamic time warping, the actual &lt;em&gt;warping&lt;/em&gt; happens when a sample in one time series is matched with two or more samples from the other, independently of their observation times. The figure below identifies one of these instances with blue bubbles. The sample 10 of &lt;code&gt;c&lt;/code&gt; (upper blue bubble), with date 2022-07-16, is matched with the samples 14 to 16 of &lt;code&gt;a&lt;/code&gt; (lower bubble), with dates 2023-12-13 to 2024-03-09. This matching structure represents a time compression in &lt;code&gt;a&lt;/code&gt; for the range of involved dates.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;750&#34; /&gt;
&lt;p&gt;This ability to warp time makes DTW incredibly useful for analyzing time series that are similar in shape but don&amp;rsquo;t have the same length or are not fully synchronized.&lt;/p&gt;
&lt;p&gt;The next section delves into the computational steps of DTW.&lt;/p&gt;
&lt;h1 id=&#34;dtw-step-by-step&#34;&gt;DTW Step by Step&lt;/h1&gt;
&lt;p&gt;Time series comparison via Dynamic Time Warping (DTW) involves several key steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Detrending and z-score normalization&lt;/strong&gt; of the time series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computation of the distance matrix&lt;/strong&gt; between all pairs of samples.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computation of a cost matrix&lt;/strong&gt; from the distance matrix.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finding the least-cost path&lt;/strong&gt; within the cost matrix.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computation of a similarity metric&lt;/strong&gt; based on the least-cost path.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;detrending-and-z-score-normalization&#34;&gt;Detrending and Z-score Normalization&lt;/h2&gt;
&lt;p&gt;DTW is highly sensitive to differences in trends and ranges between time series (see the &lt;em&gt;Pitfalls&lt;/em&gt; section). To address this, 
&lt;a href=&#34;https://sherbold.github.io/intro-to-data-science/09_Time-Series-Analysis.html#Trend-and-Seasonal-Effects&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;detrending&lt;/a&gt; and 
&lt;a href=&#34;https://developers.google.com/machine-learning/crash-course/numerical-data/normalization#z-score_scaling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;z-score normalization&lt;/a&gt; are important preprocessing steps. The former removes any upwards or downwards trend in the time series, while the later scales the time series values to a mean of zero and a standard deviation of one.&lt;/p&gt;
&lt;p&gt;In this example, the time series &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; already have matching ranges, so normalization is not strictly necessary. For demonstration purposes, however, the figure below shows them normalized using z-score scaling:&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;750&#34; /&gt;
&lt;h2 id=&#34;distance-matrix&#34;&gt;Distance Matrix&lt;/h2&gt;
&lt;p&gt;This step involves computing the distance matrix, which contains pairwise distances between all combinations of samples in the two time series.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;600&#34; /&gt;
&lt;p&gt;Choosing an appropriate distance metric is crucial. While Euclidean distance works well in many cases, other metrics may be more suitable depending on the data.&lt;/p&gt;
&lt;h2 id=&#34;cost-matrix&#34;&gt;Cost Matrix&lt;/h2&gt;
&lt;p&gt;The cost matrix is derived from the distance matrix by accumulating distances recursively, neighbor to neighbor, from the starting corner (lower-left) to the ending one (upper-right).&lt;/p&gt;
&lt;p&gt;Different rules for cell neighborhood determine how these costs propagate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Orthogonal only&lt;/strong&gt;: Accumulation occurs in the &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; directions only, ignoring diagonals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orthogonal and diagonal&lt;/strong&gt;: Diagonal movements are also considered, typically weighted by a factor of &lt;code&gt;â2&lt;/code&gt; (1.414) to balance with orthogonal movements.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The figure below illustrates the cost matrix with both orthogonal and diagonal paths.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;600&#34; /&gt;
&lt;p&gt;The result of the cost matrix is similar to the topographic map of a valley, in which the value of each cell represents the slope we have to overcome to walk through it.&lt;/p&gt;
&lt;p&gt;Now that we have a valley, let&amp;rsquo;s go create a river!&lt;/p&gt;
&lt;h2 id=&#34;least-cost-path&#34;&gt;Least-cost Path&lt;/h2&gt;
&lt;p&gt;This is the step where the actual time warping happens!&lt;/p&gt;
&lt;p&gt;The least-cost path minimizes the total cost from the start to the end of the cost matrix, aligning the time series optimally.&lt;/p&gt;
&lt;p&gt;The algorithm building the least-cost path starts on the upper right corner of the cost matrix, and recursively selects the next neighbor with the lowest cumulative cost to build the least-cost path step by step. This process is similar to letting a river find its shortest path way down a valley.&lt;/p&gt;
&lt;p&gt;The figure below shows the least-cost path (black line). Deviations from the diagonal represent adjustments made to align the time series.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;600&#34; /&gt;
&lt;h2 id=&#34;similarity-metric&#34;&gt;Similarity Metric&lt;/h2&gt;
&lt;p&gt;Finally, DTW produces a similarity metric based on the least-cost path. The simplest approach is to sum the distances of all points along the path.&lt;/p&gt;
&lt;p&gt;For this example, the total cost is 7.588.
However, when comparing time series of varying lengths, normalization is often useful. Common options include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sum of lengths&lt;/strong&gt;: Normalize by the combined lengths of the time series, e.g., &lt;code&gt;Normalized Cost = Total Cost / (Length(a) + Length(c))&lt;/code&gt;. For &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;, this would be 0.152.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Auto-sum of distances&lt;/strong&gt;: Normalize by the sum of distances between adjacent samples in each series, as in &lt;code&gt;Normalized Cost = Total Cost / (Auto-sum(a) + Auto-sum(c))&lt;/code&gt;. For &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;, this results in 0.358.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These normalized metrics allow comparisons across datasets with varying characteristics.&lt;/p&gt;
&lt;h1 id=&#34;real-world-applications&#34;&gt;Real World Applications&lt;/h1&gt;
&lt;p&gt;Dynamic Time Warping is a well-studied topic in academic community, with more than 86k research articles listed in 
&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=%22dynamic&amp;#43;time&amp;#43;warping%22&amp;amp;btnG=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Scholar&lt;/a&gt;. However, the real-world impact of an academic concept often differs from its academic popularity. Examining DTW-related patents provides a clearer view of its practical applications.&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://www.uspto.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;United States Patent and Trademark Office&lt;/a&gt; includes the class 
&lt;a href=&#34;https://patents.justia.com/patents-by-us-classification/704/241&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;704/241&lt;/a&gt; specifically for &amp;ldquo;Dynamic Time Warping Patents&amp;rdquo;. Similarly, the 
&lt;a href=&#34;https://www.cooperativepatentclassification.org/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cooperative Patent Classification&lt;/a&gt; includes the classification 
&lt;a href=&#34;https://www.uspto.gov/web/patents/classification/cpc/pdf/cpc-scheme-G10L.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(G10L 15/12)&lt;/a&gt; with the title &amp;ldquo;Speech recognition using dynamic programming techniques, e.g. dynamic time warping (DTW)&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Using the 
&lt;a href=&#34;https://www.epo.org/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;European Patent Office&lt;/a&gt; search tool 
&lt;a href=&#34;https://worldwide.espacenet.com/patent/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spacenet&lt;/a&gt; for &amp;ldquo;Dynamic Time Warping&amp;rdquo; returns approximately 
&lt;a href=&#34;https://worldwide.espacenet.com/patent/search/family/007654522/publication/US2002049591A1?q=%22dynamic%20time%20warping%22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;12.000 results&lt;/a&gt;. 
&lt;a href=&#34;https://patents.google.com/?q=%28%22dynamic&amp;#43;time&amp;#43;warping%22%29&amp;amp;oq=%22dynamic&amp;#43;time&amp;#43;warping%22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Patents&lt;/a&gt; reports over 35.000 results.&lt;/p&gt;
&lt;p&gt;While patents illustrate the technical implementation of DTW, uncovering its application in company blogs, wikis, or manuals is more challenging. Nonetheless, a few compelling examples demonstrate DTW&amp;rsquo;s real-world utility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Media and Entertainment&lt;/strong&gt;*&lt;/p&gt;
&lt;p&gt;Closed caption alignment is perhaps the most pervasive yet invisible application of DTW. Companies like 
&lt;a href=&#34;https://netflixtechblog.com/detecting-scene-changes-in-audiovisual-content-77a61d3eaad6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Netflix&lt;/a&gt; and 
&lt;a href=&#34;https://patents.google.com/patent/US20150271442A1/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft&lt;/a&gt; use DTW to synchronize subtitles with soundtracks in movies, TV shows, and video games, ensuring an accurate match regardless of pacing or timing inconsistencies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Wearables and Fitness Devices&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many wearables employ DTW to classify user activities by aligning accelerometer and gyroscope data with predefined templates. For example, 
&lt;a href=&#34;https://patents.google.com/patent/US11517789B2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goertek&amp;rsquo;s&lt;/a&gt; &lt;em&gt;Comma 2&lt;/em&gt; smart ring 
&lt;a href=&#34;https://sleepreviewmag.com/sleep-diagnostics/consumer-sleep-tracking/wearable-sleep-trackers/goertek-reveals-smart-ring-reference-designs-voice-gesture-controls/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;presented in January 2025&lt;/a&gt; uses DTW to recognize user movements. Another creative example is the 
&lt;a href=&#34;https://genkiinstruments.com/products/wave&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wave&lt;/a&gt; MIDI controller ring, 
&lt;a href=&#34;https://patents.google.com/patent/US20220085841A1/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;patented by Genki&lt;/a&gt;. This device applies DTW with a nearest-neighbor classifier to analyze hand movements and trigger musical effects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Biomechanics&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In biomechanics, DTW helps analyze movement patterns and detect anomalies. For instance, the software 
&lt;a href=&#34;https://wiki.has-motion.com/doku.php?id=sift:sift_overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sift&lt;/a&gt; by 
&lt;a href=&#34;https://www.has-motion.ca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HAS Motion&lt;/a&gt; 
&lt;a href=&#34;https://wiki.has-motion.com/doku.php?id=sift:dynamic_time_warping:dynamic_time_warping&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;uses DTW&lt;/a&gt; to compare large datasets of movement traces and identify deviations. Similarly, the 
&lt;a href=&#34;https://orthoload.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OrthoLoad&lt;/a&gt; processes load measurements on joint implants 
&lt;a href=&#34;https://orthoload.com/software/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;using DTW&lt;/a&gt; to analyze patterns and identify irregularities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Industrial Applications&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DTW is also used in manufacturing to monitor machinery health.  Toshiba&amp;rsquo;s 
&lt;a href=&#34;https://www.global.toshiba/ww/technology/corporate/rdc/rd/topics/20/2006-01.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LAMTSS&lt;/a&gt; technology applies DTW to noise and motion data from manufacturing equipment, helping detect and predict operational failures before they occur.&lt;/p&gt;
&lt;p&gt;These examples highlight the versatility and practical relevance of DTW, spanning industries from entertainment to biomechanics and industrial maintenance. Its ability to adapt to diverse time series challenges underscores its value in real-world problem-solving.&lt;/p&gt;
&lt;h1 id=&#34;closing-thoughts&#34;&gt;Closing Thoughts&lt;/h1&gt;
&lt;p&gt;Dynamic Time Warping exemplifies how a sophisticated algorithm, initially developed for niche applications, has evolved into a versatile tool with real-world significance. From aligning movie subtitles to monitoring machinery health, DTW bridges the gap between academic theory and practical innovation. Its ability to adapt to various industries highlights the importance of robust time series analysis techniques, and further cements its place in both research and applied fields.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Reading List: Data Science</title>
      <link>https://blasbenito.com/post/my-reading-list-data-science/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/my-reading-list-data-science/</guid>
      <description>&lt;p&gt;This is a live post listing links to Data Science related posts and videos I consider to be interesting, high-quality, or even essential to better understand particular topics within such a wide field.&lt;/p&gt;
&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://towardsdatascience.com/extending-target-encoding-443aa9414cae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Extending Target Encoding&lt;/strong&gt;&lt;/a&gt;: post by 
&lt;a href=&#34;https://www.aitimejournal.com/interview-with-daniele-micci-barreca-product-analytics-lead-data-science-google/30110/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniele Micci-Barreca&lt;/a&gt; explaining how he came up with the idea of target encoding, and its possible extensions.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://maxhalford.github.io/blog/target-encoding/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Target encoding done the right way&lt;/strong&gt;&lt;/a&gt;: post by 
&lt;a href=&#34;https://maxhalford.github.io/bio/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Max Halford&lt;/a&gt;, Head of Data at 
&lt;a href=&#34;https://www.carbonfact.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Carbonfact&lt;/a&gt;, explaining in detail how to combine additive smoothing and target encoding.&lt;/p&gt;
&lt;h2 id=&#34;handling-and-management&#34;&gt;Handling and Management&lt;/h2&gt;
&lt;h3 id=&#34;apache-parquet&#34;&gt;Apache Parquet&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://airbyte.com/data-engineering-resources/parquet-data-format&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A Deep Dive into Parquet: The Data Format Engineers Need to Know&lt;/strong&gt;&lt;/a&gt;: This by Aditi Prakash, published in the 
&lt;a href=&#34;https://airbyte.com/blog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Airbyte Blog&lt;/a&gt; offers a complete guide about the 
&lt;a href=&#34;https://parquet.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apache Parquet&lt;/a&gt; file format.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.influxdata.com/blog/querying-parquet-millisecond-latency/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Querying Parquet with Millisecond Latency&lt;/strong&gt;&lt;/a&gt; this post from by Raphael Taylor-Davies and Andrew Lamb explains in deep the optimization methods used in 
&lt;a href=&#34;https://parquet.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apache Parquet&lt;/a&gt; files. Warning, this is a very technical read!&lt;/p&gt;
&lt;h3 id=&#34;duckdb&#34;&gt;DuckDB&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://duckdb.org/2024/01/26/multi-database-support-in-duckdb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Multi-Database Support in DuckDB&lt;/strong&gt;&lt;/a&gt; This post by Mark Raasveldt published in the DuckDB blog explains how to query together data from different databases at once.&lt;/p&gt;
&lt;h1 id=&#34;analysis-and-modeling&#34;&gt;Analysis and Modeling&lt;/h1&gt;
&lt;h2 id=&#34;modeling-methods&#34;&gt;Modeling Methods&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://loreley.one/2024-09-pca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Unraveling Principal Component Analysis&lt;/strong&gt;&lt;/a&gt;: This book, reviewed 
&lt;a href=&#34;https://loreley.one/2024-09-pca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, is a &lt;em&gt;tour of linear algebra&lt;/em&gt; focused on intuitive explanations rather than mathematical demonstrations.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://m-clark.github.io/posts/2019-10-20-big-mixed-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Mixed Models for Big Data&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://m-clark.github.io/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Clark&lt;/a&gt; (see entry below by the same author) reviews several mixed modelling approach for large data in R.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://m-clark.github.io/generalized-additive-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Generalized Additive Models&lt;/strong&gt;&lt;/a&gt;: A good online book on Generalized Additive Models by 
&lt;a href=&#34;https://m-clark.github.io/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Clark&lt;/a&gt;, Senior Machine Learning Scientist at 
&lt;a href=&#34;https://www.strong.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Strong Analytics&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;model-explainability&#34;&gt;Model Explainability&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://towardsdatascience.com/a-simple-model-independent-score-explanation-method-c17002d66da7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Model-Independent Score Explanation&lt;/strong&gt;&lt;/a&gt;: Post by 
&lt;a href=&#34;https://www.aitimejournal.com/interview-with-daniele-micci-barreca-product-analytics-lead-data-science-google/30110/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniele Micci-Barreca&lt;/a&gt; on model explainability. It also explains a very clever method to better understand any model just from it&amp;rsquo;s predictions.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AI Explanations whitepaper&lt;/strong&gt;&lt;/a&gt;: White paper of Google&amp;rsquo;s &amp;ldquo;AI Explanations&amp;rdquo; product with a pretty good overall view of the state of the art of model explainability.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1702.08608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Towards A Rigorous Science of Interpretable Machine Learning&lt;/strong&gt;&lt;/a&gt;: Pre-print by Finale Doshi-Velez and Been Kim offering a rigorous definition and evaluation of model interpretability.&lt;/p&gt;
&lt;h2 id=&#34;spatial-analysis&#34;&gt;Spatial Analysis&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://duckdb.org/2023/04/28/spatial.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;PostGEESE? Introducing The DuckDB Spatial Extension&lt;/strong&gt;&lt;/a&gt;: In this post, the authors of 
&lt;a href=&#34;https://duckdb.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DuckDB&lt;/a&gt; present the new PostGIS-like &lt;em&gt;spatial&lt;/em&gt; extension for this popular in-process data base engine.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://py.geocompx.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Geocomputation with Python&lt;/strong&gt;&lt;/a&gt;: A very nice book on geographic data analysis with Python.&lt;/p&gt;
&lt;h1 id=&#34;coding&#34;&gt;Coding&lt;/h1&gt;
&lt;h2 id=&#34;general-concepts&#34;&gt;General Concepts&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://archive.org/details/a-philosophy-of-software-design/mode/2up&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A Philosophy Of Software Design&lt;/strong&gt;&lt;/a&gt;: This book by 
&lt;a href=&#34;https://web.stanford.edu/~ouster/cgi-bin/home.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Ousterhout&lt;/a&gt; is full of high-level concepts and tips to help tackle software complexity. It&amp;rsquo;s so good I had to buy a hard copy that now lives in my desk. 
&lt;a href=&#34;https://blog.pragmaticengineer.com/a-philosophy-of-software-design-review/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This post&lt;/a&gt; by 
&lt;a href=&#34;https://blog.pragmaticengineer.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gergely Orosz&lt;/a&gt; offers a balanced review of the book.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://youtu.be/CFRhGnuXG-4?si=7Xr3E9L7GFvoRJqA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Why You Shouldn&amp;rsquo;t Nest Your Code&lt;/strong&gt;&lt;/a&gt;: In this wonderful video, 
&lt;a href=&#34;https://www.youtube.com/@CodeAesthetic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CodeAesthetic&lt;/a&gt; explains in detail (and beautiful graphics!) a couple of methods to reduce the level of nesting in our code to improve readability and maintainability. This video has truly changed how I code in R!&lt;/p&gt;
&lt;h2 id=&#34;r&#34;&gt;R&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://ropensci.org/blog/2024/02/22/beautiful-code/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Beautiful Code, Because Weâre Worth It!&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://mastodon.social/@maelle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MaÃ«lle Salmon&lt;/a&gt; (research software engineer), and 
&lt;a href=&#34;https://fosstodon.org/@yabellini&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yanina Bellini Saibene&lt;/a&gt; (rOpenSci Community Manager) provides simple tips to help write more visually pleasant R code.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://journal.r-project.org/articles/RJ-2023-071/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Coloring in Râs Blind Spot&lt;/strong&gt;&lt;/a&gt;: This article published in 
&lt;a href=&#34;https://journal.r-project.org/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The R Journal&lt;/a&gt; by 
&lt;a href=&#34;https://www.zeileis.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Achim Zeileis&lt;/a&gt; (he has a 
&lt;a href=&#34;https://www.zeileis.org/blog/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;great analytics blog&lt;/a&gt; too!) and 
&lt;a href=&#34;https://www.stat.auckland.ac.nz/~paul/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paul Murrel&lt;/a&gt; offers a great overview of the base R color functions, and offers specific advice on what color palettes work better in different scenarios.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://peerj.com/preprints/26605v1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Taking R to its limits: 70+ tips&lt;/strong&gt;&lt;/a&gt;: This pre-print (not peer-reviewed AFAIK) by Tsagris and Papadakis offers a long list of tips to speed-up computation with the R language. I think a few of these tips lack enough context or are poorly explained, but it&amp;rsquo;s still a good resource to help optimize our R code.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.njtierney.com/post/2023/12/06/long-errors-smell/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Code Smell: Error Handling Eclipse&lt;/strong&gt; &lt;/a&gt;: This post by 
&lt;a href=&#34;https://fosstodon.org/@njtierney@aus.social&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nick Tierney&lt;/a&gt; explains how to address these situations when &lt;em&gt;error checking code totally eclipses the intent of the code&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.emilyriederer.com/post/team-of-packages/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Building a team of internal R packages&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://www.emilyriederer.com/about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emily Riederer&lt;/a&gt; delves into the particularities of building a team of R packages to do jobs helping a organization answer impactful questions.&lt;/p&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://archive.org/details/francois-chollet-deep-learning-with-python-manning-2018&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Deep Learning With Python&lt;/strong&gt;&lt;/a&gt;: This book by 
&lt;a href=&#34;https://fchollet.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FranÃ§ois Chollet&lt;/a&gt;, Software Engineer at Google and creator of the Keras library, seems to me like the best resource out there for those wanting to understand and build deep learning models from scratch. I have a hard copy on my desk, and I am finding it pretty easy to follow. Also, the code examples are clearly explained, and they ramp up in a very consistent manner.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.emilyriederer.com/post/py-rgo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Python Rgonomics&lt;/strong&gt;&lt;/a&gt;: In this post, 
&lt;a href=&#34;https://www.emilyriederer.com/about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emily Riederer&lt;/a&gt; offers a list of Python libraries with an &amp;ldquo;R feeling&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;coding-workflow&#34;&gt;Coding Workflow&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://graphite.dev/blog/stacked-prs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;How to use stacked PRs to unblock your entire team&lt;/strong&gt;&lt;/a&gt;: This post in 
&lt;a href=&#34;https://graphite.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graphite&lt;/a&gt;&amp;rsquo;s blog explains how to split large coding changes into small managed PRs (aka &amp;ldquo;stacked PRs&amp;rdquo;) to avoid blocks when PR reviews are hard to come by.&lt;/p&gt;
&lt;h1 id=&#34;other-fancy-things&#34;&gt;Other Fancy Things&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://vickiboykis.com/2024/01/15/whats-new-with-ml-in-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;What&amp;rsquo;s new with ML in production&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://vickiboykis.com/about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vicki Boykis&lt;/a&gt;, machine learning engineer at Mozilla.ai, goes deep into the differences and similarities between classical Machine Learning approaches and Large Language Models. I learned a lot from this read!&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://youtu.be/T-D1OfcDW1M?si=sAZO-5NGD8yF2WYe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;What is Retrieval-Augmented Generation (RAG)?&lt;/strong&gt;&lt;/a&gt;: In this video, Marina Danilevsky, Senior Data Scientist at IBM, offers a pretty good explanation on how the 
&lt;a href=&#34;https://research.ibm.com/blog/retrieval-augmented-generation-RAG?utm_id=YT-101-What-is-RAG&amp;amp;_gl=1*p6ef17*_ga*MTQwMzQ5NjMwMi4xNjkxNDE2MDc0*_ga_FYECCCS21D*MTY5MjcyMjgyNy40My4xLjE2OTI3MjMyMTcuMC4wLjA.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Retrieval-Augmented Generation&lt;/a&gt; method can improve the credibility of large language models.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.nature.com/articles/s41598-020-79148-7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A novel framework for spatio-temporal prediction of environmental data using deep learning&lt;/strong&gt;&lt;/a&gt;: This paper by 
&lt;a href=&#34;https://www.linkedin.com/in/federico-amato-66208637&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Federico Amato&lt;/a&gt; and collaborators describes an intriguing regression method combining a feedforward neural network with empirical orthogonal functions for spatio-temporal interpolation. Regrettably, the paper offers no code or data at all, but it&amp;rsquo;s still an interesting read.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2310.10196.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Large Models for Time Series and
Spatio-Temporal Data A Survey and Outlook&lt;/strong&gt;&lt;/a&gt;: This pre-print by Weng and collaborators reviews the current state of the art in spatio-temporal modelling with Large Language Models and Pre-Trained Foundation Models.&lt;/p&gt;
&lt;h1 id=&#34;management-and-leadership&#34;&gt;Management and Leadership&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://zaidesanton.substack.com/p/using-fake-deadlines-without-driving?publication_id=1804629&amp;amp;post_id=152010688&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Using fake deadlines without driving your engineers crazy&lt;/strong&gt;&lt;/a&gt;: In this post, 
&lt;a href=&#34;https://substack.com/@jstanier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;James Stanier&lt;/a&gt; explains how fake deadlines can help push projects forward in healthy work environments.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://zaidesanton.substack.com/p/when-engineering-managers-become&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;You are hurting your team without even noticing&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://substack.com/@zaidesanton&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anton Zaides&lt;/a&gt; (Development Team Leader), and 
&lt;a href=&#34;https://substack.com/@crushingtecheducation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eugene Shulga&lt;/a&gt;, (Software Engineer) offers insight on the harmful effects of a manager&amp;rsquo;s ego in their team dynamics.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://the.managers.guide/p/teamwork-habits-for-leaders&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Teamwork Habits for Leaders&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://substack.com/@ochronus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Csaba Okrona&lt;/a&gt; focuses on how shifting from talker to listener in team meetings offers a good insight to better address the team&amp;rsquo;s needs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mapping Categorical Predictors to Numeric With Target Encoding</title>
      <link>https://blasbenito.com/post/target-encoding/</link>
      <pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/target-encoding/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;Categorical predictors are annoying stringy monsters that can turn any data analysis and modeling effort into a real annoyance. The explains how to deal with these types of predictors using methods such as one-hot encoding (please don&amp;rsquo;t) or target encoding, and provides insights into their mechanisms and quirks.&lt;/p&gt;
&lt;h2 id=&#34;key-highlights&#34;&gt;Key Highlights:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Categorical Predictors are Kinda Annoying:&lt;/strong&gt; This section discusses the common issues encountered with categorical predictors during data analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;One-Hot Encoding Pitfalls:&lt;/strong&gt; While discussing one-hot encoding, the post focuses on its limitations, including dimensionality explosion, increased multicollinearity, and sparsity in tree-based models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro to Target Encoding:&lt;/strong&gt; Introducing target encoding as an alternative, the post explains its concept, illustrating the basic form with mean encoding and subsequent enhancements with additive smoothing, leave-one-out encoding, and more.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Handling Sparsity and Repetition:&lt;/strong&gt; It emphasizes the potential pitfalls of target encoding, such as repeated values within categories and their impact on model performance, prompting the exploration of strategies like white noise addition and random encoding to mitigate these issues.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Target Encoding Lab:&lt;/strong&gt; The post concludes with a detailed demonstration using the &lt;code&gt;collinear::target_encoding_lab()&lt;/code&gt; function, offering a hands-on exploration of various target encoding methods, parameter combinations, and their visual representations.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The post intends to serve as a useful resource for data scientists exploring alternative encoding techniques for categorical predictors.&lt;/p&gt;
&lt;h1 id=&#34;resources&#34;&gt;Resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/BlasBenito/notebooks/blob/main/target_encoding.Rmd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactive notebook of this post&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://doi.org/10.1145/507533.507538&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://towardsdatascience.com/extending-target-encoding-443aa9414cae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Extending Target Encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://maxhalford.github.io/blog/target-encoding/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Target encoding done the right way&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;r-packages&#34;&gt;R packages&lt;/h1&gt;
&lt;p&gt;This tutorial requires the development version (&amp;gt;= 1.0.3) of the newly released R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;, and a few more.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#required
install.packages(&amp;quot;remotes&amp;quot;)
remotes::install_github(
  repo = &amp;quot;blasbenito/collinear&amp;quot;, 
  ref = &amp;quot;development&amp;quot;,
  force = TRUE
  )
install.packages(&amp;quot;fastDummies&amp;quot;)
install.packages(&amp;quot;rpart&amp;quot;)
install.packages(&amp;quot;rpart.plot&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)
install.packages(&amp;quot;ggplot2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;categorical-predictors-can-be-annoying&#34;&gt;Categorical Predictors Can Be Annoying&lt;/h1&gt;
&lt;p&gt;I bet you have experienced it during an Exploratory Data Analysis (EDA) or a feature selection for model training and the likes. You likely had a nice bunch of numerical predictors, and then some things like &amp;ldquo;sampling_location&amp;rdquo;, &amp;ldquo;region_name&amp;rdquo;, &amp;ldquo;favorite_color&amp;rdquo;, or any other type of character or factor columns. And you had to branch your code to deal with numeric and categorical variables separately. Or maybe chose to ignore them, as I have done plenty of times.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s why many efforts have been made to convert them to numeric and kill the problem at once. And now we all have two problems to solve instead.&lt;/p&gt;
&lt;p&gt;Let me go ahead and illustrate the issue. There is a data frame in the &lt;code&gt;collinear&lt;/code&gt; R package named &lt;code&gt;vi&lt;/code&gt;, with one response variable named &lt;code&gt;vi_numeric&lt;/code&gt;, and several numeric and categorical predictors in the vector &lt;code&gt;vi_predictors&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(
  vi,
  vi_predictors
)

dplyr::glimpse(vi)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 30,000
## Columns: 68
## $ longitude                  &amp;lt;dbl&amp;gt; -114.254306, 114.845693, -122.145972, 108.3â¦
## $ latitude                   &amp;lt;dbl&amp;gt; 45.0540272, 26.2706940, 56.3790272, 29.9456â¦
## $ vi_numeric                 &amp;lt;dbl&amp;gt; 0.38, 0.53, 0.45, 0.69, 0.42, 0.68, 0.70, 0â¦
## $ vi_counts                  &amp;lt;int&amp;gt; 380, 530, 450, 690, 420, 680, 700, 260, 550â¦
## $ vi_binomial                &amp;lt;dbl&amp;gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0â¦
## $ vi_categorical             &amp;lt;chr&amp;gt; &amp;quot;medium&amp;quot;, &amp;quot;high&amp;quot;, &amp;quot;medium&amp;quot;, &amp;quot;very_high&amp;quot;, &amp;quot;mâ¦
## $ vi_factor                  &amp;lt;fct&amp;gt; medium, high, medium, very_high, medium, veâ¦
## $ koppen_zone                &amp;lt;chr&amp;gt; &amp;quot;BSk&amp;quot;, &amp;quot;Cfa&amp;quot;, &amp;quot;Dfc&amp;quot;, &amp;quot;Cfb&amp;quot;, &amp;quot;Aw&amp;quot;, &amp;quot;Cfa&amp;quot;, &amp;quot;Aâ¦
## $ koppen_group               &amp;lt;chr&amp;gt; &amp;quot;Arid&amp;quot;, &amp;quot;Temperate&amp;quot;, &amp;quot;Cold&amp;quot;, &amp;quot;Temperate&amp;quot;, &amp;quot;â¦
## $ koppen_description         &amp;lt;chr&amp;gt; &amp;quot;steppe, cold&amp;quot;, &amp;quot;no dry season, hot summer&amp;quot;â¦
## $ soil_type                  &amp;lt;fct&amp;gt; Cambisols, Acrisols, Luvisols, Alisols, Gleâ¦
## $ topo_slope                 &amp;lt;int&amp;gt; 6, 2, 0, 10, 0, 10, 6, 0, 2, 0, 0, 1, 0, 1,â¦
## $ topo_diversity             &amp;lt;int&amp;gt; 29, 24, 21, 25, 19, 30, 26, 20, 26, 22, 25,â¦
## $ topo_elevation             &amp;lt;int&amp;gt; 1821, 143, 765, 1474, 378, 485, 604, 1159, â¦
## $ swi_mean                   &amp;lt;dbl&amp;gt; 27.5, 56.1, 41.4, 59.3, 37.4, 56.3, 52.3, 2â¦
## $ swi_max                    &amp;lt;dbl&amp;gt; 62.9, 74.4, 81.9, 81.1, 83.2, 73.8, 55.8, 3â¦
## $ swi_min                    &amp;lt;dbl&amp;gt; 24.5, 33.3, 42.2, 31.3, 8.3, 28.8, 25.3, 11â¦
## $ swi_range                  &amp;lt;dbl&amp;gt; 38.4, 41.2, 39.7, 49.8, 74.9, 45.0, 30.5, 2â¦
## $ soil_temperature_mean      &amp;lt;dbl&amp;gt; 4.8, 19.9, 1.2, 13.0, 28.2, 18.1, 21.5, 23.â¦
## $ soil_temperature_max       &amp;lt;dbl&amp;gt; 29.9, 32.6, 20.4, 24.6, 41.6, 29.1, 26.4, 4â¦
## $ soil_temperature_min       &amp;lt;dbl&amp;gt; -12.4, 3.9, -16.0, -0.4, 16.8, 4.1, 17.3, 5â¦
## $ soil_temperature_range     &amp;lt;dbl&amp;gt; 42.3, 28.8, 36.4, 25.0, 24.8, 24.9, 9.1, 38â¦
## $ soil_sand                  &amp;lt;int&amp;gt; 41, 39, 27, 29, 48, 33, 30, 78, 23, 64, 54,â¦
## $ soil_clay                  &amp;lt;int&amp;gt; 20, 24, 28, 31, 27, 29, 40, 15, 26, 22, 23,â¦
## $ soil_silt                  &amp;lt;int&amp;gt; 38, 35, 43, 38, 23, 36, 29, 6, 49, 13, 22, â¦
## $ soil_ph                    &amp;lt;dbl&amp;gt; 6.5, 5.9, 5.6, 5.5, 6.5, 5.8, 5.2, 7.1, 7.3â¦
## $ soil_soc                   &amp;lt;dbl&amp;gt; 43.1, 14.6, 36.4, 34.9, 8.1, 20.8, 44.5, 4.â¦
## $ soil_nitrogen              &amp;lt;dbl&amp;gt; 2.8, 1.3, 2.9, 3.6, 1.2, 1.9, 2.8, 0.6, 3.1â¦
## $ solar_rad_mean             &amp;lt;dbl&amp;gt; 17.634, 19.198, 13.257, 14.163, 24.512, 17.â¦
## $ solar_rad_max              &amp;lt;dbl&amp;gt; 31.317, 24.498, 25.283, 17.237, 28.038, 22.â¦
## $ solar_rad_min              &amp;lt;dbl&amp;gt; 5.209, 13.311, 1.587, 9.642, 19.102, 12.196â¦
## $ solar_rad_range            &amp;lt;dbl&amp;gt; 26.108, 11.187, 23.696, 7.595, 8.936, 10.20â¦
## $ growing_season_length      &amp;lt;dbl&amp;gt; 139, 365, 164, 333, 228, 365, 365, 60, 365,â¦
## $ growing_season_temperature &amp;lt;dbl&amp;gt; 12.65, 19.35, 11.55, 12.45, 26.45, 17.75, 2â¦
## $ growing_season_rainfall    &amp;lt;dbl&amp;gt; 224.5, 1493.4, 345.4, 1765.5, 984.4, 1860.5â¦
## $ growing_degree_days        &amp;lt;dbl&amp;gt; 2140.5, 7080.9, 2053.2, 4162.9, 10036.7, 64â¦
## $ temperature_mean           &amp;lt;dbl&amp;gt; 3.65, 19.35, 1.45, 11.35, 27.55, 17.65, 22.â¦
## $ temperature_max            &amp;lt;dbl&amp;gt; 24.65, 33.35, 21.15, 23.75, 38.35, 30.55, 2â¦
## $ temperature_min            &amp;lt;dbl&amp;gt; -14.05, 3.05, -18.25, -3.55, 19.15, 2.45, 1â¦
## $ temperature_range          &amp;lt;dbl&amp;gt; 38.7, 30.3, 39.4, 27.3, 19.2, 28.1, 7.0, 29â¦
## $ temperature_seasonality    &amp;lt;dbl&amp;gt; 882.6, 786.6, 1070.9, 724.7, 219.3, 747.2, â¦
## $ rainfall_mean              &amp;lt;int&amp;gt; 446, 1493, 560, 1794, 990, 1860, 3150, 356,â¦
## $ rainfall_min               &amp;lt;int&amp;gt; 25, 37, 24, 29, 0, 60, 122, 1, 10, 12, 0, 0â¦
## $ rainfall_max               &amp;lt;int&amp;gt; 62, 209, 87, 293, 226, 275, 425, 62, 256, 3â¦
## $ rainfall_range             &amp;lt;int&amp;gt; 37, 172, 63, 264, 226, 215, 303, 61, 245, 2â¦
## $ evapotranspiration_mean    &amp;lt;dbl&amp;gt; 78.32, 105.88, 50.03, 64.65, 156.60, 108.50â¦
## $ evapotranspiration_max     &amp;lt;dbl&amp;gt; 164.70, 190.86, 117.53, 115.79, 187.71, 191â¦
## $ evapotranspiration_min     &amp;lt;dbl&amp;gt; 13.67, 50.44, 3.53, 28.01, 128.59, 51.39, 8â¦
## $ evapotranspiration_range   &amp;lt;dbl&amp;gt; 151.03, 140.42, 113.99, 87.79, 59.13, 139.9â¦
## $ cloud_cover_mean           &amp;lt;int&amp;gt; 31, 48, 42, 64, 38, 52, 60, 13, 53, 20, 11,â¦
## $ cloud_cover_max            &amp;lt;int&amp;gt; 39, 61, 49, 71, 58, 67, 77, 18, 60, 27, 23,â¦
## $ cloud_cover_min            &amp;lt;int&amp;gt; 16, 34, 33, 54, 19, 39, 45, 6, 45, 14, 2, 1â¦
## $ cloud_cover_range          &amp;lt;int&amp;gt; 23, 27, 15, 17, 38, 27, 32, 11, 15, 12, 21,â¦
## $ aridity_index              &amp;lt;dbl&amp;gt; 0.54, 1.27, 0.90, 2.08, 0.55, 1.67, 2.88, 0â¦
## $ humidity_mean              &amp;lt;dbl&amp;gt; 55.56, 62.14, 59.87, 69.32, 51.60, 62.76, 7â¦
## $ humidity_max               &amp;lt;dbl&amp;gt; 63.98, 65.00, 68.19, 71.90, 67.07, 65.68, 7â¦
## $ humidity_min               &amp;lt;dbl&amp;gt; 48.41, 58.97, 53.75, 67.21, 33.89, 59.92, 7â¦
## $ humidity_range             &amp;lt;dbl&amp;gt; 15.57, 6.03, 14.44, 4.69, 33.18, 5.76, 3.99â¦
## $ biogeo_ecoregion           &amp;lt;chr&amp;gt; &amp;quot;South Central Rockies forests&amp;quot;, &amp;quot;Jian Nan â¦
## $ biogeo_biome               &amp;lt;chr&amp;gt; &amp;quot;Temperate Conifer Forests&amp;quot;, &amp;quot;Tropical &amp;amp; Suâ¦
## $ biogeo_realm               &amp;lt;chr&amp;gt; &amp;quot;Nearctic&amp;quot;, &amp;quot;Indomalayan&amp;quot;, &amp;quot;Nearctic&amp;quot;, &amp;quot;Palâ¦
## $ country_name               &amp;lt;chr&amp;gt; &amp;quot;United States of America&amp;quot;, &amp;quot;China&amp;quot;, &amp;quot;Canadâ¦
## $ country_population         &amp;lt;dbl&amp;gt; 313973000, 1338612970, 33487208, 1338612970â¦
## $ country_gdp                &amp;lt;dbl&amp;gt; 15094000, 7973000, 1300000, 7973000, 15860,â¦
## $ country_income             &amp;lt;chr&amp;gt; &amp;quot;1. High income: OECD&amp;quot;, &amp;quot;3. Upper middle inâ¦
## $ continent                  &amp;lt;chr&amp;gt; &amp;quot;North America&amp;quot;, &amp;quot;Asia&amp;quot;, &amp;quot;North America&amp;quot;, &amp;quot;â¦
## $ region                     &amp;lt;chr&amp;gt; &amp;quot;Americas&amp;quot;, &amp;quot;Asia&amp;quot;, &amp;quot;Americas&amp;quot;, &amp;quot;Asia&amp;quot;, &amp;quot;Afâ¦
## $ subregion                  &amp;lt;chr&amp;gt; &amp;quot;Northern America&amp;quot;, &amp;quot;Eastern Asia&amp;quot;, &amp;quot;Northeâ¦
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The categorical variables in this data frame are identified below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vi_categorical &amp;lt;- collinear::identify_predictors_categorical(
  df = vi,
  predictors = vi_predictors
)
vi_categorical
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;koppen_zone&amp;quot;        &amp;quot;koppen_group&amp;quot;       &amp;quot;koppen_description&amp;quot;
##  [4] &amp;quot;soil_type&amp;quot;          &amp;quot;biogeo_ecoregion&amp;quot;   &amp;quot;biogeo_biome&amp;quot;      
##  [7] &amp;quot;biogeo_realm&amp;quot;       &amp;quot;country_name&amp;quot;       &amp;quot;country_income&amp;quot;    
## [10] &amp;quot;continent&amp;quot;          &amp;quot;region&amp;quot;             &amp;quot;subregion&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And finally, their number of categories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data.frame(
  name = vi_categorical,
  categories = lapply(
  X = vi_categorical,
  FUN = function(x) length(unique(vi[[x]]))
) |&amp;gt; 
  unlist()
) |&amp;gt; 
  dplyr::arrange(
    dplyr::desc(categories)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  name categories
## 1    biogeo_ecoregion        604
## 2        country_name        176
## 3           soil_type         29
## 4         koppen_zone         25
## 5           subregion         21
## 6  koppen_description         19
## 7        biogeo_biome         13
## 8        biogeo_realm          7
## 9           continent          7
## 10     country_income          6
## 11             region          6
## 12       koppen_group          5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some of them, like &lt;code&gt;country_name&lt;/code&gt; and &lt;code&gt;biogeo_ecoregion&lt;/code&gt;, have a cardinality high enough to ruin our day, don&amp;rsquo;t they? But ok, let&amp;rsquo;s start with one with a moderate number of categories, like &lt;code&gt;koppen_zone&lt;/code&gt;. This variable has 25 categories representing climate zones.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sort(unique(vi$koppen_zone))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Af&amp;quot;  &amp;quot;Am&amp;quot;  &amp;quot;Aw&amp;quot;  &amp;quot;BSh&amp;quot; &amp;quot;BSk&amp;quot; &amp;quot;BWh&amp;quot; &amp;quot;BWk&amp;quot; &amp;quot;Cfa&amp;quot; &amp;quot;Cfb&amp;quot; &amp;quot;Cfc&amp;quot; &amp;quot;Csa&amp;quot; &amp;quot;Csb&amp;quot;
## [13] &amp;quot;Cwa&amp;quot; &amp;quot;Cwb&amp;quot; &amp;quot;Dfa&amp;quot; &amp;quot;Dfb&amp;quot; &amp;quot;Dfc&amp;quot; &amp;quot;Dfd&amp;quot; &amp;quot;Dsa&amp;quot; &amp;quot;Dsb&amp;quot; &amp;quot;Dsc&amp;quot; &amp;quot;Dwa&amp;quot; &amp;quot;Dwb&amp;quot; &amp;quot;Dwc&amp;quot;
## [25] &amp;quot;ET&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;one-hot-encoding-is-here&#34;&gt;One-hot Encoding is here&amp;hellip;&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s use it as predictor of &lt;code&gt;vi_numeric&lt;/code&gt; in a linear model and take a look at the summary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;lm(
  formula = vi_numeric ~ koppen_zone, 
  data = vi
  ) |&amp;gt; 
  summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = vi_numeric ~ koppen_zone, data = vi)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.57090 -0.05592 -0.00305  0.05695  0.49212 
## 
## Coefficients:
##                 Estimate Std. Error  t value Pr(&amp;gt;|t|)    
## (Intercept)     0.670899   0.002054  326.651  &amp;lt; 2e-16 ***
## koppen_zoneAm  -0.022807   0.003151   -7.239 4.64e-13 ***
## koppen_zoneAw  -0.143375   0.002434  -58.903  &amp;lt; 2e-16 ***
## koppen_zoneBSh -0.347894   0.002787 -124.839  &amp;lt; 2e-16 ***
## koppen_zoneBSk -0.422162   0.002823 -149.523  &amp;lt; 2e-16 ***
## koppen_zoneBWh -0.537854   0.002392 -224.859  &amp;lt; 2e-16 ***
## koppen_zoneBWk -0.543022   0.002906 -186.883  &amp;lt; 2e-16 ***
## koppen_zoneCfa -0.104730   0.003087  -33.928  &amp;lt; 2e-16 ***
## koppen_zoneCfb -0.081909   0.003949  -20.744  &amp;lt; 2e-16 ***
## koppen_zoneCfc -0.120899   0.017419   -6.941 3.99e-12 ***
## koppen_zoneCsa -0.274720   0.005145  -53.399  &amp;lt; 2e-16 ***
## koppen_zoneCsb -0.136575   0.006142  -22.237  &amp;lt; 2e-16 ***
## koppen_zoneCwa -0.149006   0.003318  -44.910  &amp;lt; 2e-16 ***
## koppen_zoneCwb -0.177753   0.004579  -38.817  &amp;lt; 2e-16 ***
## koppen_zoneDfa -0.214981   0.004437  -48.453  &amp;lt; 2e-16 ***
## koppen_zoneDfb -0.179080   0.003347  -53.499  &amp;lt; 2e-16 ***
## koppen_zoneDfc -0.237050   0.003937  -60.207  &amp;lt; 2e-16 ***
## koppen_zoneDfd -0.395899   0.065900   -6.008 1.91e-09 ***
## koppen_zoneDsa -0.462494   0.011401  -40.567  &amp;lt; 2e-16 ***
## koppen_zoneDsb -0.330969   0.008056  -41.084  &amp;lt; 2e-16 ***
## koppen_zoneDsc -0.327097   0.011244  -29.090  &amp;lt; 2e-16 ***
## koppen_zoneDwa -0.282620   0.005248  -53.850  &amp;lt; 2e-16 ***
## koppen_zoneDwb -0.254027   0.005981  -42.473  &amp;lt; 2e-16 ***
## koppen_zoneDwc -0.306156   0.005660  -54.096  &amp;lt; 2e-16 ***
## koppen_zoneET  -0.297869   0.011649  -25.571  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.09315 on 29975 degrees of freedom
## Multiple R-squared:  0.805,	Adjusted R-squared:  0.8049 
## F-statistic:  5157 on 24 and 29975 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look at that. What the hell happened there? Well, linear models cannot deal with categorical predictors, so they create numeric &lt;strong&gt;dummy variables&lt;/strong&gt; instead. The function &lt;code&gt;stats::model.matrix()&lt;/code&gt; does exactly that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dummy_variables &amp;lt;- stats::model.matrix( 
  ~ koppen_zone,
  data = vi
  )
ncol(dummy_variables)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 25
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dummy_variables[1:10, 1:10]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    (Intercept) koppen_zoneAm koppen_zoneAw koppen_zoneBSh koppen_zoneBSk
## 1            1             0             0              0              1
## 2            1             0             0              0              0
## 3            1             0             0              0              0
## 4            1             0             0              0              0
## 5            1             0             1              0              0
## 6            1             0             0              0              0
## 7            1             0             0              0              0
## 8            1             0             0              1              0
## 9            1             0             0              0              0
## 10           1             0             0              0              0
##    koppen_zoneBWh koppen_zoneBWk koppen_zoneCfa koppen_zoneCfb koppen_zoneCfc
## 1               0              0              0              0              0
## 2               0              0              1              0              0
## 3               0              0              0              0              0
## 4               0              0              0              1              0
## 5               0              0              0              0              0
## 6               0              0              1              0              0
## 7               0              0              0              0              0
## 8               0              0              0              0              0
## 9               0              0              0              0              0
## 10              1              0              0              0              0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function first creates an Intercept column with all ones. Then, for each original category except the first one (&amp;ldquo;Af&amp;rdquo;), a new column with value 1 in the cases where the given category was present and 0 otherwise is created. The category with no column (&amp;ldquo;Af&amp;rdquo;) is represented in these cases in the intercept where all other dummy columns are zero. This is, essentially, &lt;strong&gt;one-hot encoding&lt;/strong&gt; with a little twist. You will find most people use the terms &lt;em&gt;dummy variables&lt;/em&gt; and &lt;em&gt;one-hot encoding&lt;/em&gt; interchangeably, and that&amp;rsquo;s ok. But in the end, the little twist of omitting the first category is what differentiates them. Most functions performing one-hot encoding, no matter their name, are creating as many columns as categories. That is for example the case of &lt;code&gt;fastDummies::dummy_cols()&lt;/code&gt;, from the R package 
&lt;a href=&#34;https://jacobkap.github.io/fastDummies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;fastDummies&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- fastDummies::dummy_cols(
  .data = vi[, &amp;quot;koppen_zone&amp;quot;, drop = FALSE],
  select_columns = &amp;quot;koppen_zone&amp;quot;,
  remove_selected_columns = TRUE
)
dplyr::glimpse(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 30,000
## Columns: 25
## $ koppen_zone_Af  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Am  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Aw  &amp;lt;int&amp;gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_BSh &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_BSk &amp;lt;int&amp;gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_BWh &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, â¦
## $ koppen_zone_BWk &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, â¦
## $ koppen_zone_Cfa &amp;lt;int&amp;gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Cfb &amp;lt;int&amp;gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, â¦
## $ koppen_zone_Cfc &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Csa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Csb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Cwa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Cwb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Dfa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, â¦
## $ koppen_zone_Dfb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Dfc &amp;lt;int&amp;gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Dfd &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Dsa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Dsb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Dsc &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Dwa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Dwb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_Dwc &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
## $ koppen_zone_ET  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â¦
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;to-mess-up-your-models&#34;&gt;&amp;hellip;to mess-up your models&lt;/h1&gt;
&lt;p&gt;As good as one-hot encoding is to fit linear models when predictors are categorical, it creates a couple of glaring issues that are hard to address when the number of encoded categories is high.&lt;/p&gt;
&lt;p&gt;The first issue can easily be named the &lt;strong&gt;dimensionality explosion&lt;/strong&gt;. If we created dummy variables for all categorical predictors in &lt;code&gt;vi&lt;/code&gt;, then we&amp;rsquo;d go from the original 61 predictors to a total of 967 new columns to handle. This alone can degrade the computational performance of a model due to increased data size.&lt;/p&gt;
&lt;p&gt;The second issue is &lt;strong&gt;increased multicollinearity&lt;/strong&gt;. One-hot encoded features are highly collinear, which makes obtaining accurate estimates for the coefficients of the encoded categories very hard. Look at the Variance Inflation Factors of the encoded Koppen zones, they have incredibly high values!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::vif_df(
  df = df,
  quiet = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          predictor          vif
## 6  koppen_zone_BWh 2.403991e+15
## 3   koppen_zone_Aw 2.177627e+15
## 4  koppen_zone_BSh 1.158438e+15
## 5  koppen_zone_BSk 1.100300e+15
## 1   koppen_zone_Af 9.879589e+14
## 7  koppen_zone_BWk 9.866240e+14
## 8  koppen_zone_Cfa 7.966760e+14
## 2   koppen_zone_Am 7.440723e+14
## 13 koppen_zone_Cwa 6.309241e+14
## 16 koppen_zone_Dfb 6.139201e+14
## 17 koppen_zone_Dfc 3.863684e+14
## 9  koppen_zone_Cfb 3.834325e+14
## 15 koppen_zone_Dfa 2.838687e+14
## 14 koppen_zone_Cwb 2.624933e+14
## 11 koppen_zone_Csa 1.984881e+14
## 22 koppen_zone_Dwa 1.894423e+14
## 24 koppen_zone_Dwc 1.592088e+14
## 23 koppen_zone_Dwb 1.405032e+14
## 12 koppen_zone_Csb 1.323997e+14
## 20 koppen_zone_Dsb 7.338609e+13
## 21 koppen_zone_Dsc 3.652432e+13
## 19 koppen_zone_Dsa 3.549784e+13
## 25  koppen_zone_ET 3.395786e+13
## 10 koppen_zone_Cfc 1.493932e+13
## 18 koppen_zone_Dfd 1.031226e+12
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On top of those issues, one-hot encoding also causes &lt;strong&gt;sparsity&lt;/strong&gt; in tree-based models. Let me show you an example. Below I train a recursive partition tree using &lt;code&gt;vi_numeric&lt;/code&gt; as response, and the one-hot encoded version of &lt;code&gt;koppen_zone&lt;/code&gt; we have in &lt;code&gt;df&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#add response variable to df
df$vi_numeric &amp;lt;- vi$vi_numeric

#fit model using all one-hot encoded variables
koppen_zone_one_hot &amp;lt;- rpart::rpart(
  formula = vi_numeric ~ .,
  data = df
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I do the same using the categorical version of &lt;code&gt;koppen_zone&lt;/code&gt; in &lt;code&gt;vi&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;koppen_zone_categorical &amp;lt;- rpart::rpart(
  formula = vi_numeric ~ koppen_zone,
  data = vi
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I am plotting the skeletons of these trees side by side (we don&amp;rsquo;t care about numbers here).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#plot tree skeleton
par(mfrow = c(1, 2))
plot(koppen_zone_one_hot, main = &amp;quot;One-hot encoding&amp;quot;)
plot(koppen_zone_categorical, main = &amp;quot;Categorical&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/target-encoding/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;576&#34; /&gt;
&lt;p&gt;Notice the stark differences in tree structure between both options. On the left, the tree trained on the one-hot encoded data only shows growth on one side! This is the &lt;em&gt;sparsity&lt;/em&gt; I was talking about before. On the right side, however, the tree based on the categorical variable shows a balanced and healthy structure. One-hot encoded data can easily mess up a single univariate regression tree, so imagine what it can do to your fancy random forest model with hundreds of these trees.&lt;/p&gt;
&lt;p&gt;In the end, the magic of one-hot encoding is in its inherent ability to create two or three problems for each one it promised to solve. We all know someone like that. Not so hot, if you ask me.&lt;/p&gt;
&lt;h1 id=&#34;target-encoding-mean-encoding-and-dummy-variables-all-the-same&#34;&gt;Target Encoding, Mean Encoding, and Dummy Variables (All The Same)&lt;/h1&gt;
&lt;p&gt;On a bright summer day of 2001, 
&lt;a href=&#34;https://www.aitimejournal.com/interview-with-daniele-micci-barreca-product-analytics-lead-data-science-google/30110/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniele Micci-Barreca&lt;/a&gt; finally got sick of the one-hot encoding wonders and decided to publish 
&lt;a href=&#34;https://doi.org/10.1145/507533.507538&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;his ideas on a suitable alternative&lt;/a&gt; others later named &lt;em&gt;mean encoding&lt;/em&gt; or &lt;em&gt;target encoding&lt;/em&gt;. He told the story himself 20 years later, in a nice blog post titled 
&lt;a href=&#34;https://towardsdatascience.com/extending-target-encoding-443aa9414cae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Extending Target Encoding&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But what is target encoding? Let&amp;rsquo;s start with a continuous response variable &lt;code&gt;y&lt;/code&gt; (a.k.a &lt;em&gt;the target&lt;/em&gt;) and a categorical predictor &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;mean-encoding&#34;&gt;Mean Encoding&lt;/h2&gt;
&lt;p&gt;In &lt;em&gt;it&amp;rsquo;s simplest form&lt;/em&gt;, target encoding replaces each category in &lt;code&gt;x&lt;/code&gt; with the mean of &lt;code&gt;y&lt;/code&gt; across the category cases. This results in a new numeric version of &lt;code&gt;x&lt;/code&gt; named &lt;code&gt;x_encoded&lt;/code&gt; in the example below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx |&amp;gt; 
  dplyr::group_by(x) |&amp;gt; 
  dplyr::mutate(
    x_encoded = mean(y)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 Ã 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a             2
## 2     2 a             2
## 3     3 a             2
## 4     4 b             5
## 5     5 b             5
## 6     6 b             5
## 7     7 c             7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simple is good, right? But sometimes it&amp;rsquo;s not. In our toy case, the category &amp;ldquo;c&amp;rdquo; has only one case that maps directly to an actual value of &lt;code&gt;y&lt;/code&gt;.Imagine the worst case scenario of &lt;code&gt;x&lt;/code&gt; having one different category per row, then &lt;code&gt;x_encoded&lt;/code&gt; would be identical to &lt;code&gt;y&lt;/code&gt;!&lt;/p&gt;
&lt;h2 id=&#34;mean-encoding-with-additive-smoothing&#34;&gt;Mean Encoding With Additive Smoothing&lt;/h2&gt;
&lt;p&gt;The issue can be solved by pushing the mean of &lt;code&gt;y&lt;/code&gt; for each category in &lt;code&gt;x&lt;/code&gt; towards the global mean of &lt;code&gt;y&lt;/code&gt; by the weighted sample size of the category, as suggested by the expression&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$x_{encoded_i} = \frac{n_i \times \overline{y}_i + m \times \overline{y}}{n_i + m}$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;\(n_i\)&lt;/code&gt; is the size of the category &lt;code&gt;\(i\)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\(\overline{y}_i\)&lt;/code&gt; is the mean of the target over the category &lt;code&gt;\(i\)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\(m\)&lt;/code&gt; is the smoothing parameter.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\(\overline{y}\)&lt;/code&gt; is the global mean of the target.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y_mean &amp;lt;- mean(yx$y)

m &amp;lt;- 3

yx |&amp;gt; 
  dplyr::group_by(x) |&amp;gt; 
  dplyr::mutate(
    x_encoded = 
      (dplyr::n() * mean(y) + m * y_mean) / (dplyr::n() + m)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 Ã 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a          3   
## 2     2 a          3   
## 3     3 a          3   
## 4     4 b          4.5 
## 5     5 b          4.5 
## 6     6 b          4.5 
## 7     7 c          4.75
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far so good! But still, the simplest implementations of target encoding generate repeated values for all cases within a category. This can still mess-up tree-based models a bit, because splits may happen again and again in the same values of the predictor. However, there are several strategies to limit this issue as well.&lt;/p&gt;
&lt;h2 id=&#34;leave-one-out-target-encoding&#34;&gt;Leave-one-out Target Encoding&lt;/h2&gt;
&lt;p&gt;In this version of target encoding, the encoded value of one case within a category is the mean of all other cases within the same category. This results in a robust encoding that avoids direct reference to the target value of the sample being encoded, and does not generate repeated values.&lt;/p&gt;
&lt;p&gt;The code below implements the idea in a way so simple that it cannot even deal with one-case categories.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx |&amp;gt;
  dplyr::group_by(x) |&amp;gt;
  dplyr::mutate(
    x_encoded = (sum(y) - y) / (dplyr::n() - 1)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 Ã 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a           2.5
## 2     2 a           2  
## 3     3 a           1.5
## 4     4 b           5.5
## 5     5 b           5  
## 6     6 b           4.5
## 7     7 c         NaN
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;mean-encoding-with-white-noise&#34;&gt;Mean Encoding with White Noise&lt;/h2&gt;
&lt;p&gt;Another way to avoid repeated values while keeping the encoding as simple as possible consists of just adding a white noise to the encoded values. The code below adds noise generated by &lt;code&gt;stats::runif()&lt;/code&gt; to the mean-encoded values, but other options such as &lt;code&gt;stats::rnorm()&lt;/code&gt; (noise from a normal distribution) can be useful here. Since white noise is random, we need to set the seed of the pseudo-random number generator (with &lt;code&gt;set.seed()&lt;/code&gt;) to obtain constant results every time we run the code below.&lt;/p&gt;
&lt;p&gt;When using this method we have to be careful with the amount of noise we add. It should be a harmless fraction of target, small enough to not throw a model off the signal provided by the encoded variable. In our toy case &lt;code&gt;y&lt;/code&gt; is between 1 and 7, so something like &amp;ldquo;one percent of the maximum&amp;rdquo; could work well here.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#maximum noise to add
max_noise &amp;lt;- max(yx$y)/100

#set seed for reproducibility
set.seed(1)

yx |&amp;gt; 
  dplyr::group_by(x) |&amp;gt; 
  dplyr::mutate(
    x_encoded = mean(y) + runif(n = dplyr::n(), max = max_noise)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 Ã 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a          2.02
## 2     2 a          2.03
## 3     3 a          2.04
## 4     4 b          5.06
## 5     5 b          5.01
## 6     6 b          5.06
## 7     7 c          7.07
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This method can deal with one-case categories without issues, and does not generate repeated values, but in exchange, we have to be mindful of the amount of noise we add, and we have to set a random seed to ensure reproducibility.&lt;/p&gt;
&lt;h2 id=&#34;rank-encoding-plus-white-noise&#34;&gt;Rank Encoding plus White Noise&lt;/h2&gt;
&lt;p&gt;This is a little different from all the other methods, because it does not map the categories to values from the target, but to the rank/order of the target means per category. It basically converts the categorical variable into an ordinal one arranged along with the target, and then adds white noise on top to avoid value repetition.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#maximum noise as function of the number of categories
max_noise &amp;lt;- length(unique(yx$x))/100

yx |&amp;gt; 
  dplyr::arrange(y) |&amp;gt; 
  dplyr::group_by(x) |&amp;gt; 
  dplyr::mutate(
    x_encoded = dplyr::cur_group_id() + runif(n = dplyr::n(), max = max_noise)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 Ã 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a          1.02
## 2     2 a          1.02
## 3     3 a          1.00
## 4     4 b          2.01
## 5     5 b          2.01
## 6     6 b          2.02
## 7     7 c          3.01
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-target-encoding-lab&#34;&gt;The Target Encoding Lab&lt;/h2&gt;
&lt;p&gt;The function &lt;code&gt;collinear::target_encoding_lab()&lt;/code&gt; implements all these encoding methods, and allows defining different combinations of parameters. It was designed to help understand how they work, and maybe help make choices about what&amp;rsquo;s the right encoding for a given categorical predictor.&lt;/p&gt;
&lt;p&gt;In the example below, the methods rank, mean, and leave-one-out are computed with white noise of 0 and 0.1 (that&amp;rsquo;s the width of the uniform distribution the noise is extracted from), the mean is also with and without smoothing, and the rnorm is computed using two different multipliers of the standard deviation of the normal distribution computed for each group in the predictor, just to help control the data spread.&lt;/p&gt;
&lt;p&gt;The function also uses a random seed to generate the same noise across the encoded versions of the predictor to make them as comparable as possible. Every time you change the seed, results using white noise and the rnorm method should change as well.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx_encoded &amp;lt;- target_encoding_lab(
  df = yx,
  response = &amp;quot;y&amp;quot;,
  predictors = &amp;quot;x&amp;quot;,
  white_noise = c(0, 0.1),
  smoothing = c(0, 2),
  quiet = FALSE,
  seed = 1, #for reproducibility
  overwrite = FALSE #to overwrite or not the predictors with their encodings
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## collinear::target_encoding_lab(): using response &#39;y&#39; to encode categorical predictors:
##  - x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dplyr::glimpse(yx_encoded)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 7
## Columns: 14
## $ y                                               &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7
## $ x                                               &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;bâ¦
## $ x__encoded_loo                                  &amp;lt;dbl&amp;gt; 2.5, 2.0, 1.5, 5.5, 5.â¦
## $ x__encoded_loo__noise_0.1__seed_1               &amp;lt;dbl&amp;gt; 2.554579, 1.564069, 2.â¦
## $ x__encoded_loo                                  &amp;lt;dbl&amp;gt; 2.5, 2.0, 1.5, 5.5, 5.â¦
## $ x__encoded_loo__noise_0.1__seed_1               &amp;lt;dbl&amp;gt; 2.554579, 1.564069, 2.â¦
## $ x__encoded_mean                                 &amp;lt;dbl&amp;gt; 2, 2, 2, 5, 5, 5, 7
## $ x__encoded_mean__noise_0.1__seed_1              &amp;lt;dbl&amp;gt; 2.054579, 1.564069, 2.â¦
## $ x__encoded_mean__smoothing_2                    &amp;lt;dbl&amp;gt; 2.8, 2.8, 2.8, 4.6, 4.â¦
## $ x__encoded_mean__smoothing_2__noise_0.1__seed_1 &amp;lt;dbl&amp;gt; 2.854579, 2.364069, 3.â¦
## $ x__encoded_rank                                 &amp;lt;int&amp;gt; 1, 1, 1, 2, 2, 2, 3
## $ x__encoded_rank__noise_0.1__seed_1              &amp;lt;dbl&amp;gt; 1.0545786, 0.5640694, â¦
## $ x__encoded_rank                                 &amp;lt;int&amp;gt; 1, 1, 1, 2, 2, 2, 3
## $ x__encoded_rank__noise_0.1__seed_1              &amp;lt;dbl&amp;gt; 1.0545786, 0.5640694, â¦
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx_encoded |&amp;gt; 
  tidyr::pivot_longer(
    cols = dplyr::contains(&amp;quot;__encoded&amp;quot;),
    values_to = &amp;quot;x_encoded&amp;quot;
  ) |&amp;gt; 
  ggplot() + 
  facet_wrap(&amp;quot;name&amp;quot;) +
  aes(
    x = x_encoded,
    y = y,
    color = x
  ) +
  geom_point(size = 3) + 
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/target-encoding/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1152&#34; /&gt;
The function also allows to replace a given predictor with their selected encoding.
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx_encoded &amp;lt;- collinear::target_encoding_lab(
  df = yx,
  response = &amp;quot;y&amp;quot;,
  predictors = &amp;quot;x&amp;quot;,
  methods = &amp;quot;mean&amp;quot;, #selected encoding method
  smoothing = 2,
  quiet = FALSE,
  overwrite = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## collinear::target_encoding_lab(): using response &#39;y&#39; to encode categorical predictors:
##  - x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dplyr::glimpse(yx_encoded)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 7
## Columns: 2
## $ y &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7
## $ x &amp;lt;dbl&amp;gt; 2.8, 2.8, 2.8, 4.6, 4.6, 4.6, 5.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s all about target encoding so far!&lt;/p&gt;
&lt;p&gt;I have a post in my TODO list with a little real experiment comparing target encoding with one-hot encoding in tree-based models. If you are interested, stay tuned!&lt;/p&gt;
&lt;p&gt;Cheers,&lt;/p&gt;
&lt;p&gt;Blas&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Everything You Don&#39;t Need to Know About Variance Inflation Factors</title>
      <link>https://blasbenito.com/post/variance-inflation-factor/</link>
      <pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/variance-inflation-factor/</guid>
      <description>&lt;h1 id=&#34;resources&#34;&gt;Resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/BlasBenito/notebooks/blob/main/variance_inflation_factors.Rmd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rmarkdown notebook used in this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://blasbenito.com/post/multicollinearity-model-interpretability/&#34;&gt;Multicollinearity Hinders Model Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R package &lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;This post focuses on Variance Inflation Factors (VIF) and their crucial role in identifying multicollinearity within linear models.&lt;/p&gt;
&lt;p&gt;The post covers the following main points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VIF meaning and interpretation&lt;/strong&gt;: Through practical examples, I demonstrate how to compute VIF values and their significance in model design. Particularly, I try to shed light on their influence on coefficient estimates and their confidence intervals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Impact of High VIF&lt;/strong&gt;: I use a small simulation to show how having a model design with a high VIF hinders the identification of predictors with moderate effects, particularly in situations with limited data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Effective VIF Management&lt;/strong&gt;: I introduce how to use the &lt;code&gt;collinear&lt;/code&gt; package and its &lt;code&gt;vif_select()&lt;/code&gt; function. to aid in the selection of predictors with low VIF, thereby enhancing model stability and interpretability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ultimately, this post serves as a comprehensive resource for understanding, interpreting, and managing VIF in the context of linear modeling. It caters to those with a strong command of R and a keen interest in statistical modeling.&lt;/p&gt;
&lt;h1 id=&#34;r-packages&#34;&gt;R packages&lt;/h1&gt;
&lt;p&gt;This tutorial requires the development version (&amp;gt;= 1.0.3) of the newly released R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;, and a few more.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#required
install.packages(&amp;quot;remotes&amp;quot;)
remotes::install_github(
  repo = &amp;quot;blasbenito/collinear&amp;quot;, 
  ref = &amp;quot;development&amp;quot;
  )
install.packages(&amp;quot;ranger&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)
install.packages(&amp;quot;ggplot2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;example-data&#34;&gt;Example data&lt;/h1&gt;
&lt;p&gt;This post uses the &lt;code&gt;toy&lt;/code&gt; data set shipped with the version &amp;gt;= 1.0.3 of the R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;. It is a data frame of centered and scaled variables representing a model design of the form &lt;code&gt;y ~ a + b + c + d&lt;/code&gt;, where the predictors show varying degrees of relatedness. Let&amp;rsquo;s load and check it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
library(ggplot2)
library(collinear)

toy |&amp;gt; 
  round(3) |&amp;gt; 
  head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        y      a      b      c      d
## 1  0.655  0.342 -0.158  0.254  0.502
## 2  0.610  0.219  1.814  0.450  1.373
## 3  0.316  1.078 -0.643  0.580  0.673
## 4  0.202  0.956 -0.815  1.168 -0.147
## 5 -0.509 -0.149 -0.356 -0.456  0.187
## 6  0.675  0.465  1.292 -0.020  0.983
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The columns in &lt;code&gt;toy&lt;/code&gt; are related as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: response generated from &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; using the expression &lt;code&gt;y = a * 0.75 + b * 0.25 + noise&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt;: predictor of &lt;code&gt;y&lt;/code&gt; uncorrelated with &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt;: predictor of &lt;code&gt;y&lt;/code&gt; uncorrelated with &lt;code&gt;a&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: predictor generated as &lt;code&gt;c = a + noise&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d&lt;/code&gt;: predictor generated as &lt;code&gt;d = (a + b)/2 + noise&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The pairwise correlations between all predictors in &lt;code&gt;toy&lt;/code&gt; are shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::cor_df(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x y correlation
## 1 c a  0.96154984
## 2 d b  0.63903887
## 3 d a  0.63575882
## 4 d c  0.61480312
## 5 b a -0.04740881
## 6 c b -0.04218308
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep these pairwise correlations in mind for what comes next!&lt;/p&gt;
&lt;h1 id=&#34;the-meaning-of-variance-inflation-factors&#34;&gt;The Meaning of Variance Inflation Factors&lt;/h1&gt;
&lt;p&gt;There are two general cases of multicollinearity in model designs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When there are pairs of predictors highly correlated.&lt;/li&gt;
&lt;li&gt;When there are &lt;strong&gt;predictors that are linear combinations of other predictors&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The focus of this post is on the second one.&lt;/p&gt;
&lt;p&gt;We can say a predictor is a linear combination of other predictors when it can be reasonably predicted from a multiple regression model against all other predictors.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say we focus on &lt;code&gt;a&lt;/code&gt; and fit the multiple regression model &lt;code&gt;a ~ b + c + d&lt;/code&gt;. The higher the R-squared of this model, the more confident we are to say that &lt;code&gt;a&lt;/code&gt; is a linear combination of &lt;code&gt;b + c + d&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#model of a against all other predictors
abcd_model &amp;lt;- lm(
  formula = a ~ b + c + d,
  data = toy
)

#r-squared of the a_model
abcd_R2 &amp;lt;- summary(abcd_model)$r.squared
abcd_R2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9381214
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the R-squared of &lt;code&gt;a&lt;/code&gt; against all other predictors is pretty high, it definitely seems that &lt;code&gt;a&lt;/code&gt; is a linear combination of the other predictors, and we can conclude that there is multicollinearity in the model design.&lt;/p&gt;
&lt;p&gt;However, as informative as this R-squared is, it tells us nothing about the consequences of having multicollinearity in our model design. And this is where &lt;strong&gt;Variance Inflation Factors&lt;/strong&gt;, or &lt;strong&gt;VIF&lt;/strong&gt; for short, come into play.&lt;/p&gt;
&lt;h2 id=&#34;what-are-variance-inflation-factors&#34;&gt;What are Variance Inflation Factors?&lt;/h2&gt;
&lt;p&gt;The Variance Inflation Factor (VIF) of a predictor is computed as &lt;code&gt;\(1/(1 - R^2)\)&lt;/code&gt;, where &lt;code&gt;\(R^Â²\)&lt;/code&gt; is the R-squared of the multiple linear regression of the predictor against all other predictors.&lt;/p&gt;
&lt;p&gt;In the case of &lt;code&gt;a&lt;/code&gt;, we just have to apply the VIF expression to the R-squared of the regression model against all other predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;abcd_vif &amp;lt;- 1/(1-abcd_R2)
abcd_vif
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16.16067
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This VIF score is relative to the other predictors in the model design. If we change the model design, so does the VIF of all predictors! For example, if we remove &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt; from the model design, we are left with this VIF for &lt;code&gt;a&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ab_model &amp;lt;- lm(
  formula = a ~ b,
  data = toy
)

ab_vif &amp;lt;- 1/(1 - summary(ab_model)$r.squared)
ab_vif
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.002253
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An almost perfect VIF score!&lt;/p&gt;
&lt;p&gt;We can simplify the VIF computation using &lt;code&gt;collinear::vif_df()&lt;/code&gt;, which returns the VIF of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; at once.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::vif_df(
  df = toy[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   predictor    vif
## 1         a 1.0023
## 2         b 1.0023
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In plot below, the worst and best VIF scores of &lt;code&gt;a&lt;/code&gt; are shown in the context of the relationship between R-squared and VIF, and three VIF thresholds commonly mentioned in the literature. These thresholds are represented as vertical dashed lines at VIF 2.5, 5, and 10, and are used as criteria to control multicollinearity in model designs. I will revisit this topic later in the post.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/variance-inflation-factor/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;When the R-squared of the linear regression model is 0, then the VIF expression becomes &lt;code&gt;\(1/(1 - 0) = 1\)&lt;/code&gt; and returns the minimum possible VIF. On the other end, when R-squared is 1, then we get &lt;code&gt;\(1/(1 - 1) = Inf\)&lt;/code&gt;, the maximum VIF.&lt;/p&gt;
&lt;p&gt;So far, we have learned that to assess whether the predictor &lt;code&gt;a&lt;/code&gt; induces multicollinearity in the model design &lt;code&gt;y ~ a + b + c + d&lt;/code&gt; we can compute it&amp;rsquo;s Variance Inflation Factor from the R-squared of the model &lt;code&gt;a ~ b + c + d&lt;/code&gt;. We have also learned that if the model design changes, so does the VIF of &lt;code&gt;a&lt;/code&gt;. We also know that there are some magic numbers (the VIF thresholds) we can use as reference.&lt;/p&gt;
&lt;p&gt;But still, we have no indication of what these VIF values actually mean! I will try to fix that in the next section.&lt;/p&gt;
&lt;h2 id=&#34;but-really-what-are-variance-inflation-factors&#34;&gt;But really, what are Variance Inflation Factors?&lt;/h2&gt;
&lt;p&gt;Variance Inflation Factors are inherently linked to these fundamental linear modeling concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Coefficient Estimate&lt;/strong&gt; (&lt;code&gt;\(\hat{\beta}\)&lt;/code&gt;): The estimated slope of the relationship between a predictor and the response in a linear model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Standard Error&lt;/strong&gt; (&lt;code&gt;\(\text{SE}\)&lt;/code&gt;): Represents the uncertainty around the estimation of the coefficient due to data variability.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Significance level&lt;/strong&gt; (&lt;code&gt;\(1.96\)&lt;/code&gt;): The acceptable level of error when determining the significance of the &lt;em&gt;coefficient estimate&lt;/em&gt;. Here it is simplified to 1.96, the 97.5th percentile of a normal distribution, to approximate a significance level of 0.05.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Confidence Interval&lt;/strong&gt; (&lt;code&gt;\(CI\)&lt;/code&gt;): The range of values containing the true value of the &lt;em&gt;coefficient estimate&lt;/em&gt; withing a certain &lt;em&gt;significance level&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These terms are related by the expression to compute the confidence interval of the coefficient estimate:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$\text{CI} = \beta \pm 1.96 \cdot \text{SE}$$&lt;/code&gt;
Let me convert this equation into a small function to compute confidence intervals of coefficient estimates named &lt;code&gt;ci()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ci &amp;lt;- function(b, se){
  x &amp;lt;- se * 1.96
  as.numeric(c(b-x, b+x))
}
#note: stats::confint() which uses t-critical values to compute more precise confidence intervals. 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are going to look at the coefficient estimate and standard error of &lt;code&gt;a&lt;/code&gt; in the model &lt;code&gt;y ~ a + b&lt;/code&gt;. We know that &lt;code&gt;a&lt;/code&gt; in this model has a vif of 1.0022527.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yab_model &amp;lt;- lm(
  formula = y ~ a + b,
  data = toy
) |&amp;gt; 
  summary()

#coefficient estimate and standard error of a
a_coef &amp;lt;- yab_model$coefficients[2, 1:2]
a_coef
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Estimate  Std. Error 
## 0.747689326 0.006636511
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we plug them into our little function to compute the confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;a_ci &amp;lt;- ci(
  b = a_coef[1], 
  se = a_coef[2]
  )
a_ci
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7346818 0.7606969
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And, finally, we compute the width of the confidence interval for &lt;code&gt;a&lt;/code&gt; as the difference between the extremes of the confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;old_width &amp;lt;- diff(a_ci)
old_width
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02601512
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep this number in mind, it&amp;rsquo;s important.&lt;/p&gt;
&lt;p&gt;Now, let me tell you something weird: &lt;strong&gt;The confidence interval of a predictor is widened by a factor equal to the square root of its Variance Inflation Factor&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So, if the VIF of a predictor is, let&amp;rsquo;s say, 16, then this means that, in a linear model, multicollinearity is inflating the width of its confidence interval by a factor of 4.&lt;/p&gt;
&lt;p&gt;In case you don&amp;rsquo;t want to take my word for it, here goes a demonstration. Now we fit the model &lt;code&gt;y ~ a + b + c + d&lt;/code&gt;, where &lt;code&gt;a&lt;/code&gt; has a vif of 16.1606674. If we follow the definition above, we could now expect an inflation of the confidence interval for &lt;code&gt;a&lt;/code&gt; of about 4.0200333. Let&amp;rsquo;s find out if that&amp;rsquo;s the case!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#model y against all predictors and get summary
yabcd_model &amp;lt;- lm(
  formula = y ~ a + b + c + d,
  data = toy
) |&amp;gt; 
  summary()

#compute confidence interval of a
a_ci &amp;lt;- ci(
  b = yabcd_model$coefficients[&amp;quot;a&amp;quot;, &amp;quot;Estimate&amp;quot;], 
  se = yabcd_model$coefficients[&amp;quot;a&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )

#compute width of confidence interval of a
new_width &amp;lt;- diff(a_ci)
new_width
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1044793
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to find out the inflation factor of this new confidence interval, we divide it by the width of the old one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;new_width/old_width
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.016101
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the result is VERY CLOSE to the square root of the VIF of &lt;code&gt;a&lt;/code&gt; (4.0200333) in this model. &lt;strong&gt;Notice that this works because in the model &lt;code&gt;y ~ a + b&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt; has a perfect VIF of 1.0022527. This demonstration needs a model with a quasi-perfect VIF as reference.&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now we can confirm our experiment about the meaning of VIF by repeating the exercise with &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;First we compute the VIF of &lt;code&gt;b&lt;/code&gt; against &lt;code&gt;a&lt;/code&gt; alone, and against &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;, and the expected level of inflation of the confidence interval as the square root of the second VIF.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#vif of b vs a
ba_vif &amp;lt;- collinear::vif_df(
  df = toy[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]
) |&amp;gt; 
  dplyr::filter(predictor == &amp;quot;b&amp;quot;)

#vif of b vs a c d
bacd_vif &amp;lt;- collinear::vif_df(
  df = toy[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]
) |&amp;gt; 
  dplyr::filter(predictor == &amp;quot;b&amp;quot;)

#expeced inflation of the confidence interval
sqrt(bacd_vif$vif)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.015515
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, since &lt;code&gt;b&lt;/code&gt; is already in the models &lt;code&gt;y ~ a + b&lt;/code&gt; and &lt;code&gt;y ~ a + b + c + d&lt;/code&gt;, we just need to extract its coefficients, compute their confidence intervals, and divide one by the other to obtain the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#compute confidence interval of b in y ~ a + b
b_ci_old &amp;lt;- ci(
  b = yab_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;], 
  se = yab_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )

#compute confidence interval of b in y ~ a + b + c + d
b_ci_new &amp;lt;- ci(
  b = yabcd_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;], 
  se = yabcd_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )

#compute inflation
diff(b_ci_new)/diff(b_ci_old)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.013543
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, the square root of the VIF of &lt;code&gt;b&lt;/code&gt; in &lt;code&gt;y ~ a + b + c + d&lt;/code&gt; is a great indicator of how much the confidence interval of &lt;code&gt;b&lt;/code&gt; is inflated by multicollinearity in the model.&lt;/p&gt;
&lt;p&gt;And that, folks, is the meaning of VIF.&lt;/p&gt;
&lt;h1 id=&#34;when-the-vif-hurts&#34;&gt;When the VIF Hurts&lt;/h1&gt;
&lt;p&gt;In the previous sections we acquired an intuition of how Variance Inflation Factors measure the effect of multicollinearity in the precision of the coefficient estimates in a linear model. But there is more to that!&lt;/p&gt;
&lt;p&gt;A coefficient estimate divided by its standard error results in the &lt;strong&gt;T statistic&lt;/strong&gt;. This number is named &amp;ldquo;t value&amp;rdquo; in the table of coefficients shown below, and represents the distance (in number of standard errors) between the estimate and zero.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yabcd_model$coefficients[-1, ] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Estimate Std. Error t value Pr(&amp;gt;|t|)
## a   0.7184     0.0267 26.9552   0.0000
## b   0.2596     0.0134 19.4253   0.0000
## c   0.0273     0.0232  1.1757   0.2398
## d   0.0039     0.0230  0.1693   0.8656
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;p-value&lt;/strong&gt;, named &amp;ldquo;Pr(&amp;gt;|t|)&amp;rdquo; above, is the probability of getting the T statistic when there is &lt;em&gt;no effect of the predictor over the response&lt;/em&gt;. The part in italics is named the &lt;em&gt;null hypothesis&lt;/em&gt; (H0), and happens when the confidence interval of the estimate intersects with zero, as in &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ci(
  b = yabcd_model$coefficients[&amp;quot;c&amp;quot;, &amp;quot;Estimate&amp;quot;],
  se = yabcd_model$coefficients[&amp;quot;c&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.01819994  0.07276692
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ci(
  b = yabcd_model$coefficients[&amp;quot;d&amp;quot;, &amp;quot;Estimate&amp;quot;],
  se = yabcd_model$coefficients[&amp;quot;d&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.04111146  0.04888457
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value of any predictor in the coefficients table above is computed as:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#predictor
predictor &amp;lt;- &amp;quot;d&amp;quot;

#number of cases
n &amp;lt;- nrow(toy)

#number of model terms
p &amp;lt;- nrow(yabcd_model$coefficients)

#one-tailed p-value
#q = absolute t-value
#df = degrees of freedom
p_value_one_tailed &amp;lt;- stats::pt(
  q = abs(yabcd_model$coefficients[predictor, &amp;quot;t value&amp;quot;]), 
  df = n - p #degrees of freedom
  )

#two-tailed p-value
2 * (1 - p_value_one_tailed)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8655869
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This p-value is then compared to a &lt;strong&gt;significance level&lt;/strong&gt; (for example, 0.05 for a 95% confidence), which is just the lowest p-value acceptable as strong evidence to make a claim:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;p-value &amp;gt; significance&lt;/strong&gt;: Evidence to claim that the predictor has no effect on the response. If the claim is wrong (we&amp;rsquo;ll see whey we could be wrong), we fall into a &lt;em&gt;false negative&lt;/em&gt; (also &lt;em&gt;Type II Error&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;p-value &amp;lt;= significance&lt;/strong&gt;: Evidence to claim that the predictor has an effect on the response. If the claim is wrong, we fall into a &lt;em&gt;false positive&lt;/em&gt; (also &lt;em&gt;Type I Error&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, how does all this matter when talking about the Variance Inflation Factor? Because a high VIF triggers a cascade of effects that increases p-values that can mess up your claims about the importance of the predictors!&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    â VIF âº â Std. Error âº â T statistic  âº â p-value  âº â  false negatives (Type II Error)
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;This cascade becomes a problem when the predictor has a small effect on the response, and the number of cases is small.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see how this works with &lt;code&gt;b&lt;/code&gt;. This predictor has a solid effect on the response &lt;code&gt;y&lt;/code&gt; (nonetheless, &lt;code&gt;y&lt;/code&gt; was created as &lt;code&gt;a * 0.75 + b * 0.25 + noise&lt;/code&gt;). It has a coefficient around 0.25, and a p-value of 0, so there is little to no risk of falling into a false negative when claiming that it is important to explain &lt;code&gt;y&lt;/code&gt;, even when its confidence interval is inflated by a factor of two in the full model.&lt;/p&gt;
&lt;p&gt;But let&amp;rsquo;s try a little experiment. We are going to create many small versions of &lt;code&gt;toy&lt;/code&gt;, using only 30 cases selected by chance over a number of iterations, we are going to fit models in which &lt;code&gt;b&lt;/code&gt; has a lower and a higher VIF, to monitor its p-values and estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#number of repetitions
repetitions &amp;lt;- 1000

#number of cases to subset in toy
sample_size &amp;lt;- 30

#vectors to store results
lowvif_p_value &amp;lt;- 
  highvif_p_value &amp;lt;- 
  lowvif_estimate &amp;lt;-
  highvif_estimate &amp;lt;- 
  vector(length = repetitions)

#repetitions
for(i in 1:repetitions){
  
  #seed to make randomization reproducible
  set.seed(i)
  
  #toy subset
  toy.i &amp;lt;- toy[sample(x = 1:nrow(toy), size = sample_size), ]
  
  #high vif model
  highvif_model &amp;lt;- lm(
    formula =  y ~ a + b + c + d,
    data = toy.i
  ) |&amp;gt; 
    summary()
  
  #gather results of high vif model
  highvif_p_value[i] &amp;lt;- highvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
  highvif_estimate[i] &amp;lt;- highvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;]
  
  #low_vif_model
  lowvif_model &amp;lt;- lm(
    formula =  y ~ a + b,
    data = toy.i
  ) |&amp;gt; 
    summary()
  
  #gather results of lowvif
  lowvif_p_value[i] &amp;lt;- lowvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
  lowvif_estimate[i] &amp;lt;- lowvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;]
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below shows all p-values of the predictor &lt;code&gt;b&lt;/code&gt; for the high and low VIF models across the experiment repetitions.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/variance-inflation-factor/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;At a significance level of 0.05, the &lt;strong&gt;high VIF&lt;/strong&gt; model rejects &lt;code&gt;b&lt;/code&gt; as an important predictor of &lt;code&gt;y&lt;/code&gt; on 53.5% of the model repetitions, while the &lt;strong&gt;low VIf model&lt;/strong&gt; does the same on 2.2% of repetitions. This is a clear case of increase in Type II Error (false negatives) under multicollinearity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Under multicollinearity, the probability of overlooking predictors with moderate effects increases dramatically!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The plot below shifts the focus towards the coefficient estimates for &lt;code&gt;b&lt;/code&gt; across repetitions.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/variance-inflation-factor/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;The gray vertical line represents the real value of the slope of &lt;code&gt;b&lt;/code&gt;, and each dot represents a model repetition. The coefficients of the &lt;strong&gt;high VIF&lt;/strong&gt; model are all over the place when compared to the &lt;strong&gt;low VIF&lt;/strong&gt; one. Probably you have read somewhere that &amp;ldquo;multicollinearity induces model instability&amp;rdquo;, or something similar, and that is exactly what we are seeing here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finding the true effect of a predictor with a moderate effect becomes harder under multicollinearity.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;managing-vif-in-a-model-design&#34;&gt;Managing VIF in a Model Design&lt;/h1&gt;
&lt;p&gt;The second most common form of modeling self-sabotage is &lt;em&gt;having high VIF predictors in a model design&lt;/em&gt;, just right after &lt;em&gt;throwing deep learning at tabular problems to see what sticks&lt;/em&gt;. I don&amp;rsquo;t have solutions for the deep learning issue, but I have some pointers for the VIFs one: &lt;strong&gt;letting things go!&lt;/strong&gt;. And with &lt;em&gt;things&lt;/em&gt; I mean &lt;em&gt;predictors&lt;/em&gt;, not the pictures of your old love. There is no rule &lt;em&gt;the more predictors the better&lt;/em&gt; rule written anywhere relevant, and letting your model shed some fat is the best way to go here.&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt; package has something to help here. The function 
&lt;a href=&#34;https://blasbenito.github.io/collinear/reference/vif_select.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear::vif_select()&lt;/code&gt;&lt;/a&gt; is specifically designed to help reduce VIF in a model design. And it can do it in two ways: either using domain knowledge to guide the process, or applying quantitative criteria instead.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s follow the domain knowledge route first. Imagine you know a lot about &lt;code&gt;y&lt;/code&gt;, you have read that &lt;code&gt;a&lt;/code&gt; is very important to explain it, and you need to discuss this predictor in your results. But you are on the fence about the other predictors, so you don&amp;rsquo;t really care about what others are in the design. You can express such an idea using the argument &lt;code&gt;preference_order&lt;/code&gt;, as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = &amp;quot;a&amp;quot;,
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you have it, your new model design with a VIF below 2.5 is now &lt;code&gt;y ~ a + b&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;But what if you get new information and it turns out that &lt;code&gt;d&lt;/code&gt; is also a variable of interest? Then you should just modify &lt;code&gt;preference_order&lt;/code&gt; to include this new information.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = c(&amp;quot;a&amp;quot;, &amp;quot;d&amp;quot;),
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;d&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that if your favorite variables are highly correlated, some of them are going to be removed anyway. For example, if &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; are your faves, since they are highly correlated, &lt;code&gt;c&lt;/code&gt; is removed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = c(&amp;quot;a&amp;quot;, &amp;quot;c&amp;quot;),
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In either case, you can now build your model while being sure that the coefficients of these predictors are going to be stable and precise.&lt;/p&gt;
&lt;p&gt;Now, what if &lt;code&gt;y&lt;/code&gt; is totally new for you, and you have no idea about what to use? In this case, the function 
&lt;a href=&#34;https://blasbenito.github.io/collinear/reference/preference_order.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear::preference_order()&lt;/code&gt;&lt;/a&gt; helps you rank the predictors following a quantiative criteria, and after that, &lt;code&gt;collinear::vif_select()&lt;/code&gt; can use it to reduce your VIFs.&lt;/p&gt;
&lt;p&gt;By default, &lt;code&gt;collinear::preference_order()&lt;/code&gt; calls 
&lt;a href=&#34;https://blasbenito.github.io/collinear/reference/f_rsquared.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear::f_rsquared()&lt;/code&gt;&lt;/a&gt; to compute the R-squared between each predictor and the response variable (that&amp;rsquo;s why the argument &lt;code&gt;response&lt;/code&gt; is required here), to return a data frame with the variables ranked from &amp;ldquo;better&amp;rdquo; to &amp;ldquo;worse&amp;rdquo;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;preference &amp;lt;- collinear::preference_order(
  df = toy,
  response = &amp;quot;y&amp;quot;,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  f = collinear::f_r2_pearson,
  quiet = TRUE
)

preference
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   response predictor                       f preference
## 1        y         a collinear::f_r2_pearson 0.77600503
## 2        y         c collinear::f_r2_pearson 0.72364944
## 3        y         d collinear::f_r2_pearson 0.59345954
## 4        y         b collinear::f_r2_pearson 0.07343563
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can use this data frame as input for the argument &lt;code&gt;preference_order&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = preference,
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;d&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now at least you can be sure that the predictors in your model design have low VIF, and were selected taking their correlation with the response as criteria.&lt;/p&gt;
&lt;p&gt;Well, I think that&amp;rsquo;s enough for today. I hope you found this post helpful. Have a great time!&lt;/p&gt;
&lt;p&gt;Blas&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multicollinearity Hinders Model Interpretability</title>
      <link>https://blasbenito.com/post/multicollinearity-model-interpretability/</link>
      <pubDate>Sun, 29 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/multicollinearity-model-interpretability/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This post is written for beginner to intermediate R users wishing to learn what multicollinearity is and how it can turn model interpretation into a challenge.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post, I delve into the intricacies of model interpretation under the influence of multicollinearity, and use R and a toy data set to demonstrate how this phenomenon impacts both linear and machine learning models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The section &lt;em&gt;Multicollinearity Explained&lt;/em&gt; explains the origin of the word and the nature of the problem.&lt;/li&gt;
&lt;li&gt;The section &lt;em&gt;Model Interpretation Challenges&lt;/em&gt; describes how to create the toy data set, and applies it to &lt;em&gt;Linear Models&lt;/em&gt; and &lt;em&gt;Random Forest&lt;/em&gt; to explain how multicollinearity can make model interpretation a challenge.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;Appendix&lt;/em&gt; shows extra examples of linear and machine learning models affected by multicollinearity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I hope you&amp;rsquo;ll enjoy it!&lt;/p&gt;
&lt;h1 id=&#34;r-packages&#34;&gt;R packages&lt;/h1&gt;
&lt;p&gt;This tutorial requires the newly released R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;, and a few more listed below. The optional ones are used only in the &lt;em&gt;Appendix&lt;/em&gt; at the end of the post.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#required
install.packages(&amp;quot;collinear&amp;quot;)
install.packages(&amp;quot;ranger&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)

#optional
install.packages(&amp;quot;nlme&amp;quot;)
install.packages(&amp;quot;glmnet&amp;quot;)
install.packages(&amp;quot;xgboost&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;multicollinearity-explained&#34;&gt;Multicollinearity Explained&lt;/h1&gt;
&lt;p&gt;This cute word comes from the amalgamation of these three Latin terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;multus&lt;/em&gt;: adjective meaning &lt;em&gt;many&lt;/em&gt; or &lt;em&gt;multiple&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;con&lt;/em&gt;: preposition often converted to &lt;em&gt;co-&lt;/em&gt; (as in &lt;em&gt;co-worker&lt;/em&gt;) meaning &lt;em&gt;together&lt;/em&gt; or &lt;em&gt;mutually&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;linealis&lt;/em&gt; (later converted to &lt;em&gt;linearis&lt;/em&gt;): from &lt;em&gt;linea&lt;/em&gt; (line), adjective meaning &amp;ldquo;resembling a line&amp;rdquo; or &amp;ldquo;belonging to a line&amp;rdquo;, among others.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After looking at these serious words, we can come up with a (VERY) liberal translation: &amp;ldquo;several things together in the same line&amp;rdquo;. From here, we just have to replace the word &amp;ldquo;things&amp;rdquo; with &amp;ldquo;predictors&amp;rdquo; (or &amp;ldquo;features&amp;rdquo;, or &amp;ldquo;independent variables&amp;rdquo;, whatever rocks your boat) to build an intuition of the whole meaning of the word in the context of statistical and machine learning modeling.&lt;/p&gt;
&lt;p&gt;If I lost you there, we can move forward with this idea instead: &lt;strong&gt;multicollinearity happens when there are redundant predictors in a modeling dataset&lt;/strong&gt;. A predictor can be redundant because it shows a high pairwise correlation with other predictors, or because it is a linear combination of other predictors. For example, in a data frame with the columns &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt;, if the correlation between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; is high, we can say that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are mutually redundant and there is multicollinearity. But also, if &lt;code&gt;c&lt;/code&gt; is the result of a linear operation between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, like &lt;code&gt;c &amp;lt;- a + b&lt;/code&gt;, or &lt;code&gt;c &amp;lt;- a * 1 + b * 0.5&lt;/code&gt;, then we can also say that there is multicollinearity between &lt;code&gt;c&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt;, and &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Multicollinearity is a fact of life that lurks in most data sets. For example, in climate data, variables like temperature, humidity and air pressure are closely intertwined, leading to multicollinearity. That&amp;rsquo;s the case as well in medical research, where parameters like blood pressure, heart rate, and body mass index frequently display common patterns. Economic analysis is another good example, as variables such as Gross Domestic Product (GDP), unemployment rate, and consumer spending often exhibit multicollinearity.&lt;/p&gt;
&lt;h1 id=&#34;model-interpretation-challenges&#34;&gt;Model Interpretation Challenges&lt;/h1&gt;
&lt;p&gt;Multicollinearity isn&amp;rsquo;t inherently problematic, but it can be a real buzz kill when the goal is interpreting predictor importance in explanatory models. In the presence of highly correlated predictors, most modelling methods, from the veteran linear models to the fancy gradient boosting, attribute a large part of the importance to only one of the predictors and not the others. In such cases, neglecting multicollinearity will certainly lead to underestimate the relevance of certain predictors.&lt;/p&gt;
&lt;p&gt;Let me go ahead and develop a toy data set to showcase this issue. But let&amp;rsquo;s load the required libraries first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#load the collinear package and its example data
library(collinear)
data(vi)

#other required libraries
library(ranger)
library(dplyr)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;vi&lt;/code&gt; data frame shipped with the 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt; package, the variables &amp;ldquo;soil_clay&amp;rdquo; and &amp;ldquo;humidity_range&amp;rdquo; are not correlated at all (Pearson correlation = -0.06).&lt;/p&gt;
&lt;p&gt;In the code block below, the &lt;code&gt;dplyr::transmute()&lt;/code&gt; command selects and renames them as &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. After that, the two variables are scaled and centered, and &lt;code&gt;dplyr::mutate()&lt;/code&gt; generates a few new columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: response variable resulting from a linear model where &lt;code&gt;a&lt;/code&gt; has a slope of 0.75, &lt;code&gt;b&lt;/code&gt; has a slope of 0.25, plus a bit of white noise generated with &lt;code&gt;runif()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: a new predictor highly correlated with &lt;code&gt;a&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d&lt;/code&gt;: a new predictor resulting from a linear combination of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(1)
df &amp;lt;- vi |&amp;gt;
  dplyr::slice_sample(n = 2000) |&amp;gt;
  dplyr::transmute(
    a = soil_clay,
    b = humidity_range
  ) |&amp;gt;
  scale() |&amp;gt;
  as.data.frame() |&amp;gt; 
  dplyr::mutate(
    y = a * 0.75 + b * 0.25 + runif(n = dplyr::n(), min = -0.5, max = 0.5),
    c = a + runif(n = dplyr::n(), min = -0.5, max = 0.5),
    d = (a + b)/2 + runif(n = dplyr::n(), min = -0.5, max = 0.5)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Pearson correlation between all pairs of these predictors is shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::cor_df(
  df = df,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x y correlation
## 1 c a  0.96154984
## 2 d b  0.63903887
## 3 d a  0.63575882
## 4 d c  0.61480312
## 5 b a -0.04740881
## 6 c b -0.04218308
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, we have are two groups of predictors useful to understand how multicollinearity muddles model interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictors with &lt;strong&gt;no&lt;/strong&gt; multicollinearity: &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Predictors with multicollinearity: &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the next two sections and the &lt;em&gt;Appendix&lt;/em&gt;, I show how and why model interpretation becomes challenging when multicollinearity is high. Let&amp;rsquo;s start with linear models.&lt;/p&gt;
&lt;h3 id=&#34;linear-models&#34;&gt;Linear Models&lt;/h3&gt;
&lt;p&gt;The code below fits &lt;em&gt;multiple linear regression models&lt;/em&gt; for both groups of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#non-collinear predictors
lm_ab &amp;lt;- lm(
  formula = y ~ a + b,
  data = df
  )

#collinear predictors
lm_abcd &amp;lt;- lm(
  formula = y ~ a + b + c + d,
  data = df
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I would like you to pay attention to the estimates of the predictors &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; for both models. The estimates are the slopes in the linear model, a direct indication of the effect of a predictor over the response.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coefficients(lm_ab)[2:3] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coefficients(lm_abcd)[2:5] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On one hand, the model with no multicollinearity (&lt;code&gt;lm_ab&lt;/code&gt;) achieved a pretty good solution for the coefficients of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. Remember that we created &lt;code&gt;y&lt;/code&gt; as &lt;code&gt;a * 0.75 + b * 0.25&lt;/code&gt; plus some noise, and that&amp;rsquo;s exactly what the model is telling us here, so the interpretation is pretty straightforward.&lt;/p&gt;
&lt;p&gt;On the other hand, the model with multicollinearity (&lt;code&gt;lm_abcd&lt;/code&gt;) did well with &lt;code&gt;b&lt;/code&gt;, but there are a few things in there that make the interpretation harder.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The coefficient of &lt;code&gt;a&lt;/code&gt; (0.7165) is slightly smaller than the true one (0.75), which could lead us to downplay its relationship with &lt;code&gt;y&lt;/code&gt; by a tiny bit. This is kinda OK though, as long as one is not using the model&amp;rsquo;s results to build nukes in the basement.&lt;/li&gt;
&lt;li&gt;The coefficient of &lt;code&gt;c&lt;/code&gt; is so small that it could led us to believe that this predictor not important at all to explain &lt;code&gt;y&lt;/code&gt;. But we know that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; are almost identical copies, so model interpretation here is being definitely muddled by multicollinearity.&lt;/li&gt;
&lt;li&gt;The coefficient of &lt;code&gt;d&lt;/code&gt; is tiny. Since &lt;code&gt;d&lt;/code&gt; results from the sum of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, we could expect this predictor to be important in explaining &lt;code&gt;y&lt;/code&gt;, but it got the shorter end of the stick in this case.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is not that the model it&amp;rsquo;s wrong though. This behavior of the linear model results from the &lt;em&gt;QR decomposition&lt;/em&gt; (also &lt;em&gt;QR factorization&lt;/em&gt;) applied by functions like &lt;code&gt;lm()&lt;/code&gt;, &lt;code&gt;glm()&lt;/code&gt;, &lt;code&gt;glmnet::glmnet()&lt;/code&gt;, and &lt;code&gt;nlme::gls()&lt;/code&gt; to improve numerical stability and computational efficiency, and to&amp;hellip; address multicollinearity in the model predictors.&lt;/p&gt;
&lt;p&gt;The QR decomposition transforms the original predictors into a set of orthogonal predictors with no multicollinearity. This is the &lt;em&gt;Q matrix&lt;/em&gt;, created in a fashion that resembles the way in which a Principal Components Analysis generates uncorrelated components from a set of correlated variables.&lt;/p&gt;
&lt;p&gt;The code below applies QR decomposition to our multicollinear predictors, extracts the Q matrix, and shows the correlation between the new versions of &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#predictors names
predictors &amp;lt;- c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)

#QR decomposition of predictors
df.qr &amp;lt;- qr(df[, predictors])

#extract Q matrix
df.q &amp;lt;- qr.Q(df.qr)
colnames(df.q) &amp;lt;- predictors

#correlation between transformed predictors
collinear::cor_df(df = df.q)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x y   correlation
## 1 d c  4.823037e-04
## 2 c b -5.439825e-17
## 3 d b  3.896078e-17
## 4 d a  3.837016e-17
## 5 c a -3.786884e-17
## 6 b a -1.098207e-17
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new set of predictors we are left with after the QR decomposition have exactly zero correlation! And now they are not our original predictors anymore, and have a different interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt; is now &amp;ldquo;the part of &lt;code&gt;a&lt;/code&gt; not in &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt; is now &amp;ldquo;the part of &lt;code&gt;b&lt;/code&gt; not in &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&amp;hellip;and so on&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The result of the QR decomposition can be plugged into the &lt;code&gt;solve()&lt;/code&gt; function along with the response vector to estimate the coefficients of the linear model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solve(a = df.qr, b = df$y) |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7189 0.2595 0.0268 0.0040
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are almost exactly the ones we got for our model with multicollinearity. In the end, the coefficients resulting from a linear model are not those of the original predictors, but the ones of their uncorrelated versions generated by the QR decomposition.&lt;/p&gt;
&lt;p&gt;But this is not the only issue of model interpretability under multicollinearity. Let&amp;rsquo;s take a look at the standard errors of the estimates. These are a measure of the coefficient estimation uncertainty, and are used to compute the p-values of the estimates. As such, they are directly linked with the &amp;ldquo;statistical significance&amp;rdquo; (whatever that means) of the predictors within the model.&lt;/p&gt;
&lt;p&gt;The code below shows the standard errors of the model without and with multicollinearity.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(lm_ab)$coefficients[, &amp;quot;Std. Error&amp;quot;][2:3] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.0066 0.0066
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(lm_abcd)$coefficients[, &amp;quot;Std. Error&amp;quot;][2:5] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.0267 0.0134 0.0232 0.0230
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These standard errors of the model with multicollinearity are an order of magnitude higher than the ones of the model without multicollinearity.&lt;/p&gt;
&lt;p&gt;Since our toy dataset is relatively large (2000 cases) and the relationship between the response and a few of the predictors pretty robust, there are no real issues arising, as these differences in estimation precision are not enough to change the p-values of the estimates. However, in a small data set with high multicollinearity and a weaker relationship between the response and the predictors, standard errors of the estimate become wide, which increases p-values and reduces &amp;ldquo;significance&amp;rdquo;. Such a situation might lead us to believe that a predictor does not explain the response, when in fact it does. And this, again, is a model interpretability issue caused by multicollinearity.&lt;/p&gt;
&lt;p&gt;At the end of this post there is an appendix with code examples of other types of linear models that use QR decomposition and become challenging to interpret in the presence of multicollinearity. Play with them as you please!&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s take a look at how multicollinearity can also mess up the interpretation of a commonly used machine learning algorithm.&lt;/p&gt;
&lt;h3 id=&#34;random-forest&#34;&gt;Random Forest&lt;/h3&gt;
&lt;p&gt;It is not uncommon to hear something like &amp;ldquo;random forest is insensitive to multicollinearity&amp;rdquo;. Actually, I cannot confirm nor deny that I have said that before. Anyway, it is kind of true if one is focused on prediction problmes. However, when the aim is interpreting predictor importance scores, then one has to be mindful about multicollinearity as well.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see an example. The code below fits two random forest models with our two sets of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#non-collinear predictors
rf_ab &amp;lt;- ranger::ranger(
  formula = y ~ a + b,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1 #for reproducibility
)

#collinear predictors
rf_abcd &amp;lt;- ranger::ranger(
  formula = y ~ a + b + c + d,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at the prediction error the two models on the out-of-bag data. While building each regression tree, Random Forest leaves a random subset of the data out. Then, each case gets a prediction from all trees that had it in the out-of-bag data, and the prediction error is averaged across all cases to get the numbers below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_ab$prediction.error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1026779
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abcd$prediction.error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1035678
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to these numbers, these two models are basically equivalent in their ability to predict our response &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But now, you noticed that I set the argument &lt;code&gt;importance&lt;/code&gt; to &amp;ldquo;permutation&amp;rdquo;. Permutation importance quantifies how the out-of-bag error increases when a predictor is permuted across all trees where the predictor is used. It is pretty robust importance metric that bears no resemblance whatsoever with the coefficients of a linear model. Think of it as a very different way to answer the question &amp;ldquo;what variables are important in this model?&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The permutation importance scores of the two random forest models are show below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_ab$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 1.0702 0.1322
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abcd$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.5019 0.0561 0.1662 0.0815
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is one interesting detail here. The predictor &lt;code&gt;a&lt;/code&gt; has a permutation error three times higher than &lt;code&gt;c&lt;/code&gt; in the second model, even though we could expect them to be similar due to their very high correlation. There are two reasons for this mismatch:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Random Forest is much more sensitive to the white noise in &lt;code&gt;c&lt;/code&gt; than linear models, especially in the deep parts of the regression trees, due to local (within-split data) decoupling with the response &lt;code&gt;y&lt;/code&gt;. In consequence, it does not get selected as often as &lt;code&gt;a&lt;/code&gt; in these deeper areas of the trees, and has less overall importance.&lt;/li&gt;
&lt;li&gt;The predictor &lt;code&gt;c&lt;/code&gt; competes with &lt;code&gt;d&lt;/code&gt;, that has around 50% of the information in &lt;code&gt;c&lt;/code&gt; (and &lt;code&gt;a&lt;/code&gt;). If we remove &lt;code&gt;d&lt;/code&gt; from the model, then the permutation importance of &lt;code&gt;c&lt;/code&gt; doubles up. Then, with &lt;code&gt;d&lt;/code&gt; in the model, we underestimate the real importance of &lt;code&gt;c&lt;/code&gt; due to multicollinearity alone.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abc &amp;lt;- ranger::ranger(
  formula = y ~ a + b + c,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1
)
rf_abc$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c 
## 0.5037 0.1234 0.3133
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With all that in mind, we can conclude that interpreting importance scores in Random Forest models is challenging when multicollinearity is high. But Random Forest is not the only machine learning affected by this issue. In the Appendix below I have left an example with Extreme Gradient Boosting so you can play with it.&lt;/p&gt;
&lt;p&gt;And that&amp;rsquo;s all for now, folks, I hope you found this post useful!&lt;/p&gt;
&lt;h1 id=&#34;appendix&#34;&gt;Appendix&lt;/h1&gt;
&lt;p&gt;This section shows several extra examples of linear and machine learning models you can play with.&lt;/p&gt;
&lt;h2 id=&#34;other-linear-models-using-qr-decomposition&#34;&gt;Other linear models using QR decomposition&lt;/h2&gt;
&lt;p&gt;As I commented above, many linear modeling functions use QR decomposition, and you will have to be careful interpreting model coefficients in the presence of strong multicollinearity in the predictors.&lt;/p&gt;
&lt;p&gt;Here I show several examples with &lt;code&gt;glm()&lt;/code&gt; (Generalized Linear Models), &lt;code&gt;nlme::gls()&lt;/code&gt; (Generalized Least Squares), and &lt;code&gt;glmnet::cv.glmnet()&lt;/code&gt; (Elastic Net Regularization). In all them, no matter how fancy, the interpretation of coefficients becomes tricky when multicollinearity is high.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generalized Linear Models with glm()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Generalized Linear Models
#non-collinear predictors
glm_ab &amp;lt;- glm(
  formula = y ~ a + b,
  data = df
  )

round(coefficients(glm_ab), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
glm_abcd &amp;lt;- glm(
  formula = y ~ a + b + c + d,
  data = df
  )

round(coefficients(glm_abcd), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Generalized Least Squares with nlme::gls()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(nlme)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;nlme&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:dplyr&#39;:
## 
##     collapse
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Generalized Least Squares
#non-collinear predictors
gls_ab &amp;lt;- nlme::gls(
  model = y ~ a + b,
  data = df
  )

round(coefficients(gls_ab), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
gls_abcd &amp;lt;- nlme::gls(
  model = y ~ a + b + c + d,
  data = df
  )

round(coefficients(gls_abcd), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Elastic Net Regularization and Lasso penalty with glmnet::glmnet()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(glmnet)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loaded glmnet 4.1-8
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Elastic net regularization with Lasso penalty
#non-collinear predictors
glmnet_ab &amp;lt;- glmnet::cv.glmnet(
  x = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]),
  y = df$y,
  alpha = 1 #lasso penalty
)

round(coef(glmnet_ab$glmnet.fit, s = glmnet_ab$lambda.min), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7438 0.2578
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
glmnet_abcd &amp;lt;- glmnet::cv.glmnet(
  x = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  y = df$y,
  alpha = 1 
)

#notice that the lasso regularization nuked the coefficients of predictors b and c
round(coef(glmnet_abcd$glmnet.fit, s = glmnet_abcd$lambda.min), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7101 0.2507 0.0267 0.0149
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;extreme-gradient-boosting-under-multicollinearity&#34;&gt;Extreme Gradient Boosting under multicollinearity&lt;/h2&gt;
&lt;p&gt;Gradient Boosting models trained with multicollinear predictors behave in a way similar to linear models with QR decomposition. When two variables are highly correlated, one of them is going to have an importance much higher than the other.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(xgboost)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;xgboost&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:dplyr&#39;:
## 
##     slice
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#without multicollinearity
gb_ab &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
  )

#with multicollinearity
gb_abcd &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xgb.importance(model = gb_ab)[, c(1:2)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature      Gain
##     &amp;lt;char&amp;gt;     &amp;lt;num&amp;gt;
## 1:       a 0.8463005
## 2:       b 0.1536995
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xgb.importance(model = gb_abcd)[, c(1:2)] |&amp;gt; 
  dplyr::arrange(Feature)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature       Gain
##     &amp;lt;char&amp;gt;      &amp;lt;num&amp;gt;
## 1:       a 0.78129661
## 2:       b 0.07386393
## 3:       c 0.03595619
## 4:       d 0.10888327
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But there is a twist too. When two variables are perfectly correlated, one of them is removed right away from the model!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#replace c with perfect copy of a
df$c &amp;lt;- df$a

#with multicollinearity
gb_abcd &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
)

xgb.importance(model = gb_abcd)[, c(1:2)] |&amp;gt; 
  dplyr::arrange(Feature)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature       Gain
##     &amp;lt;char&amp;gt;      &amp;lt;num&amp;gt;
## 1:       a 0.79469959
## 2:       b 0.07857141
## 3:       d 0.12672900
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>The global biogeography and environmental drivers of fairy circles</title>
      <link>https://blasbenito.com/publication/2023_guirado_pnas/</link>
      <pubDate>Tue, 03 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2023_guirado_pnas/</guid>
      <description></description>
    </item>
    
    <item>
      <title>R package spatialRF</title>
      <link>https://blasbenito.com/project/post-title/</link>
      <pubDate>Tue, 26 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/project/post-title/</guid>
      <description>&lt;script src=&#34;https://blasbenito.com/project/post-title/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://blasbenito.com/project/post-title/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://blasbenito.com/project/post-title/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://blasbenito.com/project/post-title/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://blasbenito.com/project/post-title/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://blasbenito.com/project/post-title/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://blasbenito.com/project/post-title/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://blasbenito.com/project/post-title/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://blasbenito.com/project/post-title/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://blasbenito.com/project/post-title/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://blasbenito.com/project/post-title/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://blasbenito.com/project/post-title/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://blasbenito.com/project/post-title/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://blasbenito.com/project/post-title/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://blasbenito.com/project/post-title/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://blasbenito.com/project/post-title/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;!---
[![R-CMD-check](https://github.com/BlasBenito/spatialRF/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/BlasBenito/spatialRF/actions/workflows/R-CMD-check.yaml)
--&gt;
&lt;!-- badges: start --&gt;
&lt;p&gt;
&lt;a href=&#34;https://cran.r-project.org/package=spatialRF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version/spatialRF&#34; alt=&#34;CRAN status&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://CRAN.R-project.org/package=spatialRF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://cranlogs.r-pkg.org/badges/grand-total/spatialRF&#34; alt=&#34;CRAN\_Download\_Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;!-- badges: end --&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The package &lt;strong&gt;spatialRF&lt;/strong&gt; facilitates fitting spatial regression models on regular or irregular data with Random Forest. It does so by generating &lt;em&gt;spatial predictors&lt;/em&gt; that help the model &amp;ldquo;understand&amp;rdquo; the spatial structure of the training data with the end goal of minimizing the spatial autocorrelation of the model residuals and offering honest variable importance scores.&lt;/p&gt;
&lt;p&gt;Two main methods to generate &lt;em&gt;spatial predictors&lt;/em&gt; from the distance matrix of the data points are implemented in the package:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Moran&amp;rsquo;s Eigenvector Maps 
&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0304380006000925&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Dray, Legendre, and Peres-Neto 2006)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Distance matrix columns as explanatory variables 
&lt;a href=&#34;https://peerj.com/articles/5518/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Hengl et al. 2018)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The package is designed to minimize the code required to fit a spatial model from a training dataset, the names of the response and the predictors, and a distance matrix, as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatial.model &amp;lt;- spatialRF::rf_spatial(
  data = your_dataframe,
  dependent.variable.name = &amp;quot;your_response_variable&amp;quot;,
  predictor.variable.names = c(&amp;quot;predictor1&amp;quot;, &amp;quot;predictor2&amp;quot;, ..., &amp;quot;predictorN&amp;quot;),
  distance.matrix = your_distance_matrix
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;spatialRF&lt;/strong&gt; uses the fast and efficient &lt;code&gt;ranger&lt;/code&gt; package under the hood 
&lt;a href=&#34;https://arxiv.org/abs/1508.04409&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Wright and Ziegler 2017)&lt;/a&gt;, so please, cite the &lt;code&gt;ranger&lt;/code&gt; package when using &lt;code&gt;spatialRF&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;This package also provides tools to identify potentially interesting variable interactions, tune random forest hyperparameters, assess model performance on spatially independent data folds, and examine the resulting models via importance plots, response curves, and response surfaces.&lt;/p&gt;
&lt;h1 id=&#34;development&#34;&gt;Development&lt;/h1&gt;
&lt;p&gt;This package is reaching its final form, and big changes are not expected at this stage. However, it has many functions, and even though all them have been tested, only one dataset has been used for those tests. You will find bugs, and something will go wrong almost surely. If you have time to report bugs, please, do so in any of the following ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open a new issue in the 
&lt;a href=&#34;https://github.com/BlasBenito/spatialRF/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Issues GitHub page of the package&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Send me an email explaining the issue and the error messages with enough detail at blasbenito at gmail dot com.&lt;/li&gt;
&lt;li&gt;Send a direct message to 
&lt;a href=&#34;https://twitter.com/blasbenito&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my twitter account&lt;/a&gt; explaining the issue.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will do my best to solve any issues ASAP!&lt;/p&gt;
&lt;h1 id=&#34;applications&#34;&gt;Applications&lt;/h1&gt;
&lt;p&gt;The goal of &lt;code&gt;spatialRF&lt;/code&gt; is to help fitting &lt;em&gt;explanatory spatial regression&lt;/em&gt;, where the target is to understand how a set of predictors and the spatial structure of the data influences response variable. Therefore, the spatial analyses implemented in the package can be applied to any spatial dataset, regular or irregular, with a sample size between ~100 and ~5000 cases (the higher end will depend on the RAM memory available), a quantitative or binary (values 0 and 1) response variable, and a more or less large set of predictive variables.&lt;/p&gt;
&lt;p&gt;All functions but &lt;code&gt;rf_spatial()&lt;/code&gt; work with non-spatial data as well if the arguments &lt;code&gt;distance.matrix&lt;/code&gt; and &lt;code&gt;distance.thresholds&lt;/code&gt; are not provided In such case, the number of training cases is no longer limited by the size of the distance matrix, and models can be trained with hundreds of thousands of rows. In such case, the spatial autocorrelation of the model&amp;rsquo;s residuals is not assessed.&lt;/p&gt;
&lt;p&gt;However, &lt;strong&gt;when the focus is on fitting spatial models&lt;/strong&gt;, and due to the nature of the &lt;em&gt;spatial predictors&lt;/em&gt; used to represent the spatial structure of the training data, &lt;strong&gt;there are many things this package cannot do&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Predict model results over raster data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Predict a model result over another region with a different spatial structure.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Work with &amp;ldquo;big data&amp;rdquo;, whatever that means.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Imputation or extrapolation (it can be done, but models based on spatial predictors are hardly transferable).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Take temporal autocorrelation into account (but this is something that might be implemented later on).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If after considering these limitations you are still interested, follow me, I will show you how it works.&lt;/p&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;p&gt;There is a paper in the making about this package. In the meantime, if you find it useful for your academic work, please cite the &lt;code&gt;ranger&lt;/code&gt; package as well, it is the true core of &lt;code&gt;spatialRF&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Marvin N. Wright, Andreas Ziegler (2017). ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software, 77(1), 1-17. doi:10.18637/jss.v077.i01&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Blas M. Benito (2021). spatialRF: Easy Spatial Regression with Random Forest. R package version 1.1.0. doi: 10.5281/zenodo.4745208. url: 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://blasbenito.github.io/spatialRF/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;install&#34;&gt;Install&lt;/h1&gt;
&lt;p&gt;The version 1.1.3 can be installed from CRAN:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;spatialRF&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The package can also be installed from GitHub as follows. There are several branches in the repository:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;main&lt;/code&gt;: latest stable version (1.1.0 currently).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;development&lt;/code&gt;: development version, usually very broken.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;v.1.0.9&lt;/code&gt; to &lt;code&gt;v.1.1.2&lt;/code&gt;: archived versions.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;remotes::install_github(
  repo = &amp;quot;blasbenito/spatialRF&amp;quot;, 
  ref = &amp;quot;main&amp;quot;,
  force = TRUE,
  quiet = TRUE
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are a few other libraries that will be useful during this tutorial.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(spatialRF)
library(kableExtra)
library(rnaturalearth)
library(rnaturalearthdata)
library(tidyverse)
library(randomForestExplainer)
library(pdp)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;data-requirements&#34;&gt;Data requirements&lt;/h1&gt;
&lt;p&gt;The data required to fit random forest models with &lt;code&gt;spatialRF&lt;/code&gt; must fulfill several conditions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The input format is data.frame&lt;/strong&gt;. At the moment, tibbles are not fully supported.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The number of rows must be somewhere between 100 and ~5000&lt;/strong&gt;, at least if your target is fitting spatial models. This limitation comes from the fact that the distance matrix grows very fast with an increasing number of training records, so for large datasets, there might not be enough RAM in your machine.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The number of predictors should be larger than 3&lt;/strong&gt;. Fitting a Random Forest model is moot otherwise.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Factors in the response or the predictors are not explicitly supported in the package&lt;/strong&gt;. They may work, or they won&amp;rsquo;t, but in any case, I designed this package for quantitative data alone. However, binary responses with values 0 and 1 are partially supported.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Must be free of &lt;code&gt;NA&lt;/code&gt;&lt;/strong&gt;. You can check if there are NA records with &lt;code&gt;sum(apply(df, 2, is.na))&lt;/code&gt;. If the result is larger than 0, then just execute &lt;code&gt;df &amp;lt;- na.omit(df)&lt;/code&gt; to remove rows with empty cells.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Columns cannot have zero variance&lt;/strong&gt;. This condition can be checked with &lt;code&gt;apply(df, 2, var) == 0&lt;/code&gt;. Columns yielding TRUE should be removed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Columns must not yield &lt;code&gt;NaN&lt;/code&gt; or &lt;code&gt;Inf&lt;/code&gt; when scaled&lt;/strong&gt;. You can check each condition with &lt;code&gt;sum(apply(scale(df), 2, is.nan))&lt;/code&gt; and &lt;code&gt;sum(apply(scale(df), 2, is.infinite))&lt;/code&gt;. If higher than 0, you can find what columns are giving issues with &lt;code&gt;sapply(as.data.frame(scale(df)), function(x)any(is.nan(x)))&lt;/code&gt; and &lt;code&gt;sapply(as.data.frame(scale(df)), function(x)any(is.infinite(x)))&lt;/code&gt;. Any column yielding &lt;code&gt;TRUE&lt;/code&gt; will generate issues while trying to fit models with &lt;code&gt;spatialRF&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;example-data&#34;&gt;Example data&lt;/h1&gt;
&lt;p&gt;The package includes an example dataset that fulfills the conditions mentioned above, named 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/plant_richness_df.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;plant_richness_df&lt;/code&gt;&lt;/a&gt;. It is a data frame with plant species richness and predictors for 227 ecoregions in the Americas, and a distance matrix among the ecoregion edges named, well, 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/distance_matrix.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;distance_matrix&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The package follows a convention throughout functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The argument &lt;code&gt;data&lt;/code&gt; requires a training data frame.&lt;/li&gt;
&lt;li&gt;The argument &lt;code&gt;dependent.variable.name&lt;/code&gt; is the column name of the response variable.&lt;/li&gt;
&lt;li&gt;The argument &lt;code&gt;predictor.variable.names&lt;/code&gt; contains the column names of the predictors.&lt;/li&gt;
&lt;li&gt;The argument &lt;code&gt;xy&lt;/code&gt; takes a data frame or matrix with two columns named &amp;ldquo;x&amp;rdquo; and &amp;ldquo;y&amp;rdquo;, in that order, with the case coordinates.&lt;/li&gt;
&lt;li&gt;The argument &lt;code&gt;distance.matrix&lt;/code&gt; requires a matrix of distances between the cases in &lt;code&gt;data&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The argument &lt;code&gt;distance.thresholds&lt;/code&gt; is a numeric vector of distances at with spatial autocorrelation wants to be computed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is convenient to define these arguments at the beginning of the workflow.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#loading training data and distance matrix from the package
data(plant_richness_df)
data(distance_matrix)

#names of the response variable and the predictors
dependent.variable.name &amp;lt;- &amp;quot;richness_species_vascular&amp;quot;
predictor.variable.names &amp;lt;- colnames(plant_richness_df)[5:21]

#coordinates of the cases
xy &amp;lt;- plant_richness_df[, c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;)]

#distance matrix
distance.matrix &amp;lt;- distance_matrix

#distance thresholds (same units as distance_matrix)
distance.thresholds &amp;lt;- c(0, 1000, 2000, 4000, 8000)

#random seed for reproducibility
random.seed &amp;lt;- 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The response variable of &lt;code&gt;plant_richness_df&lt;/code&gt; is &amp;ldquo;richness_species_vascular&amp;rdquo;, that represents the total count of vascular plant species found on each ecoregion. The figure below shows the centroids of each ecoregion along with their associated value of the response variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;world &amp;lt;- rnaturalearth::ne_countries(
  scale = &amp;quot;medium&amp;quot;, 
  returnclass = &amp;quot;sf&amp;quot;
  )

ggplot2::ggplot() +
  ggplot2::geom_sf(
    data = world, 
    fill = &amp;quot;white&amp;quot;
    ) +
  ggplot2::geom_point(
    data = plant_richness_df,
    ggplot2::aes(
      x = x,
      y = y,
      color = richness_species_vascular
    ),
    size = 2.5
  ) +
  ggplot2::scale_color_viridis_c(
    direction = -1, 
    option = &amp;quot;F&amp;quot;
    ) +
  ggplot2::theme_bw() +
  ggplot2::labs(color = &amp;quot;Plant richness&amp;quot;) +
  ggplot2::scale_x_continuous(limits = c(-170, -30)) +
  ggplot2::scale_y_continuous(limits = c(-58, 80))  +
  ggplot2::ggtitle(&amp;quot;Plant richness of the American ecoregions&amp;quot;) + 
  ggplot2::xlab(&amp;quot;Longitude&amp;quot;) + 
  ggplot2::ylab(&amp;quot;Latitude&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;576&#34; /&gt;
&lt;p&gt;The predictors (columns 5 to 21) represent diverse factors that may influence plant richness such as sampling bias, the area of the ecoregion, climatic variables, human presence and impact, topography, geographical fragmentation, and features of the neighbors of each ecoregion. The figure below shows the scatterplots of the response variable (y axis) against each predictor (x axis).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Every plotting function in the package now allows changing the colors of their main features via specific arguments such as &lt;code&gt;point.color&lt;/code&gt;, &lt;code&gt;line.color&lt;/code&gt;, or &lt;code&gt;fill.color&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::plot_training_df(
  data = plant_richness_df,
  dependent.variable.name = dependent.variable.name,
  predictor.variable.names = predictor.variable.names,
  ncol = 3,
  point.color = viridis::viridis(100, option = &amp;quot;F&amp;quot;),
  line.color = &amp;quot;gray30&amp;quot;
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;960&#34; /&gt;
&lt;p&gt;The function 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/plot_training_df_moran.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;plot_training_df_moran()&lt;/code&gt;&lt;/a&gt; helps assessing the spatial autocorrelation of the response variable and the predictors across different distance thresholds. Low Moran&amp;rsquo;s I and p-values equal or larger than 0.05 indicate that there is no spatial autocorrelation for the given variable and distance threshold.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::plot_training_df_moran(
  data = plant_richness_df,
  dependent.variable.name = dependent.variable.name,
  predictor.variable.names = predictor.variable.names,
  distance.matrix = distance.matrix,
  distance.thresholds = distance.thresholds,
  fill.color = viridis::viridis(
    100,
    option = &amp;quot;F&amp;quot;,
    direction = -1
    ),
  point.color = &amp;quot;gray40&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;768&#34; /&gt;
&lt;h1 id=&#34;reducing-multicollinearity-in-the-predictors&#34;&gt;Reducing multicollinearity in the predictors&lt;/h1&gt;
&lt;p&gt;The functions 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/auto_cor.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;auto_cor()&lt;/code&gt;&lt;/a&gt; and 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/auto_vif.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;auto_vif()&lt;/code&gt;&lt;/a&gt; help reduce redundancy in the predictors by using different criteria (bivariate R squared vs. 
&lt;a href=&#34;https://www.statisticshowto.com/variance-inflation-factor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;variance inflation factor&lt;/a&gt;), while allowing the user to define an &lt;em&gt;order of preference&lt;/em&gt;, which can be based either on domain expertise or on a quantitative assessment (e.g., order of preference based on variable importance scores or model coefficients). The preference order is defined as a character vector in the &lt;code&gt;preference.order&lt;/code&gt; argument of both functions, and does not need to include the names of all predictors, but just the ones the user would like to keep in the analysis.&lt;/p&gt;
&lt;p&gt;Notice that I have set &lt;code&gt;cor.threshold&lt;/code&gt; and &lt;code&gt;vif.threshold&lt;/code&gt; to low values because the predictors in &lt;code&gt;plant_richness_df&lt;/code&gt; already have little multicollinearity,. The default values (&lt;code&gt;cor.threshold = 0.75&lt;/code&gt; and &lt;code&gt;vif.threshold = 5&lt;/code&gt;) should work well when combined together for any other set of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;preference.order &amp;lt;- c(
    &amp;quot;climate_bio1_average_X_bias_area_km2&amp;quot;,
    &amp;quot;climate_aridity_index_average&amp;quot;,
    &amp;quot;climate_hypervolume&amp;quot;,
    &amp;quot;climate_bio1_average&amp;quot;,
    &amp;quot;climate_bio15_minimum&amp;quot;,
    &amp;quot;bias_area_km2&amp;quot;
  )

predictor.variable.names &amp;lt;- spatialRF::auto_cor(
  x = plant_richness_df[, predictor.variable.names],
  cor.threshold = 0.6,
  preference.order = preference.order
) %&amp;gt;% 
  spatialRF::auto_vif(
    vif.threshold = 2.5,
    preference.order = preference.order
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [auto_cor()]: Removed variables: human_footprint_average
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [auto_vif()]: Variables are not collinear.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output of &lt;code&gt;auto_cor()&lt;/code&gt; or &lt;code&gt;auto_vif()&lt;/code&gt; has the class &amp;ldquo;variable_selection&amp;rdquo;, which can be used as input in every function having the argument &lt;code&gt;predictor.variable.names&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(predictor.variable.names)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;vif&amp;quot;                   &amp;quot;selected.variables&amp;quot;    &amp;quot;selected.variables.df&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The slot &lt;code&gt;selected.variables&lt;/code&gt; contains the names of the selected predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;predictor.variable.names$selected.variables
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;climate_aridity_index_average&amp;quot;   &amp;quot;climate_hypervolume&amp;quot;            
##  [3] &amp;quot;climate_bio1_average&amp;quot;            &amp;quot;climate_bio15_minimum&amp;quot;          
##  [5] &amp;quot;bias_area_km2&amp;quot;                   &amp;quot;bias_species_per_record&amp;quot;        
##  [7] &amp;quot;climate_velocity_lgm_average&amp;quot;    &amp;quot;neighbors_count&amp;quot;                
##  [9] &amp;quot;neighbors_percent_shared_edge&amp;quot;   &amp;quot;human_population_density&amp;quot;       
## [11] &amp;quot;topography_elevation_average&amp;quot;    &amp;quot;landcover_herbs_percent_average&amp;quot;
## [13] &amp;quot;fragmentation_cohesion&amp;quot;          &amp;quot;fragmentation_division&amp;quot;         
## [15] &amp;quot;neighbors_area&amp;quot;                  &amp;quot;human_population&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;finding-promising-variable-interactions&#34;&gt;Finding promising variable interactions&lt;/h1&gt;
&lt;p&gt;Random Forests already takes into account variable interactions of the form &amp;ldquo;variable &lt;code&gt;a&lt;/code&gt; becomes important when &lt;code&gt;b&lt;/code&gt; is higher than x&amp;rdquo;. However, Random Forest can also take advantage of variable interactions of the form &lt;code&gt;a * b&lt;/code&gt;, across the complete ranges of the predictors, as they are commonly defined in regression models, and &amp;ldquo;interactions&amp;rdquo; (not the proper name, but used here for simplicity) represented by the first component of a PCA on the predictors &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The function 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/the_feature_engineer.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;the_feature_engineer()&lt;/code&gt;&lt;/a&gt; tests all possible interactions of both types among the most important predictors, and suggesting the ones not correlated among themselves and with the other predictors inducing an increase in the model&amp;rsquo;s R squared (or AUC when the response is binary) on independent data via spatial cross-validation (see &lt;code&gt;rf_evaluate()&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;interactions &amp;lt;- spatialRF::the_feature_engineer(
  data = plant_richness_df,
  dependent.variable.name = dependent.variable.name,
  predictor.variable.names = predictor.variable.names,
  xy = xy,
  importance.threshold = 0.50, #uses 50% best predictors
  cor.threshold = 0.60, #max corr between interactions and predictors
  seed = random.seed,
  repetitions = 100,
  verbose = TRUE
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Fitting and evaluating a model without interactions.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Testing 28 candidate interactions.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Interactions identified: 5
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  ââââââââââââââââââââ¬âââââââââââââââââââ¬âââââââââââââââââââ¬âââââââââââââââââââ
##  â Interaction      â Importance (% of â        R-squared â     Max cor with â
##  â                  â             max) â      improvement â       predictors â
##  ââââââââââââââââââââ¼âââââââââââââââââââ¼âââââââââââââââââââ¼âââââââââââââââââââ¤
##  â bias_area_km2..x â             60.8 â            0.096 â            0.60Â  â
##  â ..bias_species_p â                  â                  â                  â
##  â er_record        â                  â                  â                  â
##  ââââââââââââââââââââ¼âââââââââââââââââââ¼âââââââââââââââââââ¼âââââââââââââââââââ¤
##  â climate_bio1_ave â             97.9 â            0.067 â            0.34Â  â
##  â rage..pca..human â                  â                  â                  â
##  â _population_dens â                  â                  â                  â
##  â ity              â                  â                  â                  â
##  ââââââââââââââââââââ¼âââââââââââââââââââ¼âââââââââââââââââââ¼âââââââââââââââââââ¤
##  â climate_bio1_ave â             98.7 â            0.049 â            0.24Â  â
##  â rage..pca..neigh â                  â                  â                  â
##  â bors_count       â                  â                  â                  â
##  ââââââââââââââââââââ¼âââââââââââââââââââ¼âââââââââââââââââââ¼âââââââââââââââââââ¤
##  â human_population â             66.5 â            0.021 â            0.55Â  â
##  â ..x..bias_specie â                  â                  â                  â
##  â s_per_record     â                  â                  â                  â
##  ââââââââââââââââââââ¼âââââââââââââââââââ¼âââââââââââââââââââ¼âââââââââââââââââââ¤
##  â bias_area_km2..p â             62.7 â            0.029 â            0.305 â
##  â ca..neighbors_pe â                  â                  â                  â
##  â rcent_shared_edg â                  â                  â                  â
##  â e                â                  â                  â                  â
##  ââââââââââââââââââââ´âââââââââââââââââââ´âââââââââââââââââââ´âââââââââââââââââââ
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Comparing models with and without interactions via spatial cross-validation.
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;1248&#34; /&gt;
&lt;p&gt;The upper panel in the plot plot above shows the relationship between the interaction and the response variable. It also indicates the gain in R squared (+R2), the importance, in percentage, when used in a model along the other predictors (Imp. (%)), and the maximum Pearson correlation of the interaction with the predictors. The violin-plot shows a comparison of the model with and without the selected interaction made via spatial cross-validation using 100 repetitions (see 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/rf_evaluate.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;rf_evaluate()&lt;/code&gt;&lt;/a&gt; and 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/rf_compare.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;rf_compare()&lt;/code&gt;&lt;/a&gt; for further details).&lt;/p&gt;
&lt;p&gt;The function also returns a data frame with all the interactions considered.  The columns are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;interaction.name&lt;/code&gt;: Interactions computed via multiplication are named &lt;code&gt;a..x..b&lt;/code&gt;, while interactions computed via PCA are named &lt;code&gt;a..pca..b&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;interaction.importance&lt;/code&gt;: Importance of the interaction expressed as a percentage. If &lt;code&gt;interaction.importance == 100&lt;/code&gt;, that means that the interaction is the most important predictor in the model fitted with the interaction and the predictors named in &lt;code&gt;predictor.variable.names&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;interaction.metric.gain&lt;/code&gt;: Difference in R squared (or AUC for models fitting a binary response) between a model with and a model without the interaction.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max.cor.with.predictors&lt;/code&gt;: The maximum Pearson correlation of the interaction with the predictors named in &lt;code&gt;predictor.variable.names&lt;/code&gt;. Gives an idea of the amount of multicollinearity the interaction introduces in the model.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;variable.a.name&lt;/code&gt; and &lt;code&gt;variable.b.name&lt;/code&gt;: Names of the predictors involved in the interaction.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;selected&lt;/code&gt;: &lt;code&gt;TRUE&lt;/code&gt; if the interaction fulfills the selection criteria (importance higher than a threshold, positive gain in R squared or AUC, and Pearson correlation with other predictors lower than a threshold). The selected interactions have a correlation among themselves always lower than the value of the argument &lt;code&gt;cor.threshold&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;kableExtra::kbl(
  head(interactions$screening, 10),
  format = &amp;quot;html&amp;quot;
) %&amp;gt;%
  kableExtra::kable_paper(&amp;quot;hover&amp;quot;, full_width = F)
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34; lightable-paper lightable-hover&#34; style=&#39;color: black; font-family: &#34;Arial Narrow&#34;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;&#39;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; interaction.name &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; interaction.importance &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; interaction.metric.gain &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; max.cor.with.predictors &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; variable.a.name &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; variable.b.name &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; selected &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_area_km2..x..bias_species_per_record &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 60.779 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.096 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.5962899 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_area_km2 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_species_per_record &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 97.944 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.067 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.3369664 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..neighbors_count &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 98.742 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.049 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.2441858 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; neighbors_count &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..neighbors_percent_shared_edge &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 84.196 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.066 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.2215337 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; neighbors_percent_shared_edge &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; human_population..x..bias_species_per_record &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 66.512 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.021 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.5462406 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; human_population &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_species_per_record &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..bias_species_per_record &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 70.082 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.018 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.3469834 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_species_per_record &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_area_km2..pca..neighbors_percent_shared_edge &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 62.678 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.029 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.3046471 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_area_km2 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; neighbors_percent_shared_edge &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_hypervolume..x..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 49.367 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.006 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.5599486 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_hypervolume &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; neighbors_count..pca..neighbors_percent_shared_edge &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 63.808 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.016 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.1584006 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; neighbors_count &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; neighbors_percent_shared_edge &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_area_km2..pca..neighbors_count &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 58.662 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.012 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.2166191 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_area_km2 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; neighbors_count &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The function returns a data frame with the response variables, the predictors, and the selected interactions that can be used right away as a training data frame. However, the function cannot say whether an interaction &lt;em&gt;makes sense&lt;/em&gt;, and it is up to the user to choose wisely whether to select an interaction or not. In this particular case, and just for the sake of simplicity, we will be using the resulting data frame as training data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#adding interaction column to the training data
plant_richness_df &amp;lt;- interactions$data

#adding interaction name to predictor.variable.names
predictor.variable.names &amp;lt;- interactions$predictor.variable.names
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;fitting-a-non-spatial-random-forest-model-with-rf&#34;&gt;Fitting a non-spatial Random Forest model with &lt;code&gt;rf()&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;The function 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/rf.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;rf()&lt;/code&gt;&lt;/a&gt; is a convenient wrapper for &lt;code&gt;ranger::ranger()&lt;/code&gt; used in every modelling function of the &lt;em&gt;spatialRF&lt;/em&gt; package. It takes the training data, the names of the response and the predictors, and optionally (to assess the spatial autocorrelation of the residuals), the distance matrix, and a vector of distance thresholds (in the same units as the distances in &lt;strong&gt;distance_matrix&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;These distance thresholds are the neighborhoods at which the model will check the spatial autocorrelation of the residuals. Their values may depend on the spatial scale of the data, and the ecological system under study.&lt;/p&gt;
&lt;p&gt;Notice that here I plug the object &lt;code&gt;predictor.variable.names&lt;/code&gt;, output of &lt;code&gt;auto_cor()&lt;/code&gt; and &lt;code&gt;auto_vif()&lt;/code&gt;, directly into the &lt;code&gt;predictor.variable.names&lt;/code&gt; argument of the &lt;code&gt;rf()&lt;/code&gt; function to fit a random forest model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model.non.spatial &amp;lt;- spatialRF::rf(
  data = plant_richness_df,
  dependent.variable.name = dependent.variable.name,
  predictor.variable.names = predictor.variable.names,
  distance.matrix = distance.matrix,
  distance.thresholds = distance.thresholds,
  xy = xy, #not needed by rf, but other functions read it from the model
  seed = random.seed,
  verbose = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is a list with several slots containing the information required to interpret the model. The information available in these slots can be plotted (functions named &lt;code&gt;plot_...()&lt;/code&gt;), printed to screen (&lt;code&gt;print_...()&lt;/code&gt;) and captured for further analyses (&lt;code&gt;get_...()&lt;/code&gt;).&lt;/p&gt;
&lt;h2 id=&#34;residuals&#34;&gt;Residuals&lt;/h2&gt;
&lt;p&gt;The slot &lt;strong&gt;residuals&lt;/strong&gt; (&lt;code&gt;model.non.spatial$residuals&lt;/code&gt;) stores the values of the residuals and the results of the normality and spatial autocorrelation tests, and its content can be plotted with 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/plot_residuals_diagnostics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;plot_residuals_diagnostics()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::plot_residuals_diagnostics(
  model.non.spatial,
  verbose = FALSE
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;576&#34; /&gt;
&lt;p&gt;The upper panels show the results of the normality test (interpretation in the title), the middle panel shows the relationship between the residuals and the fitted values, important to understand the behavior of the residuals, and the lower panel shows the Moran&amp;rsquo;s I of the residuals across distance thresholds and their respective p-values (positive for 0 and 1000 km).&lt;/p&gt;
&lt;h2 id=&#34;variable-importance&#34;&gt;Variable importance&lt;/h2&gt;
&lt;h3 id=&#34;global-variable-importance&#34;&gt;Global variable importance&lt;/h3&gt;
&lt;p&gt;The slot &lt;strong&gt;importance&lt;/strong&gt; (&lt;code&gt;model.non.spatial$variable.importance&lt;/code&gt;) contains the variable importance scores. These can be plotted with 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/plot_importance.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;plot_importance()&lt;/code&gt;&lt;/a&gt;, printed with 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/print_importance.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;print_importance()&lt;/code&gt;&lt;/a&gt;, and the dataframe retrieved with 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/get_importance.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;get_importance()&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::plot_importance(
  model.non.spatial,
  verbose = FALSE
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;576&#34; /&gt;
&lt;p&gt;Variable importance represents the increase in mean error (computed on the out-of-bag data) across trees when a predictor is permuted. Values lower than zero would indicate that the variable performs worse than a random one.&lt;/p&gt;
&lt;p&gt;If the argument &lt;code&gt;scaled.importance = TRUE&lt;/code&gt; is used, the variable importance scores are computed from the scaled data, making the importance scores easier to compare across different models.&lt;/p&gt;
&lt;p&gt;The package 
&lt;a href=&#34;https://github.com/ModelOriented/randomForestExplainer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;randomForestExplainer&lt;/code&gt;&lt;/a&gt; offers a couple of interesting options to deepen our understanding on variable importance scores. The first one is &lt;code&gt;measure_importance()&lt;/code&gt;, which analyzes the forest to find out the average minimum tree depth at which each variable can be found (&lt;code&gt;mean_min_depth&lt;/code&gt;), the number of nodes in which a variable was selected to make a split (&lt;code&gt;no_of_nodes&lt;/code&gt;), the number of times the variable was selected as the first one to start a tree (&lt;code&gt;times_a_root&lt;/code&gt;), and the probability of a variable to be in more nodes than what it would be expected by chance (&lt;code&gt;p_value&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;importance.df &amp;lt;- randomForestExplainer::measure_importance(
  model.non.spatial,
  measures = c(&amp;quot;mean_min_depth&amp;quot;, &amp;quot;no_of_nodes&amp;quot;, &amp;quot;times_a_root&amp;quot;, &amp;quot;p_value&amp;quot;)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;kableExtra::kbl(
  importance.df %&amp;gt;% 
    dplyr::arrange(mean_min_depth) %&amp;gt;% 
    dplyr::mutate(p_value = round(p_value, 4)),
  format = &amp;quot;html&amp;quot;
) %&amp;gt;%
  kableExtra::kable_paper(&amp;quot;hover&amp;quot;, full_width = F)
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34; lightable-paper lightable-hover&#34; style=&#39;color: black; font-family: &#34;Arial Narrow&#34;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;&#39;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; variable &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; mean_min_depth &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; no_of_nodes &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; times_a_root &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; p_value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..neighbors_count &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 2.811087 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 2098 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 86 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; human_population &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 2.989940 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 2222 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 51 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3.117996 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1979 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 57 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_hypervolume &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3.200133 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 2072 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 44 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3.201070 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1781 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 64 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.4909 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_area_km2..x..bias_species_per_record &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3.379787 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1797 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 46 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.3406 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3.454233 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1900 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 23 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0020 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_species_per_record &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3.564676 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 2321 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 2 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_area_km2..pca..neighbors_percent_shared_edge &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3.959485 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1712 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 33 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.9519 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; human_population..x..bias_species_per_record &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3.977952 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1780 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 32 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.5006 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_area_km2 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 4.186849 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1800 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 10 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.3144 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; neighbors_count &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 4.204326 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1325 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 29 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1.0000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; topography_elevation_average &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 4.306636 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1732 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.8795 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; neighbors_percent_shared_edge &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 4.409251 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1700 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 5 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.9749 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; neighbors_area &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 4.643590 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1621 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 2 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1.0000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; fragmentation_cohesion &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 4.770930 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1518 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 8 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1.0000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_velocity_lgm_average &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 4.845155 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1658 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.9986 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_aridity_index_average &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 4.858302 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1682 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.9919 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; landcover_herbs_percent_average &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 4.984346 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1656 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.9988 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio15_minimum &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 5.057002 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1552 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 2 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1.0000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; fragmentation_division &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 5.229187 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1468 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1.0000 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;contribution-of-predictors-to-model-transferability&#34;&gt;Contribution of predictors to model transferability&lt;/h3&gt;
&lt;p&gt;The new function &lt;code&gt;rf_importance()&lt;/code&gt; offers a way to assess to what extent each predictor contributes to model transferability (predictive ability on independent spatial folds measured with &lt;code&gt;rf_evaluate()&lt;/code&gt;, see below). It does so by comparing the performance of the full model with models fitted without each one of the predictors. The difference in performance between the full model and a model without a given predictor represents the contribution of such predictor to model transferability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model.non.spatial &amp;lt;- spatialRF::rf_importance(
  model = model.non.spatial
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;768&#34; /&gt;
&lt;p&gt;The function results are added to the &amp;ldquo;importance&amp;rdquo; slot of the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(model.non.spatial$importance)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;per.variable&amp;quot;          &amp;quot;local&amp;quot;                 &amp;quot;oob.per.variable.plot&amp;quot;
## [4] &amp;quot;cv.per.variable.plot&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data frame &amp;ldquo;per.variable&amp;rdquo; contains the columns &amp;ldquo;importance.cv&amp;rdquo; (median importance), &amp;ldquo;importance.cv.mad&amp;rdquo; (median absolute deviation), &amp;ldquo;importance.cv.percent&amp;rdquo; (median importance in percentage), and &amp;ldquo;importance.cv.percent.mad&amp;rdquo; (median absolute deviation of the importance in percent). The ggplot object &amp;ldquo;cv.per.variable.plot&amp;rdquo; contains the importance plot with the median and the median absolute deviation shown above.&lt;/p&gt;
&lt;p&gt;The importance computed by random forest on the out-of-bag data by permutating each predictor (as computed by &lt;code&gt;rf()&lt;/code&gt;) and the contribution of each predictor to model transferability (as computed by &lt;code&gt;rf_importance()&lt;/code&gt;) show a moderate correlation, indicating that both importance measures capture different aspects of the effect of the variables on the model results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model.non.spatial$importance$per.variable %&amp;gt;% 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = importance.oob,
    y = importance.cv
  ) + 
  ggplot2::geom_point(size = 3) + 
  ggplot2::theme_bw() +
  ggplot2::xlab(&amp;quot;Importance (out-of-bag)&amp;quot;) + 
  ggplot2::ylab(&amp;quot;Contribution to transferability&amp;quot;) + 
  ggplot2::geom_smooth(method = &amp;quot;lm&amp;quot;, formula = y ~ x, color = &amp;quot;red4&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;576&#34; /&gt;
&lt;h3 id=&#34;local-variable-importance&#34;&gt;Local variable importance&lt;/h3&gt;
&lt;p&gt;Random forest also computes the average increase in error when a variable is permuted for each case. This is named &amp;ldquo;local importance&amp;rdquo;, is stored in &lt;code&gt;model.non.spatial$importance$local&lt;/code&gt; as a data frame, and can be retrieved with 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/get_importance_local.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;get_importance_local()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;local.importance &amp;lt;- spatialRF::get_importance_local(model.non.spatial)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The table below shows the first few records and columns. Larger values indicate larger average errors when estimating a case with the permuted version of the variable, so more important variables will show larger values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;kableExtra::kbl(
  round(local.importance[1:10, 1:5], 0),
  format = &amp;quot;html&amp;quot;
) %&amp;gt;%
  kableExtra::kable_paper(&amp;quot;hover&amp;quot;, full_width = F)
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34; lightable-paper lightable-hover&#34; style=&#39;color: black; font-family: &#34;Arial Narrow&#34;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;&#39;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; climate_aridity_index_average &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; climate_hypervolume &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; climate_bio1_average &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; climate_bio15_minimum &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; bias_area_km2 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -120 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 705 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 214 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 231 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -274 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 502 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -400 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -431 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 375 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 470 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -182 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -155 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1152 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 46 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -75 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 384 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 769 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 704 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 5 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -538 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -399 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -706 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -669 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 350 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -711 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 248 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1113 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 715 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 483 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 475 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 195 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 705 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 513 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 286 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 332 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -247 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -629 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 506 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -332 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -125 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 335 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -519 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1016 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 95 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -246 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 275 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1154 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 429 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 71 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 234 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;When case coordinates are joined with the local importance scores, it is possible to draw maps showing how variable importance changes over space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#adding coordinates
local.importance &amp;lt;- cbind(
  xy,
  local.importance
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#colors
color.low &amp;lt;- viridis::viridis(
    3,
    option = &amp;quot;F&amp;quot;
    )[2]
color.high &amp;lt;- viridis::viridis(
    3,
    option = &amp;quot;F&amp;quot;
    )[1]

#plot of climate_bio1_average
p1 &amp;lt;- ggplot2::ggplot() +
  ggplot2::geom_sf(
    data = world,
    fill = &amp;quot;white&amp;quot;
  ) +
  ggplot2::geom_point(
    data = local.importance,
    ggplot2::aes(
      x = x,
      y = y,
      color = climate_bio1_average
    )
  ) +
  ggplot2::scale_x_continuous(limits = c(-170, -30)) +
  ggplot2::scale_y_continuous(limits = c(-58, 80)) +
  ggplot2::scale_color_gradient2(
    low = color.low, 
    high = color.high
    ) +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = &amp;quot;bottom&amp;quot;) + 
  ggplot2::ggtitle(&amp;quot;climate_bio1_average&amp;quot;) +
  ggplot2::theme(
    plot.title = ggplot2::element_text(hjust = 0.5),
    legend.key.width = ggplot2::unit(1,&amp;quot;cm&amp;quot;)
    ) + 
  ggplot2::labs(color = &amp;quot;Importance&amp;quot;) + 
  ggplot2::xlab(&amp;quot;Longitude&amp;quot;) + 
  ggplot2::ylab(&amp;quot;Latitude&amp;quot;)

p2 &amp;lt;- ggplot2::ggplot() +
  ggplot2::geom_sf(
    data = world,
    fill = &amp;quot;white&amp;quot;
  ) +
  ggplot2::geom_point(
    data = local.importance,
    ggplot2::aes(
      x = x,
      y = y,
      color = human_population
    )
  ) +
  ggplot2::scale_x_continuous(limits = c(-170, -30)) +
  ggplot2::scale_y_continuous(limits = c(-58, 80)) +
  ggplot2::scale_color_gradient2(
    low = color.low, 
    high = color.high
    ) +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = &amp;quot;bottom&amp;quot;) +
  ggplot2::ggtitle(&amp;quot;human_population&amp;quot;) +
  ggplot2::theme(
    plot.title = ggplot2::element_text(hjust = 0.5),
    legend.key.width = ggplot2::unit(1,&amp;quot;cm&amp;quot;)
    ) + 
  ggplot2::labs(color = &amp;quot;Importance&amp;quot;) + 
  ggplot2::xlab(&amp;quot;Longitude&amp;quot;) + 
  ggplot2::ylab(&amp;quot;Latitude&amp;quot;)

p1 + p2
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;768&#34; /&gt;
&lt;p&gt;In these maps, values lower than 0 indicate that for a given record, the permuted version of the variable led to an accuracy score even higher than the one of the non-permuted variable, so again these negative values can be interpreted as &amp;ldquo;worse than chance&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;response-curves-and-surfaces&#34;&gt;Response curves and surfaces&lt;/h2&gt;
&lt;p&gt;The variable importance scores are also used by the function 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/plot_response_curves.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;plot_response_curves()&lt;/code&gt;&lt;/a&gt; to plot partial dependence curves for the predictors (by default, only the ones with an importance score above the median). Building the partial dependency curve of a predictor requires setting the other predictors to their quantiles (0.1, 0.5, and 0.9 by default). This helps to understand how the response curve of a variable changes when all the other variables have low, centered, or high values. The function also allows to see the training data&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::plot_response_curves(
  model.non.spatial,
  quantiles = c(0.1, 0.5, 0.9),
  line.color = viridis::viridis(
    3, #same number of colors as quantiles
    option = &amp;quot;F&amp;quot;, 
    end = 0.9
    ),
  ncol = 3,
  show.data = TRUE
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;864&#34; /&gt;
&lt;p&gt;Setting the argument &lt;code&gt;quantiles&lt;/code&gt; to 0.5 and setting &lt;code&gt;show.data&lt;/code&gt; to &lt;code&gt;FALSE&lt;/code&gt; (default optioin) accentuates the shape of the response curves.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::plot_response_curves(
  model.non.spatial,
  quantiles = 0.5,
  ncol = 3
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;864&#34; /&gt;
&lt;p&gt;The package 
&lt;a href=&#34;https://bgreenwell.github.io/pdp/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;pdp&lt;/code&gt;&lt;/a&gt; provides a general way to plot partial dependence plots.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pdp::partial(
  model.non.spatial, 
  train = plant_richness_df, 
  pred.var = &amp;quot;climate_bio1_average&amp;quot;, 
  plot = TRUE, 
  grid.resolution = 200
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;384&#34; /&gt;
&lt;p&gt;If you need to do your own plots in a different way, the function 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/get_response_curves.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;get_response_curves()&lt;/code&gt;&lt;/a&gt; returns a data frame with the required data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;reponse.curves.df &amp;lt;- spatialRF::get_response_curves(model.non.spatial)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;kableExtra::kbl(
  head(reponse.curves.df, n = 10),
  format = &amp;quot;html&amp;quot;
) %&amp;gt;%
  kableExtra::kable_paper(&amp;quot;hover&amp;quot;, full_width = F)
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34; lightable-paper lightable-hover&#34; style=&#39;color: black; font-family: &#34;Arial Narrow&#34;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;&#39;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; response &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; predictor &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; quantile &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; model &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; predictor.name &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; response.name &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3081.941 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -4.428994 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 0.1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; richness_species_vascular &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3081.941 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -4.393562 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 0.1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; richness_species_vascular &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3081.941 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -4.358129 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 0.1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; richness_species_vascular &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3081.941 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -4.322697 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 0.1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; richness_species_vascular &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3081.941 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -4.287265 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 0.1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; richness_species_vascular &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3081.941 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -4.251833 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 0.1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; richness_species_vascular &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3081.941 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -4.216400 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 0.1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; richness_species_vascular &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3081.941 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -4.180968 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 0.1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; richness_species_vascular &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3081.941 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -4.145536 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 0.1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; richness_species_vascular &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3081.941 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -4.110104 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 0.1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; richness_species_vascular &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Interactions between two variables can be plotted with 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/plot_response_surface.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;plot_response_surface()&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::plot_response_surface(
  model.non.spatial,
  a = &amp;quot;climate_bio1_average&amp;quot;,
  b = &amp;quot;neighbors_count&amp;quot;
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;432&#34; /&gt;
&lt;p&gt;This can be done as well with the &lt;code&gt;pdp&lt;/code&gt; package, that uses a slightly different algorithm to plot interaction surfaces.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pdp::partial(
  model.non.spatial, 
  train = plant_richness_df, 
  pred.var = c(&amp;quot;climate_bio1_average&amp;quot;, &amp;quot;neighbors_count&amp;quot;), 
  plot = TRUE
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;432&#34; /&gt;
&lt;h2 id=&#34;model-performance&#34;&gt;Model performance&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;performance&lt;/strong&gt; slot (in &lt;code&gt;model.non.spatial$performance&lt;/code&gt;) contains the values of several performance measures. It be printed via the function 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/print_performance.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;print_performance()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::print_performance(model.non.spatial)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model performance 
##   - R squared (oob):                  0.6075314
##   - R squared (cor(obs, pred)^2):     0.9543769
##   - Pseudo R squared (cor(obs, pred)):0.9769221
##   - RMSE (oob):                       2111.241
##   - RMSE:                             902.1424
##   - Normalized RMSE:                  0.2604337
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;R squared (oob)&lt;/code&gt; and &lt;code&gt;RMSE (oob)&lt;/code&gt; are the R squared of the model and its root mean squared error when predicting the out-of-bag data (fraction of data not used to train individual trees). From all the values available in the &lt;code&gt;performance&lt;/code&gt; slot, probably these the most honest ones, as it is the closer trying to get a performance estimate on independent data. However, out-of-bag data is not fully independent, and therefore will still be inflated, especially if the data is highly aggregated in space.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;R squared&lt;/code&gt; and &lt;code&gt;pseudo R squared&lt;/code&gt; are computed from the observations and the predictions, and indicate to what extent model outcomes represent the input data. These values will usually be high the data is highly aggregated in space.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;RMSE&lt;/code&gt; and its normalized version are computed via 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/root_mean_squared_error.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;root_mean_squared_error()&lt;/code&gt;&lt;/a&gt;, and are linear with &lt;code&gt;R squared&lt;/code&gt; and &lt;code&gt;pseudo R squared&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;spatial-cross-validation&#34;&gt;Spatial cross-validation&lt;/h2&gt;
&lt;p&gt;The function 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/rf_evaluate.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rf_evaluate()&lt;/a&gt; overcomes the limitations of the performance scores explained above by providing honest performance based on &lt;em&gt;spatial cross-validation&lt;/em&gt;. The function separates the data into a number of spatially independent training and testing folds. Then, it fits a model on each training fold, predicts over each testing fold, and computes statistics of performance measures across folds. Let&amp;rsquo;s see how it works.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model.non.spatial &amp;lt;- spatialRF::rf_evaluate(
  model = model.non.spatial,
  xy = xy,                  #data coordinates
  repetitions = 30,         #number of spatial folds
  training.fraction = 0.75, #training data fraction on each fold
  metrics = &amp;quot;r.squared&amp;quot;,
  seed = random.seed,
  verbose = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function generates a new slot in the model named &lt;strong&gt;evaluation&lt;/strong&gt; (&lt;code&gt;model.non.spatial$evaluation&lt;/code&gt;) with several objects that summarize the spatial cross-validation results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(model.non.spatial$evaluation)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;metrics&amp;quot;           &amp;quot;training.fraction&amp;quot; &amp;quot;spatial.folds&amp;quot;    
## [4] &amp;quot;per.fold&amp;quot;          &amp;quot;per.fold.long&amp;quot;     &amp;quot;per.model&amp;quot;        
## [7] &amp;quot;aggregated&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The slot &amp;ldquo;spatial.folds&amp;rdquo;, produced by 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/make_spatial_folds.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;make_spatial_folds()&lt;/code&gt;&lt;/a&gt;, contains the indices of the training and testing cases for each cross-validation repetition. The maps below show two sets of training and testing folds.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pr &amp;lt;- plant_richness_df[, c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;)]
pr$group.2 &amp;lt;- pr$group.1 &amp;lt;- &amp;quot;Training&amp;quot;
pr[model.non.spatial$evaluation$spatial.folds[[1]]$testing, &amp;quot;group.1&amp;quot;] &amp;lt;- &amp;quot;Testing&amp;quot;
pr[model.non.spatial$evaluation$spatial.folds[[25]]$testing, &amp;quot;group.2&amp;quot;] &amp;lt;- &amp;quot;Testing&amp;quot;

p1 &amp;lt;- ggplot2::ggplot() +
  ggplot2::geom_sf(data = world, fill = &amp;quot;white&amp;quot;) +
  ggplot2::geom_point(data = pr,
          ggplot2::aes(
            x = x,
            y = y,
            color = group.1
            ),
          size = 2
          ) +
  ggplot2::scale_color_viridis_d(
    direction = -1, 
    end = 0.5, 
    alpha = 0.8, 
    option = &amp;quot;F&amp;quot;
    ) +
  ggplot2::theme_bw() +
  ggplot2::labs(color = &amp;quot;Group&amp;quot;) +
  ggplot2::scale_x_continuous(limits = c(-170, -30)) +
  ggplot2::scale_y_continuous(limits = c(-58, 80))  +
  ggplot2::ggtitle(&amp;quot;Spatial fold 1&amp;quot;) + 
  ggplot2::theme(
    legend.position = &amp;quot;none&amp;quot;, 
    plot.title = ggplot2::element_text(hjust = 0.5)
  ) + 
  ggplot2::xlab(&amp;quot;Longitude&amp;quot;) + 
  ggplot2::ylab(&amp;quot;Latitude&amp;quot;)

p2 &amp;lt;- ggplot2::ggplot() +
  ggplot2::geom_sf(data = world, fill = &amp;quot;white&amp;quot;) +
  ggplot2::geom_point(data = pr,
          ggplot2::aes(
            x = x,
            y = y,
            color = group.2
            ),
          size = 2
          ) +
  ggplot2::scale_color_viridis_d(
    direction = -1, 
    end = 0.5, 
    alpha = 0.8, 
    option = &amp;quot;F&amp;quot;
    ) +
  ggplot2::theme_bw() +
  ggplot2::labs(color = &amp;quot;Group&amp;quot;) +
  ggplot2::scale_x_continuous(limits = c(-170, -30)) +
  ggplot2::scale_y_continuous(limits = c(-58, 80)) +
  ggplot2::theme(
    plot.title = ggplot2::element_text(hjust = 0.5)
  ) + 
  ggplot2::ggtitle(&amp;quot;Spatial fold 25&amp;quot;) + 
  ggplot2::xlab(&amp;quot;Longitude&amp;quot;) + 
  ggplot2::ylab(&amp;quot;&amp;quot;)

p1 | p2
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;960&#34; /&gt;
&lt;p&gt;The information available in this new slot can be accessed with the functions 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/print_evaluation.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;print_evaluation()&lt;/code&gt;&lt;/a&gt;, 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/plot_evaluation.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;plot_evaluation()&lt;/code&gt;&lt;/a&gt;, and 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/get_evaluation.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;get_evaluation()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::plot_evaluation(model.non.spatial)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;432&#34; /&gt;
&lt;p&gt;&lt;code&gt;Full&lt;/code&gt; represents the R squared of the model trained on the full dataset. &lt;code&gt;Training&lt;/code&gt; are the R-squared of the models fitted on the spatial folds (named &lt;code&gt;Training&lt;/code&gt; in the maps above), and &lt;code&gt;Testing&lt;/code&gt; are the R-squared of the same models on &amp;ldquo;unseen&amp;rdquo; data (data not used to train the model, named &lt;code&gt;Testing&lt;/code&gt; in the maps above). The median, median absolute deviation (MAD), minimum, and maximum R-squared values on the testing folds can be printed with &lt;code&gt;print_evaluation()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::print_evaluation(model.non.spatial)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Spatial evaluation 
##   - Training fraction:             0.75
##   - Spatial folds:                 29
## 
##     Metric Median   MAD Minimum Maximum
##  r.squared  0.517 0.085   0.122   0.781
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;other-important-things-stored-in-the-model&#34;&gt;Other important things stored in the model&lt;/h2&gt;
&lt;p&gt;The model predictions are stored in the slot &lt;strong&gt;predictions&lt;/strong&gt;, the arguments used to fit the model in &lt;strong&gt;ranger.arguments&lt;/strong&gt;, and the model itself, used to predict new values (see code chunk below), is in the &lt;strong&gt;forest&lt;/strong&gt; slot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;predicted &amp;lt;- stats::predict(
  object = model.non.spatial,
  data = plant_richness_df,
  type = &amp;quot;response&amp;quot;
  )$predictions
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;fitting-a-spatial-model-with-rf_spatial&#34;&gt;Fitting a spatial model with &lt;code&gt;rf_spatial()&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;The spatial autocorrelation of the residuals of a model like &lt;code&gt;model.non.spatial&lt;/code&gt;, measured with 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Moran%27s_I&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Moran&amp;rsquo;s I&lt;/a&gt;, can be plotted with 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/plot_moran.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;plot_moran()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::plot_moran(
  model.non.spatial, 
  verbose = FALSE
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-41-1.png&#34; width=&#34;384&#34; /&gt;
According to the plot, the spatial autocorrelation of the residuals of `model.non.spatial` is highly positive for a neighborhood of 0 and 1000 km, while it becomes non-significant (p-value \&gt; 0.05) at 2000, 4000, and 8000 km. To reduce the spatial autocorrelation of the residuals as much as possible, the non-spatial model can be transformed into a *spatial model* very easily with the function [`rf_spatial()`](https://blasbenito.github.io/spatialRF/reference/rf_spatial.html). This function is the true core of the package!
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model.spatial &amp;lt;- spatialRF::rf_spatial(
  model = model.non.spatial,
  method = &amp;quot;mem.moran.sequential&amp;quot;, #default method
  verbose = FALSE,
  seed = random.seed
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below shows the Moran&amp;rsquo;s I of the residuals of the spatial model, and indicates that the residuals are not autocorrelated at any distance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::plot_moran(
  model.spatial, 
  verbose = FALSE
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;384&#34; /&gt;
&lt;p&gt;If we compare the variable importance plots of both models, we can see that the spatial model has an additional set of dots under the name &amp;ldquo;spatial_predictors&amp;rdquo;, and that the maximum importance of a few of these &lt;em&gt;spatial predictors&lt;/em&gt; matches the importance of the most relevant non-spatial predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p1 &amp;lt;- spatialRF::plot_importance(
  model.non.spatial, 
  verbose = FALSE) + 
  ggplot2::ggtitle(&amp;quot;Non-spatial model&amp;quot;) 

p2 &amp;lt;- spatialRF::plot_importance(
  model.spatial,
  verbose = FALSE) + 
  ggplot2::ggtitle(&amp;quot;Spatial model&amp;quot;)

p1 | p2 
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-44-1.png&#34; width=&#34;960&#34; /&gt;
&lt;p&gt;If we look at the ten most important variables in &lt;code&gt;model.spatial&lt;/code&gt; we will see that a few of them are &lt;em&gt;spatial predictors&lt;/em&gt;. Spatial predictors are named &lt;code&gt;spatial_predictor_X_Y&lt;/code&gt;, where &lt;code&gt;X&lt;/code&gt; is the neighborhood distance at which the predictor has been generated, and &lt;code&gt;Y&lt;/code&gt; is the index of the predictor.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;kableExtra::kbl(
  head(model.spatial$importance$per.variable, n = 10),
  format = &amp;quot;html&amp;quot;
) %&amp;gt;%
  kableExtra::kable_paper(&amp;quot;hover&amp;quot;, full_width = F)
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34; lightable-paper lightable-hover&#34; style=&#39;color: black; font-family: &#34;Arial Narrow&#34;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;&#39;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; variable &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; importance &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..neighbors_count &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1094.792 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_hypervolume &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1067.111 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; spatial_predictor_0_2 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1035.365 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; human_population &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 997.965 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 973.243 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_area_km2..pca..neighbors_percent_shared_edge &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 902.209 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; climate_bio1_average..pca..human_population_density &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 882.910 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; spatial_predictor_0_1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 815.656 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_area_km2 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 750.278 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; bias_species_per_record &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 712.031 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But what are spatial predictors? Spatial predictors, as shown below, are smooth surfaces representing neighborhood among records at different spatial scales. They are computed from the distance matrix in different ways. The ones below are the eigenvectors of the double-centered distance matrix of weights (a.k.a, Moran&amp;rsquo;s Eigenvector Maps). They represent the effect of spatial proximity among records, helping to represent biogeographic and spatial processes not considered by the non-spatial predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatial.predictors &amp;lt;- spatialRF::get_spatial_predictors(model.spatial)
pr &amp;lt;- data.frame(spatial.predictors, plant_richness_df[, c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;)])

p1 &amp;lt;- ggplot2::ggplot() +
  ggplot2::geom_sf(data = world, fill = &amp;quot;white&amp;quot;) +
  ggplot2::geom_point(
    data = pr,
    ggplot2::aes(
      x = x,
      y = y,
      color = spatial_predictor_0_2
    ),
    size = 2.5
  ) +
  ggplot2::scale_color_viridis_c(option = &amp;quot;F&amp;quot;) +
  ggplot2::theme_bw() +
  ggplot2::labs(color = &amp;quot;Eigenvalue&amp;quot;) +
  ggplot2::scale_x_continuous(limits = c(-170, -30)) +
  ggplot2::scale_y_continuous(limits = c(-58, 80))  +
  ggplot2::ggtitle(&amp;quot;Variable: spatial_predictor_0_2&amp;quot;) + 
  ggplot2::theme(legend.position = &amp;quot;bottom&amp;quot;)+ 
  ggplot2::xlab(&amp;quot;Longitude&amp;quot;) + 
  ggplot2::ylab(&amp;quot;Latitude&amp;quot;)

p2 &amp;lt;- ggplot2::ggplot() +
  ggplot2::geom_sf(data = world, fill = &amp;quot;white&amp;quot;) +
  ggplot2::geom_point(
    data = pr,
    ggplot2::aes(
      x = x,
      y = y,
      color = spatial_predictor_0_5,
    ),
    size = 2.5
  ) +
  ggplot2::scale_color_viridis_c(option = &amp;quot;F&amp;quot;) +
  ggplot2::theme_bw() +
  ggplot2::labs(color = &amp;quot;Eigenvalue&amp;quot;) +
  ggplot2::scale_x_continuous(limits = c(-170, -30)) +
  ggplot2::scale_y_continuous(limits = c(-58, 80))  +
  ggplot2::ggtitle(&amp;quot;Variable: spatial_predictor_0_5&amp;quot;) + 
  ggplot2::theme(legend.position = &amp;quot;bottom&amp;quot;) + 
  ggplot2::xlab(&amp;quot;Longitude&amp;quot;) + 
  ggplot2::ylab(&amp;quot;&amp;quot;)

p1 | p2
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-46-1.png&#34; width=&#34;1152&#34; /&gt;
&lt;p&gt;The spatial predictors are included in the model one by one, in the order of their Moran&amp;rsquo;s I (spatial predictors with Moran&amp;rsquo;s I lower than 0 are removed). The selection procedure is performed by the function 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/select_spatial_predictors_sequential.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;select_spatial_predictors_sequential()&lt;/code&gt;&lt;/a&gt;, which finds the smaller subset of spatial predictors maximizing the model&amp;rsquo;s R squared, and minimizing the Moran&amp;rsquo;s I of the residuals. This is shown in the optimization plot below (dots linked by lines represent the selected spatial predictors).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- spatialRF::plot_optimization(model.spatial)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-47-1.png&#34; width=&#34;576&#34; /&gt;
&lt;h1 id=&#34;tuning-random-forest-hyperparameters&#34;&gt;Tuning Random Forest hyperparameters&lt;/h1&gt;
&lt;p&gt;The model fitted above was based on the default random forest hyperparameters of &lt;code&gt;ranger()&lt;/code&gt;, and those might not be the most adequate ones for a given dataset. The function 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/rf_tuning.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;rf_tuning()&lt;/code&gt;&lt;/a&gt; helps the user to choose sensible values for three Random Forest hyperparameters that are critical to model performance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;num.trees&lt;/code&gt;: number of regression trees in the forest.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mtry&lt;/code&gt;: number of variables to choose from on each tree split.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min.node.size&lt;/code&gt;: minimum number of cases on a terminal node.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These values can be modified in any model fitted with the package using the &lt;code&gt;ranger.arguments&lt;/code&gt; argument. The example below shows how to fit a spatial model with a given set of hyperparameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model.spatial &amp;lt;- spatialRF::rf_spatial(
  model = model.non.spatial,
  method = &amp;quot;mem.moran.sequential&amp;quot;, #default method
  ranger.arguments = list(
    mtry = 5,
    min.node.size = 20,
    num.trees = 1000
  ),
  verbose = FALSE,
  seed = random.seed
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The usual method for model tuning relies on a grid search exploring the results of all the combinations of hyperparameters selected by the user. In &lt;code&gt;spatialRF&lt;/code&gt;,
model tuning is done via spatial cross-validation, to ensure that the selected combination of hyperparameters maximizes the ability of the model to predict over data not used to train it. &lt;strong&gt;Warning&lt;/strong&gt;: model tuning consumes a lot of computational resources, using it on large datasets might freeze your computer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: model tunning is very RAM-hungry, but you can control RAM usage by defining a lower value for the argument &lt;code&gt;n.cores&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model.spatial &amp;lt;- rf_tuning(
  model = model.spatial,
  xy = xy,
  repetitions = 30,
  num.trees = c(500, 1000),
  mtry = seq(
    2,
    length(model.spatial$ranger.arguments$predictor.variable.names), #number of predictors
    by = 9),
  min.node.size = c(5, 15),
  seed = random.seed,
  verbose = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function returns a tuned model only if the tuning finds a solution better than the original model. Otherwise the original model is returned. The results of the tuning are stored in the model under the name &amp;ldquo;tuning&amp;rdquo;.&lt;/p&gt;
&lt;h1 id=&#34;repeating-a-model-execution&#34;&gt;Repeating a model execution&lt;/h1&gt;
&lt;p&gt;Random Forest is an stochastic algorithm that yields slightly different results on each run unless a random seed is set. This particularity has implications for the interpretation of variable importance scores and response curves. The function 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/rf_repeat.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;rf_repeat()&lt;/code&gt;&lt;/a&gt; repeats a model execution and yields the distribution of importance scores of the predictors across executions. &lt;strong&gt;NOTE&lt;/strong&gt;: this function works better when used at the end of a workflow&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model.spatial.repeat &amp;lt;- spatialRF::rf_repeat(
  model = model.spatial, 
  repetitions = 30,
  seed = random.seed,
  verbose = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The importance scores of a model fitted with &lt;code&gt;rf_repeat()&lt;/code&gt; are plotted as a violin plot, with the distribution of the importance scores of each predictor across repetitions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::plot_importance(
  model.spatial.repeat, 
  verbose = FALSE
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-51-1.png&#34; width=&#34;576&#34; /&gt;
&lt;p&gt;The response curves of models fitted with &lt;code&gt;rf_repeat()&lt;/code&gt; can be plotted with &lt;code&gt;plot_response_curves()&lt;/code&gt; as well. The median prediction is shown with a thicker line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::plot_response_curves(
  model.spatial.repeat, 
  quantiles = 0.5,
  ncol = 3
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-52-1.png&#34; width=&#34;864&#34; /&gt;
&lt;p&gt;The function &lt;code&gt;print_performance()&lt;/code&gt; generates a summary of the performance scores across model repetitions. As every other function of the package involving repetitions, the provided stats are the median, and the median absolute deviation (mad).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;spatialRF::print_performance(model.spatial.repeat)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model performance (median +/- mad) 
##   - R squared (oob):              0.593 +/- 0.0098
##   - R squared (cor(obs, pred)^2): 0.957 +/- 0.0016
##   - Pseudo R squared:             0.978 +/- 8e-04
##   - RMSE (oob):                   2151.154 +/- 25.7818
##   - RMSE:                         914.973 +/- 15.5809
##   - Normalized RMSE:              0.264 +/- 0.0045
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;taking-advantage-of-the---pipe&#34;&gt;Taking advantage of the &lt;code&gt; %&amp;gt;%&lt;/code&gt; pipe&lt;/h1&gt;
&lt;p&gt;The modeling functions of &lt;code&gt;spatialRF&lt;/code&gt; are designed to facilitate using the pipe to combine them. The code below fits a spatial model, tunes its hyperparameters, evaluates it using spatial cross-validation, and repeats the execution several times, just by passing the model from one function to another. Replace &lt;code&gt;eval = FALSE&lt;/code&gt; with &lt;code&gt;eval = TRUE&lt;/code&gt; if you want to execute the code chunk.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model.full &amp;lt;- rf_spatial(
  data = plant_richness_df,
  dependent.variable.name = dependent.variable.name,
  predictor.variable.names = predictor.variable.names,
  distance.matrix = distance_matrix,
  distance.thresholds = distance.thresholds,
  xy = xy
) %&amp;gt;%
  rf_tuning() %&amp;gt;%
  rf_evaluate() %&amp;gt;%
  rf_repeat()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code structure shown above can also be used to take advantage of a custom cluster, either defined in the local host, or a Beowulf cluster.&lt;/p&gt;
&lt;p&gt;When working with a single machine, a cluster can be defined and used as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#creating and registering the cluster
local.cluster &amp;lt;- parallel::makeCluster(
  parallel::detectCores() - 1,
  type = &amp;quot;PSOCK&amp;quot;
)
doParallel::registerDoParallel(cl = local.cluster)

#fitting, tuning, evaluating, and repeating a model
model.full &amp;lt;- rf_spatial(
  data = plant_richness_df,
  dependent.variable.name = dependent.variable.name,
  predictor.variable.names = predictor.variable.names,
  distance.matrix = distance_matrix,
  distance.thresholds = distance.thresholds,
  xy = xy,
  cluster = local.cluster #is passed via pipe to the other functions
) %&amp;gt;%
  rf_tuning() %&amp;gt;%
  rf_evaluate() %&amp;gt;%
  rf_repeat()

#stopping the cluster
parallel::stopCluster(cl = local.cluster)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To facilitate working with Beowulf clusters (
&lt;a href=&#34;https://www.blasbenito.com/post/01_home_cluster/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;just several computers connected via SSH&lt;/a&gt;), the package provides the function &lt;code&gt;beowulf_cluster()&lt;/code&gt;, that generates the cluster definition from details such as the IPs of the machines, the number of cores to be used on each machine, the user name, and the connection port.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#creating and registering the cluster
beowulf.cluster &amp;lt;- beowulf_cluster(
  cluster.ips = c(
    &amp;quot;10.42.0.1&amp;quot;,
    &amp;quot;10.42.0.34&amp;quot;,
    &amp;quot;10.42.0.104&amp;quot;
  ),
  cluster.cores = c(7, 4, 4),
  cluster.user = &amp;quot;blas&amp;quot;,
  cluster.port = &amp;quot;11000&amp;quot;
)

#fitting, tuning, evaluating, and repeating a model
model.full &amp;lt;- rf_spatial(
  data = plant_richness_df,
  dependent.variable.name = dependent.variable.name,
  predictor.variable.names = predictor.variable.names,
  distance.matrix = distance_matrix,
  distance.thresholds = distance.thresholds,
  xy = xy,
  cluster = beowulf.cluster 
) %&amp;gt;%
  rf_tuning() %&amp;gt;%
  rf_evaluate() %&amp;gt;%
  rf_repeat()

doParallel::registerDoParallel(cl = beowulf.cluster)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;comparing-several-models&#34;&gt;Comparing several models&lt;/h1&gt;
&lt;p&gt;The function 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/rf_compare.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;rf_compare()&lt;/code&gt;&lt;/a&gt; takes named list with as many models as the user needs to compare, and applies &lt;code&gt;rf_evaluate()&lt;/code&gt; to each one of them to compare their predictive performances across spatial folds.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;comparison &amp;lt;- spatialRF::rf_compare(
  models = list(
    `Non-spatial` = model.non.spatial,
    `Spatial` = model.spatial
  ),
  xy = xy,
  repetitions = 30,
  training.fraction = 0.8,
  metrics = &amp;quot;r.squared&amp;quot;,
  seed = random.seed
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-57-1.png&#34; width=&#34;576&#34; /&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- comparison$comparison.df %&amp;gt;% 
    dplyr::group_by(model, metric) %&amp;gt;% 
    dplyr::summarise(value = round(median(value), 3)) %&amp;gt;% 
    dplyr::arrange(metric) %&amp;gt;% 
    as.data.frame()
colnames(x) &amp;lt;- c(&amp;quot;Model&amp;quot;, &amp;quot;Metric&amp;quot;, &amp;quot;Median&amp;quot;)
kableExtra::kbl(
  x,
  format = &amp;quot;html&amp;quot;
  ) %&amp;gt;%
  kableExtra::kable_paper(&amp;quot;hover&amp;quot;, full_width = F)
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34; lightable-paper lightable-hover&#34; style=&#39;color: black; font-family: &#34;Arial Narrow&#34;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;&#39;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Model &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Metric &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; Median &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Non-spatial &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; r.squared &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.494 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Spatial &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; r.squared &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.305 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;working-with-a-binomial-response&#34;&gt;Working with a binomial response&lt;/h1&gt;
&lt;p&gt;This package can also perform binomial regression on response variables with zeros and ones. Let&amp;rsquo;s work on a quick example by turning the response variable of the previous models into a binomial one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plant_richness_df$response_binomial &amp;lt;- ifelse(
  plant_richness_df$richness_species_vascular &amp;gt; 5000,
  1,
  0
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new response variable, &lt;code&gt;response_binomial&lt;/code&gt;, will have ones where &lt;code&gt;richness_species_vascular &amp;gt; 5000&lt;/code&gt;, and zeros otherwise. This would be equivalent to having the classes &amp;ldquo;high richness&amp;rdquo; (represented by the ones) and &amp;ldquo;low richness&amp;rdquo;, represented by the zeros. The binomial regression model would then have as objective to compute the probability of each ecoregion to belong to the &amp;ldquo;high richness&amp;rdquo; class.&lt;/p&gt;
&lt;p&gt;There is something important to notice before moving forward though. The number of zeros in the new response variable is larger than the number of ones.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(plant_richness_df$response_binomial)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   0   1 
## 165  62
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that there is &lt;strong&gt;class imbalance&lt;/strong&gt;, and under this scenario, any random forest model is going to get better at predicting the most abundant class, while in our case the &amp;ldquo;target&amp;rdquo; is the less abundant one. But the function &lt;code&gt;rf()&lt;/code&gt; is ready to deal with this issue. Let&amp;rsquo;s fit a model to see what am I talking about.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model.non.spatial &amp;lt;- spatialRF::rf(
  data = plant_richness_df,
  dependent.variable.name = &amp;quot;response_binomial&amp;quot;,
  predictor.variable.names = predictor.variable.names,
  distance.matrix = distance.matrix,
  distance.thresholds = distance.thresholds,
  seed = random.seed,
  verbose = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function detects that the response variable is binary (using the function 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/is_binary.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;is_binary()&lt;/code&gt;&lt;/a&gt;), and computes &lt;em&gt;case weights&lt;/em&gt; for the ones and the zeros. These case weights are stored in the &lt;code&gt;ranger.arguments&lt;/code&gt; slot of the model, and are used to give preference to the cases with larger weights during the selection of the out-of-bag data (check the &lt;code&gt;case.weights&lt;/code&gt; argument in &lt;code&gt;ranger::ranger()&lt;/code&gt;). As a result, each individual tree in the forest is trained with a similar proportion of zeros and ones, which helps mitigate the class imbalance issue. This method is named &lt;em&gt;weighted Random Forest&lt;/em&gt;, and is very well explained in this 
&lt;a href=&#34;https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;white paper&lt;/a&gt; that includes the father of Random Forest, Leo Breiman, as coauthor.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unique(model.non.spatial$ranger.arguments$case.weights)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.006060606 0.016129032
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model could be projected right away onto a raster stack with maps of the predictors, so, in fact, &lt;code&gt;spatialRF&lt;/code&gt; can be used to fit Species Distribution Models, when it actually wasn&amp;rsquo;t really designed with such a purpose in mind. And as an additional advantage, the model can be evaluated with &lt;code&gt;rf_evaluate()&lt;/code&gt;, which is way better than cross-validation via random data-splitting (
&lt;a href=&#34;https://methodsblog.com/2018/11/29/blockcv-english/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this blog post&lt;/a&gt; explains explains why).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model.non.spatial &amp;lt;- spatialRF::rf_evaluate(
  model.non.spatial,
  xy = xy,
  metrics = &amp;quot;auc&amp;quot;,
  verbose = FALSE
)

spatialRF::print_evaluation(model.non.spatial)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Spatial evaluation 
##   - Training fraction:             0.75
##   - Spatial folds:                 29
## 
##  Metric Median   MAD Minimum Maximum
##     auc  0.932 0.024    0.83   0.977
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;take away message&lt;/strong&gt; here is that you can work with a binomial response with &lt;code&gt;spatialRF&lt;/code&gt;, just as you would do with a continuous response, as long as it is represented with zeros and ones. Just remember that the class imbalance problem is tackled via case weights, and that predictive performance is also measured using the Area Under the ROC Curve (AUC).&lt;/p&gt;
&lt;h1 id=&#34;generating-spatial-predictors-for-other-modelling-methods&#34;&gt;Generating spatial predictors for other modelling methods&lt;/h1&gt;
&lt;p&gt;You might not love Random Forest, but &lt;code&gt;spatialRF&lt;/code&gt; loves you, and as such, it gives you tools to generate spatial predictors for other models anyway.&lt;/p&gt;
&lt;p&gt;The first step requires generating Moran&amp;rsquo;s Eigenvector Maps (MEMs) from the distance matrix. Here there are two options, computing MEMs for a single neighborhood distance with 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/mem.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;mem()&lt;/code&gt;&lt;/a&gt;, and computing MEMs for several neighborhood distances at once with 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/mem_multithreshold.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;mem_multithreshold()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#single distance (0km by default)
mems &amp;lt;- spatialRF::mem(distance.matrix = distance_matrix)

#several distances
mems &amp;lt;- spatialRF::mem_multithreshold(
  distance.matrix = distance.matrix,
  distance.thresholds = distance.thresholds
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In either case the result is a data frame with Moran&amp;rsquo;s Eigenvector Maps (&amp;ldquo;just&amp;rdquo; the positive eigenvectors of the double-centered distance matrix).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;kableExtra::kbl(
  head(mems[, 1:4], n = 10),
  format = &amp;quot;html&amp;quot;
) %&amp;gt;%
  kableExtra::kable_paper(&amp;quot;hover&amp;quot;, full_width = F)
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34; lightable-paper lightable-hover&#34; style=&#39;color: black; font-family: &#34;Arial Narrow&#34;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;&#39;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; spatial_predictor_0_1 &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; spatial_predictor_0_2 &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; spatial_predictor_0_3 &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; spatial_predictor_0_4 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0259217 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0052203 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0416969 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0363324 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0996679 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0539713 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.1324480 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.3826928 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0010477 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0143046 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0443602 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0031386 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0165695 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0047991 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0307457 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0005170 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0225761 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0019595 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0230368 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0524239 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0155252 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0023742 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0197953 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0338956 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0229197 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0039860 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0312561 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0416697 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.2436009 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.1155295 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0791452 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0189996 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0150725 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0158684 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.1010284 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0095590 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.1187381 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0471879 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; -0.0359881 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0065211 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But not all MEMs are made equal, and you will need to rank them by their Moran&amp;rsquo;s I. The function 
&lt;a href=&#34;https://blasbenito.github.io/spatialRF/reference/rank_spatial_predictors.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;rank_spatial_predictors()&lt;/code&gt;&lt;/a&gt; will help you do so.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mem.rank &amp;lt;- spatialRF::rank_spatial_predictors(
  distance.matrix = distance_matrix,
  spatial.predictors.df = mems,
  ranking.method = &amp;quot;moran&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output of &lt;code&gt;rank_spatial_predictors()&lt;/code&gt; is a list with three slots: &amp;ldquo;method&amp;rdquo;, a character string with the name of the ranking method; &amp;ldquo;criteria&amp;rdquo;, an ordered data frame with the criteria used to rank the spatial predictors; and &amp;ldquo;ranking&amp;rdquo;, a character vector with the names of the spatial predictors in the order of their ranking (it is just the first column of the &amp;ldquo;criteria&amp;rdquo; data frame). We can use this &amp;ldquo;ranking&amp;rdquo; object to reorder or &lt;code&gt;mems&lt;/code&gt; data frame.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mems &amp;lt;- mems[, mem.rank$ranking]

#also:
#mems &amp;lt;- mem.rank$spatial.predictors.df
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here, spatial predictors can be included in any model one by one, in the order of the ranking, until the spatial autocorrelation of the residuals becomes neutral, if possible. A little example with a linear model follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#model definition
predictors &amp;lt;- c(
  &amp;quot;climate_aridity_index_average &amp;quot;,
  &amp;quot;climate_bio1_average&amp;quot;,
  &amp;quot;bias_species_per_record&amp;quot;,
  &amp;quot;human_population_density&amp;quot;,
  &amp;quot;topography_elevation_average&amp;quot;,
  &amp;quot;fragmentation_division&amp;quot;
)
model.formula &amp;lt;- as.formula(
  paste(
    dependent.variable.name,
    &amp;quot; ~ &amp;quot;,
    paste(
      predictors,
      collapse = &amp;quot; + &amp;quot;
    )
  )
)

#scaling the data
model.data &amp;lt;- scale(plant_richness_df) %&amp;gt;% 
  as.data.frame()

#fitting the model
m &amp;lt;- lm(model.formula, data = model.data)

#Moran&#39;s I test of the residuals
moran.test &amp;lt;- spatialRF::moran(
  x = residuals(m),
  distance.matrix = distance_matrix,
  verbose = FALSE
)
moran.test$plot
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-68-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;According to the Moran&amp;rsquo;s I test, the model residuals show spatial autocorrelation. Let&amp;rsquo;s introduce MEMs one by one until the problem is solved.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#add mems to the data and applies scale()
model.data &amp;lt;- data.frame(
  plant_richness_df,
  mems
) %&amp;gt;%
  scale() %&amp;gt;%
  as.data.frame()

#initialize predictors.i
predictors.i &amp;lt;- predictors

#iterating through MEMs
for(mem.i in colnames(mems)){
  
  #add mem name to model definintion
  predictors.i &amp;lt;- c(predictors.i, mem.i)
  
  #generate model formula with the new spatial predictor
  model.formula.i &amp;lt;- as.formula(
    paste(
      dependent.variable.name,
      &amp;quot; ~ &amp;quot;,
      paste(
        predictors.i,
        collapse = &amp;quot; + &amp;quot;
      )
    )
  )
  
  #fit model
  m.i &amp;lt;- lm(model.formula.i, data = model.data)
  
  #Moran&#39;s I test
  moran.test.i &amp;lt;- moran(
    x = residuals(m.i),
    distance.matrix = distance_matrix,
    verbose = FALSE
  )
  
  #stop if no autocorrelation
  if(moran.test.i$test$interpretation == &amp;quot;No spatial correlation&amp;quot;){
    break
  }
  
}#end of loop

#last moran test
moran.test.i$plot
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/project/post-title/index_files/figure-html/unnamed-chunk-69-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;Now we can compare the model without spatial predictors &lt;code&gt;m&lt;/code&gt; and the model with spatial predictors &lt;code&gt;m.i&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;comparison.df &amp;lt;- data.frame(
  Model = c(&amp;quot;Non-spatial&amp;quot;, &amp;quot;Spatial&amp;quot;),
  Predictors = c(length(predictors), length(predictors.i)),
  R_squared = round(c(summary(m)$r.squared, summary(m.i)$r.squared), 2),
  AIC = round(c(AIC(m), AIC(m.i)), 0),
  BIC = round(c(BIC(m), BIC(m.i)), 0),
  `Moran I` = round(c(moran.test$test$moran.i, moran.test.i$test$moran.i), 2)
)

kableExtra::kbl(
  comparison.df,
  format = &amp;quot;html&amp;quot;
) %&amp;gt;%
  kableExtra::kable_paper(&amp;quot;hover&amp;quot;, full_width = F)
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34; lightable-paper lightable-hover&#34; style=&#39;color: black; font-family: &#34;Arial Narrow&#34;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;&#39;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Model &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; Predictors &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; R_squared &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; AIC &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; BIC &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; Moran.I &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Non-spatial &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 6 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.38 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 551 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 578 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.21 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Spatial &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 22 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.50 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 533 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 615 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.06 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;According to the model comparison, it can be concluded that the addition of spatial predictors, in spite of the increase in complexity, has improved the model. In any case, this is just a simple demonstration of how spatial predictors generated with functions of the &lt;code&gt;spatialRF&lt;/code&gt; package can still help you fit spatial models with other modeling methods.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Species Distribution Models predict abundance and its temporal variation in a steppe bird population.</title>
      <link>https://blasbenito.com/publication/2023_monnier_global_ecology_and_conservation/</link>
      <pubDate>Sat, 24 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2023_monnier_global_ecology_and_conservation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Environmental and human factors drive the subtropical marine forests of Gongolaria abies-marina to extinction.</title>
      <link>https://blasbenito.com/publication/2022_martin_marine_environmental_research/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2022_martin_marine_environmental_research/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Human practices behind the aquatic and terrestrial ecological decoupling to climate change in the tropical Andes.</title>
      <link>https://blasbenito.com/publication/2022_benito_stoten/</link>
      <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2022_benito_stoten/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Density-dependence of reproductive success in a Houbara bustard population.</title>
      <link>https://blasbenito.com/publication/2022_monnier_global_ecology_and_conservation/</link>
      <pubDate>Thu, 24 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2022_monnier_global_ecology_and_conservation/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Fourteen years of continuous soil moisture records from plant and biocrust-dominated microsites.</title>
      <link>https://blasbenito.com/publication/2022_moreno_scientific_data/</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2022_moreno_scientific_data/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Biogeography of global drylands</title>
      <link>https://blasbenito.com/publication/2021_maestre_new_phytologist/</link>
      <pubDate>Sat, 17 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2021_maestre_new_phytologist/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Designing R Functions to Compute Betadiversity Indices</title>
      <link>https://blasbenito.com/post/betadiversity/</link>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/betadiversity/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This is a tutorial written for R users needing to compute betadiversity indices from species lists rather than from presence-absence matrices, and for R beginners or intermediate users that want to start using their own functions. If you are an advanced R user, this post will likely waste your time.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We ecologists like to measure all things in nature, and compositional changes in biological communities over time or space, a.k.a &lt;em&gt;betadiversity&lt;/em&gt;, is one of these things. I am not going to explain what betadiversity is because others that know better than me have done it already. Good examples are 
&lt;a href=&#34;https://methodsblog.com/2015/05/27/beta_diversity/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post published the blog of Methods in Ecology and Evolution&lt;/a&gt; by 
&lt;a href=&#34;https://twitter.com/andres_baselga&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andres Baselga&lt;/a&gt;, 
&lt;a href=&#34;https://www.youtube.com/watch?v=WQGN30YSc_U&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;and this lecture by Tim Seipel&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;What I am actually going to do in this post is to explain how to write functions to compute betadiversity indices in R from species lists rather than from presence-absence matrices. For the latter there are a few packages such as 
&lt;a href=&#34;https://cran.r-project.org/package=vegan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vegan&lt;/a&gt;, 
&lt;a href=&#34;https://cran.r-project.org/package=BAT&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BAT&lt;/a&gt;,  
&lt;a href=&#34;https://cran.r-project.org/package=MBI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MBI&lt;/a&gt;, or 
&lt;a href=&#34;https://cran.r-project.org/package=betapart&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;betapart&lt;/a&gt;, but for the former I was unable to find anything suitable. To make this post useful for R beginners, I will go step by step on the rationale behind the design of the functions to compute betadiversity indices, and by the end of the post I will explain how to organize them to achieve a clean R workflow.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go!&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;betadiversity-indices&#34;&gt;Betadiversity indices&lt;/h2&gt;
&lt;p&gt;There are a few betadiversity indices out there, and I totally recommend you to start with 
&lt;a href=&#34;https://besjournals.onlinelibrary.wiley.com/doi/10.1046/j.1365-2656.2003.00710.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Koleff &lt;em&gt;et al.&lt;/em&gt; (2003)&lt;/a&gt; as a primer. They review the literature and analyze the properties of 24 different indices to provide guidance on how to use them.&lt;/p&gt;
&lt;h3 id=&#34;betadiversity-components-a-b-and-c&#34;&gt;Betadiversity components &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;, and &lt;em&gt;c&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;Betadiversity indices are designed to compare the taxa pools of two sites at a time, and require the computation of three components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;a&lt;/strong&gt;: number of common taxa of both sites.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;b&lt;/strong&gt;: number of exclusive taxa of one site.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;c&lt;/strong&gt;: number of exclusive taxa of the other site.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s see how can we use these diversity components to compute betadiversity indices.&lt;/p&gt;
&lt;h3 id=&#34;sÃ¸rensens-beta&#34;&gt;SÃ¸rensen&amp;rsquo;s Beta&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start with the &lt;strong&gt;SÃ¸rensen&amp;rsquo;s Beta&lt;/strong&gt; (&lt;code&gt;\(\beta_{sor}\)&lt;/code&gt; hereafter), as presented in Koleff &lt;em&gt;et al.&lt;/em&gt; (2003).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$\beta_{sor} = \frac{2a}{2a + b + c}$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;\(\beta_{sor}\)&lt;/code&gt; is a similarity index in the range [0, 1] (the closer to one, the more similar the taxa pools of both sites are) that puts a lot of weight in the &lt;code&gt;\(a\)&lt;/code&gt; component, and is therefore a measure of &lt;em&gt;continuity&lt;/em&gt;, as it focuses the most in the common taxa among sites.&lt;/p&gt;
&lt;h3 id=&#34;simpsons-beta&#34;&gt;Simpson&amp;rsquo;s Beta&lt;/h3&gt;
&lt;p&gt;Another popular betadiversity index is the &lt;strong&gt;Simpson&amp;rsquo;s Beta&lt;/strong&gt; (&lt;code&gt;\(\beta_{sim}\)&lt;/code&gt; hereafter).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$\beta_{sim} = \frac{min(b, c)}{min(b, c) + a}$$&lt;/code&gt;
where &lt;code&gt;\(min()\)&lt;/code&gt; is a function that takes the minimum value among the diversity components within the parenthesis. &lt;code&gt;\(\beta_{sim}\)&lt;/code&gt; is a dissimilarity measure that focuses on compositional turnover among sites because it focuses the most on the values of &lt;code&gt;\(b\)&lt;/code&gt; and &lt;code&gt;\(c\)&lt;/code&gt;. It has its lower bound in zero, and an open upper value.&lt;/p&gt;
&lt;p&gt;To bring these ideas into R, first we have to load a few R packages, and generate some fake data to help us develop the functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(foreach, quietly = TRUE)
library(doParallel, quietly = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code chunk below generates 15 fake taxa names, from &lt;code&gt;taxon_1&lt;/code&gt; to &lt;code&gt;taxon_15&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;taxa &amp;lt;- paste0(&amp;quot;taxon_&amp;quot;, 1:15)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these fake taxa we are going to generate taxa lists for four hypothetical sites named &lt;em&gt;site1&lt;/em&gt;, &lt;em&gt;site2&lt;/em&gt;, &lt;em&gt;site3&lt;/em&gt;, and &lt;em&gt;site4&lt;/em&gt;. Two of the sites will have identical taxa lists, two will have non-overlapping taxa lists, and two of them will have some overlap.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;site1 &amp;lt;- site2 &amp;lt;- taxa[1:7]
site3 &amp;lt;- taxa[8:12]
site4 &amp;lt;- taxa[10:15]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now we have these taxa lists:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;site1 #and site2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;taxon_1&amp;quot; &amp;quot;taxon_2&amp;quot; &amp;quot;taxon_3&amp;quot; &amp;quot;taxon_4&amp;quot; &amp;quot;taxon_5&amp;quot; &amp;quot;taxon_6&amp;quot; &amp;quot;taxon_7&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;site3
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;taxon_8&amp;quot;  &amp;quot;taxon_9&amp;quot;  &amp;quot;taxon_10&amp;quot; &amp;quot;taxon_11&amp;quot; &amp;quot;taxon_12&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;site4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;taxon_10&amp;quot; &amp;quot;taxon_11&amp;quot; &amp;quot;taxon_12&amp;quot; &amp;quot;taxon_13&amp;quot; &amp;quot;taxon_14&amp;quot; &amp;quot;taxon_15&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;step-by-step-computation-of-betadiversity-indices-with-r&#34;&gt;Step-by-step computation of betadiversity indices with R&lt;/h2&gt;
&lt;p&gt;For a given pair of sites, how can we compute the diversity components &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;, and &lt;em&gt;c&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;Looking at it from an R perspective, each site is a character vector, so &lt;em&gt;a&lt;/em&gt; can be found by counting the number of common elements between two vectors. These common elements can be found with the function &lt;code&gt;intersect()&lt;/code&gt;, and the number of elements can be computed by applying &lt;code&gt;length()&lt;/code&gt; on the result of &lt;code&gt;intersect()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;a &amp;lt;- length(intersect(site3, site4))
a
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To compute &lt;em&gt;b&lt;/em&gt; and &lt;em&gt;c&lt;/em&gt; we can use the function &lt;code&gt;setdiff()&lt;/code&gt;, that finds the exclusive elements of one character vector when comparing it with another. In this case, &lt;em&gt;b&lt;/em&gt; is computed for the first vector introduced in the function, &lt;em&gt;site3&lt;/em&gt; in this case&amp;hellip;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;b &amp;lt;- length(setdiff(site3, site4))
b
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;hellip; so to compute the &lt;em&gt;c&lt;/em&gt; component we only need to switch the sites.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;c &amp;lt;- length(setdiff(site4, site3))
c
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we know &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;, and &lt;em&gt;c&lt;/em&gt;, we can compute &lt;code&gt;\(\beta_{sor}\)&lt;/code&gt; and &lt;code&gt;\(\beta_{sim}\)&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Bsor &amp;lt;- 2 * a / (2 * a + b + c)
Bsor
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5454545
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Bsim&amp;lt;- min(b, c) / (min(b, c) + a)
Bsim
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, if we have a long list of sites, computing betadiversity indices like this can get quite boring quite fast. Let&amp;rsquo;s put everything in a set of functions to make it easier to work with.&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;writing-functions-to-compute-betadiversity-indices&#34;&gt;Writing functions to compute betadiversity indices&lt;/h2&gt;
&lt;p&gt;The basic structure of a function definition in R looks as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;function_name &amp;lt;- function(x, y, ...){
  output &amp;lt;- [body]
  output #also return(output)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;function_name&lt;/code&gt; is the name of your function. Ideally, a verb, or otherwise, something indicating somehow what the function will do with the input data and arguments.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;function()&lt;/code&gt; is a function to define functions, there isn&amp;rsquo;t much more to it&amp;hellip;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is the first argument of the function, and ideally, represents the input data. If that is the case, you can later use 
&lt;a href=&#34;https://r4ds.had.co.nz/pipes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pipes&lt;/a&gt; (&lt;code&gt;|&amp;gt;&lt;/code&gt;) to chain functions together.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; (it could have any other name) is another function argument, an can be either another input dataset, or an argument defining how the function has to behave.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;...&lt;/code&gt; refers to other arguments the function may require.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;body&lt;/code&gt; is the code that operates with the data and function arguments. This can be one line of code, or a thousand, it all comes down to the function&amp;rsquo;s objective. In any case, the &lt;code&gt;body&lt;/code&gt; must return an object (or an error if something went wrong) that will be the function&amp;rsquo;s &lt;code&gt;output&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;output&lt;/code&gt; is the object ultimately produced by the function. It can have any name, and can be any kind of structure, such a number, a vector, a data frame, a list, etc. R functions return one output object only. Since R functions return the last evaluated value, it is good practice to put the &lt;code&gt;output&lt;/code&gt; object at the end of the function as an explicit way to state what the actual output of the function is.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s start writing a function to compute &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;, and &lt;em&gt;c&lt;/em&gt; from a pair of sites.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#x: taxa list of one site
#y: taxa list of another site
abc &amp;lt;- function(x, y){
  
  #list to store output
  out &amp;lt;- list()
  
  #filling the list
  out$a &amp;lt;- length(intersect(x, y))
  out$b &amp;lt;- length(setdiff(x, y))
  out$c &amp;lt;- length(setdiff(y, x))
  
  #returning the output
  out
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that to to return the three values I am wrapping them in a list. Let&amp;rsquo;s run a little test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- abc(
  x = site3,
  y = site4
)
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $a
## [1] 3
## 
## $b
## [1] 2
## 
## $c
## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far so good! From here we build the functions &lt;code&gt;sorensen_beta()&lt;/code&gt; and &lt;code&gt;simpson_beta()&lt;/code&gt; making sure they can accept the output of &lt;code&gt;abc()&lt;/code&gt;, and return it with an added slot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sorensen_beta &amp;lt;- function(x){
  
  x$bsor &amp;lt;- round(2 * x$a / (2 * x$a + x$b + x$c), 3)
  
  x
  
}


simpson_beta &amp;lt;- function(x){
  
  x$bsim &amp;lt;- round(min(x$b, x$c) / (min(x$b, x$c) + x$a), 3)
  
  x
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that both functions are returning the input &lt;code&gt;x&lt;/code&gt; with an added slot named after the given betadiversity index. Let&amp;rsquo;s test them first, to later see why returning the input object gives these functions a lot of flexibility.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sorensen_beta(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $a
## [1] 3
## 
## $b
## [1] 2
## 
## $c
## [1] 3
## 
## $bsor
## [1] 0.545
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;simpson_beta(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $a
## [1] 3
## 
## $b
## [1] 2
## 
## $c
## [1] 3
## 
## $bsim
## [1] 0.4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When I said that returning the input object with an added slot gave these functions a lot of flexibility I was talking about this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- abc(
  x = site3, 
  y = site4
  ) |&amp;gt; 
  sorensen_beta() |&amp;gt; 
  simpson_beta()
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $a
## [1] 3
## 
## $b
## [1] 2
## 
## $c
## [1] 3
## 
## $bsor
## [1] 0.545
## 
## $bsim
## [1] 0.4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Chaining the functions through the pipe &lt;code&gt;|&amp;gt;&lt;/code&gt; pipe allows us combining their results in a single output no matter whether we use &lt;code&gt;sorensen_beta()&lt;/code&gt; or &lt;code&gt;sorensen_beta()&lt;/code&gt; first, or whether we omit one of them. The only thing the pipe is doing here is moving the output of the first function into the next.&lt;/p&gt;
&lt;p&gt;We can put that idea right away into a function to compute both betadiversity indices at once from the taxa list of a pair of sites.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;betadiversity &amp;lt;- function(x, y){
  
  abc(x, y) |&amp;gt;
    sorensen_beta() |&amp;gt;
    simpson_beta()
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function now works as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- betadiversity(
  x = site3, 
  y = site4
  )
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $a
## [1] 3
## 
## $b
## [1] 2
## 
## $c
## [1] 3
## 
## $bsor
## [1] 0.545
## 
## $bsim
## [1] 0.4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far we have four functions&amp;hellip;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;abc()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;simpson_beta()&lt;/code&gt;, that requires &lt;code&gt;abc()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sorensen_beta()&lt;/code&gt;, that requires &lt;code&gt;abc()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;betadiversity()&lt;/code&gt;, that requires &lt;code&gt;abc()&lt;/code&gt;, &lt;code&gt;simpson_beta()&lt;/code&gt;, and &lt;code&gt;sorensen_beta()&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;hellip; and one limitation: so far we can only return betadiversity indices for two sites at a time. So at the moment, to compute betadiversity indices for all combinations of sites we have to do a pretty ridiculous thing:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x1 &amp;lt;- betadiversity(x = site1, y = site2)
x2 &amp;lt;- betadiversity(x = site1, y = site3)
x3 &amp;lt;- betadiversity(x = site1, y = site4)
#... and so on
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I see you doing this I&amp;rsquo;ll come to haunt you in your nightmares! Since a real analysis may involve hundreds of sites, the next step is to use the functions above to build a new one able to intake an arbitrary number of sites.&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;writing-a-function-to-compute-betadiversity-indices-for-an-arbitrary-number-of-sites&#34;&gt;Writing a function to compute betadiversity indices for an arbitrary number of sites.&lt;/h2&gt;
&lt;p&gt;First we have to organize our sites in a data frame with a &lt;em&gt;long format&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sites &amp;lt;- data.frame(
  site = c(
    rep(&amp;quot;site1&amp;quot;, length(site1)),
    rep(&amp;quot;site2&amp;quot;, length(site2)),
    rep(&amp;quot;site3&amp;quot;, length(site3)),
    rep(&amp;quot;site4&amp;quot;, length(site4))
    ),
  taxon = c(
    site1,
    site2,
    site3,
    site4
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;site&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;taxon&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;taxon_15&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Our new function will need to do several things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generate combinations of the unique values of the column &lt;code&gt;site&lt;/code&gt; two by two without repetition.&lt;/li&gt;
&lt;li&gt;Iterate through these combinations of two sites to compute betadiversity components and indices.&lt;/li&gt;
&lt;li&gt;Return a dataframe with the results to facilitate further analyses.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The combinations of site pairs are done with &lt;code&gt;utils::combn()&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;site.combinations &amp;lt;- utils::combn(
  x = unique(sites$site),
  m = 2
  )
site.combinations
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]    [,2]    [,3]    [,4]    [,5]    [,6]   
## [1,] &amp;quot;site1&amp;quot; &amp;quot;site1&amp;quot; &amp;quot;site1&amp;quot; &amp;quot;site2&amp;quot; &amp;quot;site2&amp;quot; &amp;quot;site3&amp;quot;
## [2,] &amp;quot;site2&amp;quot; &amp;quot;site3&amp;quot; &amp;quot;site4&amp;quot; &amp;quot;site3&amp;quot; &amp;quot;site4&amp;quot; &amp;quot;site4&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is a matrix, and each pair of rows in a column contain a pair of sites. The idea now is to iterate over the matrix columns, obtain the set of taxa from each site from the &lt;code&gt;taxon&lt;/code&gt; column of the &lt;code&gt;sites&lt;/code&gt; data frame, and use these taxa lists to compute the betadiversity components and indices.&lt;/p&gt;
&lt;p&gt;To easily generate the output data frame, I use the &lt;code&gt;foreach::foreach()&lt;/code&gt; function to iterate through pairs instead of a more traditional &lt;code&gt;for&lt;/code&gt; loop. You can read more about &lt;code&gt;foreach()&lt;/code&gt; in a 
&lt;a href=&#34;https://www.blasbenito.com/post/02_parallelizing_loops_with_r/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;betadiversity.df &amp;lt;- foreach::foreach(
  i = 1:ncol(site.combinations), #iterates through columns of site.combinations
  .combine = &#39;rbind&#39; #to produce a data frame
  ) %do% {
  
  #site names
  site.one &amp;lt;- site.combinations[1, i] #from column i, row 1
  site.two &amp;lt;- site.combinations[2, i] #from column i, row 2
  
  #getting taxa lists
  taxa.list.one &amp;lt;- sites[sites$site %in% site.one, &amp;quot;taxon&amp;quot;]
  taxa.list.two &amp;lt;- sites[sites$site %in% site.two, &amp;quot;taxon&amp;quot;]
  
  #betadiversity
  beta &amp;lt;- betadiversity(
    x = taxa.list.one,
    y = taxa.list.two
  )
  
  #adding site names
  beta$site.one &amp;lt;- site.one
  beta$site.two &amp;lt;- site.two
  
  #returning output
  beta
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;a&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;b&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;c&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;bsor&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;bsim&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;site.one&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;site.two&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0.545&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0.4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now that we know it works, we can put everything together in a function. Notice that to make the function more general, I have added arguments requesting the names of the columns with the site and the taxa names.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;betadiversity_multisite &amp;lt;- function(
  x, 
  site.column, #column with site names
  taxa.column #column with taxa names
){
  
  #get site combinations
  site.combinations &amp;lt;- utils::combn(
    x = unique(x[, site.column]),
    m = 2
  )
  
  #iterating through site pairs
  betadiversity.df &amp;lt;- foreach::foreach(
    i = 1:ncol(site.combinations),
    .combine = &#39;rbind&#39;
  ) %do% {
    
    #site names
    site.one &amp;lt;- site.combinations[1, i]
    site.two &amp;lt;- site.combinations[2, i]
    
    #getting taxa lists
    taxa.list.one &amp;lt;- x[x[, site.column] %in% site.one, taxa.column]
    taxa.list.two &amp;lt;- x[x[, site.column] %in% site.two, taxa.column]
    
    #betadiversity
    beta &amp;lt;- betadiversity(
      x = taxa.list.one,
      y = taxa.list.two
    )
    
    #adding site names
    beta$site.one &amp;lt;- site.one
    beta$site.two &amp;lt;- site.two
    
    #returning output
    beta
    
  }
  
  #remove bad rownames
  rownames(betadiversity.df) &amp;lt;- NULL
  
  #reordering columns
  betadiversity.df &amp;lt;- betadiversity.df[, c(
    &amp;quot;site.one&amp;quot;,
    &amp;quot;site.two&amp;quot;,
    &amp;quot;a&amp;quot;,
    &amp;quot;b&amp;quot;,
    &amp;quot;c&amp;quot;,
    &amp;quot;bsor&amp;quot;,
    &amp;quot;bsim&amp;quot;
    )]
  
  #returning output
  return(betadiversity.df)
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the test!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sites.betadiversity &amp;lt;- betadiversity_multisite(
  x = sites, 
  site.column = &amp;quot;site&amp;quot;,
  taxa.column = &amp;quot;taxon&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;site.one&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;site.two&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;a&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;b&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;c&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;bsor&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;bsim&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0.545&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0.4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;That went well!&lt;/p&gt;
&lt;p&gt;Finally, to have these functions available in my R session I always put them all in a single file in the same folder where my Rstudio project lives, name it something like &lt;code&gt;functions_betadiversity.R&lt;/code&gt;, and source it at the beginning of my script or .Rmd file by running a line like the one below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;source(&amp;quot;functions_betadiversity.R&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have placed the file &lt;code&gt;functions_betadiversity.R&lt;/code&gt; in 
&lt;a href=&#34;https://gist.github.com/BlasBenito/4c3740b056a0c9bb3602f33dfd35990c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this GitHub Gist&lt;/a&gt; in case you want to give it a look. You can also source it right away to your R environment by executing the following line:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;source(&amp;quot;https://gist.githubusercontent.com/BlasBenito/4c3740b056a0c9bb3602f33dfd35990c/raw/bbb40d868787fc5d10e391a2121045eb5d75f165/functions_betadiversity.R&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I hope this post helped you to better understand how to write and organize R functions!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Setup of a shared folder in a home cluster</title>
      <link>https://blasbenito.com/post_projects/template/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post_projects/template/</guid>
      <description>&lt;p&gt;In the previous posts I have covered how to 
&lt;a href=&#34;https://www.blasbenito.com/post/01_home_cluster/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;setup a home cluster&lt;/a&gt;, and how to 
&lt;a href=&#34;https://www.blasbenito.com/post/02_parallelizing_loops_with_r/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;run parallel processes with &lt;code&gt;foreach&lt;/code&gt; in R&lt;/a&gt;. However, so far I haven&amp;rsquo;t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.&lt;/p&gt;
&lt;p&gt;This post (developed on Ubuntu and Ubuntu Server 20.04) assumes that the home cluster runs on Linux, and covers the following topics.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basics of the Network File System protocol (NFS).&lt;/li&gt;
&lt;li&gt;Setup of an NFS folder in a home cluster.&lt;/li&gt;
&lt;li&gt;Using an NFS folder in a parallelized loop.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;the-network-file-system-protocol-nfs&#34;&gt;The Network File System protocol (NFS)&lt;/h2&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Network_File_System&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Network File System&lt;/a&gt; protocol offers the means for a &lt;em&gt;host&lt;/em&gt; computer to allow other computers in the network (&lt;em&gt;clients&lt;/em&gt;) to read and write the contents of a given folder. The clients access such folder like if it was a local one, while in fact it is a &lt;em&gt;reference&lt;/em&gt; to the one in the host computer.&lt;/p&gt;
&lt;p&gt;The image at the beginning of the post illustrates the concept. There is a &lt;em&gt;host&lt;/em&gt; computer with a folder in the path &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; (were &lt;code&gt;user&lt;/code&gt; is your user name) that is broadcasted to the network, and there are one or several &lt;em&gt;clients&lt;/em&gt; that are mounting &lt;em&gt;mounting&lt;/em&gt; (making accessible) the same folder in their local paths &lt;code&gt;/home/user/cluster_shared&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If the host writes a file to the shared folder, it is available right away for the clients, and the other way around. At the end, the idea is to have a folder shared among all computers in the cluster, while having the same exact path on each one of them to write or read files from such shared folder.&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;setup-of-an-nfs-folder-in-a-home-cluster&#34;&gt;Setup of an NFS folder in a home cluster&lt;/h2&gt;
&lt;p&gt;To setup the shared folder we&amp;rsquo;ll need to do some things in the &lt;em&gt;host&lt;/em&gt;, and some things in the &lt;em&gt;clients&lt;/em&gt;. Let&amp;rsquo;s start with the host.&lt;/p&gt;
&lt;h4 id=&#34;preparing-the-host-computer&#34;&gt;Preparing the host computer&lt;/h4&gt;
&lt;p&gt;First we need to install the &lt;code&gt;nfs-kernel-server&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt update
sudo apt install nfs-kernel-server
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create the shared folder. Remember to replace &lt;code&gt;user&lt;/code&gt; with your user name, and &lt;code&gt;cluster_shared&lt;/code&gt; with the actual folder name you want to use.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir /home/user/cluster_shared
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To broadcast it we need to open the file &lt;code&gt;/etc/exports&lt;/code&gt;&amp;hellip;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo gedit /etc/exports
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;hellip; and add the following line&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/home/user/cluster_shared IP_CLIENT1(rw,no_subtree_check) IP_CLIENT2(rw,no_subtree_check) IP_CLIENT3(rw,no_subtree_check)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/home/user/cluster_shared&lt;/code&gt; is the path of the shared folder.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IP_CLIENTx&lt;/code&gt; are the IPs of each one of the clients.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rw&lt;/code&gt; gives reading and writing permission on the shared folder to the given client.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;no_subtree_check&lt;/code&gt; prevents the host from checking the complete tree of shares before attending a request (read or write) by a client.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, the last line of my &lt;code&gt;/etc/exports&lt;/code&gt; file looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/home/blas/cluster_shared 10.42.0.34(rw,async,no_subtree_check) 10.42.0.104(rw,async,no_subtree_check)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Save the file, and to make the changes effective, execute:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo exportfs -ra
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To end preparing the host we have to update the firewall rules to allow nfs connections from the clients. Notice that one rule per client needs to be defined, using the clients IPs to identify them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ufw allow from IP_CLIENT1 to any port nfs
sudo ufw allow from IP_CLIENT2 to any port nfs
sudo ufw status
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;preparing-the-clients&#34;&gt;Preparing the clients&lt;/h4&gt;
&lt;p&gt;First we have to install the Linux package &lt;code&gt;nfs-common&lt;/code&gt; on each client.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt update
sudp apt install nfs-common
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create a folder in the clients and use it to mount the NFS folder of the host.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p /home/user/cluster_shared
sudo mount IP_HOST:/home/user/cluster_shared /home/user/cluster_shared
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second line of code is mounting the folder &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; of the host in the folder &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; of the client.&lt;/p&gt;
&lt;p&gt;To make the mount permanent, we have to open &lt;code&gt;/etc/fstab&lt;/code&gt; with super-user privilege in the clients&amp;hellip;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo gedit /etc/fstab
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;hellip; and add the line&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;IP_HOST:/home/user/cluster_shared /home/user/cluster_shared   nfs     defaults 0 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember to replace &lt;code&gt;IP_HOST&lt;/code&gt; and &lt;code&gt;user&lt;/code&gt; with the right values!&lt;/p&gt;
&lt;p&gt;Now we can test that the shared folder works as intended by writing one file into it from each computer in the network using the code below in the command line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd cluster_shared
touch filename.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;terminator.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once the files are created, we can check they are visible from each computer using the &lt;code&gt;ls&lt;/code&gt; command.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;terminator2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;using-an-nfs-folder-in-a-parallelized-loop&#34;&gt;Using an NFS folder in a parallelized loop&lt;/h2&gt;
&lt;p&gt;In a 
&lt;a href=&#34;https://www.blasbenito.com/post/02_parallelizing_loops_with_r/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt; I described how to run parallelized tasks with &lt;code&gt;foreach&lt;/code&gt; in R. This section intends to complete that post by showing how a shared NFS folder can be used to store input and output files during the execution of a parallelized loop&lt;/p&gt;
&lt;h3 id=&#34;the-task&#34;&gt;The task&lt;/h3&gt;
&lt;p&gt;In this hypothetical example we have a large number of data frames stored in &lt;code&gt;/home/user/cluster_shared/input&lt;/code&gt;. Each data frame has the same predictors &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;, and a different response variable, named &lt;code&gt;y1&lt;/code&gt; for the data frame &lt;code&gt;y1&lt;/code&gt;, &lt;code&gt;y2&lt;/code&gt; for the data frame &lt;code&gt;y2&lt;/code&gt;, and so on. In the example we will be using 100 data frames, but there could be thousands. This example would be valid as well for any other kind of object, such as raster files, time-series, etc etc.&lt;/p&gt;
&lt;p&gt;First we have to load the libraries we&amp;rsquo;ll be using.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#automatic install of packages if they are not installed already
list.of.packages &amp;lt;- c(
  &amp;quot;foreach&amp;quot;,
  &amp;quot;doParallel&amp;quot;,
  &amp;quot;ranger&amp;quot;
  )

new.packages &amp;lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&amp;quot;Package&amp;quot;])]

if(length(new.packages) &amp;gt; 0){
  install.packages(new.packages, dep=TRUE)
}

#loading packages
for(package.i in list.of.packages){
  suppressPackageStartupMessages(
    library(
      package.i, 
      character.only = TRUE
      )
    )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code chunk below generates the folder &lt;code&gt;/home/user/cluster_shared/input&lt;/code&gt; and populates it with the dummy files.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#creating the input folder
input.folder &amp;lt;- &amp;quot;/home/blas/cluster_shared/input&amp;quot;
dir.create(input.folder)

#data frame names
df.names &amp;lt;- paste0(&amp;quot;y&amp;quot;, 1:100)

#filling it with files
for(i in df.names){
  
  #creating the df
  df.i &amp;lt;- data.frame(
    y = rnorm(1000),
    a = rnorm(1000),
    b = rnorm(1000),
    c = rnorm(1000),
    d = rnorm(1000)
  )
  
  #changing name of the response variable
  colnames(df.i)[1] &amp;lt;- i
  
  #assign to a variable with name i
  assign(i, df.i)
  
  #saving the object
  save(
    list = i,
    file = paste0(input.folder, &amp;quot;/&amp;quot;, i, &amp;quot;.RData&amp;quot;)
  )
  
  #removing the generated data frame form the environment
  rm(list = i, df.i, i)
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our target now will be to fit one &lt;code&gt;ranger::ranger()&lt;/code&gt; model per data frame stored in &lt;code&gt;/home/blas/cluster_shared/input&lt;/code&gt;, save the model result to a folder with the path &lt;code&gt;/home/blas/cluster_shared/input&lt;/code&gt;, and write a small summary of the model to the output of &lt;code&gt;foreach&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Such target is based on this rationale: When executing a &lt;code&gt;foreach&lt;/code&gt; loop as in &lt;code&gt;x &amp;lt;- foreach(...) %dopar% {...}&lt;/code&gt;, the variable &lt;code&gt;x&lt;/code&gt; is going to grow in memory very fast, competing for RAM resources with the worker nodes. Furthermore, since &lt;code&gt;x&lt;/code&gt; is being written on the fly, the results would be lost if the computer crashes. When the size of the input and the output of our parallelized operation is larger than memory, we can use an NFS folder to store inputs and outputs, while keeping the RAM memory free for computational tasks only, with the positive side effect of having our outputs already stored should our computer decide to crash.&lt;/p&gt;
&lt;p&gt;Also, please notice that here I am focusing in a cluster setting, but using a folder to read and write data during a loop paralellized with &lt;code&gt;foreach&lt;/code&gt; can indeed be done in a single computer without an NFS folder. Any folder in your system will do the trick as well!&lt;/p&gt;
&lt;p&gt;So, from here, we are going to prepare the cluster, and execute a parallelized loop fitting one model per data frame that reads the inputs and writes the outputs to the shared folder.&lt;/p&gt;
&lt;h3 id=&#34;cluster-setup&#34;&gt;Cluster setup&lt;/h3&gt;
&lt;p&gt;We will also need the function I showed in the previous post to generate the cluster specification from a 
&lt;a href=&#34;https://gist.github.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Gist&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;source(&amp;quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below I use the function to create a cluster specification and initiate the cluster with &lt;code&gt;parallel::makeCluster()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#generate cluster specification
spec &amp;lt;- cluster_spec(
  ips = c(&#39;10.42.0.1&#39;, &#39;10.42.0.34&#39;, &#39;10.42.0.104&#39;),
  cores = c(7, 4, 4),
  user = &amp;quot;blas&amp;quot;
)

#define parallel port
Sys.setenv(R_PARALLEL_PORT = 11000)
Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;)

#setting up cluster
my.cluster &amp;lt;- parallel::makeCluster(
  master = &#39;10.42.0.1&#39;, 
  spec = spec,
  port = Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;),
  outfile = &amp;quot;&amp;quot;,
  homogeneous = TRUE
)

#check cluster definition (optional)
print(my.cluster)

#register cluster
doParallel::registerDoParallel(cl = my.cluster)

#check number of workers
foreach::getDoParWorkers()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;parallelized-loop&#34;&gt;Parallelized loop&lt;/h3&gt;
&lt;p&gt;For everything to work as intended, we first need to create the output folder.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;output.folder &amp;lt;- &amp;quot;/home/blas/cluster_shared/output&amp;quot;
dir.create(output.folder)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we are ready to execute the parallelized loop. Notice that I am using the output of &lt;code&gt;list.files()&lt;/code&gt; to have a vector of file names the loop will iterate over. The steps inside of the loop go as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;1.&lt;/em&gt; Remove the extension &lt;code&gt;.RData&lt;/code&gt; from the file name. We&amp;rsquo;ll later use the result to use &lt;code&gt;assign()&lt;/code&gt; on the fitted model to change its name to the same as the input file before saving it.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2.&lt;/em&gt; Read the input data frame and store in an object named &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;3.&lt;/em&gt; Fit the model with ranger, using the first column of &lt;code&gt;df&lt;/code&gt; as respose variable.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;4.&lt;/em&gt; Change the model name to the name of the input file without extension, resulting from the first step described above.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;5.&lt;/em&gt; Save the model into the output folder with the extension &lt;code&gt;.RData&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;6.&lt;/em&gt; Return a data frame with one line with the name of the response variable, the r-squared, and the importance of each predictor.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#list of input files as iterator
input.files &amp;lt;- list.files(
  path = input.folder,
  full.names = FALSE
)

modelling.summary &amp;lt;- foreach(
  input.file = input.files,
  .combine = &#39;rbind&#39;, 
  .packages = &amp;quot;ranger&amp;quot;
) %dopar% {
  
  # 1. input file name without extension
  input.file.name &amp;lt;- tools::file_path_sans_ext(input.file)
  
  # 2. read input file
  df &amp;lt;- get(load(paste0(input.folder, &amp;quot;/&amp;quot;, input.file)))
  
  # 3. fit model
  m.i &amp;lt;- ranger::ranger(
    data = df,
    dependent.variable.name = colnames(df)[1],
    importance = &amp;quot;permutation&amp;quot;
  )
  
  # 4. change name of the model to one of the response variable
  assign(input.file.name, m.i)
  
  # 5. save model
  save(
    list = input.file.name,
    file = paste0(output.folder, &amp;quot;/&amp;quot;, input.file)
  )
  
  # 6. returning summary
  return(
    data.frame(
      response.variable = input.file.name,
      r.squared = m.i$r.squared,
      importance.a = m.i$variable.importance[&amp;quot;a&amp;quot;],
      importance.b = m.i$variable.importance[&amp;quot;b&amp;quot;],
      importance.c = m.i$variable.importance[&amp;quot;c&amp;quot;],
      importance.d = m.i$variable.importance[&amp;quot;d&amp;quot;]
    )
  )
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once this parallelized loop is executed, the folder &lt;code&gt;/home/blas/cluster_shared/output&lt;/code&gt; should be filled with the results from the cluster workers, and the &lt;code&gt;modelling.summary&lt;/code&gt; data frame contains the summary of each fitted model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;output.png&#34; alt=&#34;Listing outputs in the shared folder&#34;&gt;
Now that the work is done, we can stop the cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parallel::stopCluster(cl = my.cluster)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you know how to work with data larger than memory in a parallelized loop!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Setup of a Shared Folder in a Home Cluster</title>
      <link>https://blasbenito.com/post/shared-folder-beowulf-cluster/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/shared-folder-beowulf-cluster/</guid>
      <description>&lt;p&gt;In the previous posts I have covered how to 
&lt;a href=&#34;https://www.blasbenito.com/post/01_home_cluster/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;setup a home cluster&lt;/a&gt;, and how to 
&lt;a href=&#34;https://www.blasbenito.com/post/02_parallelizing_loops_with_r/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;run parallel processes with &lt;code&gt;foreach&lt;/code&gt; in R&lt;/a&gt;. However, so far I haven&amp;rsquo;t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.&lt;/p&gt;
&lt;p&gt;This post (developed on Ubuntu and Ubuntu Server 20.04) assumes that the home cluster runs on Linux, and covers the following topics.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basics of the Network File System protocol (NFS).&lt;/li&gt;
&lt;li&gt;Setup of an NFS folder in a home cluster.&lt;/li&gt;
&lt;li&gt;Using an NFS folder in a parallelized loop.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;the-network-file-system-protocol-nfs&#34;&gt;The Network File System protocol (NFS)&lt;/h2&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Network_File_System&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Network File System&lt;/a&gt; protocol offers the means for a &lt;em&gt;host&lt;/em&gt; computer to allow other computers in the network (&lt;em&gt;clients&lt;/em&gt;) to read and write the contents of a given folder. The clients access such folder like if it was a local one, while in fact it is a &lt;em&gt;reference&lt;/em&gt; to the one in the host computer.&lt;/p&gt;
&lt;p&gt;The image at the beginning of the post illustrates the concept. There is a &lt;em&gt;host&lt;/em&gt; computer with a folder in the path &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; (were &lt;code&gt;user&lt;/code&gt; is your user name) that is broadcasted to the network, and there are one or several &lt;em&gt;clients&lt;/em&gt; that are mounting &lt;em&gt;mounting&lt;/em&gt; (making accessible) the same folder in their local paths &lt;code&gt;/home/user/cluster_shared&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If the host writes a file to the shared folder, it is available right away for the clients, and the other way around. At the end, the idea is to have a folder shared among all computers in the cluster, while having the same exact path on each one of them to write or read files from such shared folder.&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;setup-of-an-nfs-folder-in-a-home-cluster&#34;&gt;Setup of an NFS folder in a home cluster&lt;/h2&gt;
&lt;p&gt;To setup the shared folder we&amp;rsquo;ll need to do some things in the &lt;em&gt;host&lt;/em&gt;, and some things in the &lt;em&gt;clients&lt;/em&gt;. Let&amp;rsquo;s start with the host.&lt;/p&gt;
&lt;h4 id=&#34;preparing-the-host-computer&#34;&gt;Preparing the host computer&lt;/h4&gt;
&lt;p&gt;First we need to install the &lt;code&gt;nfs-kernel-server&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt update
sudo apt install nfs-kernel-server
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create the shared folder. Remember to replace &lt;code&gt;user&lt;/code&gt; with your user name, and &lt;code&gt;cluster_shared&lt;/code&gt; with the actual folder name you want to use.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir /home/user/cluster_shared
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To broadcast it we need to open the file &lt;code&gt;/etc/exports&lt;/code&gt;&amp;hellip;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo gedit /etc/exports
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;hellip; and add the following line&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/home/user/cluster_shared IP_CLIENT1(rw,no_subtree_check) IP_CLIENT2(rw,no_subtree_check) IP_CLIENT3(rw,no_subtree_check)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/home/user/cluster_shared&lt;/code&gt; is the path of the shared folder.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IP_CLIENTx&lt;/code&gt; are the IPs of each one of the clients.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rw&lt;/code&gt; gives reading and writing permission on the shared folder to the given client.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;no_subtree_check&lt;/code&gt; prevents the host from checking the complete tree of shares before attending a request (read or write) by a client.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, the last line of my &lt;code&gt;/etc/exports&lt;/code&gt; file looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/home/blas/cluster_shared 10.42.0.34(rw,async,no_subtree_check) 10.42.0.104(rw,async,no_subtree_check)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Save the file, and to make the changes effective, execute:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo exportfs -ra
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To end preparing the host we have to update the firewall rules to allow nfs connections from the clients. Notice that one rule per client needs to be defined, using the clients IPs to identify them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ufw allow from IP_CLIENT1 to any port nfs
sudo ufw allow from IP_CLIENT2 to any port nfs
sudo ufw status
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;preparing-the-clients&#34;&gt;Preparing the clients&lt;/h4&gt;
&lt;p&gt;First we have to install the Linux package &lt;code&gt;nfs-common&lt;/code&gt; on each client.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt update
sudp apt install nfs-common
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create a folder in the clients and use it to mount the NFS folder of the host.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p /home/user/cluster_shared
sudo mount IP_HOST:/home/user/cluster_shared /home/user/cluster_shared
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second line of code is mounting the folder &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; of the host in the folder &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; of the client.&lt;/p&gt;
&lt;p&gt;To make the mount permanent, we have to open &lt;code&gt;/etc/fstab&lt;/code&gt; with super-user privilege in the clients&amp;hellip;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo gedit /etc/fstab
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;hellip; and add the line&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;IP_HOST:/home/user/cluster_shared /home/user/cluster_shared   nfs     defaults 0 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember to replace &lt;code&gt;IP_HOST&lt;/code&gt; and &lt;code&gt;user&lt;/code&gt; with the right values!&lt;/p&gt;
&lt;p&gt;Now we can test that the shared folder works as intended by writing one file into it from each computer in the network using the code below in the command line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd cluster_shared
touch filename.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;terminator.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once the files are created, we can check they are visible from each computer using the &lt;code&gt;ls&lt;/code&gt; command.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;terminator2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;using-an-nfs-folder-in-a-parallelized-loop&#34;&gt;Using an NFS folder in a parallelized loop&lt;/h2&gt;
&lt;p&gt;In a 
&lt;a href=&#34;https://www.blasbenito.com/post/02_parallelizing_loops_with_r/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt; I described how to run parallelized tasks with &lt;code&gt;foreach&lt;/code&gt; in R. This section intends to complete that post by showing how a shared NFS folder can be used to store input and output files during the execution of a parallelized loop&lt;/p&gt;
&lt;h3 id=&#34;the-task&#34;&gt;The task&lt;/h3&gt;
&lt;p&gt;In this hypothetical example we have a large number of data frames stored in &lt;code&gt;/home/user/cluster_shared/input&lt;/code&gt;. Each data frame has the same predictors &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;, and a different response variable, named &lt;code&gt;y1&lt;/code&gt; for the data frame &lt;code&gt;y1&lt;/code&gt;, &lt;code&gt;y2&lt;/code&gt; for the data frame &lt;code&gt;y2&lt;/code&gt;, and so on. In the example we will be using 100 data frames, but there could be thousands. This example would be valid as well for any other kind of object, such as raster files, time-series, etc etc.&lt;/p&gt;
&lt;p&gt;First we have to load the libraries we&amp;rsquo;ll be using.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#automatic install of packages if they are not installed already
list.of.packages &amp;lt;- c(
  &amp;quot;foreach&amp;quot;,
  &amp;quot;doParallel&amp;quot;,
  &amp;quot;ranger&amp;quot;
  )

new.packages &amp;lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&amp;quot;Package&amp;quot;])]

if(length(new.packages) &amp;gt; 0){
  install.packages(new.packages, dep=TRUE)
}

#loading packages
for(package.i in list.of.packages){
  suppressPackageStartupMessages(
    library(
      package.i, 
      character.only = TRUE
      )
    )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code chunk below generates the folder &lt;code&gt;/home/user/cluster_shared/input&lt;/code&gt; and populates it with the dummy files.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#creating the input folder
input.folder &amp;lt;- &amp;quot;/home/blas/cluster_shared/input&amp;quot;
dir.create(input.folder)

#data frame names
df.names &amp;lt;- paste0(&amp;quot;y&amp;quot;, 1:100)

#filling it with files
for(i in df.names){
  
  #creating the df
  df.i &amp;lt;- data.frame(
    y = rnorm(1000),
    a = rnorm(1000),
    b = rnorm(1000),
    c = rnorm(1000),
    d = rnorm(1000)
  )
  
  #changing name of the response variable
  colnames(df.i)[1] &amp;lt;- i
  
  #assign to a variable with name i
  assign(i, df.i)
  
  #saving the object
  save(
    list = i,
    file = paste0(input.folder, &amp;quot;/&amp;quot;, i, &amp;quot;.RData&amp;quot;)
  )
  
  #removing the generated data frame form the environment
  rm(list = i, df.i, i)
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our target now will be to fit one &lt;code&gt;ranger::ranger()&lt;/code&gt; model per data frame stored in &lt;code&gt;/home/blas/cluster_shared/input&lt;/code&gt;, save the model result to a folder with the path &lt;code&gt;/home/blas/cluster_shared/input&lt;/code&gt;, and write a small summary of the model to the output of &lt;code&gt;foreach&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Such target is based on this rationale: When executing a &lt;code&gt;foreach&lt;/code&gt; loop as in &lt;code&gt;x &amp;lt;- foreach(...) %dopar% {...}&lt;/code&gt;, the variable &lt;code&gt;x&lt;/code&gt; is going to grow in memory very fast, competing for RAM resources with the worker nodes. Furthermore, since &lt;code&gt;x&lt;/code&gt; is being written on the fly, the results would be lost if the computer crashes. When the size of the input and the output of our parallelized operation is larger than memory, we can use an NFS folder to store inputs and outputs, while keeping the RAM memory free for computational tasks only, with the positive side effect of having our outputs already stored should our computer decide to crash.&lt;/p&gt;
&lt;p&gt;Also, please notice that here I am focusing in a cluster setting, but using a folder to read and write data during a loop paralellized with &lt;code&gt;foreach&lt;/code&gt; can indeed be done in a single computer without an NFS folder. Any folder in your system will do the trick as well!&lt;/p&gt;
&lt;p&gt;So, from here, we are going to prepare the cluster, and execute a parallelized loop fitting one model per data frame that reads the inputs and writes the outputs to the shared folder.&lt;/p&gt;
&lt;h3 id=&#34;cluster-setup&#34;&gt;Cluster setup&lt;/h3&gt;
&lt;p&gt;We will also need the function I showed in the previous post to generate the cluster specification from a 
&lt;a href=&#34;https://gist.github.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Gist&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;source(&amp;quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below I use the function to create a cluster specification and initiate the cluster with &lt;code&gt;parallel::makeCluster()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#generate cluster specification
spec &amp;lt;- cluster_spec(
  ips = c(&#39;10.42.0.1&#39;, &#39;10.42.0.34&#39;, &#39;10.42.0.104&#39;),
  cores = c(7, 4, 4),
  user = &amp;quot;blas&amp;quot;
)

#define parallel port
Sys.setenv(R_PARALLEL_PORT = 11000)
Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;)

#setting up cluster
my.cluster &amp;lt;- parallel::makeCluster(
  master = &#39;10.42.0.1&#39;, 
  spec = spec,
  port = Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;),
  outfile = &amp;quot;&amp;quot;,
  homogeneous = TRUE
)

#check cluster definition (optional)
print(my.cluster)

#register cluster
doParallel::registerDoParallel(cl = my.cluster)

#check number of workers
foreach::getDoParWorkers()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;parallelized-loop&#34;&gt;Parallelized loop&lt;/h3&gt;
&lt;p&gt;For everything to work as intended, we first need to create the output folder.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;output.folder &amp;lt;- &amp;quot;/home/blas/cluster_shared/output&amp;quot;
dir.create(output.folder)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we are ready to execute the parallelized loop. Notice that I am using the output of &lt;code&gt;list.files()&lt;/code&gt; to have a vector of file names the loop will iterate over. The steps inside of the loop go as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;1.&lt;/em&gt; Remove the extension &lt;code&gt;.RData&lt;/code&gt; from the file name. We&amp;rsquo;ll later use the result to use &lt;code&gt;assign()&lt;/code&gt; on the fitted model to change its name to the same as the input file before saving it.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2.&lt;/em&gt; Read the input data frame and store in an object named &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;3.&lt;/em&gt; Fit the model with ranger, using the first column of &lt;code&gt;df&lt;/code&gt; as respose variable.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;4.&lt;/em&gt; Change the model name to the name of the input file without extension, resulting from the first step described above.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;5.&lt;/em&gt; Save the model into the output folder with the extension &lt;code&gt;.RData&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;6.&lt;/em&gt; Return a data frame with one line with the name of the response variable, the r-squared, and the importance of each predictor.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#list of input files as iterator
input.files &amp;lt;- list.files(
  path = input.folder,
  full.names = FALSE
)

modelling.summary &amp;lt;- foreach(
  input.file = input.files,
  .combine = &#39;rbind&#39;, 
  .packages = &amp;quot;ranger&amp;quot;
) %dopar% {
  
  # 1. input file name without extension
  input.file.name &amp;lt;- tools::file_path_sans_ext(input.file)
  
  # 2. read input file
  df &amp;lt;- get(load(paste0(input.folder, &amp;quot;/&amp;quot;, input.file)))
  
  # 3. fit model
  m.i &amp;lt;- ranger::ranger(
    data = df,
    dependent.variable.name = colnames(df)[1],
    importance = &amp;quot;permutation&amp;quot;
  )
  
  # 4. change name of the model to one of the response variable
  assign(input.file.name, m.i)
  
  # 5. save model
  save(
    list = input.file.name,
    file = paste0(output.folder, &amp;quot;/&amp;quot;, input.file)
  )
  
  # 6. returning summary
  return(
    data.frame(
      response.variable = input.file.name,
      r.squared = m.i$r.squared,
      importance.a = m.i$variable.importance[&amp;quot;a&amp;quot;],
      importance.b = m.i$variable.importance[&amp;quot;b&amp;quot;],
      importance.c = m.i$variable.importance[&amp;quot;c&amp;quot;],
      importance.d = m.i$variable.importance[&amp;quot;d&amp;quot;]
    )
  )
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once this parallelized loop is executed, the folder &lt;code&gt;/home/blas/cluster_shared/output&lt;/code&gt; should be filled with the results from the cluster workers, and the &lt;code&gt;modelling.summary&lt;/code&gt; data frame contains the summary of each fitted model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;output.png&#34; alt=&#34;Listing outputs in the shared folder&#34;&gt;
Now that the work is done, we can stop the cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parallel::stopCluster(cl = my.cluster)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you know how to work with data larger than memory in a parallelized loop!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Palaeo fire modeling</title>
      <link>https://blasbenito.com/project/palaeo_fire_modeling/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/project/palaeo_fire_modeling/</guid>
      <description>&lt;p&gt;This is a spatio-temporal simulation of the effect of fire regimes on the population dynamics of five forest species (Pinus sylvestris, Pinus uncinata, Betula pendula, Corylus avellana, and Quercus petraea) during the Lateglacial-Holocene transition (15-7 cal Kyr BP) at El Portalet, a subalpine bog located in the central Pyrenees region (1802m asl, Spain), that has served for palaeoenvironmental studies (GonzÃ¡lez-SmapÃ©riz et al. 2006; Gil-Romera et al., 2014). This model is described in the paper in prep. titled &lt;em&gt;Forest - fire interactions in the Central Pyrenees: a data-model comparison for the Lateglacial-Holocene transition&lt;/em&gt;, and authored by Graciela Gil-Romera, Blas M. Benito, Juli G. Pausas, PenÃ©lope GonzÃ¡lez-SampÃ©riz, J. Julio. Camarero, Jens-Christian Svenning, and Blas Valero-GarcÃ©s.&lt;/p&gt;
&lt;h3 id=&#34;how-does-it-work&#34;&gt;HOW DOES IT WORK&lt;/h3&gt;
&lt;h4 id=&#34;abiotic-component&#34;&gt;Abiotic component&lt;/h4&gt;
&lt;p&gt;The abiotic layer of the model is represented by three main environmental factors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Topography&lt;/strong&gt; derived from a digital elevation model at 200 x 200 meters resolution. Slope (along temperature) is used to impose restrictions to species distributions. Northness (in the range [0, 1]) is used to restrict fire spread. Aspect is used to draw a shaded relief map (at the user&amp;rsquo;s request). Elevation is used to compute a lapse rate map (see below).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Temperature&lt;/strong&gt; (average of montly minimum temperatures) time series for the study area computed from palaeoclimatic data at annual resolution provided by the 
&lt;a href=&#34;http://www.cgd.ucar.edu/ccr/TraCE/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TraCe simulation&lt;/a&gt;, a transient model for the global climate evolution of the last 21K years with an annual resolution. The single temperature value of every year is converted into a temperature map (200 x 200 m resolution) using a lapse rate map based on the elevation map. Temperature, along with slope, is used to compute habitat suitability by using a logistic equation. Habitat suitability affects plant growth and survival.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fire&lt;/strong&gt;: The charcoal accumulation rate record (CHAR) from El Portalet palaeoenvironmental sequence (Gil-Romera et al., 2014) is used as input to simulate forest fires. A value of this time series is read each year, and a random number in the range [0, 1] is generated. If the random number is lower than the &lt;em&gt;Fire-probability-per-year&lt;/em&gt; (FPY) parameter defined by the user, the value from the charcoal time series is multiplied by the parameter &lt;em&gt;Number-ignitions-per-fire-event&lt;/em&gt; (NIF) (defined by the user) to compute the number of ignitions for the given year. As many adult tree as ignitions are selected to start spreading fire. Fire spreads to a neighbor patch if there is an adult tree in there, and a random number in the range [0, 1] is higher than the northness value of the patch.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;biotic-component&#34;&gt;Biotic component&lt;/h4&gt;
&lt;p&gt;The biotic layer of the model is composed by five tree species. We have introduced the following elements to represent their ecological dynamics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Topoclimatic niche&lt;/strong&gt;, inferred from their present day distributions and high resolution temperature maps (presence data taken from GBIF, temperature maps taken from Worldclim and the Digital Climatic Atlas of the Iberian Peninsula). The ecological niche is represented by a logistic equation (see below). The results of this equation plus the dispersal dynamics of each species defines changes in distribution over time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Population dynamics&lt;/strong&gt;, driven by species traits such as dispersal distance, longevity, fecundity, mortality, growth rate, post-fire response to fire, and heliophity (competition for light). The data is based on the literature and/or expert opinion from forest and fire ecologists, and it is used to simulate growth (using logistic equations), competition for light and space, decay due to senescence, and mortality due to climate, fire, or plagues.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model doesn&amp;rsquo;t simulate the entire populations of the target species. Instead, on each 200 x 200 meters patch it simulates the dynamics of an small forest plot (around 10 x 10 meters) where a maximum of one individual per species can exist.&lt;/p&gt;
&lt;h4 id=&#34;model-dynamics&#34;&gt;Model dynamics&lt;/h4&gt;
&lt;iframe src=&#34;https://player.vimeo.com/video/274750111&#34; width=&#34;640&#34; height=&#34;424&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; fullscreen&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&#34;https://vimeo.com/274750111&#34;&gt;Let&amp;rsquo;s burn it! Simulating fire-vegetation dynamics at millennial timescales in the central Pyrenees.&lt;/a&gt; from &lt;a href=&#34;https://vimeo.com/blasbenito&#34;&gt;blas benito&lt;/a&gt; on &lt;a href=&#34;https://vimeo.com&#34;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The life of an individual&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;During the model setup seeds of every species are created on every patch. From there, every seed will go through the following steps every simulated year:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Its age increases by one year, and its life-stage is changed to &amp;ldquo;seedling&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The minimum average temperature of its patch is updated.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The individual computes its habitat suitability using the logistic equation &lt;em&gt;1 / ( 1 + exp( -(intercept + coefficient * patch-temperature)))&lt;/em&gt;, where the &lt;em&gt;intercept&lt;/em&gt; and the &lt;em&gt;coefficient&lt;/em&gt; are user defined. These parameters are hardcoded to save space in the GUI, and have been computed beforehand by using current presence data and temperature maps.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If habitat suitability is higher than a random number in the range [0, 1], the habitat is considered suitable (NOTE: this random number is defined for the patch, and it changes every ~10 years following a random walk drawn from a normal distribution with the average set to the previous value, and a standard deviation of 0.001).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If it is lower, the habitat is considered unsuitable, and the number of years under unsuitable habitat is increased by 1.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the number of years unders unsuitable habitat becomes higher than &lt;em&gt;seedling-tolerance&lt;/em&gt;, the seedling dies, and another seed from the seed bank takes its place. Otherwise it stays alive.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mortality: If a random number in the range [0, 1] is lower than the seedling mortality of the species the plant dies, and it is replaced by a seed from the seed bank. Otherwise it stays alive.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Competition and growth:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If the patch total biomass of the individuals in the patch equals &lt;em&gt;Max-biomass-per-patch&lt;/em&gt;, the individual loses an amount of biomass between 0 and the 20% of its current biomass. This number is randomly selected.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If &lt;em&gt;Max-biomass-per-patch&lt;/em&gt; has not been reached yet:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;An &lt;em&gt;interaction term&lt;/em&gt; is computed as &lt;em&gt;(1 - (biomass of other individuals in the patch / Max-biomass-per-patch)) * (1 - heliophilia))&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The interaction term is introduced in the growth equation &lt;em&gt;max-biomass / (1 + max-biomass * exp(- growth-rate * interaction-term * habitat-suitability * age))&lt;/em&gt; to compute the current biomass of the individual. The lower the interaction term and habitat suitability are, the lower the growth becomes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If a fire reaches the patch and there are adult individuals of other species on it, the plant dies, and it is replaced by a seed (this seed inherites the traits of the parent).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These steps continue while the individual is still a seedling, but once it reaches its maturity some steps become slightly different:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If a random number in the range [0, 1] is lower than the adults mortality of the species, or the maximum age of the species is reached, the individual is marked for decay. The current biomass of decaying individuals is computed as &lt;em&gt;previous-biomass - years-of-decay&lt;/em&gt;. To add the effect of climatic variability to this decreasing function, its result is multiplied by &lt;em&gt;1 - habitat-suitability x random[0, 10]&lt;/em&gt;. If the biomass is higher than zero, pollen productivity is computed as &lt;em&gt;current-biomass x species-pollen-productivity&lt;/em&gt;. The individual dies and is replaced by a seed when the biomass is below 1.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dispersal: If the individual is in suitable habitat, a seed from it is placed in one of the neighboring patches within a radius given by the dispersal distance of the species (which is measured in &amp;ldquo;number of patches&amp;rdquo; and hardcoded) with no individuals of the same species.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the individual starts a fire, or if fire spreads in from neighboring patches, it is marked as &amp;ldquo;burned&amp;rdquo;, spreads fire to its neighbors, dies, and is replaced by a seed. If the individual belongs to an species with post-fire resprouting, the growth-rate of the seed is multiplied by 2 to boost growth after fire.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Simulating pollen and charcoal deposition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The user defines the radius of a catchment area round the core location (10 km by default, that is 50 patches). All patches within this radius define the RSAP (relevant source area of pollen).&lt;/p&gt;
&lt;p&gt;At the end of every simulated year the pollen productivity of every adult of each species within the RSAP is summed, and this value is used to compose the simulated pollen curves. The same is done with the biomass of the burned individuals to compose the virtual charcoal curve.&lt;/p&gt;
&lt;h4 id=&#34;output&#34;&gt;Output&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;In GUI&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The simulation GUI shows the following results in real time:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Plots of the input values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Minimum Temperature of the coldest month.&lt;/li&gt;
&lt;li&gt;Real charcoal data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simulated pollen curves for the target taxa.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simulated charcoal curve.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Map showing the distribution of every species and the forest fires.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Written to disk&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The simulated pollen counts and charcoal is exported to the path defined by the user as a table in csv format named &lt;strong&gt;output_table.csv&lt;/strong&gt;. It contains one row per simulated year and the following columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;age: simulated year.&lt;/li&gt;
&lt;li&gt;temperature_minimum_average: average minimum winter temperature of the study area.&lt;/li&gt;
&lt;li&gt;pollen_Psylvestris: pollen sum for Pinus sylvestris.&lt;/li&gt;
&lt;li&gt;pollen_Puncinata&lt;/li&gt;
&lt;li&gt;pollen_Bpendula&lt;/li&gt;
&lt;li&gt;pollen_Cavellana&lt;/li&gt;
&lt;li&gt;pollen_Qpetraea&lt;/li&gt;
&lt;li&gt;real_charcoal: real charcoal values from El Portalet core.&lt;/li&gt;
&lt;li&gt;ignitions: number of fire ignitions.&lt;/li&gt;
&lt;li&gt;charcoal_sum: biomass sum of all burned individuals.&lt;/li&gt;
&lt;li&gt;charcoal_Psylvestris: sum of the biomass of burned individuals of Pinus sylvestris.&lt;/li&gt;
&lt;li&gt;charcoal_Puncinata&lt;/li&gt;
&lt;li&gt;charcoal_Bpendula&lt;/li&gt;
&lt;li&gt;charcoal_Cavellana&lt;/li&gt;
&lt;li&gt;charcoal_Qpetraea&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Snapshots of the simulation map taken at 1 or 10 years intervals are stored in the output folder is requested by the user. These snapshots are useful to compose a video of the simulation.&lt;/p&gt;
&lt;h3 id=&#34;how-to-use-it&#34;&gt;HOW TO USE IT&lt;/h3&gt;
&lt;h4 id=&#34;input-files&#34;&gt;Input files&lt;/h4&gt;
&lt;p&gt;Input files are stored in a folder named &amp;ldquo;data&amp;rdquo;. These are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;age&lt;/strong&gt;: text file with no extension and a single column with no header containing age values from -15000 to -5701&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fire&lt;/strong&gt;: text file with no extension and a single column with no header containing actual charcoal counts expresed in the range [0, 1]. There are as many rows as in the &lt;strong&gt;age&lt;/strong&gt; file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;t_minimum_average&lt;/strong&gt;: text file with same features as the ones above containing minimum winter temperatures for the study area extracted from the TraCe simualtion.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;correct_t_minimum_average.asc&lt;/strong&gt;: Map at 200m resolution containing the minimum winter temperature difference (period 1970-2000) between the TraCe simulation and the Digital Climatic Atlas of the Iberian Peninsula. It is used to transform the values of &lt;strong&gt;t_minimum_average&lt;/strong&gt; into a high resolution temperature map.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;elevation.asc&lt;/strong&gt;: digital elevation model of the study area at 200m resolution, coordinate system with EPSG code 23030.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;slope.asc&lt;/strong&gt;: topographic slope.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;topography.asc&lt;/strong&gt;: shaded relief map. It is used for plotting purposes only.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;input-parameters&#34;&gt;Input parameters&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;General configuration of the simulation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The user can set-up the following parameters throught the GUI controls.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Output-path&lt;/strong&gt;: Character. Path of the output folder. This parameter cannot be empty, and the output folder must exist.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Snapshots?&lt;/strong&gt;: Boolean. If on, creates snapshots of the GUI to make videos.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Snapshots-frequency&lt;/strong&gt;: Character. Defines the frequency of snapshots. Only two options: &amp;ldquo;every year&amp;rdquo; and &amp;ldquo;every 10 years&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Draw-topography?&lt;/strong&gt;: Boolean. If on, plots a shaded relief map (stored in &lt;strong&gt;topography.asc&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RSAP-radius&lt;/strong&gt;: Numeric[5, 50]. Radius of the RSAP in number of patches. Each patch is 200 x 200 m, so an RSAP-radius of 10 equals 2 kilometres.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Randommness-settings&lt;/strong&gt;: Character. Allows to choose between &amp;ldquo;fixed seed&amp;rdquo; to obtain deterministic results, or &amp;ldquo;free seed&amp;rdquo; to obtain different results on each run.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Max-biomass-per-patch&lt;/strong&gt;: Numeric, integer. Maximum charge capacity of a patch.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fire?&lt;/strong&gt;: Boolean. If on, fires are produced whenever the data &lt;strong&gt;fire&lt;/strong&gt; triggers a fire event. If off, fires are not produced (control simulation).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fire-probability-per-year&lt;/strong&gt;: Numeric [0, 1]. Whenever the &lt;strong&gt;fire&lt;/strong&gt; file provides a number higher than 0, if a random number in the range [0, 1] is lower than &lt;strong&gt;Fire-probability-per-year&lt;/strong&gt;, a number of ignitions is computed (see below) and fires are triggered.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fire-ignitions-amplification-factor&lt;/strong&gt;: Numeric  The &lt;strong&gt;fire&lt;/strong&gt; file provides values in the range [0, 1], and this multiplication factor converts these values in an integer number of ignitions. If &lt;strong&gt;fire&lt;/strong&gt; equals one, and &lt;strong&gt;Fire-ignitions-amplification-factor&lt;/strong&gt; equals 10, the number of ignitions will be 10 for the given year.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mortality?&lt;/strong&gt;: Boolean. If on, mortality due to predation, plagues and other unpredictable sources is active (see &lt;strong&gt;Xx-seedling-mortality&lt;/strong&gt; and &lt;strong&gt;Xx-adult-mortality&lt;/strong&gt; parameters below).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Burn-in-iterations&lt;/strong&gt;: Numeric, integer. Number of years to run the model at a constant temperature (the initial one in the &lt;strong&gt;t_minimum_average&lt;/strong&gt; file) and no fires to allow the population model to reach an equilibrium before to start the actual simulation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;P.sylvestris?&lt;/strong&gt;, &lt;strong&gt;P.uncinata?&lt;/strong&gt;, &lt;strong&gt;B.pendula?&lt;/strong&gt;, &lt;strong&gt;Q.petraea?&lt;/strong&gt;, and &lt;strong&gt;C.avellana?&lt;/strong&gt;: Boolean. If off, the given species is removed from the simulation. Used for testing purposes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Species traits&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each species has a set of traits to be filled by the user. Note that a particular species can be removed from the simulation by switching it to &amp;ldquo;off&amp;rdquo;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Xx-max-age&lt;/strong&gt;: Numeric, integer. Maximum longevity. Every individual reaching this age is marked for decay.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-maturity-age&lt;/strong&gt;: Numeric, integer. Age of sexual maturity. Individuals reaching this age are considered adults.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-pollen-productivity&lt;/strong&gt;: Numeric. Multiplier of biomass to obtain a relative measure of pollen productivity among species.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-growth-rate&lt;/strong&gt;: Numeric. Growth rate of the given species.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-max-biomass&lt;/strong&gt;: Numeric, integer. Maximum biomass reachable by the given species.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-heliophilia&lt;/strong&gt;: Numeric, [0, 1]. Dependance of the species on solar light to grow. It is used to compute the effect of competence in plant growth.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-seedling-tolerance&lt;/strong&gt;: Numeric, integer. Numer of years a seedling can tolerate unsuitable climate.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-adult-tolerance&lt;/strong&gt;: Numeric, integer. Numer of years an adult can tolerate unsuitable climate.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-seedling-mortality&lt;/strong&gt;: Numeric, [0, 1]. Proportion of seedlings dying due to predation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-adult-mortality&lt;/strong&gt;: Numeric, [0, 1]. Proportion of adults dying due to plagues or other mortality sources.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-resprout-after-fire&lt;/strong&gt;: Boolean. If 0 the species doesn&amp;rsquo;t show a post-fire response. If 1, &lt;strong&gt;growth-rate&lt;/strong&gt; is multiplied by two in the resprouted individual to increase growth rate.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-min-temperature&lt;/strong&gt;: Numeric. Minimum temperature at which the species has been found using GBIF presence data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-max-temperature&lt;/strong&gt;: Numeric. Maximum temperature at which the species has been found using GBIF presence data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-min-slope&lt;/strong&gt;: Numeric. Minimum topographic slope at which the species has been found.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-max-slope&lt;/strong&gt;: Numeric. Maximum topographic slope at which the species has been found.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-intercept&lt;/strong&gt;: Numeric. Intercept of the logistic equation to compute habitat suitability fitted to presence data and minimum temperature maps.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xx-coefficient&lt;/strong&gt;: Numeric. Coefficient of the logistic equation to compute habitat suitability.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Ecological Diversity within Rear-Edge: A Case Study from Mediterranean Quercus pyrenaica Willd.</title>
      <link>https://blasbenito.com/publication/2020_perez-luque_forests/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2020_perez-luque_forests/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Range-shift simulation</title>
      <link>https://blasbenito.com/project/quercus_range_shift/</link>
      <pubDate>Thu, 31 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/project/quercus_range_shift/</guid>
      <description>&lt;p&gt;This Netlogo model simulates the dispersal of Quercus pyrenaica populations in Sierra Nevada (Spain) at a yearly resolution until 2100 while considering different levels of model complexity, from random dispersal and seedling establishment, to realistic dispersal based on the dispersal behavior of the Eurasian Jay.&lt;/p&gt;
&lt;p&gt;The data required to run the model can be downloaded from 
&lt;a href=&#34;https://www.dropbox.com/s/zsjja2g3yrgqyl1/data.zip?dl=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. It must be decompressed in the same folder containing the netlogo code of the model.&lt;/p&gt;
&lt;p&gt;The video below shows the model in action for one population of &lt;em&gt;Quercus pyrenaica&lt;/em&gt;. On the left, it shows the effect of a random dispersal model, and on the right, a realistic dispersal model based on observations of the dispersal behavior of the Eurasian Jay.&lt;/p&gt;
&lt;iframe src=&#34;https://player.vimeo.com/video/66052823&#34; width=&#34;640&#34; height=&#34;363&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; fullscreen&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Parallelized loops with R</title>
      <link>https://blasbenito.com/post/02_parallelizing_loops_with_r/</link>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/02_parallelizing_loops_with_r/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; to better follow this tutorial you can download the .Rmd file 
&lt;a href=&#34;https://www.dropbox.com/s/wsl2hcex3w0lr6u/parallelized_loops.Rmd?dl=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;from here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In 
&lt;a href=&#34;https://www.blasbenito.com/post/01_home_cluster/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a previous post&lt;/a&gt; I explained how to set up a small home cluster. Many things can be done with a cluster, and parallelizing loops is one of them. But there is no need of a cluster to parallelize loops and improve the efficiency of your coding!&lt;/p&gt;
&lt;p&gt;I believe that coding parallelized loops is an important asset for anyone working with R. That&amp;rsquo;s why this post covers the following topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Beyond &lt;code&gt;for&lt;/code&gt;: building loops with &lt;code&gt;foreach&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;What is a parallel backend?&lt;/li&gt;
&lt;li&gt;Setup of a parallel backend for a single computer.&lt;/li&gt;
&lt;li&gt;Setup for a Beowulf cluster.&lt;/li&gt;
&lt;li&gt;Practical examples.
&lt;ul&gt;
&lt;li&gt;Tuning of random forest hyperparameters.&lt;/li&gt;
&lt;li&gt;Confidence intervals of the importance scores of the predictors in random forest models.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;for-loops-are-fine-but&#34;&gt;&lt;code&gt;for&lt;/code&gt; loops are fine, but&amp;hellip;&lt;/h2&gt;
&lt;p&gt;Many experienced R users frequently say that nobody should write loops with R because they are tacky or whatever. However, I find loops easy to write, read, and debug, and are therefore my workhorse whenever I need to repeat a task and I don&amp;rsquo;t feel like using &lt;code&gt;apply()&lt;/code&gt; and the likes. However, regular &lt;code&gt;for&lt;/code&gt; loops in R are highly inefficient, because they only use one of your computer cores to perform the iterations.&lt;/p&gt;
&lt;p&gt;For example, the &lt;code&gt;for&lt;/code&gt; loop below sorts vectors of random numbers a given number of times, and will only work on one of your computer cores for a few seconds, while the others are there, procrastinating with no shame.&lt;/p&gt;
&lt;div class=&#34;tenor-gif-embed&#34; data-postid=&#34;16563810&#34; data-share-method=&#34;host&#34; data-width=&#34;60%&#34; data-aspect-ratio=&#34;1.7785714285714287&#34;&gt;&lt;a href=&#34;https://tenor.com/view/wot-cpu-danceing-break-dancing-cool-gif-16563810&#34;&gt;Wot Cpu GIF&lt;/a&gt; from &lt;a href=&#34;https://tenor.com/search/wot-gifs&#34;&gt;Wot GIFs&lt;/a&gt;&lt;/div&gt;&lt;script type=&#34;text/javascript&#34; async src=&#34;https://tenor.com/embed.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;(gif kindly suggested by 
&lt;a href=&#34;https://twitter.com/AndrosSpica&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andreas Angourakis&lt;/a&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;for(i in 1:10000){
  sort(runif(10000))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If every &lt;code&gt;i&lt;/code&gt; could run in a different core, the operation would indeed run a bit faster, and we would get rid of lazy cores. This is were packages like 
&lt;a href=&#34;https://cran.r-project.org/web/packages/foreach&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;foreach&lt;/code&gt;&lt;/a&gt; and 
&lt;a href=&#34;https://cran.r-project.org/web/packages/doParallel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;doParallel&lt;/code&gt;&lt;/a&gt; come into play. Let&amp;rsquo;s start installing these packages and a few others that will be useful throughout this tutorial.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#automatic install of packages if they are not installed already
list.of.packages &amp;lt;- c(
  &amp;quot;foreach&amp;quot;,
  &amp;quot;doParallel&amp;quot;,
  &amp;quot;ranger&amp;quot;,
  &amp;quot;palmerpenguins&amp;quot;,
  &amp;quot;tidyverse&amp;quot;,
  &amp;quot;kableExtra&amp;quot;
  )

new.packages &amp;lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&amp;quot;Package&amp;quot;])]

if(length(new.packages) &amp;gt; 0){
  install.packages(new.packages, dep=TRUE)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Installing packages into &#39;/home/blas/R/x86_64-pc-linux-gnu-library/4.4&#39;
## (as &#39;lib&#39; is unspecified)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## also installing the dependencies &#39;SQUAREM&#39;, &#39;diagram&#39;, &#39;lava&#39;, &#39;AsioHeaders&#39;, &#39;cpp11&#39;, &#39;prodlim&#39;, &#39;websocket&#39;, &#39;clock&#39;, &#39;gower&#39;, &#39;hardhat&#39;, &#39;ipred&#39;, &#39;timeDate&#39;, &#39;gargle&#39;, &#39;cellranger&#39;, &#39;ids&#39;, &#39;selectr&#39;, &#39;chromote&#39;, &#39;recipes&#39;, &#39;conflicted&#39;, &#39;dtplyr&#39;, &#39;forcats&#39;, &#39;googledrive&#39;, &#39;googlesheets4&#39;, &#39;haven&#39;, &#39;readxl&#39;, &#39;reprex&#39;, &#39;rvest&#39;, &#39;feather&#39;, &#39;mockr&#39;, &#39;magick&#39;, &#39;formattable&#39;, &#39;sparkline&#39;, &#39;webshot2&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in install.packages(new.packages, dep = TRUE): installation of package
## &#39;magick&#39; had non-zero exit status
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#loading packages
for(package.i in list.of.packages){
  suppressPackageStartupMessages(
    library(
      package.i, 
      character.only = TRUE
      )
    )
}

#loading example data
data(&amp;quot;penguins&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;beyond-for-building-loops-with-foreach&#34;&gt;Beyond &lt;code&gt;for&lt;/code&gt;: building loops with &lt;code&gt;foreach&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;foreach&lt;/code&gt; package (the vignette is 
&lt;a href=&#34;https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;) provides a way to build loops that support parallel execution, and easily gather the results provided by each iteration in the loop.&lt;/p&gt;
&lt;p&gt;For example, this classic &lt;code&gt;for&lt;/code&gt; loop computes the square root of the numbers 1 to 5 with &lt;code&gt;sqrt()&lt;/code&gt; (the function is vectorized, but let&amp;rsquo;s conveniently forget that for a moment). Notice that I have to create a vector &lt;code&gt;x&lt;/code&gt; to gather the results before executing the loop.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- vector()
for(i in 1:10){
  x[i] &amp;lt;- sqrt(i)
  }
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;foreach&lt;/code&gt; version returns a list with the results automatically. Notice that &lt;code&gt;%do%&lt;/code&gt; operator after the loop definition, I&amp;rsquo;ll talk more about it later.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- foreach(i = 1:10) %do% {
  sqrt(i)
  }
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1.414214
## 
## [[3]]
## [1] 1.732051
## 
## [[4]]
## [1] 2
## 
## [[5]]
## [1] 2.236068
## 
## [[6]]
## [1] 2.44949
## 
## [[7]]
## [1] 2.645751
## 
## [[8]]
## [1] 2.828427
## 
## [[9]]
## [1] 3
## 
## [[10]]
## [1] 3.162278
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the &lt;code&gt;.combine&lt;/code&gt; argument of &lt;code&gt;foreach&lt;/code&gt; to arrange the list as a vector. Other options such as &lt;code&gt;cbind&lt;/code&gt;, &lt;code&gt;rbind&lt;/code&gt;, or even custom functions can be used as well, only depending on the structure of the output of each iteration.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- foreach(
  i = 1:10, 
  .combine = &#39;c&#39;
) %do% {
    sqrt(i)
  }
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another interesting capability of &lt;code&gt;foreach&lt;/code&gt; is that it supports several iterators of the same length at once. Notice that the values of the iterators are not combined. When the first value of one iterator is being used, the first value of the other iterators will be used as well.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- foreach(
  i = 1:3, 
  j = 1:3, 
  k = 1:3, 
  .combine = &#39;c&#39;
  ) %do% {
  i + j + k
  }
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 6 9
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;running-foreach-loops-in-parallel&#34;&gt;Running &lt;code&gt;foreach&lt;/code&gt; loops in parallel&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;foreach&lt;/code&gt; loops shown above use the operator &lt;code&gt;%do%&lt;/code&gt;, that processes the tasks sequentially. To run tasks in parallel, &lt;code&gt;foreach&lt;/code&gt; uses the operator &lt;code&gt;%dopar%&lt;/code&gt;, that has to be supported by a parallel &lt;em&gt;backend&lt;/em&gt;. If there is no parallel backend, &lt;code&gt;%dopar%&lt;/code&gt; warns the user that it is being run sequentially, as shown below. But what the heck is a parallel backend?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- foreach(
  i = 1:10, 
  .combine = &#39;c&#39;
) %dopar% {
    sqrt(i)
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: executing %dopar% sequentially: no parallel backend registered
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h3 id=&#34;what-is-a-parallel-backend&#34;&gt;What is a parallel backend?&lt;/h3&gt;
&lt;p&gt;When running tasks in parallel, there should be a &lt;em&gt;director&lt;/em&gt; node that tells a group of &lt;em&gt;workers&lt;/em&gt; what to do with a given set of data and functions. The &lt;em&gt;workers&lt;/em&gt; execute the iterations, and the &lt;em&gt;director&lt;/em&gt; manages execution and gathers the results provided by the &lt;em&gt;workers&lt;/em&gt;. A parallel backend provides the means for the director and workers to communicate, while allocating and managing the required computing resources (processors, RAM memory, and network bandwidth among others).&lt;/p&gt;
&lt;p&gt;There are two types of parallel backends that can be used with &lt;code&gt;foreach&lt;/code&gt;, &lt;strong&gt;FORK&lt;/strong&gt; and &lt;strong&gt;PSOCK&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h4 id=&#34;fork&#34;&gt;FORK&lt;/h4&gt;
&lt;p&gt;FORK backends are only available on UNIX machines (Linux, Mac, and the likes), and do not work in clusters [sad face], so only single-machine environments are appropriate for this backend. In a FORK backend, the workers share the same environment (data, loaded packages, and functions) as the director. This setup is highly efficient because the main environment doesn&amp;rsquo;t have to be copied, and only worker outputs need to be sent back to the director.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;FORK.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h4 id=&#34;psock&#34;&gt;PSOCK&lt;/h4&gt;
&lt;p&gt;PSOCK backends (Parallel Socket Cluster) are available for both UNIX and WINDOWS systems, and are the default option provided with &lt;code&gt;foreach&lt;/code&gt;. As their main disadvantage, the environment of the director needs to be copied to the environment of each worker, which increases network overhead while decreasing the overall efficiency of the cluster. By default, all the functions available in base R are copied to each worker, and if a particular set of R packages are needed in the workers, they need to be copied to the respective environments of the workers as well.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.r-bloggers.com/2019/06/parallel-r-socket-or-fork/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This post&lt;/a&gt; compares both backends and concludes that FORK is about a 40% faster than PSOCK.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;PSOCK.png&#34; alt=&#34;&#34;&gt;
Â &lt;/p&gt;
&lt;h2 id=&#34;setup-of-a-parallel-backend&#34;&gt;Setup of a parallel backend&lt;/h2&gt;
&lt;p&gt;Here I explain how to setup the parallel backend for a simple computer and for a Beowulf cluster as 
&lt;a href=&#34;%28https://www.blasbenito.com/post/01_home_cluster/%29&#34;&gt;the one I described in a previous post&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;setup-for-a-single-computer&#34;&gt;Setup for a single computer&lt;/h3&gt;
&lt;p&gt;Setting up a cluster in a single computer requires first to find out how many cores we want to use from the ones we have available. It is recommended to leave one free core for other tasks.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parallel::detectCores()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;n.cores &amp;lt;- parallel::detectCores() - 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to define the cluster with &lt;code&gt;parallel::makeCluster()&lt;/code&gt; and register it so it can be used by &lt;code&gt;%dopar%&lt;/code&gt; with &lt;code&gt;doParallel::registerDoParallel(my.cluster)&lt;/code&gt;. The &lt;code&gt;type&lt;/code&gt; argument of &lt;code&gt;parallel::makeCluster()&lt;/code&gt; accepts the strings &amp;ldquo;PSOCK&amp;rdquo; and &amp;ldquo;FORK&amp;rdquo; to define the type of parallel backend to be used.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#create the cluster
my.cluster &amp;lt;- parallel::makeCluster(
  n.cores, 
  type = &amp;quot;PSOCK&amp;quot;
  )

#check cluster definition (optional)
print(my.cluster)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## socket cluster with 15 nodes on host &#39;localhost&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#register it to be used by %dopar%
doParallel::registerDoParallel(cl = my.cluster)

#check if it is registered (optional)
foreach::getDoParRegistered()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#how many workers are available? (optional)
foreach::getDoParWorkers()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can run a set of tasks in parallel!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- foreach(
  i = 1:10, 
  .combine = &#39;c&#39;
) %dopar% {
    sqrt(i)
  }
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If everything went well, now &lt;code&gt;%dopar%&lt;/code&gt; should not be throwing the warning &lt;code&gt;executing %dopar% sequentially: no parallel backend registered&lt;/code&gt;, meaning that the parallel execution is working as it should. In this little example there is no gain in execution speed, because the operation being executed is extremely fast, but this will change when the operations running inside of the loop take longer times to run.&lt;/p&gt;
&lt;p&gt;Finally, it is always recommendable to stop the cluster when we are done working with it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parallel::stopCluster(cl = my.cluster)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;setup-for-a-beowulf-cluster&#34;&gt;Setup for a Beowulf cluster&lt;/h3&gt;
&lt;p&gt;This setup is a bit more complex, because it requires to open a &lt;em&gt;port&lt;/em&gt; in every computer of the cluster. Ports are virtual communication channels, and are identified by a number.&lt;/p&gt;
&lt;p&gt;First, lets tell R what port we want to use:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#define port
Sys.setenv(R_PARALLEL_PORT = 11000)

#check that it
Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we need to open the selected port in every computer of the network. In Linux we need to setup the firewall to allow connections from the network &lt;code&gt;10.42.1.0/24&lt;/code&gt; (replace this with your network range if different!) to the port &lt;code&gt;11000&lt;/code&gt; by splitting the window of the 
&lt;a href=&#34;https://gnometerminator.blogspot.com/p/introduction.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Terminator console&lt;/a&gt; in as many computers available in your network (the figure below shows three, one for my PC and two for my Intel NUCs), opening an ssh session on each remote machine, and setting Terminator with &lt;em&gt;Grouping&lt;/em&gt; equal to &lt;em&gt;Broadcast all&lt;/em&gt; so we only need to type the commands once.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;terminator.png&#34; alt=&#34;Opening port 11000 in three computers at once with Terminator&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we have to create an object defining the IPs of the computers in the network, the number of cores to use from each computer, the user name, and the identity of the &lt;em&gt;director&lt;/em&gt;. This will be the &lt;code&gt;spec&lt;/code&gt; argument required by &lt;code&gt;parallel::makeCluster()&lt;/code&gt; to create the cluster throughtout the machines in the network. It is a list of lists, with as many lists as nodes are defined. Each &lt;em&gt;sub-list&lt;/em&gt; has a slot named &lt;em&gt;host&lt;/em&gt; with the IP of the computer where the given node is, and &lt;em&gt;user&lt;/em&gt;, with the name of the user in each computer.&lt;/p&gt;
&lt;p&gt;The code below shows how this would be done, step by step. Yes, this is CUMBERSOME.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#main parameters
director &amp;lt;- &#39;10.42.0.1&#39;
nuc2 &amp;lt;- &#39;10.42.0.34&#39;
nuc1 &amp;lt;- &#39;10.42.0.104&#39;
user &amp;lt;- &amp;quot;blas&amp;quot;

#list of machines, user names, and cores
spec &amp;lt;- list(
  list(
    host = director, 
    user = user,
    ncore = 7
  ), 
  list(
    host = nuc1, 
    user = user,
    ncore = 4
  ),
  list(
    host = nuc2, 
    user = user,
    ncore = 4
  )
)

#generating nodes from the list of machines
spec &amp;lt;- lapply(
  spec, 
  function(spec.i) rep(
    list(
      list(
        host = spec.i$host, 
        user = spec.i$user)
      ), 
    spec.i$ncore
    )
)

#formating into a list of lists
spec &amp;lt;- unlist(
  spec, 
  recursive = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Generating the &lt;code&gt;spec&lt;/code&gt; definition is a bit easier with the function below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#function to generate cluster specifications from a vector of IPs, a vector with the number of cores to use on each IP, and a user name
cluster_spec &amp;lt;- function(
  ips,
  cores,
  user
){
  
  #creating initial list
  spec &amp;lt;- list()
  
  for(i in 1:length(ips)){
    spec[[i]] &amp;lt;- list()
    spec[[i]]$host &amp;lt;- ips[i]
    spec[[i]]$user &amp;lt;- user
    spec[[i]]$ncore &amp;lt;- cores[i]
  }

  #generating nodes from the list of machines
  spec &amp;lt;- lapply(
    spec, 
    function(spec.i) rep(
      list(
        list(
          host = spec.i$host, 
          user = spec.i$user)
        ), 
      spec.i$ncore
      )
  )

  #formating into a list of lists
  spec &amp;lt;- unlist(
    spec, 
    recursive = FALSE
  )
  
  return(spec)
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function is also available in 
&lt;a href=&#34;https://gist.github.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this GitHub Gist&lt;/a&gt;, so you can load it into your R environment by executing:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;source(&amp;quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below I use it to generate the input to the &lt;code&gt;spec&lt;/code&gt; argument to start the cluster with &lt;code&gt;parallel::makeCluster()&lt;/code&gt;. Notice that I have added several arguments.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The argument &lt;code&gt;outfile&lt;/code&gt; determines where the workers write a log. In this case it is set to &lt;em&gt;nowhere&lt;/em&gt; with the double quotes, but the path to a text file in the director could be provided here.&lt;/li&gt;
&lt;li&gt;The argument &lt;code&gt;homogeneous = TRUE&lt;/code&gt; indicates that all machines have the &lt;code&gt;Rscript&lt;/code&gt; in the same location. In this case all three machines have it at &amp;ldquo;/usr/lib/R/bin/Rscript&amp;rdquo;. Otherwise, set it up to &lt;code&gt;FALSE&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#generate cluster specification
spec &amp;lt;- cluster_spec(
  ips = c(&#39;10.42.0.1&#39;, &#39;10.42.0.34&#39;, &#39;10.42.0.104&#39;),
  cores = c(7, 4, 4),
  user = &amp;quot;blas&amp;quot;
)

#setting up cluster
my.cluster &amp;lt;- parallel::makeCluster(
  master = &#39;10.42.0.1&#39;, 
  spec = spec,
  port = Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;),
  outfile = &amp;quot;&amp;quot;,
  homogeneous = TRUE
)

#check cluster definition (optional)
print(my.cluster)

#register cluster
doParallel::registerDoParallel(cl = my.cluster)

#how many workers are available? (optional)
foreach::getDoParWorkers()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can use the cluster to execute a dummy operation in parallel using all machines in the network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- foreach(
  i = 1:20, 
  .combine = &#39;c&#39;
) %dopar% {
    sqrt(i)
  }
x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once everything is done, remember to close the cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parallel::stopCluster(cl = my.cluster)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;practical-examples&#34;&gt;Practical examples&lt;/h2&gt;
&lt;p&gt;In this section I cover two examples on how to use parallelized loops to explore model outputs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tuning random forest 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Hyperparameter_%28machine_learning%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;hyperparameters&lt;/em&gt;&lt;/a&gt; to maximize classification accuracy.&lt;/li&gt;
&lt;li&gt;Obtain a confidence interval for the importance score of each predictor from a set random forest models fitted with 
&lt;a href=&#34;https://github.com/imbs-hl/ranger&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ranger()&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the examples I use the &lt;code&gt;penguins&lt;/code&gt; data from the 
&lt;a href=&#34;https://github.com/allisonhorst/palmerpenguins&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;palmerpenguins&lt;/code&gt;&lt;/a&gt; package to fit classification models with random forest using &lt;em&gt;species&lt;/em&gt; as a response, and &lt;em&gt;bill_length_mm&lt;/em&gt;, &lt;em&gt;bill_depth_mm&lt;/em&gt;, &lt;em&gt;flipper_length_mm&lt;/em&gt;, and &lt;em&gt;body_mass_g&lt;/em&gt; as predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#removing NA and subsetting columns
penguins &amp;lt;- as.data.frame(
  na.omit(
    penguins[, c(
      &amp;quot;species&amp;quot;,
      &amp;quot;bill_length_mm&amp;quot;,
      &amp;quot;bill_depth_mm&amp;quot;,
      &amp;quot;flipper_length_mm&amp;quot;,
      &amp;quot;body_mass_g&amp;quot;
    )]
    )
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;species&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;bill_length_mm&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;bill_depth_mm&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;flipper_length_mm&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;body_mass_g&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;39.1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;18.7&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;181&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3750&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;39.5&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;17.4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;186&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;40.3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;18.0&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;195&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;36.7&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;19.3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;193&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3450&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;39.3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20.6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;190&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3650&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;38.9&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;17.8&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;181&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3625&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;39.2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;19.6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;195&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4675&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;34.1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;18.1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;193&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3475&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;42.0&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20.2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;190&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;37.8&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;17.1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;186&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3300&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;37.8&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;17.3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;180&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3700&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;41.1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;17.6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;182&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3200&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;38.6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;21.2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;191&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;34.6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;21.1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;198&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4400&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;36.6&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;17.8&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;185&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3700&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;38.7&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;19.0&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;195&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3450&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;42.5&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20.7&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;197&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4500&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;34.4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;18.4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;184&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3325&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;46.0&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;21.5&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;194&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4200&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adelie&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;37.8&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;18.3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;174&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3400&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We&amp;rsquo;ll fit random forest models with the 
&lt;a href=&#34;https://cran.r-project.org/package=ranger&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ranger&lt;/code&gt;&lt;/a&gt; package, which works as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#fitting classification model
m &amp;lt;- ranger::ranger(
  data = penguins,
  dependent.variable.name = &amp;quot;species&amp;quot;,
  importance = &amp;quot;permutation&amp;quot;
)

#summary
m
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Ranger result
## 
## Call:
##  ranger::ranger(data = penguins, dependent.variable.name = &amp;quot;species&amp;quot;,      importance = &amp;quot;permutation&amp;quot;) 
## 
## Type:                             Classification 
## Number of trees:                  500 
## Sample size:                      342 
## Number of independent variables:  4 
## Mtry:                             2 
## Target node size:                 1 
## Variable importance mode:         permutation 
## Splitrule:                        gini 
## OOB prediction error:             2.63 %
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#variable importance
m$variable.importance
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    bill_length_mm     bill_depth_mm flipper_length_mm       body_mass_g 
##        0.30913224        0.16916726        0.20410227        0.08218859
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output shows that the percentage of misclassified cases is 2.63, and that &lt;em&gt;bill_length_mm&lt;/em&gt; is the variable that contributes the most to the accuracy of the classification.&lt;/p&gt;
&lt;p&gt;If you are not familiar with random forest, 
&lt;a href=&#34;https://victorzhou.com/blog/intro-to-random-forests/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt; and the video below do a pretty good job in explaining the basics:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/D_2LkhMJcfY&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;tuning-random-forest-hyperparameters&#34;&gt;Tuning random forest hyperparameters&lt;/h3&gt;
&lt;p&gt;Random forest has several hyperparameters that influence model fit:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;num.trees&lt;/code&gt; is the total number of trees to fit. The default value is 500.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mtry&lt;/code&gt; is the number of variables selected by chance (from the total pool of variables) as candidates for a tree split. The minimum is 2, and the maximum is the total number of predictors.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min.node.size&lt;/code&gt; is the minimum number of cases that shall go together in the terminal nodes of each tree. For classification models as the ones we are going to fit, 1 is the minimum.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we are going to explore how combinations of these values increase or decrease the prediction error of the model (percentage of misclassified cases) on the out-of-bag data (not used to train each decision tree). This operation is usually named &lt;strong&gt;grid search for hyperparameter optimization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To create these combinations of hyperparameters we use &lt;code&gt;expand.grid()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sensitivity.df &amp;lt;- expand.grid(
  num.trees = c(500, 1000, 1500),
  mtry = 2:4,
  min.node.size = c(1, 10, 20)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;num.trees&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;mtry&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;min.node.size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1000&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Each row in &lt;code&gt;sensitivity.df&lt;/code&gt; corresponds to a combination of parameters to test, so there are 27 models to fit. The code below prepares the cluster, and uses the ability of &lt;code&gt;foreach&lt;/code&gt; to work with several iterators at once to easily introduce the right set of hyperparameters to each fitted model.&lt;/p&gt;
&lt;p&gt;Notice how in the &lt;code&gt;foreach&lt;/code&gt; definition I use the &lt;code&gt;.packages&lt;/code&gt; argument to export the &lt;code&gt;ranger&lt;/code&gt; package to the environments of the workers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#create and register cluster
my.cluster &amp;lt;- parallel::makeCluster(n.cores)
doParallel::registerDoParallel(cl = my.cluster)
  
#fitting each rf model with different hyperparameters
prediction.error &amp;lt;- foreach(
  num.trees = sensitivity.df$num.trees,
  mtry = sensitivity.df$mtry,
  min.node.size = sensitivity.df$min.node.size,
  .combine = &#39;c&#39;, 
  .packages = &amp;quot;ranger&amp;quot;
) %dopar% {
  
  #fit model
  m.i &amp;lt;- ranger::ranger(
    data = penguins,
    dependent.variable.name = &amp;quot;species&amp;quot;,
    num.trees = num.trees,
    mtry = mtry,
    min.node.size = min.node.size
  )
  
  #returning prediction error as percentage
  return(m.i$prediction.error * 100)
  
}

#adding the prediction error column
sensitivity.df$prediction.error &amp;lt;- prediction.error
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To plot the results:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot2::ggplot(data = sensitivity.df) + 
  ggplot2::aes(
    x = mtry,
    y = as.factor(min.node.size),
    fill = prediction.error
  ) + 
  ggplot2::facet_wrap(as.factor(num.trees)) +
  ggplot2::geom_tile() + 
  ggplot2::scale_y_discrete(breaks = c(1, 10, 20)) +
  ggplot2::scale_fill_viridis_c() + 
  ggplot2::ylab(&amp;quot;min.node.size&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;sensitivity.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The figure shows that combinations of lower values of &lt;code&gt;min.node.size&lt;/code&gt; and &lt;code&gt;mtry&lt;/code&gt; generally lead to models with a lower prediction error across different numbers of trees. Retrieving the first line of &lt;code&gt;sensitivity.df&lt;/code&gt; ordered by ascending &lt;code&gt;prediction.error&lt;/code&gt; will give us the values of the hyperparameters we need to use to reduce the prediction error as much as possible.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;best.hyperparameters &amp;lt;- sensitivity.df %&amp;gt;% 
  dplyr::arrange(prediction.error) %&amp;gt;% 
  dplyr::slice(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;num.trees&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;mtry&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;min.node.size&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;prediction.error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;500&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.339181&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;confidence-intervals-of-variable-importance-scores&#34;&gt;Confidence intervals of variable importance scores&lt;/h3&gt;
&lt;p&gt;Random forest has an important stochastic component during model fitting, and as consequence, the same model will return slightly different results in different runs (unless &lt;code&gt;set.seed()&lt;/code&gt; or the &lt;code&gt;seed&lt;/code&gt; argument of &lt;code&gt;ranger&lt;/code&gt; are used). This variability also affects the importance scores of the predictors, and can be use to our advantage to assess whether the importance scores of different variables do really overlap or not.&lt;/p&gt;
&lt;p&gt;I have written a little function to transform the vector of importance scores returned by &lt;code&gt;ranger&lt;/code&gt; into a data frame (of one row). It helps arranging the importance scores of different runs into a long format, which helps a lot to plot a boxplot with &lt;code&gt;ggplot2&lt;/code&gt; right away. This function could have been just some code thrown inside the &lt;code&gt;foreach&lt;/code&gt; loop, but I want to illustrate how &lt;code&gt;foreach&lt;/code&gt; automatically transfers functions available in the R environment into the environments of the workers when required, without the intervention of the user. The same will happen with the &lt;code&gt;best.hyperparameters&lt;/code&gt; tiny data frame we created in the previous section.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;importance_to_df &amp;lt;- function(model){
  x &amp;lt;- as.data.frame(model$variable.importance)
  x$variable &amp;lt;- rownames(x)
  colnames(x)[1] &amp;lt;- &amp;quot;importance&amp;quot;
  rownames(x) &amp;lt;- NULL
  return(x)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code chunk below setups the cluster and runs 1000 random forest models in parallel (using the best hyperparameters computed in the previous section) while using &lt;code&gt;system.time()&lt;/code&gt; to assess running time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#we don&#39;t need to create the cluster, it is still up
print(my.cluster)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## socket cluster with 15 nodes on host &#39;localhost&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#assessing execution time
system.time(
  
  #performing 1000 iterations in parallel
  importance.scores &amp;lt;- foreach(
    i = 1:1000, 
    .combine = &#39;rbind&#39;, 
    .packages = &amp;quot;ranger&amp;quot;
  ) %dopar% {
    
    #fit model
    m.i &amp;lt;- ranger::ranger(
      data = penguins,
      dependent.variable.name = &amp;quot;species&amp;quot;,
      importance = &amp;quot;permutation&amp;quot;,
      mtry = best.hyperparameters$mtry,
      num.trees = best.hyperparameters$num.trees,
      min.node.size = best.hyperparameters$min.node.size
    )
    
    #format importance
    m.importance.i &amp;lt;- importance_to_df(model = m.i)
    
    #returning output
    return(m.importance.i)
    
  }
  
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.331   0.061   4.283
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output of &lt;code&gt;system.time()&lt;/code&gt; goes as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;user&lt;/em&gt;: seconds the R session has been using the CPU.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;system&lt;/em&gt;: seconds the operating system has been using the CPU.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;elapsed&lt;/em&gt;: the total execution time experienced by the user.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This will make sense in a minute. In the meantime, let&amp;rsquo;s plot our results!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot2::ggplot(data = importance.scores) + 
  ggplot2::aes(
    y = reorder(variable, importance), 
    x = importance
  ) +
  ggplot2::geom_boxplot() + 
  ggplot2::ylab(&amp;quot;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;boxplot.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The figure shows that the variable &lt;em&gt;bill_length_mm&lt;/em&gt; is the most important in helping the model classifying penguin species, with no overlap with any other variable. In this particular case, since the distributions of the importance scores do not overlap, this analysis isn&amp;rsquo;t truly helpful, but now you know how to do it!&lt;/p&gt;
&lt;p&gt;I assessed the running time with &lt;code&gt;system.time()&lt;/code&gt; because &lt;code&gt;ranger()&lt;/code&gt; can run in parallel by itself just by setting the &lt;code&gt;num.threads&lt;/code&gt; argument to the number of cores available in the machine. This capability cannot be used when executing &lt;code&gt;ranger()&lt;/code&gt; inside a parallelized &lt;code&gt;foreach&lt;/code&gt; loop though, and it is only useful inside classic &lt;code&gt;for&lt;/code&gt; loops.&lt;/p&gt;
&lt;p&gt;What option is more efficient then? The code below executes a regular &lt;code&gt;for&lt;/code&gt; loop running the function sequentially to evaluate whether it is more efficient to run &lt;code&gt;ranger()&lt;/code&gt; in parallel using one core per model, as we did above, or sequentially while using several cores per model on each iteration.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#list to save results
importance.scores.list &amp;lt;- list()

#performing 1000 iterations sequentially
system.time(
  
  for(i in 1:1000){
    
    #fit model
    m.i &amp;lt;- ranger::ranger(
      data = penguins,
      dependent.variable.name = &amp;quot;species&amp;quot;,
      importance = &amp;quot;permutation&amp;quot;,
      seed = i,
      num.threads = parallel::detectCores() - 1
    )
    
    #format importance
    importance.scores.list[[i]] &amp;lt;- importance_to_df(model = m.i)
    
  }
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##  44.377   3.010   9.909
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, &lt;code&gt;ranger()&lt;/code&gt; takes longer to execute in a regular &lt;code&gt;for&lt;/code&gt; loop using several cores at once than in a parallel &lt;code&gt;foreach&lt;/code&gt; loop using one core at once. That&amp;rsquo;s a win for the parallelized loop!&lt;/p&gt;
&lt;p&gt;We can stop our cluster now, we are done with it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parallel::stopCluster(cl = my.cluster)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;a-few-things-to-take-in-mind&#34;&gt;A few things to take in mind&lt;/h2&gt;
&lt;p&gt;As I have shown in this post, using parallelized &lt;code&gt;foreach&lt;/code&gt; loops can accelerate long computing processes, even when some functions have the ability to run in parallel on their own. However, there are things to take in mind, that might vary depending on whether we are executing the parallelized task on a single computer or on a small cluster.&lt;/p&gt;
&lt;p&gt;In a single computer, the communication between workers and the director is usually pretty fast, so there are no obvious bottlenecks to take into account here. The only limitation that might arise comes from the availability of RAM memory. For example, if a computer has 8 cores and 8GB of RAM, less than 1GB of RAM will be available for each worker. So, if you need to repeat a process that consumes a significant amount of RAM, the ideal number of cores running in parallel might be lower than the total number of cores available in your system. Don&amp;rsquo;t be greedy, and try to understand the capabilities of your machine while designing a parallelized task.&lt;/p&gt;
&lt;p&gt;When running &lt;code&gt;foreach&lt;/code&gt; loops as in &lt;code&gt;x &amp;lt;- foreach(...){...}&lt;/code&gt;, the variable &lt;code&gt;x&lt;/code&gt; is receiving whatever results the workers are producing. For example, if you are only returning the prediction error of a model, or its importance scores, &lt;code&gt;x&lt;/code&gt; will have a very manageable size. But if you are returning heavy objects such as complete random forest models, the size of &lt;code&gt;x&lt;/code&gt; is going to grow VERY FAST, and at the end it will be competing for RAM resources with the workers, which might even crash your R session. Again, don&amp;rsquo;t be greedy, and size your outputs carefully.&lt;/p&gt;
&lt;p&gt;Clusters spanning several computers are a different beast, since the workers and the director communicate through a switch and network wires and interfaces. If the amount of data going to and coming from the workers is large, the network can get clogged easily, reducing the cluster&amp;rsquo;s efficiency drastically. In general, if the amount of data produced by a worker on each iteration takes longer to arrive to the director than the time it takes the worker to produce it, then a cluster is not going to be more efficient than a single machine. But this is not important if you don&amp;rsquo;t care about efficiency.&lt;/p&gt;
&lt;p&gt;Other issues you might come across while parallelizing tasks in R are thoroughly commented in 
&lt;a href=&#34;https://towardsdatascience.com/parallelization-caveats-in-r-1-the-basics-multiprocessing-and-multithreading-performance-eb584b7e850e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt;, by Imre Gera.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s all for now folks, happy parallelization!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R package memoria</title>
      <link>https://blasbenito.com/project/memoria/</link>
      <pubDate>Sat, 19 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/project/memoria/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://zenodo.org/badge/latestdoi/179102027&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/179102027.svg&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://CRAN.R-project.org/package=memoria&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version-ago/memoria&#34; alt=&#34;CRAN\_Release\_Badge&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://CRAN.R-project.org/package=memoria&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://cranlogs.r-pkg.org/badges/memoria&#34; alt=&#34;CRAN\_Download\_Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The goal of &lt;em&gt;memoria&lt;/em&gt; is to provide the tools to quantify &lt;strong&gt;ecological
memory&lt;/strong&gt; in long time-series involving environmental drivers and biotic
responses, including palaeoecological datasets.&lt;/p&gt;
&lt;p&gt;Ecological memory has two main components: the &lt;em&gt;endogenous&lt;/em&gt; component,
which represents the effect of antecedent values of the response on
itself, and &lt;em&gt;endogenous&lt;/em&gt; component, which represents the effect of
antecedent values of the driver or drivers on the current state of the
biotic response. Additionally, the &lt;em&gt;concurrent effect&lt;/em&gt;, which represents
the synchronic effect of the environmental drivers over the response is
measured. The functions in the package allow the user&lt;/p&gt;
&lt;p&gt;The package &lt;em&gt;memoria&lt;/em&gt; uses the fast implementation of Random Forest
available in the 
&lt;a href=&#34;https://CRAN.R-project.org/package=ranger&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ranger&lt;/a&gt;
package to fit a model of the form shown in &lt;strong&gt;Equation 1&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Equation 1&lt;/strong&gt; (simplified from the one in the paper):
$$p_{t} = p_{t-1} +&amp;hellip;+ p_{t-n} + d_{t} + d_{t-1} +&amp;hellip;+ d_{t-n}$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p$ is the response variable, &lt;em&gt;Pollen&lt;/em&gt; counts were used in this particular case..&lt;/li&gt;
&lt;li&gt;$d$ is an environmental &lt;em&gt;Driver&lt;/em&gt; influencing the response variable.&lt;/li&gt;
&lt;li&gt;$t$ is the time of any given value of the response $p$.&lt;/li&gt;
&lt;li&gt;$t-1$ is the lag 1.&lt;/li&gt;
&lt;li&gt;$p_{t-1} +&amp;hellip;+ p_{t-n}$ represents the endogenous component of
ecological memory.&lt;/li&gt;
&lt;li&gt;$d_{t-1} +&amp;hellip;+ d_{t-n}$ represents the exogenous component of
ecological memory.&lt;/li&gt;
&lt;li&gt;$d_{t}$ represents the concurrent effect of the driver over the
response.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Random Forest returns an importance score for each model term, and the
functions in &lt;em&gt;memoria&lt;/em&gt; let the user to plot the importance scores across
time lags for each ecological memory components, and to compute
different features of each memory component (length, strength, and
dominance).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;output.png&#34; alt=&#34;Outputs produced by memoria from the analysis of a multivariate time series&#34;&gt;&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://github.com/BlasBenito/memoria&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub page&lt;/a&gt; of the package features complete examples on how to use the package. The 
&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.04772&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper published in the Ecography journal&lt;/a&gt; describes ecological memory concepts and the method based on Random Forest used to assess ecological memory components. The code used to generate the supplementary materials can be found in 
&lt;a href=&#34;https://github.com/BlasBenito/EcologicalMemory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; and 
&lt;a href=&#34;https://zenodo.org/record/3236128#.X941v9Yo-1c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zenodo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you ever use the package, please, cite it as:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Benito, B.M., GilâRomera, G. and Birks, H.J.B. (2020), Ecological memory at millennial timeâscales: the importance of data constraints, species longevity and niche features. Ecography, 43: 1-10. 
&lt;a href=&#34;https://doi.org/10.1111/ecog.04772&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1111/ecog.04772&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R package virtualPollen</title>
      <link>https://blasbenito.com/project/virtualpollen/</link>
      <pubDate>Sat, 19 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/project/virtualpollen/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://CRAN.R-project.org/package=virtualPollen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version-ago/virtualPollen&#34; alt=&#34;CRAN\_Release\_Badge&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://CRAN.R-project.org/package=virtualPollen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://cranlogs.r-pkg.org/badges/virtualPollen&#34; alt=&#34;CRAN\_Download\_Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The goal of &lt;code&gt;virtualPollen&lt;/code&gt; is to provide the tools to simulate pollen
curves over millenial time-scales generated by virtual taxa with
different life traits (life-span, fecundity, growth-rate) and niche
features (niche position and breadth) as a response to virtual
environmental drivers with a given temporal autocorrelation. It furthers
allow to simulate specific data properties of fossil pollen datasets,
such as sediment accumulation rate, and depth intervals between
consecutive pollen samples. The simulation outcomes are useful to better
understand the role of plant traits, niche properties, and climatic
variability in defining the shape of pollen curves.&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://github.com/BlasBenito/virtualPollen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub page&lt;/a&gt; of the package offers a complete tutorial on how to use the package. The 
&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.04772&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper published in the Ecography journal&lt;/a&gt; describes the foundations of the model in brief.&lt;/p&gt;
&lt;p&gt;If you ever use the package, please, cite it as:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Benito, B.M., GilâRomera, G. and Birks, H.J.B. (2020), Ecological memory at millennial timeâscales: the importance of data constraints, species longevity and niche features. Ecography, 43: 1-10. 
&lt;a href=&#34;https://doi.org/10.1111/ecog.04772&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1111/ecog.04772&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Setting up a Beowulf Home Cluster</title>
      <link>https://blasbenito.com/post/beowulf-cluster/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/beowulf-cluster/</guid>
      <description>&lt;p&gt;In this post I explain how to setup a small 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Beowulf_cluster&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beowulf cluster&lt;/a&gt; with a personal PC running Ubuntu 20.04 and a couple of 
&lt;a href=&#34;https://www.intel.com/content/www/us/en/products/boards-kits/nuc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intel NUCs&lt;/a&gt; running Ubuntu Server 20.04, with the end-goal of parallelizing R tasks.&lt;/p&gt;
&lt;p&gt;The topics I cover here are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Required material&lt;/li&gt;
&lt;li&gt;Network setting&lt;/li&gt;
&lt;li&gt;Installing the secure shell protocol&lt;/li&gt;
&lt;li&gt;Installing Ubuntu server in the NUCs&lt;/li&gt;
&lt;li&gt;Installing R in the NUCs&lt;/li&gt;
&lt;li&gt;Managing the cluster&amp;rsquo;s network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;preamble&#34;&gt;Preamble&lt;/h2&gt;
&lt;p&gt;I have a little but nice HP ENVY model &lt;em&gt;TE01-0008ns&lt;/em&gt; with 32 GB RAM, 8 CPUs, and 3TB of hard disk running Ubuntu 20.04 that I use to do all my computational work (and most of my tweeting). A few months ago I connected it with my two laptops (one of them deceased now, RIP my dear &lt;em&gt;skynet&lt;/em&gt;) to create a little cluster to run parallel tasks in R.&lt;/p&gt;
&lt;p&gt;It was just a draft cluster running on a wireless network, but it served me to think about getting a more permanent solution not requiring two additional laptops in my desk.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s were the nice INTEL NUCs (from 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Next_Unit_of_Computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Next Unit of Computing&lt;/em&gt;&lt;/a&gt;) come into play. NUCs are full-fledged computers fitted in small boxes usually sold without RAM memory sticks and no hard disk (hence the term &lt;em&gt;barebone&lt;/em&gt;). Since they have a low energy consumption footprint, I thought these would be ideal units for my soon-to-be home cluster.&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;p&gt;I gifted myself with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 
&lt;a href=&#34;https://ark.intel.com/content/www/us/en/ark/products/95062/intel-nuc-kit-nuc6cayh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intel Barebone BOXNUC6CAYH&lt;/a&gt;, each with 4 cores, and a maximum RAM memory of 32GB (you might read they only accept 8GB, but that&amp;rsquo;s not the case anymore). Notice that these NUCs aren&amp;rsquo;t state-of-the-art now, they were released by the end of 2016.&lt;/li&gt;
&lt;li&gt;2 Hard disks SSD 2.5&amp;quot; 
&lt;a href=&#34;https://shop.westerndigital.com/es-es/products/internal-drives/wd-blue-sata-ssd#WDS250G2B0A&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Western Digital WDS250G2B0A WD Blue&lt;/a&gt; (250GB)&lt;/li&gt;
&lt;li&gt;4 Crucial CT102464BF186D DDR3 SODIMM (204 pins) RAM sticks with 8GB each.&lt;/li&gt;
&lt;li&gt;1 ethernet switch Netgear GS308-300PES with 8 ports.&lt;/li&gt;
&lt;li&gt;3 ethernet wires NanoCable 10.20.0400-BL of 
&lt;a href=&#34;https://www.electronics-notes.com/articles/connectivity/ethernet-ieee-802-3/how-to-buy-best-ethernet-cables-cat-5-6-7.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cat 6&lt;/a&gt; quality.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The whole set came to cost around 530â¬, but please notice that I had a clear goal in mind: &amp;ldquo;duplicating&amp;rdquo; my computing power with the minimum number of NUCs, while preserving a share of 4GB of RAM memory per CPU throughout the cluster (based on the features of my desk computer). A more basic setting with more modest NUCs and smaller RAM would cost half of that.&lt;/p&gt;
&lt;p&gt;This instructive video by 
&lt;a href=&#34;https://www.youtube.com/channel/UCYa3XeSHenvosy5wMRpeIww&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Harry&lt;/a&gt; shows how to install the SSD and the RAM sticks in an Intel NUC. It really takes 5 minutes tops, one only has to be a bit careful with the RAM sticks, the pins need to go all the way in into their slots before securing the sticks in place.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/6hzj7DogqXU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;network-settings&#34;&gt;Network settings&lt;/h2&gt;
&lt;p&gt;Before starting to install an operating system in the NUCS, the network setup goes as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;My desktop PC is connected to a router via WIFI and dynamic IP (DHCP).&lt;/li&gt;
&lt;li&gt;The PC and each NUC are connected to the switch with cat6 ethernet wires.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;network.png&#34; alt=&#34;Network diagram&#34;&gt;&lt;/p&gt;
&lt;p&gt;To share my PC&amp;rsquo;s WIFI connection with the NUCs I have to prepare a new &lt;em&gt;connection profile&lt;/em&gt; with the command line tool of Ubuntu&amp;rsquo;s 
&lt;a href=&#34;https://en.wikipedia.org/wiki/NetworkManager&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;NetworkManager&lt;/code&gt;&lt;/a&gt;, named &lt;code&gt;nmcli&lt;/code&gt;, as follows.&lt;/p&gt;
&lt;p&gt;First, I need to find the name of my ethernet interface by checking the status of my network devices with the command line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nmcli device status
DEVICE  TYPE      STATE        CONNECTION  
wlp3s0  wifi      connected    my_wifi 
enp2s0  ethernet  unavailable  --          
lo      loopback  unmanaged    --      
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There I can see that my ethernet interface is named &lt;code&gt;enp2s0&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Second, I have to configure the shared connection.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nmcli connection add type ethernet ifname enp2s0 ipv4.method shared con-name cluster
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Were &lt;code&gt;ifname enp2s0&lt;/code&gt; is the name of the interface I want to use for the new connection, &lt;code&gt;ipv4.method shared&lt;/code&gt; is the type of connection, and &lt;code&gt;con-name cluster&lt;/code&gt; is the name I want the connection to have. This operation adds firewall rules to manage traffic within the &lt;code&gt;cluster&lt;/code&gt; network, starts a DHCP server in the computer that serves IPs to the NUCS, and a DNS server that allows the NUCs to translate internet addresses.&lt;/p&gt;
&lt;p&gt;After turning on the switch, I can check the connection status again with&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nmcli device status
DEVICE  TYPE      STATE      CONNECTION  
enp2s0  ethernet  connected  cluster     
wlp3s0  wifi      connected  my_wifi 
lo      loopback  unmanaged  --    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When checking the IP of the device with &lt;code&gt;bash ifconfig&lt;/code&gt; it should yield &lt;code&gt;10.42.0.1&lt;/code&gt;. Any other computer in the &lt;code&gt;cluster&lt;/code&gt; network will have a dynamic IP in the range &lt;code&gt;10.42.0.1/24&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Further details about how to set a shared connection with &lt;code&gt;NetworkManager&lt;/code&gt; can be found in 
&lt;a href=&#34;https://fedoramagazine.org/internet-connection-sharing-networkmanager/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this nice post by Beniamino Galvani&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;ssh-setup&#34;&gt;SSH setup&lt;/h2&gt;
&lt;p&gt;My PC, as the director of the cluster, needs an &lt;code&gt;SSH client&lt;/code&gt; running, while the NUCs need an &lt;code&gt;SSH server&lt;/code&gt;. 
&lt;a href=&#34;https://www.ionos.com/digitalguide/server/tools/ssh-secure-shell/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;SSH&lt;/code&gt; (&lt;strong&gt;S&lt;/strong&gt;ecure &lt;strong&gt;Sh&lt;/strong&gt;ell)&lt;/a&gt; is a remote authentication protocol that allows secure connections to remote servers that I will be using all the time to manage the cluster. To install, run, and check its status I just have to run these lines in the console:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install ssh 
sudo systemctl enable --now ssh
sudo systemctl status ssh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, a secure certificate of the identity of a given computer, named &lt;code&gt;ssh-key&lt;/code&gt;, that grants access to remote ssh servers and services needs to be generated.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh-keygen &amp;quot;label&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, substitute &amp;ldquo;label&amp;rdquo; by the name of the computer to be used as cluster&amp;rsquo;s &amp;ldquo;director&amp;rdquo;. The system will ask for a file name and a 
&lt;a href=&#34;https://www.ssh.com/ssh/passphrase&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;passphrase&lt;/a&gt; that will be used to encrypt the ssh-key.&lt;/p&gt;
&lt;p&gt;The ssh-key needs to be added to the 
&lt;a href=&#34;https://www.ssh.com/ssh/agent&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ssh-agent&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh-add ~/.ssh/id_rsa
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To copy the ssh-key to my GitHub account, I have to copy the contents of the file &lt;code&gt;~/.ssh/id_rsa.pub&lt;/code&gt; (can be done just opening it with &lt;code&gt;gedit ~/.ssh/id_rsa.pub&lt;/code&gt; + &lt;code&gt;Ctrl + a&lt;/code&gt; + &lt;code&gt;Ctrl + c&lt;/code&gt;), and paste it on &lt;code&gt;GitHub account &amp;gt; Settings &amp;gt;  SSH and GPG keys &amp;gt; New SSH Key&lt;/code&gt; (green button in the upper right part of the window).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you don&amp;rsquo;t use GitHub, you&amp;rsquo;ll need to copy your ssh-key to the NUCs once they are up and running with &lt;code&gt;ssh-copy-id -i ~/.ssh/id_rsa.pub user_name@nuc_IP&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&#34;installing-and-preparing-ubuntu-server-in-each-nuc&#34;&gt;Installing and preparing ubuntu server in each NUC&lt;/h2&gt;
&lt;p&gt;The NUCs don&amp;rsquo;t need to waste resources in a user graphical interface I won&amp;rsquo;t be using whatsoever. Since they will work in a 
&lt;a href=&#34;https://www.howtogeek.com/660841/what-is-a-headless-server/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;headless&lt;/em&gt; configuration&lt;/a&gt; once the cluster is ready, a Linux distro without graphical user interface such as Ubuntu server is the way to go.&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h3 id=&#34;installing-ubuntu-server&#34;&gt;Installing Ubuntu server&lt;/h3&gt;
&lt;p&gt;First it is important to connect a display, a keyboard, and a mouse to the NUC in preparation, and turn it on while pushing F2 to start the visual BIOS. These BIOS parameters need to be modified:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Advanced (upper right) &amp;gt; Boot &amp;gt; Boot Configuration &amp;gt; UEFI Boot &amp;gt; OS Selection: Linux&lt;/li&gt;
&lt;li&gt;Advanced &amp;gt; Boot &amp;gt; Boot Configuration &amp;gt; UEFI Boot &amp;gt; OS Selection: mark &amp;ldquo;Boot USB Devices First&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;[optional] Advanced &amp;gt; Power &amp;gt; Secondary Power Settings &amp;gt; After Power Failure: &amp;ldquo;Power On&amp;rdquo;. I have the switch and nucs connected to an outlet plug extender with an interrupter. When I switch it on, the NUCs (and the switch) boot automatically after this option is enabled, so I only need to push one button to power up the cluster.&lt;/li&gt;
&lt;li&gt;F10 to save, and shutdown.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To prepare the USB boot device with Ubuntu server 20.04 I first download the .iso from 
&lt;a href=&#34;https://ubuntu.com/download/server&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, by choosing &amp;ldquo;Option 3&amp;rdquo;, which leads to the manual install. Once the .iso file is downloaded, I use 
&lt;a href=&#34;https://ubuntu.com/tutorials/create-a-usb-stick-on-ubuntu#1-overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ubuntu&amp;rsquo;s &lt;code&gt;Startup Disk Creator&lt;/code&gt;&lt;/a&gt; to prepare a bootable USB stick. Now I just have to plug the stick in the NUC and reboot it.&lt;/p&gt;
&lt;p&gt;The Ubuntu server install is pretty straightforward, and only a few things need to be decided along the way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As user name I choose the same I have in my personal computer.&lt;/li&gt;
&lt;li&gt;As name for the NUCs I choose &amp;ldquo;nuc1&amp;rdquo; and &amp;ldquo;nuc2&amp;rdquo;, but any other option will work well.&lt;/li&gt;
&lt;li&gt;As password, for comfort I use the same I have in my personal computer.&lt;/li&gt;
&lt;li&gt;During the network setup, choose DHCP. If the network is properly configured and the switch is powered on, after a few seconds the NUC will acquire an IP in the range &lt;code&gt;10.42.0.1/24&lt;/code&gt;, as any other machine within the &lt;code&gt;cluster&lt;/code&gt; network.&lt;/li&gt;
&lt;li&gt;When asked, mark the option &amp;ldquo;Install in the whole disk&amp;rdquo;, unless you have other plans for your NUC.&lt;/li&gt;
&lt;li&gt;Mark &amp;ldquo;Install OpenSSH&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Provide it with your GitHub user name if you have your ssh-key there, and it will download it right away, facilitating a lot the ssh setup.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reboot once the install is completed. Now I keep configuring the NUC&amp;rsquo;s operating system from my PC through ssh.&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h3 id=&#34;configuring-a-nuc&#34;&gt;Configuring a NUC&lt;/h3&gt;
&lt;p&gt;First, to learn the IP of the NUC:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo arp-scan 10.42.0.1/24
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other alternatives to this command are &lt;code&gt;arp -a&lt;/code&gt; and &lt;code&gt;sudo arp-scan -I enp2s0 --localnet&lt;/code&gt;. Once I learn the IP of the NUC, I add it to the file &lt;code&gt;etc/hosts&lt;/code&gt; of my personal computer as follows.&lt;/p&gt;
&lt;p&gt;First I open the file as root.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo gedit /etc/hosts
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add a new line there: &lt;code&gt;10.42.0.XXX nuc1&lt;/code&gt; and save the file.&lt;/p&gt;
&lt;p&gt;Now I access the NUC trough ssh to keep preparing it without a keyboard and a display. I do it from &lt;code&gt;Tilix&lt;/code&gt;, that allows to open different command line tabs in the same window, which is quite handy to manage several NUCs at once.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;htop.png&#34; alt=&#34;Tilix showing htop on my PC and the two NUCS&#34;&gt;&lt;/p&gt;
&lt;p&gt;Another great option to manage the NUCs through ssh is &lt;code&gt;terminator&lt;/code&gt;, that allows to 
&lt;a href=&#34;https://opensource.com/article/20/2/terminator-ssh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;broadcast the same commands to several ssh sessions at once&lt;/a&gt;. I have been trying it, and it is much better for cluster management purposes than Tilix. Actually, using it would simplify this workflow a lot, because once Ubuntu server is installed on each NUC, the rest of the configuration commands can be broadcasted at once to both NUCs. It&amp;rsquo;s a bummer I discovered this possibility way too late!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh blas@10.42.0.XXX
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The NUC&amp;rsquo;s operating system probably has a bunch of pending software updates. To install these:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get upgrade
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I have to install a set of software packages that will facilitate managing the cluster&amp;rsquo;s network and the NUC itself.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install net-tools arp-scan lm-sensors dirmngr gnupg apt-transport-https ca-certificates software-properties-common samba libopenmpi3 libopenmpi-dev openmpi-bin openmpi-common htop
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h3 id=&#34;setting-the-system-time&#34;&gt;Setting the system time&lt;/h3&gt;
&lt;p&gt;To set the system time of the NUC to the same you have in your computer, just repeat these steps in every computer in the cluster network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#list time zones: 
timedatectl list-timezones
#set time zone
sudo timedatectl set-timezone Europe/Madrid
#enable timesyncd
sudo timedatectl set-ntp on
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h3 id=&#34;setting-the-locale&#34;&gt;Setting the locale&lt;/h3&gt;
&lt;p&gt;The operating systems of the NUCs and the PC need to have the same locale. It can be set by editing the file &lt;code&gt;/etc/default/locale&lt;/code&gt; with either &lt;code&gt;nano&lt;/code&gt; (in the NUCS) or &lt;code&gt;gedit&lt;/code&gt; (in the PC) and adding these lines, just replacing &lt;code&gt;en_US.UTF-8&lt;/code&gt; with your preferred locale.&lt;/p&gt;
&lt;p&gt;LANG=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LANGUAGE=&amp;ldquo;en_US:en&amp;rdquo;&lt;br&gt;
LC_NUMERIC=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_TIME=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_MONETARY=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_PAPER=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_IDENTIFICATION=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_NAME=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_ADDRESS=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_TELEPHONE=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_MEASUREMENT=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h3 id=&#34;temperature-monitoring&#34;&gt;Temperature monitoring&lt;/h3&gt;
&lt;p&gt;NUCs are 
&lt;a href=&#34;https://www.intel.com/content/www/us/en/support/articles/000033327/intel-nuc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;prone to overheating&lt;/a&gt; when under heavy loads for prolonged times. Therefore, monitoring the temperature of the NUCs CPUs is kinda important. In a step before I installed &lt;code&gt;lm-sensors&lt;/code&gt; in the NUC, which provides the tools to do so. To setup the sensors from an ssh session in the NUC:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo sensors-detect
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The program will request permission to find sensors in the NUC. I answered &amp;ldquo;yes&amp;rdquo; to every request. Once all sensors are identified, to check them&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sensors

iwlwifi_1-virtual-0
Adapter: Virtual device
temp1:            N/A  

acpitz-acpi-0
Adapter: ACPI interface
temp1:        +32.0Â°C  (crit = +100.0Â°C)

coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +30.0Â°C  (high = +105.0Â°C, crit = +105.0Â°C)
Core 0:        +30.0Â°C  (high = +105.0Â°C, crit = +105.0Â°C)
Core 1:        +30.0Â°C  (high = +105.0Â°C, crit = +105.0Â°C)
Core 2:        +29.0Â°C  (high = +105.0Â°C, crit = +105.0Â°C)
Core 3:        +30.0Â°C  (high = +105.0Â°C, crit = +105.0Â°C)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which gives the cpu temperatures at the moment the command was executed. The command &lt;code&gt;watch sensors&lt;/code&gt; gives continuous temperature readings instead.&lt;/p&gt;
&lt;p&gt;To control overheating in my NUCs I removed their top lids, and installed them into a custom LEGO &amp;ldquo;rack&amp;rdquo; with 
&lt;a href=&#34;http://www.eluteng.com/module/fan/12cm/details003.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;external USB fans&lt;/a&gt; with velocity control, as shown in the picture at the beginning of the post.&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h3 id=&#34;installing-r&#34;&gt;Installing R&lt;/h3&gt;
&lt;p&gt;To install R in the NUCs I just proceed as I would when installing it in my personal computer. There is a thorough guide 
&lt;a href=&#34;https://linuxize.com/post/how-to-install-r-on-ubuntu-20-04/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In a step above I installed all the pre-required software packages. Now I only have to add the security key of the R repository, add the repository itself, update the information on the packages available in the new repository, and finally install R.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9
sudo add-apt-repository &#39;deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/&#39;
sudo apt update
sudo apt install r-base
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If R has issues to recognize the system locale&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nano ~/.profile
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;add the following lines, replacing &lt;code&gt;en_US.UTF-8&lt;/code&gt; with your preferred locale&lt;/p&gt;
&lt;p&gt;&lt;code&gt;export LANG=en_US.UTF-8&lt;/code&gt;
&lt;code&gt;export LC_ALL=en_US.UTF-8&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;save, and execute the file to export the locale so R can read it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;. ~/.profile
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h3 id=&#34;finalizing-the-network-configuration&#34;&gt;Finalizing the network configuration&lt;/h3&gt;
&lt;p&gt;Each NUC needs firewall rules to grant access from other computers withinn the cluster network. To activate the NUC&amp;rsquo;s firewall and check what ports are open:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ufw enable
sudo ufw status
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To grant access from the PC to the NUC through ssh, and later through R for parallel computing, the ports &lt;code&gt;22&lt;/code&gt; and &lt;code&gt;11000&lt;/code&gt; must be open for the IP of the PC (&lt;code&gt;10.42.0.1&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ufw allow ssh
sudo ufw allow from 10.42.0.1 to any port 11000
sudo ufw allow from 10.42.0.1 to any port 22
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the other members of the cluster network must be declared in the &lt;code&gt;/etc/hosts&lt;/code&gt; file of each computer.&lt;/p&gt;
&lt;p&gt;In each NUC edit the file through ssh with &lt;code&gt;bash sudo nano /etc/hosts&lt;/code&gt; and add the lines&lt;/p&gt;
&lt;p&gt;&lt;code&gt;10.42.0.1 pc_name&lt;/code&gt;&lt;br&gt;
&lt;code&gt;10.42.0.XXX name_of_the_other_nuc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In the PC, add the lines&lt;/p&gt;
&lt;p&gt;&lt;code&gt;10.42.0.XXX name_of_one_nuc&lt;/code&gt;&lt;br&gt;
&lt;code&gt;10.42.0.XXX name_of_the_other_nuc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;At this point, after rebooting every machine, the NUCs must be accessible through ssh by using their names (&lt;code&gt;ssh username@nuc_name&lt;/code&gt;) instead of their IPs (&lt;code&gt;ssh username@n10.42.0.XXX&lt;/code&gt;). Just take in mind that, since the &lt;code&gt;cluster&lt;/code&gt; network works with dynamic IPs (and such setting cannot be changed in a shared connection), the IPs of the NUCs might change if a new device is added to the network. That&amp;rsquo;s something you need to check from the PC with &lt;code&gt;sudo arp-scan 10.42.0.1/24&lt;/code&gt;, to update every &lt;code&gt;/etc/hosts&lt;/code&gt; file accordingly.&lt;/p&gt;
&lt;p&gt;I think that&amp;rsquo;s all folks. Good luck setting your home cluster! Next time I will describe how to use it for parallel computing in R.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Explainable artificial intelligence enhances the ecological interpretability of blackâbox species distribution models</title>
      <link>https://blasbenito.com/publication/2020_ryo_ecography/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2020_ryo_ecography/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Comment on âA global-scale ecological niche model to predict SARS-CoV-2 coronavirus infection rateâ, author Coro</title>
      <link>https://blasbenito.com/publication/2020_contina_ecological_modelling/</link>
      <pubDate>Mon, 21 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2020_contina_ecological_modelling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Donât gamble the COVID-19 response on ecological hypotheses</title>
      <link>https://blasbenito.com/publication/2020_carlson_nature_ecology_and_evolution/</link>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2020_carlson_nature_ecology_and_evolution/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evaluating fossil charcoal representation in small peat bogs: Detailed Holocene fire records from southern Sweden</title>
      <link>https://blasbenito.com/publication/2020_cui_the_holocene/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2020_cui_the_holocene/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Late-spring frost risk between 1959 and 2017 decreased in North America but increased in Europe and Asia</title>
      <link>https://blasbenito.com/publication/2020_zohner_pnas/</link>
      <pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2020_zohner_pnas/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Species distribution models are inappropriate for COVID-19</title>
      <link>https://blasbenito.com/publication/2020_carlson_nature_ecology_and_evolution_b/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2020_carlson_nature_ecology_and_evolution_b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Holocene fire and vegetation dynamics in the Central Pyrenees (Spain)</title>
      <link>https://blasbenito.com/publication/2020_leunda_catena/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2020_leunda_catena/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Compositional turnover and variation in Eemian pollen sequences in Europe</title>
      <link>https://blasbenito.com/publication/2020_felde_vegetation_history_and_archaeobotany/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2020_felde_vegetation_history_and_archaeobotany/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On the inadequacy of species distribution models for modelling the spread of SARS-CoV-2: response to AraÃºjo and Naimi</title>
      <link>https://blasbenito.com/publication/2020_chipperfield_ecoevorxiv/</link>
      <pubDate>Sat, 28 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2020_chipperfield_ecoevorxiv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>distantia: an openâsource toolset to quantify dissimilarity between multivariate ecological timeâseries</title>
      <link>https://blasbenito.com/publication/2020_benito_ecography_distantia/</link>
      <pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2020_benito_ecography_distantia/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ecological memory at millennial timeâscales: the importance of data constraints, species longevity and niche features</title>
      <link>https://blasbenito.com/publication/2020_benito_ecography_memoria/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2020_benito_ecography_memoria/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A multi-dating approach to age-modelling long continental records: The 135 ka El CaÃ±izar de Villarquemado sequence (NE Spain)</title>
      <link>https://blasbenito.com/publication/2019_valero-garces_quaternary_geochronology/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2019_valero-garces_quaternary_geochronology/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Long-term fire resilience of the Ericaceous Belt, Bale Mountains, Ethiopia</title>
      <link>https://blasbenito.com/publication/2019_gil_romera_biology_letters/</link>
      <pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2019_gil_romera_biology_letters/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Display Jupyter Notebooks with Academic</title>
      <link>https://blasbenito.com/post_examples/jupyter/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post_examples/jupyter/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_1_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Welcome to Academic!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Welcome to Academic!
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;install-python-and-jupyterlab&#34;&gt;Install Python and JupyterLab&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Install Anaconda&lt;/a&gt; which includes Python 3 and JupyterLab.&lt;/p&gt;
&lt;p&gt;Alternatively, install JupyterLab with &lt;code&gt;pip3 install jupyterlab&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;create-or-upload-a-jupyter-notebook&#34;&gt;Create or upload a Jupyter notebook&lt;/h2&gt;
&lt;p&gt;Run the following commands in your Terminal, substituting &lt;code&gt;&amp;lt;MY-WEBSITE-FOLDER&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;SHORT-POST-TITLE&amp;gt;&lt;/code&gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
cd &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
jupyter lab index.ipynb
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;jupyter&lt;/code&gt; command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.&lt;/p&gt;
&lt;h2 id=&#34;edit-your-post-metadata&#34;&gt;Edit your post metadata&lt;/h2&gt;
&lt;p&gt;The first cell of your Jupter notebook will contain your post metadata (
&lt;a href=&#34;https://sourcethemes.com/academic/docs/front-matter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;front matter&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In Jupter, choose &lt;em&gt;Markdown&lt;/em&gt; as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: My post&#39;s title
date: 2019-09-01

# Put any other Academic metadata here...
---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Edit the metadata of your post, using the 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; as a guide to the available options.&lt;/p&gt;
&lt;p&gt;To set a 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#featured-image&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;featured image&lt;/a&gt;, place an image named &lt;code&gt;featured&lt;/code&gt; into your post&amp;rsquo;s folder.&lt;/p&gt;
&lt;p&gt;For other tips, such as using math, see the guide on 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;writing content with Academic&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;convert-notebook-to-markdown&#34;&gt;Convert notebook to Markdown&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;This post was created with Jupyter. The orginal files can be found at 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://blasbenito.com/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Potential changes in the distribution of Carnegiea gigantea under future scenarios</title>
      <link>https://blasbenito.com/publication/2018_albuquerque_peerj/</link>
      <pubDate>Wed, 19 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2018_albuquerque_peerj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Past and potential future population dynamics of three grouse species using ecological and whole genome coalescent modeling</title>
      <link>https://blasbenito.com/publication/2018_kozma_ecology_and_evolution/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2018_kozma_ecology_and_evolution/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Phylogenetic age differences in tree assemblages across the Northern Hemisphere increase with long-term climate stability in unstable regions</title>
      <link>https://blasbenito.com/publication/2017_feng_global_ecology_and_biogeography/</link>
      <pubDate>Mon, 17 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2017_feng_global_ecology_and_biogeography/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Historical anthropogenic footprints in the distribution of threatened plants in China</title>
      <link>https://blasbenito.com/publication/2017_feng_biological_conservation/</link>
      <pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2017_feng_biological_conservation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The ecological niche and distribution of Neanderthals during the Last Interglacial</title>
      <link>https://blasbenito.com/publication/2017_benito_journal_of_biogeography/</link>
      <pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2017_benito_journal_of_biogeography/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Investigating Neanderthal dispersal above 55Â°N in Europe during the Last Interglacial Complex</title>
      <link>https://blasbenito.com/publication/2017_kellberg-nielsen_quaternary_international/</link>
      <pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2017_kellberg-nielsen_quaternary_international/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spring predictability explains different leafâout strategies in the woody floras of North America, Europe and East Asia</title>
      <link>https://blasbenito.com/publication/2017_zohner_ecology_letters/</link>
      <pubDate>Tue, 14 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2017_zohner_ecology_letters/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Day length unlikely to constrain climate-driven shifts in leaf-out times of northern woody plants</title>
      <link>https://blasbenito.com/publication/2016_zohner_nature_climate_change/</link>
      <pubDate>Mon, 17 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2016_zohner_nature_climate_change/</guid>
      <description></description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>https://blasbenito.com/project_examples/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/project_examples/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>https://blasbenito.com/project_examples/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/project_examples/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Academic: the website builder for Hugo</title>
      <link>https://blasbenito.com/post_examples/getting-started/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post_examples/getting-started/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 &lt;em&gt;widgets&lt;/em&gt;, &lt;em&gt;themes&lt;/em&gt;, and &lt;em&gt;language packs&lt;/em&gt; included!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or 
&lt;a href=&#34;https://sourcethemes.com/academic/#expo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ð 
&lt;a href=&#34;#install&#34;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;View the &lt;strong&gt;documentation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð¬ 
&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Ask a question&lt;/strong&gt; on the forum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð¥ 
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chat with the &lt;strong&gt;community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð¦ Twitter: 
&lt;a href=&#34;https://twitter.com/source_themes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@source_themes&lt;/a&gt; 
&lt;a href=&#34;https://twitter.com/GeorgeCushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@GeorgeCushen&lt;/a&gt; 
&lt;a href=&#34;https://twitter.com/search?q=%23MadeWithAcademic&amp;amp;src=typd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#MadeWithAcademic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð¡ 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request a &lt;strong&gt;feature&lt;/strong&gt; or report a &lt;strong&gt;bug&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;â¬ï¸ &lt;strong&gt;Updating?&lt;/strong&gt; View the 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Guide&lt;/a&gt; and 
&lt;a href=&#34;https://sourcethemes.com/academic/updates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;â¤ï¸ &lt;strong&gt;Support development&lt;/strong&gt; of Academic:
&lt;ul&gt;
&lt;li&gt;âï¸ 
&lt;a href=&#34;https://paypal.me/cushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Donate a coffee&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðµ 
&lt;a href=&#34;https://www.patreon.com/cushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Become a backer on &lt;strong&gt;Patreon&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð¼ï¸ 
&lt;a href=&#34;https://www.redbubble.com/people/neutreno/works/34387919-academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Decorate your laptop or journal with an Academic &lt;strong&gt;sticker&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð 
&lt;a href=&#34;https://academic.threadless.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wear the &lt;strong&gt;T-shirt&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð©âð» 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/contribute/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Contribute&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


















&lt;figure id=&#34;figure-academic-is-mobile-first-with-a-responsive-design-to-ensure-that-your-site-looks-stunning-on-every-device&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png&#34; data-caption=&#34;Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34;&gt;


  &lt;img src=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/page-builder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/jupyter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable 
&lt;a href=&#34;https://sourcethemes.com/academic/themes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and 
&lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - 
&lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt;, 
&lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 15+ language packs including English, ä¸­æ, and PortuguÃªs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Academic comes with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Alternatively, visitors can  choose their preferred mode - click the sun/moon icon in the top right of the 
&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo&lt;/a&gt; to see it in action! Day/night mode can also be disabled by the site admin in &lt;code&gt;params.toml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/themes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choose a stunning &lt;strong&gt;theme&lt;/strong&gt; and &lt;strong&gt;font&lt;/strong&gt;&lt;/a&gt; for your site. Themes are fully 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/customization/#custom-theme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;customizable&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;
&lt;a href=&#34;https://github.com/sourcethemes/academic-admin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic Admin&lt;/a&gt;:&lt;/strong&gt; An admin tool to import publications from BibTeX or import assets for an offline site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;
&lt;a href=&#34;https://github.com/sourcethemes/academic-scripts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic Scripts&lt;/a&gt;:&lt;/strong&gt; Scripts to help migrate content to new versions of Academic&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;
&lt;p&gt;You can choose from one of the following four methods to install:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-web-browser&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;one-click install using your web browser (recommended)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-git&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;install on your computer using &lt;strong&gt;Git&lt;/strong&gt; with the Command Prompt/Terminal app&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-zip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;install on your computer by downloading the &lt;strong&gt;ZIP files&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;install on your computer with &lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/get-started/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;personalize and deploy your new site&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;updating&#34;&gt;Updating&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;View the Update Guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Feel free to &lt;em&gt;star&lt;/em&gt; the project on 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt; to help keep track of 
&lt;a href=&#34;https://sourcethemes.com/academic/updates&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;updates&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present 
&lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>https://blasbenito.com/post_examples/2015-07-23-r-rmarkdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      <guid>https://blasbenito.com/post_examples/2015-07-23-r-rmarkdown/</guid>
      <description>&lt;h1 id=&#34;r-markdown&#34;&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see 
&lt;a href=&#34;http://rmarkdown.rstudio.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can embed an R code chunk like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &amp;lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;including-plots&#34;&gt;Including Plots&lt;/h1&gt;
&lt;p&gt;You can also embed plots. See Figure &lt;a href=&#34;#fig:pie&#34;&gt;1&lt;/a&gt; for example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&#39;Sky&#39;, &#39;Sunny side of pyramid&#39;, &#39;Shady side of pyramid&#39;),
  col = c(&#39;#0292D8&#39;, &#39;#F7EA39&#39;, &#39;#C4B632&#39;),
  init.angle = -50, border = NA
)
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://blasbenito.com/post_examples/2015-07-23-r-rmarkdown_files/figure-html/pie-1.png&#34; alt=&#34;A fancy pie chart.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;span id=&#34;fig:pie&#34;&gt;&lt;/span&gt;Figure 1: A fancy pie chart.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A 2.5âmillionâyear perspective on coarseâfilter strategies for conserving nature&#39;s stage</title>
      <link>https://blasbenito.com/publication/2015_gill_conservation_biology/</link>
      <pubDate>Tue, 28 Apr 2015 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2015_gill_conservation_biology/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Supporting underrepresented forests in Mesoamerica</title>
      <link>https://blasbenito.com/publication/2015_albuquerque_naturaleza_and_conservacao/</link>
      <pubDate>Tue, 03 Mar 2015 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2015_albuquerque_naturaleza_and_conservacao/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Extreme habitat loss in a Mediterranean habitat: Maytenus senegalensis subsp. europaea</title>
      <link>https://blasbenito.com/publication/2015_mendoza-fernandez_plant_biosystems/</link>
      <pubDate>Tue, 13 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2015_mendoza-fernandez_plant_biosystems/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Distribution and conservation of the relict interaction between the butterfly Agriades zullichi and its larval foodplant (Androsace vitaliana nevadensis)</title>
      <link>https://blasbenito.com/publication/2014_barea-azcon_and_benito_biodiversity_and_conservation/</link>
      <pubDate>Thu, 20 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2014_barea-azcon_and_benito_biodiversity_and_conservation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Documenting, storing, and executing models in Ecology: A conceptual framework and real implementation in a global change monitoring program</title>
      <link>https://blasbenito.com/publication/2014_bonet_environmental_modelling_and_software/</link>
      <pubDate>Sat, 01 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2014_bonet_environmental_modelling_and_software/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Forecasting plant range collapse in a mediterranean hotspot: when dispersal uncertainties matter</title>
      <link>https://blasbenito.com/publication/2014_benito_diversity_and_distributions/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2014_benito_diversity_and_distributions/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interdisciplinary Climate Change Collaborations Are Essential for EarlyâCareer Scientists</title>
      <link>https://blasbenito.com/publication/2013_gornish_eos/</link>
      <pubDate>Tue, 16 Apr 2013 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2013_gornish_eos/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Comparing the performance of species distribution models of Zostera marina: Implications for conservation</title>
      <link>https://blasbenito.com/publication/2013_valle_journal_of_sea_research/</link>
      <pubDate>Fri, 15 Mar 2013 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2013_valle_journal_of_sea_research/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The impact of modelling choices in the predictive performance of richness maps derived from speciesâdistribution models: guidelines to build better diversity models</title>
      <link>https://blasbenito.com/publication/2013_benito_methods_in_ecology_and_evolution/</link>
      <pubDate>Wed, 06 Mar 2013 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2013_benito_methods_in_ecology_and_evolution/</guid>
      <description></description>
    </item>
    
    <item>
      <title>European Bird distribution is âwellâ represented by Special Protected Areas: Mission accomplished?</title>
      <link>https://blasbenito.com/publication/2013_albuquerque_biological_conservation/</link>
      <pubDate>Mon, 21 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2013_albuquerque_biological_conservation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ModeleR: An enviromental model repository as knowledge base for experts</title>
      <link>https://blasbenito.com/publication/2012_perez-perez_expert_systems_with_applications/</link>
      <pubDate>Sat, 04 Feb 2012 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2012_perez-perez_expert_systems_with_applications/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Habitat Fragmentation in Arid Zones: A Case Study of Linaria nigricans Under Land Use Changes (SE Spain)</title>
      <link>https://blasbenito.com/publication/2011_penas_environmental_management/</link>
      <pubDate>Sat, 26 Mar 2011 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2011_penas_environmental_management/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Simulating potential effects of climatic warming on altitudinal patterns of key species in Mediterranean-alpine ecosystems</title>
      <link>https://blasbenito.com/publication/2011_benito_climate_change/</link>
      <pubDate>Wed, 02 Feb 2011 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2011_benito_climate_change/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Conservation Status of the First Known Population of Polygala balansae in Europe</title>
      <link>https://blasbenito.com/publication/2010_lorite_annales_botanici_fennici/</link>
      <pubDate>Mon, 01 Mar 2010 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2010_lorite_annales_botanici_fennici/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Past and present potential distribution of the Iberian Abies species: a phytogeographic approach using fossil pollen data and species distribution models </title>
      <link>https://blasbenito.com/publication/2010_alba-sanchez_diversity_and_distributions/</link>
      <pubDate>Sun, 21 Feb 2010 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2010_alba-sanchez_diversity_and_distributions/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Assessing extinction-risk of endangered plants using species distribution models: a case study of habitat depletion caused by the spread of greenhouses</title>
      <link>https://blasbenito.com/publication/2009_benito_biodiversity_and_conservation/</link>
      <pubDate>Sat, 28 Feb 2009 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2009_benito_biodiversity_and_conservation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Greenhouses, land use change, and predictive models: MaxEnt and Geomod working together</title>
      <link>https://blasbenito.com/publication/2008_penas_phyton/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/publication/2008_penas_phyton/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
