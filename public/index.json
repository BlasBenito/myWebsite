[{"authors":["admin"],"categories":null,"content":"I am biogeographer and ecological modeler with a PhD in Plant Ecology and Global Change, a Master in Geographical Information Systems, a Degree in Biology, and quite a bit of experience in applying state-of-the-art quantitative methods to better understand the processes shaping the distribution of biodiversity across space and time.\nI have developed my research career as PhD student at the Department of Botany of the University of Granada (2006 - 2009), staff researcher at IISTA, postdoctoral researcher at ECOINFO (2014 - 2017, Aarhus University, Denmark), led by Jens-Christian Svenning, postdoctoral researcher at EECRG (2017 - 2019), and currently as senior researcher at Maestre Lab, led by Fernando T. Maestre.\nMy research activity is structured around five main lines, namely biogeography, testing and development of quantitative methods, palaeoecology, range shift, and biodiversity conservation.\n Biogeography\nWhy are species where they are? is a question I have been passionate about since I was a child.\nA keystone result of my personal interest in this topic is the multidisciplinary study I led on the distribution of Neanderthals during the Last Interglacial ( Benito et al. 2016. This paper was highlighted at the Editor’s Picks section of Science ( Sugen 2017), and was among the top 20 most downloaded papers of the Journal of Bigeography during 2018. In Kellberg-Nielsen et al. (2017), led by my brilliant colleague Trine Kellberg-Nielsen we further discuss the dispersal dynamics of Neanderthals in their northern edge.\nI have also learned a a fair deal about the biogeography of plant phenological strategies through my collaboration with the outstanding Constantin Zohner. In Zohner et al. (2016) we found that plant species from lower latitudes use spring photoperiod to trigger leaf-out, while boreal species do not use photoperiod as leaf-out signal. In a complementary study ( Zohner et al. (2017)) we found that plant species from regions with high spring temperature variability have higher winter chilling requirements than species living in more predictable environments. More recently, in Zohner et al. (2020), we report increasing risks of late spring frosts in significant portions of European and Asian forests.\nLast, but not least in this section are my collaborations with the excellent biogeographer Gang Feng on the links between climatic and anthropogenetic legacies and plant distributions. In Feng et al. (2017) we found that tree assemblages with large phylogenetic age differences among species mostly inhabit areas with relatively high long‐term climate stability. The same year, in Feng et al. (2017) we evaluated the relationship between the distribution of threatened species and land-use change legacies, to find that the current distribution of threatened plants in China happens in places where historical land-use intensity was low, but has increased in the last decades.\n Testing and development of quantitative methods\nUnderstanding how quantitative methods work, developing new ones, and finding their limits of application are key axes of my research.\nHowever, this line of research started by designing and creating an infrastructure to document, store and execute the ecological models named ModeleR. This system, used to run the ecological models required by the Global Change Observatory of Sierra Nevada, is described in Perez, Benito and Bonet (2012) and Bonet et al (2014).\nIn 2013 I compared the ability of stacked species distribution models based on different statistical and machine learning methods to predict tree species richness and composition in Mesoamerica (Benito et al. 2013), and contributed to a comparison of SDMs to forecast the distribution of the seaweed Zostera marina in the Wadden Sea (Tovar et al. 2013).\nRecently I engaged in a collaboration with modelers and epidemiologists to warn against the use of species distribution models to forecast the expansion of SARS-CoV-2. In Carlson et al. (2020) we discuss the limitations of SDMs to model the direct transmission of the virus, and in Carlson et al. (2020) we point out that speculation on the relationships between the distribution of the virus and climate hinder decision making and preparedness. In the preprint Chipperfield et al. (2020) and in Contina et al. (2020, in press) we criticize two different misapplications of SDMs to the expansion of the virus.\nR packages\nAfter 10 years working with R I recently started to develop R packages. For example, the package distantia (Benito et al. 2020) implements several methods to quantify the dissimilarity among irregular multivariate time series.\nThe package memoria implements a method based on Random Forest to evaluate ecological memory (effect of antecedent conditions on a response variable) in time series data.\nI also designed a mechanistical simulation to produce virtual pollen curves in the package virtualPollen. The model uses a set of drivers, the ecological niches of a virtual species for these drivers, the traits life-span and fecundity, and the carrying capacity of the forest plot to simulate population dynamics over thousands of years.\nCurrently I am working on two other packages: sdmflow intentds to streamline the production of species distribution models based on the concept \u0026ldquo;use versus availability\u0026rdquo;, where the background data (a comprehensive sampling of the ecological conditions available in the study area) represents the availability, and the presence represents the use. The second package (still nameless) incorporates temporal and spatial autocorrelation through Moran Eigenvector Maps to machine learning models such as random forest, gradient boosting, or artificial neural networks, among others.\n Palaeoecology\nDuring the last years I have focused on applying state-of-the-art quantitative methods to better understand past ecological dynamics. Most of this work is a result of my ongoing collaboration with the bright Graciela Gil-Romera. We recently developed a framework to apply ecological memory concepts to millennial timescales in Benito et al. 2020. This paper was highlighted as an Editor\u0026rsquo;s Choice in the number 43 of the Ecography journal, and made it to the top 10% most downloaded papers of the journal in the period 2018-2019, after it was published as early view in October 22, 2019. A very kind reviewer wrote the following to the handling editor: \u0026ldquo;In my years of reviewing papers, this is by far one of the best and cleanest reviews I have encountered and the authors should be commended for that.\u0026quot;. That was a first in my research career!\nI helped implement these memory conceptsto better understand the relationship between fire and Erica spp. in the Bale Mountains of Ethiopia during the Holocene ( Gil-Romera et al. 2019) and the Pyrenees (Leunda et al. 2020).\n Range shift\nGlobal warming is changing the geographic distribution of climate, and organisms respond by either shifting their distributions through dispersal and colonization of new habitats, resisting change in situ, or going extinct. Species distribution models (SDM), with the help of future climate simulations, allow to model changes in habitat suitability over time. For example, in Benito et al. (2011) I evaluated future suitability change for four vegetation types in the Sierra Nevada mountain range (Granada, Spain). I have also contributed to future suitability change projections for the emblematic saguaro (Carnegiea gigantea) in the Sonoran Desert (Albuquerque et al. 2018), and three grouse species in Kozma et al (2018). I have also worked with mechanistic models simulating dispersal to forecast plant range shift and extinction in the southern Iberian Peninsula (Benito et al 2014).\n Biodiversity conservation\nThe first half of my PhD thesis was focused on understanding the threat posed by the expansion of greenhouses and habitat degradation on rare and endemic annual plants in the drylands of the Iberian Southwest. For example, I analyzed the historical habitat fragmentation of the habitat of Linaria nigricans using landscape fragmentation metrics ( Peñas, Benito et al. 2011), and modelled future greenhouse expansion on protected dryland habitats through correlative ( Benito et al. 2009) and mechanistic models ( Benito and Peñas 2008).\nWithin this research line I have also also engaged in collaborations to better understand the distribution and conservation status of an endemic butterfly ( Azcón, Benito et al 2014), assess how well Special Protected Areas protect birds in Europe ( Albuquerque et al. 2013), define the role of palaeoecology in conservation strategies ( Gill et al. 2015), and find conservation gaps for tree diversity in Mesoamerica ( Albuquerque, Benito et al. 2015).\n","date":1609459200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1609459200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/blas-m.-benito/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/blas-m.-benito/","section":"authors","summary":"I am biogeographer and ecological modeler with a PhD in Plant Ecology and Global Change, a Master in Geographical Information Systems, a Degree in Biology, and quite a bit of experience in applying state-of-the-art quantitative methods to better understand the processes shaping the distribution of biodiversity across space and time.","tags":null,"title":"Blas M. Benito","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"    This is a tutorial written for R users needing to compute betadiversity indices from species lists rather than from presence-absence matrices, and for R beginners or intermediate users that want to start using their own functions. If you are an advanced R user, this post will likely waste your time.   We ecologists like to measure all things in nature, and compositional changes in biological communities over time or space, a.k.a betadiversity, is one of these things. I am not going to explain what betadiversity is because others that know better than me have done it already. Good examples are this post published the blog of Methods in Ecology and Evolution by Andres Baselga, and this lecture by Tim Seipel.\nWhat I am actually going to do in this post is to explain how to write functions to compute betadiversity indices in R from species lists rather than from presence-absence matrices. For the latter there are a few packages such as vegan, BAT, MBI, or betapart, but for the former I was unable to find anything suitable. To make this post useful for R beginners, I will go step by step on the rationale behind the design of the functions to compute betadiversity indices, and by the end of the post I will explain how to organize them to achieve a clean R workflow.\nLet’s go!\n Betadiversity indices There are a few betadiversity indices out there, and I totally recommend you to start with Koleff et al. (2003) as a primer. They review the literature and analyze the properties of 24 different indices to provide guidance on how to use them.\nBetadiversity components a, b, and c Betadiversity indices are designed to compare the taxa pools of two sites at a time, and require the computation of three components:\n a: number of common taxa of both sites. b: number of exclusive taxa of one site. c: number of exclusive taxa of the other site.  Let’s see how can we use these diversity components to compute betadiversity indices.\n Sørensen’s Beta Let’s start with the Sørensen’s Beta (\\(\\beta_{sor}\\) hereafter), as presented in Koleff et al. (2003).\n\\[\\beta_{sor} = \\frac{2a}{2a + b + c}\\]\n\\(\\beta_{sor}\\) is a similarity index in the range [0, 1] (the closer to one, the more similar the taxa pools of both sites are) that puts a lot of weight in the \\(a\\) component, and is therefore a measure of continuity, as it focuses the most in the common taxa among sites.\n Simpson’s Beta Another popular betadiversity index is the Simpson’s Beta (\\(\\beta_{sim}\\) hereafter).\n\\[\\beta_{sim} = \\frac{min(b, c)}{min(b, c) + a}\\] where \\(min()\\) is a function that takes the minimum value among the diversity components within the parenthesis. \\(\\beta_{sim}\\) is a dissimilarity measure that focuses on compositional turnover among sites because it focuses the most on the values of \\(b\\) and \\(c\\). It has its lower bound in zero, and an open upper value.\nTo bring these ideas into R, first we have to load a few R packages, and generate some fake data to help us develop the functions.\nlibrary(magrittr) library(foreach) library(doParallel) ## Loading required package: iterators ## Loading required package: parallel The code chunk below generates 15 fake taxa names, from taxon_1 to taxon_15.\ntaxa \u0026lt;- paste0(\u0026quot;taxon_\u0026quot;, 1:15) With these fake taxa we are going to generate taxa lists for four hypothetical sites named site1, site2, site3, and site4. Two of the sites will have identical taxa lists, two will have non-overlapping taxa lists, and two of them will have some overlap.\nsite1 \u0026lt;- site2 \u0026lt;- taxa[1:7] site3 \u0026lt;- taxa[8:12] site4 \u0026lt;- taxa[10:15] So now we have these taxa lists:\nsite1 #and site2 ## [1] \u0026quot;taxon_1\u0026quot; \u0026quot;taxon_2\u0026quot; \u0026quot;taxon_3\u0026quot; \u0026quot;taxon_4\u0026quot; \u0026quot;taxon_5\u0026quot; \u0026quot;taxon_6\u0026quot; \u0026quot;taxon_7\u0026quot; site3 ## [1] \u0026quot;taxon_8\u0026quot; \u0026quot;taxon_9\u0026quot; \u0026quot;taxon_10\u0026quot; \u0026quot;taxon_11\u0026quot; \u0026quot;taxon_12\u0026quot; site4 ## [1] \u0026quot;taxon_10\u0026quot; \u0026quot;taxon_11\u0026quot; \u0026quot;taxon_12\u0026quot; \u0026quot;taxon_13\u0026quot; \u0026quot;taxon_14\u0026quot; \u0026quot;taxon_15\u0026quot;    Step-by-step computation of betadiversity indices with R For a given pair of sites, how can we compute the diversity components a, b, and c?\nLooking at it from an R perspective, each site is a character vector, so a can be found by counting the number of common elements between two vectors. These common elements can be found with the function intersect(), and the number of elements can be computed by applying length() on the result of intersect().\na \u0026lt;- length(intersect(site3, site4)) a ## [1] 3 To compute b and c we can use the function setdiff(), that finds the exclusive elements of one character vector when comparing it with another. In this case, b is computed for the first vector introduced in the function, site3 in this case…\nb \u0026lt;- length(setdiff(site3, site4)) b ## [1] 2 … so to compute the c component we only need to switch the sites.\nc \u0026lt;- length(setdiff(site4, site3)) c ## [1] 3 Now that we know a, b, and c, we can compute \\(\\beta_{sor}\\) and \\(\\beta_{sim}\\).\nBsor \u0026lt;- 2 * a / (2 * a + b + c) Bsor ## [1] 0.5454545 Bsim\u0026lt;- min(b, c) / (min(b, c) + a) Bsim ## [1] 0.4 Of course, if we have a long list of sites, computing betadiversity indices like this can get quite boring quite fast. Let’s put everything in a set of functions to make it easier to work with.\n  Writing functions to compute betadiversity indices The basic structure of a function definition in R looks as follows:\nfunction_name \u0026lt;- function(x, y, ...){ output \u0026lt;- [body] output #also return(output) } Where:\n function_name is the name of your function. Ideally, a verb, or otherwise, something indicating somehow what the function will do with the input data and arguments. function() is a function to define functions, there isn’t much more to it… x is the first argument of the function, and ideally, represents the input data. If that is the case, you can later use pipes (%\u0026gt;%) to chain functions together. y (it could have any other name) is another function argument, an can be either another input dataset, or an argument defining how the function has to behave. ... refers to other arguments the function may require. body is the code that operates with the data and function arguments. This can be one line of code, or a thousand, it all comes down to the function’s objective. In any case, the body must return an object (or an error if something went wrong) that will be the function’s output. output is the object ultimately produced by the function. It can have any name, and can be any kind of structure, such a number, a vector, a data frame, a list, etc. R functions return one output object only. Since R functions return the last evaluated value, it is good practice to put the output object at the end of the function as an explicit way to state what the actual output of the function is.  Let’s start writing a function to compute a, b, and c from a pair of sites.\n#x: taxa list of one site #y: taxa list of another site abc \u0026lt;- function(x, y){ #list to store output out \u0026lt;- list() #filling the list out$a \u0026lt;- length(intersect(x, y)) out$b \u0026lt;- length(setdiff(x, y)) out$c \u0026lt;- length(setdiff(y, x)) #returning the output out } Notice that to to return the three values I am wrapping them in a list. Let’s run a little test.\nx \u0026lt;- abc( x = site3, y = site4 ) x ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 So far so good! From here we build the functions sorensen_beta() and simpson_beta() making sure they can accept the output of abc(), and return it with an added slot.\nsorensen_beta \u0026lt;- function(x){ x$bsor \u0026lt;- round(2 * x$a / (2 * x$a + x$b + x$c), 3) x } simpson_beta \u0026lt;- function(x){ x$bsim \u0026lt;- round(min(x$b, x$c) / (min(x$b, x$c) + x$a), 3) x } Notice that both functions are returning the input x with an added slot named after the given betadiversity index. Let’s test them first, to later see why returning the input object gives these functions a lot of flexibility.\nsorensen_beta(x) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 ## ## $bsor ## [1] 0.545 simpson_beta(x) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 ## ## $bsim ## [1] 0.4 When I said that returning the input object with an added slot gave these functions a lot of flexibility I was talking about this:\nx \u0026lt;- abc( x = site3, y = site4 ) %\u0026gt;% sorensen_beta() %\u0026gt;% simpson_beta() x ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 ## ## $bsor ## [1] 0.545 ## ## $bsim ## [1] 0.4 Chaining the functions through the %\u0026gt;% pipe now allows us to combine their results in a single output no matter whether we use sorensen_beta() or sorensen_beta() first, or whether we omit one of them.\nWe can put that idea right away into a function to compute both betadiversity indices at once from the taxa list of a pair of sites.\nbetadiversity \u0026lt;- function(x, y){ require(magrittr) abc(x, y) %\u0026gt;% sorensen_beta() %\u0026gt;% simpson_beta() } The function now works as follows.\nx \u0026lt;- betadiversity( x = site3, y = site4 ) x ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 ## ## $bsor ## [1] 0.545 ## ## $bsim ## [1] 0.4 So far we have four functions…\n abc() simpson_beta(), that requires abc(). sorensen_beta(), that requires abc(). betadiversity(), that requires abc(), simpson_beta(), and sorensen_beta().  … and one limitation: so far we can only return betadiversity indices for two sites at a time. So at the moment, to compute betadiversity indices for all combinations of sites we have to do a pretty ridiculous thing:\nx1 \u0026lt;- betadiversity(x = site1, y = site2) x2 \u0026lt;- betadiversity(x = site1, y = site3) x3 \u0026lt;- betadiversity(x = site1, y = site4) #... and so on If I see you doing this I’ll come to haunt you in your nightmares! Since a real analysis may involve hundreds of sites, the next step is to use the functions above to build a new one able to intake an arbitrary number of sites.\n  Writing a function to compute betadiversity indices for an arbitrary number of sites. First we have to organize our sites in a data frame with a long format.\nsites \u0026lt;- data.frame( site = c( rep(\u0026quot;site1\u0026quot;, length(site1)), rep(\u0026quot;site2\u0026quot;, length(site2)), rep(\u0026quot;site3\u0026quot;, length(site3)), rep(\u0026quot;site4\u0026quot;, length(site4)) ), taxon = c( site1, site2, site3, site4 ) )    site  taxon      site1  taxon_1    site1  taxon_2    site1  taxon_3    site1  taxon_4    site1  taxon_5    site1  taxon_6    site1  taxon_7    site2  taxon_1    site2  taxon_2    site2  taxon_3    site2  taxon_4    site2  taxon_5    site2  taxon_6    site2  taxon_7    site3  taxon_8    site3  taxon_9    site3  taxon_10    site3  taxon_11    site3  taxon_12    site4  taxon_10    site4  taxon_11    site4  taxon_12    site4  taxon_13    site4  taxon_14    site4  taxon_15     Our new function will need to do several things:\n Generate combinations of the unique values of the column site two by two without repetition. Iterate through these combinations of two sites to compute betadiversity components and indices. Return a dataframe with the results to facilitate further analyses.  The combinations of site pairs are done with utils::combn() as follows:\nsite.combinations \u0026lt;- utils::combn( x = unique(sites$site), m = 2 ) site.combinations ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] \u0026quot;site1\u0026quot; \u0026quot;site1\u0026quot; \u0026quot;site1\u0026quot; \u0026quot;site2\u0026quot; \u0026quot;site2\u0026quot; \u0026quot;site3\u0026quot; ## [2,] \u0026quot;site2\u0026quot; \u0026quot;site3\u0026quot; \u0026quot;site4\u0026quot; \u0026quot;site3\u0026quot; \u0026quot;site4\u0026quot; \u0026quot;site4\u0026quot; The result is a matrix, and each pair of rows in a column contain a pair of sites. The idea now is to iterate over the matrix columns, obtain the set of taxa from each site from the taxon column of the sites data frame, and use these taxa lists to compute the betadiversity components and indices.\nTo easily generate the output data frame, I use the foreach::foreach() function to iterate through pairs instead of a more traditional for loop. You can read more about foreach() in a previous post.\nbetadiversity.df \u0026lt;- foreach::foreach( i = 1:ncol(site.combinations), #iterates through columns of site.combinations .combine = \u0026#39;rbind\u0026#39; #to produce a data frame ) %do% { #site names site.one \u0026lt;- site.combinations[1, i] #from column i, row 1 site.two \u0026lt;- site.combinations[2, i] #from column i, row 2 #getting taxa lists taxa.list.one \u0026lt;- sites[sites$site %in% site.one, \u0026quot;taxon\u0026quot;] taxa.list.two \u0026lt;- sites[sites$site %in% site.two, \u0026quot;taxon\u0026quot;] #betadiversity beta \u0026lt;- betadiversity( x = taxa.list.one, y = taxa.list.two ) #adding site names beta$site.one \u0026lt;- site.one beta$site.two \u0026lt;- site.two #returning output beta }    a  b  c  bsor  bsim  site.one  site.two      7  0  0  1  0  site1  site2    0  7  5  0  1  site1  site3    0  7  6  0  1  site1  site4    0  7  5  0  1  site2  site3    0  7  6  0  1  site2  site4    3  2  3  0.545  0.4  site3  site4     Now that we know it works, we can put everything together in a function. Notice that to make the function more general, I have added arguments requesting the names of the columns with the site and the taxa names.\nbetadiversity_multisite \u0026lt;- function( x, site.column, #column with site names taxa.column #column with taxa names ){ #get site combinations site.combinations \u0026lt;- utils::combn( x = unique(x[, site.column]), m = 2 ) #iterating through site pairs betadiversity.df \u0026lt;- foreach::foreach( i = 1:ncol(site.combinations), .combine = \u0026#39;rbind\u0026#39; ) %do% { #site names site.one \u0026lt;- site.combinations[1, i] site.two \u0026lt;- site.combinations[2, i] #getting taxa lists taxa.list.one \u0026lt;- x[x[, site.column] %in% site.one, taxa.column] taxa.list.two \u0026lt;- x[x[, site.column] %in% site.two, taxa.column] #betadiversity beta \u0026lt;- betadiversity( x = taxa.list.one, y = taxa.list.two ) #adding site names beta$site.one \u0026lt;- site.one beta$site.two \u0026lt;- site.two #returning output beta } #remove bad rownames rownames(betadiversity.df) \u0026lt;- NULL #reordering columns betadiversity.df \u0026lt;- betadiversity.df[, c( \u0026quot;site.one\u0026quot;, \u0026quot;site.two\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;bsor\u0026quot;, \u0026quot;bsim\u0026quot; )] #returning output return(betadiversity.df) } And the test!\nsites.betadiversity \u0026lt;- betadiversity_multisite( x = sites, site.column = \u0026quot;site\u0026quot;, taxa.column = \u0026quot;taxon\u0026quot; )    site.one  site.two  a  b  c  bsor  bsim      site1  site2  7  0  0  1  0    site1  site3  0  7  5  0  1    site1  site4  0  7  6  0  1    site2  site3  0  7  5  0  1    site2  site4  0  7  6  0  1    site3  site4  3  2  3  0.545  0.4     That went well!\nFinally, to have these functions available in my R session I always put them all in a single file in the same folder where my Rstudio project lives, name it something like functions_betadiversity.R, and source it at the beginning of my script or .Rmd file by running a line like the one below.\nsource(\u0026quot;functions_betadiversity.R\u0026quot;) I have placed the file functions_betadiversity.R in this GitHub Gist in case you want to give it a look. You can also source it right away to your R environment by executing the following line:\nsource(\u0026quot;https://gist.githubusercontent.com/BlasBenito/4c3740b056a0c9bb3602f33dfd35990c/raw/bbb40d868787fc5d10e391a2121045eb5d75f165/functions_betadiversity.R\u0026quot;) I hope this post helped you to better understand how to write and organize R functions!\n ","date":1609891200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609891200,"objectID":"09127f8961ebfb017dfd29461698bb7c","permalink":"/post/04_betadiversity/","publishdate":"2021-01-06T00:00:00Z","relpermalink":"/post/04_betadiversity/","section":"post","summary":"This is a tutorial written for R users needing to compute betadiversity indices from species lists rather than from presence-absence matrices, and for R beginners or intermediate users that want to start using their own functions.","tags":null,"title":"Designing R functions to compute betadiversity indices from species lists","type":"post"},{"authors":null,"categories":null,"content":"   In the previous posts I have covered how to setup a home cluster, and how to run parallel processes with foreach in R. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.\nThis post (developed on Ubuntu and Ubuntu Server 20.04) assumes that the home cluster runs on Linux, and covers the following topics.\n Basics of the Network File System protocol (NFS). Setup of an NFS folder in a home cluster. Using an NFS folder in a parallelized loop.   The Network File System protocol (NFS) The Network File System protocol offers the means for a host computer to allow other computers in the network (clients) to read and write the contents of a given folder. The clients access such folder like if it was a local one, while in fact it is a reference to the one in the host computer.\nThe image at the beginning of the post illustrates the concept. There is a host computer with a folder in the path /home/user/cluster_shared (were user is your user name) that is broadcasted to the network, and there are one or several clients that are mounting mounting (making accessible) the same folder in their local paths /home/user/cluster_shared.\nIf the host writes a file to the shared folder, it is available right away for the clients, and the other way around. At the end, the idea is to have a folder shared among all computers in the cluster, while having the same exact path on each one of them to write or read files from such shared folder.\n  Setup of an NFS folder in a home cluster To setup the shared folder we’ll need to do some things in the host, and some things in the clients. Let’s start with the host.\nPreparing the host computer First we need to install the nfs-kernel-server.\nsudo apt update sudo apt install nfs-kernel-server Now we can create the shared folder. Remember to replace user with your user name, and cluster_shared with the actual folder name you want to use.\nmkdir /home/user/cluster_shared To broadcast it we need to open the file /etc/exports…\nsudo gedit /etc/exports … and add the following line\n/home/user/cluster_shared IP_CLIENT1(rw,no_subtree_check) IP_CLIENT2(rw,no_subtree_check) IP_CLIENT3(rw,no_subtree_check) where:\n /home/user/cluster_shared is the path of the shared folder. IP_CLIENTx are the IPs of each one of the clients. rw gives reading and writing permission on the shared folder to the given client. no_subtree_check prevents the host from checking the complete tree of shares before attending a request (read or write) by a client.  For example, the last line of my /etc/exports file looks like this:\n/home/blas/cluster_shared 10.42.0.34(rw,async,no_subtree_check) 10.42.0.104(rw,async,no_subtree_check) Save the file, and to make the changes effective, execute:\nsudo exportfs -ra To end preparing the host we have to update the firewall rules to allow nfs connections from the clients. Notice that one rule per client needs to be defined, using the clients IPs to identify them.\nsudo ufw allow from IP_CLIENT1 to any port nfs sudo ufw allow from IP_CLIENT2 to any port nfs sudo ufw status  Preparing the clients First we have to install the Linux package nfs-common on each client.\nsudo apt update sudp apt install nfs-common Now we can create a folder in the clients and use it to mount the NFS folder of the host.\nmkdir -p /home/user/cluster_shared sudo mount IP_HOST:/home/user/cluster_shared /home/user/cluster_shared The second line of code is mounting the folder /home/user/cluster_shared of the host in the folder /home/user/cluster_shared of the client.\nTo make the mount permanent, we have to open /etc/fstab with super-user privilege in the clients…\nsudo gedit /etc/fstab … and add the line\nIP_HOST:/home/user/cluster_shared /home/user/cluster_shared nfs defaults 0 0 Remember to replace IP_HOST and user with the right values!\nNow we can test that the shared folder works as intended by writing one file into it from each computer in the network using the code below in the command line.\ncd cluster_shared touch filename.txt Once the files are created, we can check they are visible from each computer using the ls command.\nls    Using an NFS folder in a parallelized loop In a previous post I described how to run parallelized tasks with foreach in R. This section intends to complete that post by showing how a shared NFS folder can be used to store input and output files during the execution of a parallelized loop\nThe task In this hypothetical example we have a large number of data frames stored in /home/user/cluster_shared/input. Each data frame has the same predictors a, b, c, and d, and a different response variable, named y1 for the data frame y1, y2 for the data frame y2, and so on. In the example we will be using 100 data frames, but there could be thousands. This example would be valid as well for any other kind of object, such as raster files, time-series, etc etc.\nFirst we have to load the libraries we’ll be using.\n#automatic install of packages if they are not installed already list.of.packages \u0026lt;- c( \u0026quot;foreach\u0026quot;, \u0026quot;doParallel\u0026quot;, \u0026quot;ranger\u0026quot; ) new.packages \u0026lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\u0026quot;Package\u0026quot;])] if(length(new.packages) \u0026gt; 0){ install.packages(new.packages, dep=TRUE) } #loading packages for(package.i in list.of.packages){ suppressPackageStartupMessages( library( package.i, character.only = TRUE ) ) } The code chunk below generates the folder /home/user/cluster_shared/input and populates it with the dummy files.\n#creating the input folder input.folder \u0026lt;- \u0026quot;/home/blas/cluster_shared/input\u0026quot; dir.create(input.folder) #data frame names df.names \u0026lt;- paste0(\u0026quot;y\u0026quot;, 1:100) #filling it with files for(i in df.names){ #creating the df df.i \u0026lt;- data.frame( y = rnorm(1000), a = rnorm(1000), b = rnorm(1000), c = rnorm(1000), d = rnorm(1000) ) #changing name of the response variable colnames(df.i)[1] \u0026lt;- i #assign to a variable with name i assign(i, df.i) #saving the object save( list = i, file = paste0(input.folder, \u0026quot;/\u0026quot;, i, \u0026quot;.RData\u0026quot;) ) #removing the generated data frame form the environment rm(list = i, df.i, i) } Our target now will be to fit one ranger::ranger() model per data frame stored in /home/blas/cluster_shared/input, save the model result to a folder with the path /home/blas/cluster_shared/input, and write a small summary of the model to the output of foreach.\nSuch target is based on this rationale: When executing a foreach loop as in x \u0026lt;- foreach(...) %dopar% {...}, the variable x is going to grow in memory very fast, competing for RAM resources with the worker nodes. Furthermore, since x is being written on the fly, the results would be lost if the computer crashes. When the size of the input and the output of our parallelized operation is larger than memory, we can use an NFS folder to store inputs and outputs, while keeping the RAM memory free for computational tasks only, with the positive side effect of having our outputs already stored should our computer decide to crash.\nAlso, please notice that here I am focusing in a cluster setting, but using a folder to read and write data during a loop paralellized with foreach can indeed be done in a single computer without an NFS folder. Any folder in your system will do the trick as well!\nSo, from here, we are going to prepare the cluster, and execute a parallelized loop fitting one model per data frame that reads the inputs and writes the outputs to the shared folder.\n Cluster setup We will also need the function I showed in the previous post to generate the cluster specification from a GitHub Gist.\nsource(\u0026quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R\u0026quot;) Below I use the function to create a cluster specification and initiate the cluster with parallel::makeCluster().\n#generate cluster specification spec \u0026lt;- cluster_spec( ips = c(\u0026#39;10.42.0.1\u0026#39;, \u0026#39;10.42.0.34\u0026#39;, \u0026#39;10.42.0.104\u0026#39;), cores = c(7, 4, 4), user = \u0026quot;blas\u0026quot; ) #define parallel port Sys.setenv(R_PARALLEL_PORT = 11000) Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;) #setting up cluster my.cluster \u0026lt;- parallel::makeCluster( master = \u0026#39;10.42.0.1\u0026#39;, spec = spec, port = Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;), outfile = \u0026quot;\u0026quot;, homogeneous = TRUE ) #check cluster definition (optional) print(my.cluster) #register cluster doParallel::registerDoParallel(cl = my.cluster) #check number of workers foreach::getDoParWorkers()  Parallelized loop For everything to work as intended, we first need to create the output folder.\noutput.folder \u0026lt;- \u0026quot;/home/blas/cluster_shared/output\u0026quot; dir.create(output.folder) And now we are ready to execute the parallelized loop. Notice that I am using the output of list.files() to have a vector of file names the loop will iterate over. The steps inside of the loop go as follows:\n 1. Remove the extension .RData from the file name. We’ll later use the result to use assign() on the fitted model to change its name to the same as the input file before saving it. 2. Read the input data frame and store in an object named df. 3. Fit the model with ranger, using the first column of df as respose variable. 4. Change the model name to the name of the input file without extension, resulting from the first step described above. 5. Save the model into the output folder with the extension .RData. 6. Return a data frame with one line with the name of the response variable, the r-squared, and the importance of each predictor.  #list of input files as iterator input.files \u0026lt;- list.files( path = input.folder, full.names = FALSE ) modelling.summary \u0026lt;- foreach( input.file = input.files, .combine = \u0026#39;rbind\u0026#39;, .packages = \u0026quot;ranger\u0026quot; ) %dopar% { # 1. input file name without extension input.file.name \u0026lt;- tools::file_path_sans_ext(input.file) # 2. read input file df \u0026lt;- get(load(paste0(input.folder, \u0026quot;/\u0026quot;, input.file))) # 3. fit model m.i \u0026lt;- ranger::ranger( data = df, dependent.variable.name = colnames(df)[1], importance = \u0026quot;permutation\u0026quot; ) # 4. change name of the model to one of the response variable assign(input.file.name, m.i) # 5. save model save( list = input.file.name, file = paste0(output.folder, \u0026quot;/\u0026quot;, input.file) ) # 6. returning summary return( data.frame( response.variable = input.file.name, r.squared = m.i$r.squared, importance.a = m.i$variable.importance[\u0026quot;a\u0026quot;], importance.b = m.i$variable.importance[\u0026quot;b\u0026quot;], importance.c = m.i$variable.importance[\u0026quot;c\u0026quot;], importance.d = m.i$variable.importance[\u0026quot;d\u0026quot;] ) ) } Once this parallelized loop is executed, the folder /home/blas/cluster_shared/output should be filled with the results from the cluster workers, and the modelling.summary data frame contains the summary of each fitted model.\nNow that the work is done, we can stop the cluster.\nparallel::stopCluster(cl = my.cluster) Now you know how to work with data larger than memory in a parallelized loop!\n  ","date":1609632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609632000,"objectID":"bf28eeedb27403ed52da5fe9b61d9449","permalink":"/post/03_shared_folder_in_cluster/","publishdate":"2021-01-03T00:00:00Z","relpermalink":"/post/03_shared_folder_in_cluster/","section":"post","summary":"In the previous posts I have covered how to setup a home cluster, and how to run parallel processes with foreach in R. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.","tags":null,"title":"Setup of a shared folder in a home cluster","type":"post"},{"authors":null,"categories":null,"content":"   In the previous posts I have covered how to setup a home cluster, and how to run parallel processes with foreach in R. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.\nThis post (developed on Ubuntu and Ubuntu Server 20.04) assumes that the home cluster runs on Linux, and covers the following topics.\n Basics of the Network File System protocol (NFS). Setup of an NFS folder in a home cluster. Using an NFS folder in a parallelized loop.   The Network File System protocol (NFS) The Network File System protocol offers the means for a host computer to allow other computers in the network (clients) to read and write the contents of a given folder. The clients access such folder like if it was a local one, while in fact it is a reference to the one in the host computer.\nThe image at the beginning of the post illustrates the concept. There is a host computer with a folder in the path /home/user/cluster_shared (were user is your user name) that is broadcasted to the network, and there are one or several clients that are mounting mounting (making accessible) the same folder in their local paths /home/user/cluster_shared.\nIf the host writes a file to the shared folder, it is available right away for the clients, and the other way around. At the end, the idea is to have a folder shared among all computers in the cluster, while having the same exact path on each one of them to write or read files from such shared folder.\n  Setup of an NFS folder in a home cluster To setup the shared folder we’ll need to do some things in the host, and some things in the clients. Let’s start with the host.\nPreparing the host computer First we need to install the nfs-kernel-server.\nsudo apt update sudo apt install nfs-kernel-server Now we can create the shared folder. Remember to replace user with your user name, and cluster_shared with the actual folder name you want to use.\nmkdir /home/user/cluster_shared To broadcast it we need to open the file /etc/exports…\nsudo gedit /etc/exports … and add the following line\n/home/user/cluster_shared IP_CLIENT1(rw,no_subtree_check) IP_CLIENT2(rw,no_subtree_check) IP_CLIENT3(rw,no_subtree_check) where:\n /home/user/cluster_shared is the path of the shared folder. IP_CLIENTx are the IPs of each one of the clients. rw gives reading and writing permission on the shared folder to the given client. no_subtree_check prevents the host from checking the complete tree of shares before attending a request (read or write) by a client.  For example, the last line of my /etc/exports file looks like this:\n/home/blas/cluster_shared 10.42.0.34(rw,async,no_subtree_check) 10.42.0.104(rw,async,no_subtree_check) Save the file, and to make the changes effective, execute:\nsudo exportfs -ra To end preparing the host we have to update the firewall rules to allow nfs connections from the clients. Notice that one rule per client needs to be defined, using the clients IPs to identify them.\nsudo ufw allow from IP_CLIENT1 to any port nfs sudo ufw allow from IP_CLIENT2 to any port nfs sudo ufw status  Preparing the clients First we have to install the Linux package nfs-common on each client.\nsudo apt update sudp apt install nfs-common Now we can create a folder in the clients and use it to mount the NFS folder of the host.\nmkdir -p /home/user/cluster_shared sudo mount IP_HOST:/home/user/cluster_shared /home/user/cluster_shared The second line of code is mounting the folder /home/user/cluster_shared of the host in the folder /home/user/cluster_shared of the client.\nTo make the mount permanent, we have to open /etc/fstab with super-user privilege in the clients…\nsudo gedit /etc/fstab … and add the line\nIP_HOST:/home/user/cluster_shared /home/user/cluster_shared nfs defaults 0 0 Remember to replace IP_HOST and user with the right values!\nNow we can test that the shared folder works as intended by writing one file into it from each computer in the network using the code below in the command line.\ncd cluster_shared touch filename.txt Once the files are created, we can check they are visible from each computer using the ls command.\nls    Using an NFS folder in a parallelized loop In a previous post I described how to run parallelized tasks with foreach in R. This section intends to complete that post by showing how a shared NFS folder can be used to store input and output files during the execution of a parallelized loop\nThe task In this hypothetical example we have a large number of data frames stored in /home/user/cluster_shared/input. Each data frame has the same predictors a, b, c, and d, and a different response variable, named y1 for the data frame y1, y2 for the data frame y2, and so on. In the example we will be using 100 data frames, but there could be thousands. This example would be valid as well for any other kind of object, such as raster files, time-series, etc etc.\nFirst we have to load the libraries we’ll be using.\n#automatic install of packages if they are not installed already list.of.packages \u0026lt;- c( \u0026quot;foreach\u0026quot;, \u0026quot;doParallel\u0026quot;, \u0026quot;ranger\u0026quot; ) new.packages \u0026lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\u0026quot;Package\u0026quot;])] if(length(new.packages) \u0026gt; 0){ install.packages(new.packages, dep=TRUE) } #loading packages for(package.i in list.of.packages){ suppressPackageStartupMessages( library( package.i, character.only = TRUE ) ) } The code chunk below generates the folder /home/user/cluster_shared/input and populates it with the dummy files.\n#creating the input folder input.folder \u0026lt;- \u0026quot;/home/blas/cluster_shared/input\u0026quot; dir.create(input.folder) #data frame names df.names \u0026lt;- paste0(\u0026quot;y\u0026quot;, 1:100) #filling it with files for(i in df.names){ #creating the df df.i \u0026lt;- data.frame( y = rnorm(1000), a = rnorm(1000), b = rnorm(1000), c = rnorm(1000), d = rnorm(1000) ) #changing name of the response variable colnames(df.i)[1] \u0026lt;- i #assign to a variable with name i assign(i, df.i) #saving the object save( list = i, file = paste0(input.folder, \u0026quot;/\u0026quot;, i, \u0026quot;.RData\u0026quot;) ) #removing the generated data frame form the environment rm(list = i, df.i, i) } Our target now will be to fit one ranger::ranger() model per data frame stored in /home/blas/cluster_shared/input, save the model result to a folder with the path /home/blas/cluster_shared/input, and write a small summary of the model to the output of foreach.\nSuch target is based on this rationale: When executing a foreach loop as in x \u0026lt;- foreach(...) %dopar% {...}, the variable x is going to grow in memory very fast, competing for RAM resources with the worker nodes. Furthermore, since x is being written on the fly, the results would be lost if the computer crashes. When the size of the input and the output of our parallelized operation is larger than memory, we can use an NFS folder to store inputs and outputs, while keeping the RAM memory free for computational tasks only, with the positive side effect of having our outputs already stored should our computer decide to crash.\nAlso, please notice that here I am focusing in a cluster setting, but using a folder to read and write data during a loop paralellized with foreach can indeed be done in a single computer without an NFS folder. Any folder in your system will do the trick as well!\nSo, from here, we are going to prepare the cluster, and execute a parallelized loop fitting one model per data frame that reads the inputs and writes the outputs to the shared folder.\n Cluster setup We will also need the function I showed in the previous post to generate the cluster specification from a GitHub Gist.\nsource(\u0026quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R\u0026quot;) Below I use the function to create a cluster specification and initiate the cluster with parallel::makeCluster().\n#generate cluster specification spec \u0026lt;- cluster_spec( ips = c(\u0026#39;10.42.0.1\u0026#39;, \u0026#39;10.42.0.34\u0026#39;, \u0026#39;10.42.0.104\u0026#39;), cores = c(7, 4, 4), user = \u0026quot;blas\u0026quot; ) #define parallel port Sys.setenv(R_PARALLEL_PORT = 11000) Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;) #setting up cluster my.cluster \u0026lt;- parallel::makeCluster( master = \u0026#39;10.42.0.1\u0026#39;, spec = spec, port = Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;), outfile = \u0026quot;\u0026quot;, homogeneous = TRUE ) #check cluster definition (optional) print(my.cluster) #register cluster doParallel::registerDoParallel(cl = my.cluster) #check number of workers foreach::getDoParWorkers()  Parallelized loop For everything to work as intended, we first need to create the output folder.\noutput.folder \u0026lt;- \u0026quot;/home/blas/cluster_shared/output\u0026quot; dir.create(output.folder) And now we are ready to execute the parallelized loop. Notice that I am using the output of list.files() to have a vector of file names the loop will iterate over. The steps inside of the loop go as follows:\n 1. Remove the extension .RData from the file name. We’ll later use the result to use assign() on the fitted model to change its name to the same as the input file before saving it. 2. Read the input data frame and store in an object named df. 3. Fit the model with ranger, using the first column of df as respose variable. 4. Change the model name to the name of the input file without extension, resulting from the first step described above. 5. Save the model into the output folder with the extension .RData. 6. Return a data frame with one line with the name of the response variable, the r-squared, and the importance of each predictor.  #list of input files as iterator input.files \u0026lt;- list.files( path = input.folder, full.names = FALSE ) modelling.summary \u0026lt;- foreach( input.file = input.files, .combine = \u0026#39;rbind\u0026#39;, .packages = \u0026quot;ranger\u0026quot; ) %dopar% { # 1. input file name without extension input.file.name \u0026lt;- tools::file_path_sans_ext(input.file) # 2. read input file df \u0026lt;- get(load(paste0(input.folder, \u0026quot;/\u0026quot;, input.file))) # 3. fit model m.i \u0026lt;- ranger::ranger( data = df, dependent.variable.name = colnames(df)[1], importance = \u0026quot;permutation\u0026quot; ) # 4. change name of the model to one of the response variable assign(input.file.name, m.i) # 5. save model save( list = input.file.name, file = paste0(output.folder, \u0026quot;/\u0026quot;, input.file) ) # 6. returning summary return( data.frame( response.variable = input.file.name, r.squared = m.i$r.squared, importance.a = m.i$variable.importance[\u0026quot;a\u0026quot;], importance.b = m.i$variable.importance[\u0026quot;b\u0026quot;], importance.c = m.i$variable.importance[\u0026quot;c\u0026quot;], importance.d = m.i$variable.importance[\u0026quot;d\u0026quot;] ) ) } Once this parallelized loop is executed, the folder /home/blas/cluster_shared/output should be filled with the results from the cluster workers, and the modelling.summary data frame contains the summary of each fitted model.\nNow that the work is done, we can stop the cluster.\nparallel::stopCluster(cl = my.cluster) Now you know how to work with data larger than memory in a parallelized loop!\n  ","date":1609632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609632000,"objectID":"4eb8b71dc831df7557610c90552ef52b","permalink":"/post_projects/template/","publishdate":"2021-01-03T00:00:00Z","relpermalink":"/post_projects/template/","section":"post_projects","summary":"In the previous posts I have covered how to setup a home cluster, and how to run parallel processes with foreach in R. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.","tags":null,"title":"Setup of a shared folder in a home cluster","type":"post_projects"},{"authors":null,"categories":null,"content":"This is a spatio-temporal simulation of the effect of fire regimes on the population dynamics of five forest species (Pinus sylvestris, Pinus uncinata, Betula pendula, Corylus avellana, and Quercus petraea) during the Lateglacial-Holocene transition (15-7 cal Kyr BP) at El Portalet, a subalpine bog located in the central Pyrenees region (1802m asl, Spain), that has served for palaeoenvironmental studies (González-Smapériz et al. 2006; Gil-Romera et al., 2014). This model is described in the paper in prep. titled Forest - fire interactions in the Central Pyrenees: a data-model comparison for the Lateglacial-Holocene transition, and authored by Graciela Gil-Romera, Blas M. Benito, Juli G. Pausas, Penélope González-Sampériz, J. Julio. Camarero, Jens-Christian Svenning, and Blas Valero-Garcés.\nA short description of the project was presented by Graciela Gil-Romera at the 1st Quaternary Palaeoecology Twitter Conference.\n1 #QUAPAL18 [Coauthored by P. González-Sampériz @jgpausas @2xeu @JCSvenning @Blas_Valero_Garces] #Palaeofire tells us a lot on past burnt biomass, emissions \u0026amp; human impact (https://t.co/biSuzCKWjX) We lack though a good understanding of past fire-vegetation interactions. pic.twitter.com/zEWaTpcKe3\n\u0026mdash; Graciela Gil-Romera (@gilromera) June 13, 2018  HOW DOES IT WORK Abiotic component The abiotic layer of the model is represented by three main environmental factors:\n  Topography derived from a digital elevation model at 200 x 200 meters resolution. Slope (along temperature) is used to impose restrictions to species distributions. Northness (in the range [0, 1]) is used to restrict fire spread. Aspect is used to draw a shaded relief map (at the user\u0026rsquo;s request). Elevation is used to compute a lapse rate map (see below).\n  Temperature (average of montly minimum temperatures) time series for the study area computed from palaeoclimatic data at annual resolution provided by the TraCe simulation, a transient model for the global climate evolution of the last 21K years with an annual resolution. The single temperature value of every year is converted into a temperature map (200 x 200 m resolution) using a lapse rate map based on the elevation map. Temperature, along with slope, is used to compute habitat suitability by using a logistic equation. Habitat suitability affects plant growth and survival.\n  Fire: The charcoal accumulation rate record (CHAR) from El Portalet palaeoenvironmental sequence (Gil-Romera et al., 2014) is used as input to simulate forest fires. A value of this time series is read each year, and a random number in the range [0, 1] is generated. If the random number is lower than the Fire-probability-per-year (FPY) parameter defined by the user, the value from the charcoal time series is multiplied by the parameter Number-ignitions-per-fire-event (NIF) (defined by the user) to compute the number of ignitions for the given year. As many adult tree as ignitions are selected to start spreading fire. Fire spreads to a neighbor patch if there is an adult tree in there, and a random number in the range [0, 1] is higher than the northness value of the patch.\n  Biotic component The biotic layer of the model is composed by five tree species. We have introduced the following elements to represent their ecological dynamics:\n  Topoclimatic niche, inferred from their present day distributions and high resolution temperature maps (presence data taken from GBIF, temperature maps taken from Worldclim and the Digital Climatic Atlas of the Iberian Peninsula). The ecological niche is represented by a logistic equation (see below). The results of this equation plus the dispersal dynamics of each species defines changes in distribution over time.\n  Population dynamics, driven by species traits such as dispersal distance, longevity, fecundity, mortality, growth rate, post-fire response to fire, and heliophity (competition for light). The data is based on the literature and/or expert opinion from forest and fire ecologists, and it is used to simulate growth (using logistic equations), competition for light and space, decay due to senescence, and mortality due to climate, fire, or plagues.\n  The model doesn\u0026rsquo;t simulate the entire populations of the target species. Instead, on each 200 x 200 meters patch it simulates the dynamics of an small forest plot (around 10 x 10 meters) where a maximum of one individual per species can exist.\nModel dynamics  Let\u0026rsquo;s burn it! Simulating fire-vegetation dynamics at millennial timescales in the central Pyrenees. from blas benito on Vimeo.\nThe life of an individual\nDuring the model setup seeds of every species are created on every patch. From there, every seed will go through the following steps every simulated year:\n  Its age increases by one year, and its life-stage is changed to \u0026ldquo;seedling\u0026rdquo;.\n  The minimum average temperature of its patch is updated.\n  The individual computes its habitat suitability using the logistic equation 1 / ( 1 + exp( -(intercept + coefficient * patch-temperature))), where the intercept and the coefficient are user defined. These parameters are hardcoded to save space in the GUI, and have been computed beforehand by using current presence data and temperature maps.\n  If habitat suitability is higher than a random number in the range [0, 1], the habitat is considered suitable (NOTE: this random number is defined for the patch, and it changes every ~10 years following a random walk drawn from a normal distribution with the average set to the previous value, and a standard deviation of 0.001).\n  If it is lower, the habitat is considered unsuitable, and the number of years under unsuitable habitat is increased by 1.\n If the number of years unders unsuitable habitat becomes higher than seedling-tolerance, the seedling dies, and another seed from the seed bank takes its place. Otherwise it stays alive.      Mortality: If a random number in the range [0, 1] is lower than the seedling mortality of the species the plant dies, and it is replaced by a seed from the seed bank. Otherwise it stays alive.\n  Competition and growth:\n  If the patch total biomass of the individuals in the patch equals Max-biomass-per-patch, the individual loses an amount of biomass between 0 and the 20% of its current biomass. This number is randomly selected.\n  If Max-biomass-per-patch has not been reached yet:\n  An interaction term is computed as (1 - (biomass of other individuals in the patch / Max-biomass-per-patch)) * (1 - heliophilia)).\n  The interaction term is introduced in the growth equation max-biomass / (1 + max-biomass * exp(- growth-rate * interaction-term * habitat-suitability * age)) to compute the current biomass of the individual. The lower the interaction term and habitat suitability are, the lower the growth becomes.\n      If a fire reaches the patch and there are adult individuals of other species on it, the plant dies, and it is replaced by a seed (this seed inherites the traits of the parent).\n  These steps continue while the individual is still a seedling, but once it reaches its maturity some steps become slightly different:\n  If a random number in the range [0, 1] is lower than the adults mortality of the species, or the maximum age of the species is reached, the individual is marked for decay. The current biomass of decaying individuals is computed as previous-biomass - years-of-decay. To add the effect of climatic variability to this decreasing function, its result is multiplied by 1 - habitat-suitability x random[0, 10]. If the biomass is higher than zero, pollen productivity is computed as current-biomass x species-pollen-productivity. The individual dies and is replaced by a seed when the biomass is below 1.\n  Dispersal: If the individual is in suitable habitat, a seed from it is placed in one of the neighboring patches within a radius given by the dispersal distance of the species (which is measured in \u0026ldquo;number of patches\u0026rdquo; and hardcoded) with no individuals of the same species.\n  If the individual starts a fire, or if fire spreads in from neighboring patches, it is marked as \u0026ldquo;burned\u0026rdquo;, spreads fire to its neighbors, dies, and is replaced by a seed. If the individual belongs to an species with post-fire resprouting, the growth-rate of the seed is multiplied by 2 to boost growth after fire.\n  Simulating pollen and charcoal deposition\nThe user defines the radius of a catchment area round the core location (10 km by default, that is 50 patches). All patches within this radius define the RSAP (relevant source area of pollen).\nAt the end of every simulated year the pollen productivity of every adult of each species within the RSAP is summed, and this value is used to compose the simulated pollen curves. The same is done with the biomass of the burned individuals to compose the virtual charcoal curve.\nOutput In GUI\nThe simulation GUI shows the following results in real time:\n  Plots of the input values:\n Minimum Temperature of the coldest month. Real charcoal data.    Simulated pollen curves for the target taxa.\n  Simulated charcoal curve.\n  Map showing the distribution of every species and the forest fires.\n  Written to disk\nThe simulated pollen counts and charcoal is exported to the path defined by the user as a table in csv format named output_table.csv. It contains one row per simulated year and the following columns:\n age: simulated year. temperature_minimum_average: average minimum winter temperature of the study area. pollen_Psylvestris: pollen sum for Pinus sylvestris. pollen_Puncinata pollen_Bpendula pollen_Cavellana pollen_Qpetraea real_charcoal: real charcoal values from El Portalet core. ignitions: number of fire ignitions. charcoal_sum: biomass sum of all burned individuals. charcoal_Psylvestris: sum of the biomass of burned individuals of Pinus sylvestris. charcoal_Puncinata charcoal_Bpendula charcoal_Cavellana charcoal_Qpetraea  Snapshots of the simulation map taken at 1 or 10 years intervals are stored in the output folder is requested by the user. These snapshots are useful to compose a video of the simulation.\nHOW TO USE IT Input files Input files are stored in a folder named \u0026ldquo;data\u0026rdquo;. These are:\n age: text file with no extension and a single column with no header containing age values from -15000 to -5701 fire: text file with no extension and a single column with no header containing actual charcoal counts expresed in the range [0, 1]. There are as many rows as in the age file t_minimum_average: text file with same features as the ones above containing minimum winter temperatures for the study area extracted from the TraCe simualtion. correct_t_minimum_average.asc: Map at 200m resolution containing the minimum winter temperature difference (period 1970-2000) between the TraCe simulation and the Digital Climatic Atlas of the Iberian Peninsula. It is used to transform the values of t_minimum_average into a high resolution temperature map. elevation.asc: digital elevation model of the study area at 200m resolution, coordinate system with EPSG code 23030. slope.asc: topographic slope. topography.asc: shaded relief map. It is used for plotting purposes only.  Input parameters General configuration of the simulation\nThe user can set-up the following parameters throught the GUI controls.\n Output-path: Character. Path of the output folder. This parameter cannot be empty, and the output folder must exist. Snapshots?: Boolean. If on, creates snapshots of the GUI to make videos. Snapshots-frequency: Character. Defines the frequency of snapshots. Only two options: \u0026ldquo;every year\u0026rdquo; and \u0026ldquo;every 10 years\u0026rdquo;. Draw-topography?: Boolean. If on, plots a shaded relief map (stored in topography.asc). RSAP-radius: Numeric[5, 50]. Radius of the RSAP in number of patches. Each patch is 200 x 200 m, so an RSAP-radius of 10 equals 2 kilometres. Randommness-settings: Character. Allows to choose between \u0026ldquo;fixed seed\u0026rdquo; to obtain deterministic results, or \u0026ldquo;free seed\u0026rdquo; to obtain different results on each run. Max-biomass-per-patch: Numeric, integer. Maximum charge capacity of a patch. Fire?: Boolean. If on, fires are produced whenever the data fire triggers a fire event. If off, fires are not produced (control simulation). Fire-probability-per-year: Numeric [0, 1]. Whenever the fire file provides a number higher than 0, if a random number in the range [0, 1] is lower than Fire-probability-per-year, a number of ignitions is computed (see below) and fires are triggered. Fire-ignitions-amplification-factor: Numeric The fire file provides values in the range [0, 1], and this multiplication factor converts these values in an integer number of ignitions. If fire equals one, and Fire-ignitions-amplification-factor equals 10, the number of ignitions will be 10 for the given year. Mortality?: Boolean. If on, mortality due to predation, plagues and other unpredictable sources is active (see Xx-seedling-mortality and Xx-adult-mortality parameters below). Burn-in-iterations: Numeric, integer. Number of years to run the model at a constant temperature (the initial one in the t_minimum_average file) and no fires to allow the population model to reach an equilibrium before to start the actual simulation. P.sylvestris?, P.uncinata?, B.pendula?, Q.petraea?, and C.avellana?: Boolean. If off, the given species is removed from the simulation. Used for testing purposes.  Species traits\nEach species has a set of traits to be filled by the user. Note that a particular species can be removed from the simulation by switching it to \u0026ldquo;off\u0026rdquo;.\n Xx-max-age: Numeric, integer. Maximum longevity. Every individual reaching this age is marked for decay. Xx-maturity-age: Numeric, integer. Age of sexual maturity. Individuals reaching this age are considered adults. Xx-pollen-productivity: Numeric. Multiplier of biomass to obtain a relative measure of pollen productivity among species. Xx-growth-rate: Numeric. Growth rate of the given species. Xx-max-biomass: Numeric, integer. Maximum biomass reachable by the given species. Xx-heliophilia: Numeric, [0, 1]. Dependance of the species on solar light to grow. It is used to compute the effect of competence in plant growth. Xx-seedling-tolerance: Numeric, integer. Numer of years a seedling can tolerate unsuitable climate. Xx-adult-tolerance: Numeric, integer. Numer of years an adult can tolerate unsuitable climate. Xx-seedling-mortality: Numeric, [0, 1]. Proportion of seedlings dying due to predation. Xx-adult-mortality: Numeric, [0, 1]. Proportion of adults dying due to plagues or other mortality sources. Xx-resprout-after-fire: Boolean. If 0 the species doesn\u0026rsquo;t show a post-fire response. If 1, growth-rate is multiplied by two in the resprouted individual to increase growth rate. Xx-min-temperature: Numeric. Minimum temperature at which the species has been found using GBIF presence data. Xx-max-temperature: Numeric. Maximum temperature at which the species has been found using GBIF presence data. Xx-min-slope: Numeric. Minimum topographic slope at which the species has been found. Xx-max-slope: Numeric. Maximum topographic slope at which the species has been found. Xx-intercept: Numeric. Intercept of the logistic equation to compute habitat suitability fitted to presence data and minimum temperature maps. Xx-coefficient: Numeric. Coefficient of the logistic equation to compute habitat suitability.  ","date":1609545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609545600,"objectID":"ccb2f0c0f3fc235a1a29cfdb9e5f5669","permalink":"/project/palaeo_fire_modeling/","publishdate":"2021-01-02T00:00:00Z","relpermalink":"/project/palaeo_fire_modeling/","section":"project","summary":"This is a spatio-temporal simulation of the effect of fire regimes on the population dynamics of five forest species during the Lateglacial-Holocene transition (15-7 cal Kyr BP) at El Portalet, a subalpine bog located in the central Pyrenees region (1802m asl, Spain)","tags":["ABM","Netlogo","Fire dynamics","Mechanistic simulation","Palaeoecology","Agent-based models"],"title":"Palaeo fire modeling","type":"project"},{"authors":["Antonio J. Pérez-Luque","Blas M. Benito","Francisco J. Bonet","Regino Zamora"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"b0b95974528b688da014b913cd112f58","permalink":"/publication/2021_perez-luque_forests/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/2021_perez-luque_forests/","section":"publication","summary":"Understanding the ecology of populations located in the rear edge of their distribution is key to assessing the response of the species to changing environmental conditions. Here, we focus on rear-edge populations of Quercus pyrenaica in Sierra Nevada (southern Iberian Peninsula) to analyze their ecological and floristic diversity. We perform multivariate analyses using high-resolution environmental information and forest inventories to determine how environmental variables differ among oak populations, and to identify population groups based on environmental and floristic composition.","tags":["Biogeography","Plant Ecology"],"title":"Ecological Diversity within Rear-Edge: A Case Study from Mediterranean Quercus pyrenaica Willd.","type":"publication"},{"authors":null,"categories":null,"content":"This Netlogo model simulates the dispersal of Quercus pyrenaica populations in Sierra Nevada (Spain) at a yearly resolution until 2100 while considering different levels of model complexity, from random dispersal and seedling establishment, to realistic dispersal based on the dispersal behavior of the Eurasian Jay.\nThe data required to run the model can be downloaded from here. It must be decompressed in the same folder containing the netlogo code of the model.\nThe video below shows the model in action for one population of Quercus pyrenaica. On the left, it shows the effect of a random dispersal model, and on the right, a realistic dispersal model based on observations of the dispersal behavior of the Eurasian Jay.\n ","date":1609372800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609372800,"objectID":"e58a80cc3e1a80acfc33794a87e4e941","permalink":"/project/quercus_range_shift/","publishdate":"2020-12-31T00:00:00Z","relpermalink":"/project/quercus_range_shift/","section":"project","summary":"Agent-based model coded with Netlogo to simulate range shift of *Quercus pyrenaica* populations in Sierra Nevada (Spain) using a realistic dispersal model with different levels of complexity.","tags":["ABM","Netlogo","Range-shift simulation","Mechanistic simulation","Dispersal","Agent-based models"],"title":"Range-shift simulation","type":"project"},{"authors":null,"categories":null,"content":"   Note: to better follow this tutorial you can download the .Rmd file from here.\nIn a previous post I explained how to set up a small home cluster. Many things can be done with a cluster, and parallelizing loops is one of them. But there is no need of a cluster to parallelize loops and improve the efficiency of your coding!\nI believe that coding parallelized loops is an important asset for anyone working with R. That’s why this post covers the following topics:\n Beyond for: building loops with foreach. What is a parallel backend? Setup of a parallel backend for a single computer. Setup for a Beowulf cluster. Practical examples.  Tuning of random forest hyperparameters. Confidence intervals of the importance scores of the predictors in random forest models.    for loops are fine, but… Many experienced R users frequently say that nobody should write loops with R because they are tacky or whatever. However, I find loops easy to write, read, and debug, and are therefore my workhorse whenever I need to repeat a task and I don’t feel like using apply() and the likes. However, regular for loops in R are highly inefficient, because they only use one of your computer cores to perform the iterations.\nFor example, the for loop below sorts vectors of random numbers a given number of times, and will only work on one of your computer cores for a few seconds, while the others are there, procrastinating with no shame.\nWot Cpu GIF from Wot GIFs   (gif kindly suggested by Andreas Angourakis)\nfor(i in 1:10000){ sort(runif(10000)) } If every i could run in a different core, the operation would indeed run a bit faster, and we would get rid of lazy cores. This is were packages like foreach and doParallel come into play. Let’s start installing these packages and a few others that will be useful throughout this tutorial.\n#automatic install of packages if they are not installed already list.of.packages \u0026lt;- c( \u0026quot;foreach\u0026quot;, \u0026quot;doParallel\u0026quot;, \u0026quot;ranger\u0026quot;, \u0026quot;palmerpenguins\u0026quot;, \u0026quot;tidyverse\u0026quot;, \u0026quot;kableExtra\u0026quot; ) new.packages \u0026lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\u0026quot;Package\u0026quot;])] if(length(new.packages) \u0026gt; 0){ install.packages(new.packages, dep=TRUE) } #loading packages for(package.i in list.of.packages){ suppressPackageStartupMessages( library( package.i, character.only = TRUE ) ) } #loading example data data(\u0026quot;penguins\u0026quot;)   Beyond for: building loops with foreach The foreach package (the vignette is here) provides a way to build loops that support parallel execution, and easily gather the results provided by each iteration in the loop.\nFor example, this classic for loop computes the square root of the numbers 1 to 5 with sqrt() (the function is vectorized, but let’s conveniently forget that for a moment). Notice that I have to create a vector x to gather the results before executing the loop.\nx \u0026lt;- vector() for(i in 1:10){ x[i] \u0026lt;- sqrt(i) } x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 The foreach version returns a list with the results automatically. Notice that %do% operator after the loop definition, I’ll talk more about it later.\nx \u0026lt;- foreach(i = 1:10) %do% { sqrt(i) } x ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 1.414214 ## ## [[3]] ## [1] 1.732051 ## ## [[4]] ## [1] 2 ## ## [[5]] ## [1] 2.236068 ## ## [[6]] ## [1] 2.44949 ## ## [[7]] ## [1] 2.645751 ## ## [[8]] ## [1] 2.828427 ## ## [[9]] ## [1] 3 ## ## [[10]] ## [1] 3.162278 We can use the .combine argument of foreach to arrange the list as a vector. Other options such as cbind, rbind, or even custom functions can be used as well, only depending on the structure of the output of each iteration.\nx \u0026lt;- foreach( i = 1:10, .combine = \u0026#39;c\u0026#39; ) %do% { sqrt(i) } x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 Another interesting capability of foreach is that it supports several iterators of the same length at once. Notice that the values of the iterators are not combined. When the first value of one iterator is being used, the first value of the other iterators will be used as well.\nx \u0026lt;- foreach( i = 1:3, j = 1:3, k = 1:3, .combine = \u0026#39;c\u0026#39; ) %do% { i + j + k } x ## [1] 3 6 9   Running foreach loops in parallel The foreach loops shown above use the operator %do%, that processes the tasks sequentially. To run tasks in parallel, foreach uses the operator %dopar%, that has to be supported by a parallel backend. If there is no parallel backend, %dopar% warns the user that it is being run sequentially, as shown below. But what the heck is a parallel backend?\nx \u0026lt;- foreach( i = 1:10, .combine = \u0026#39;c\u0026#39; ) %dopar% { sqrt(i) } ## Warning: executing %dopar% sequentially: no parallel backend registered x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278  What is a parallel backend? When running tasks in parallel, there should be a director node that tells a group of workers what to do with a given set of data and functions. The workers execute the iterations, and the director manages execution and gathers the results provided by the workers. A parallel backend provides the means for the director and workers to communicate, while allocating and managing the required computing resources (processors, RAM memory, and network bandwidth among others).\nThere are two types of parallel backends that can be used with foreach, FORK and PSOCK.\n FORK FORK backends are only available on UNIX machines (Linux, Mac, and the likes), and do not work in clusters [sad face], so only single-machine environments are appropriate for this backend. In a FORK backend, the workers share the same environment (data, loaded packages, and functions) as the director. This setup is highly efficient because the main environment doesn’t have to be copied, and only worker outputs need to be sent back to the director.\n  PSOCK PSOCK backends (Parallel Socket Cluster) are available for both UNIX and WINDOWS systems, and are the default option provided with foreach. As their main disadvantage, the environment of the director needs to be copied to the environment of each worker, which increases network overhead while decreasing the overall efficiency of the cluster. By default, all the functions available in base R are copied to each worker, and if a particular set of R packages are needed in the workers, they need to be copied to the respective environments of the workers as well.\nThis post compares both backends and concludes that FORK is about a 40% faster than PSOCK.\n   Setup of a parallel backend Here I explain how to setup the parallel backend for a simple computer and for a Beowulf cluster as the one I described in a previous post.\nSetup for a single computer Setting up a cluster in a single computer requires first to find out how many cores we want to use from the ones we have available. It is recommended to leave one free core for other tasks.\nparallel::detectCores() ## [1] 8 n.cores \u0026lt;- parallel::detectCores() - 1 Now we need to define the cluster with parallel::makeCluster() and register it so it can be used by %dopar% with doParallel::registerDoParallel(my.cluster). The type argument of parallel::makeCluster() accepts the strings “PSOCK” and “FORK” to define the type of parallel backend to be used.\n#create the cluster my.cluster \u0026lt;- parallel::makeCluster( n.cores, type = \u0026quot;PSOCK\u0026quot; ) #check cluster definition (optional) print(my.cluster) ## socket cluster with 7 nodes on host \u0026#39;localhost\u0026#39; #register it to be used by %dopar% doParallel::registerDoParallel(cl = my.cluster) #check if it is registered (optional) foreach::getDoParRegistered() ## [1] TRUE #how many workers are available? (optional) foreach::getDoParWorkers() ## [1] 7 Now we can run a set of tasks in parallel!\nx \u0026lt;- foreach( i = 1:10, .combine = \u0026#39;c\u0026#39; ) %dopar% { sqrt(i) } x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 If everything went well, now %dopar% should not be throwing the warning executing %dopar% sequentially: no parallel backend registered, meaning that the parallel execution is working as it should. In this little example there is no gain in execution speed, because the operation being executed is extremely fast, but this will change when the operations running inside of the loop take longer times to run.\nFinally, it is always recommendable to stop the cluster when we are done working with it.\nparallel::stopCluster(cl = my.cluster)  Setup for a Beowulf cluster This setup is a bit more complex, because it requires to open a port in every computer of the cluster. Ports are virtual communication channels, and are identified by a number.\nFirst, lets tell R what port we want to use:\n#define port Sys.setenv(R_PARALLEL_PORT = 11000) #check that it Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;) Now, we need to open the selected port in every computer of the network. In Linux we need to setup the firewall to allow connections from the network 10.42.1.0/24 (replace this with your network range if different!) to the port 11000 by splitting the window of the Terminator console in as many computers available in your network (the figure below shows three, one for my PC and two for my Intel NUCs), opening an ssh session on each remote machine, and setting Terminator with Grouping equal to Broadcast all so we only need to type the commands once.\nOpening port 11000 in three computers at once with Terminator\n Now we have to create an object defining the IPs of the computers in the network, the number of cores to use from each computer, the user name, and the identity of the director. This will be the spec argument required by parallel::makeCluster() to create the cluster throughtout the machines in the network. It is a list of lists, with as many lists as nodes are defined. Each sub-list has a slot named host with the IP of the computer where the given node is, and user, with the name of the user in each computer.\nThe code below shows how this would be done, step by step. Yes, this is CUMBERSOME.\n#main parameters director \u0026lt;- \u0026#39;10.42.0.1\u0026#39; nuc2 \u0026lt;- \u0026#39;10.42.0.34\u0026#39; nuc1 \u0026lt;- \u0026#39;10.42.0.104\u0026#39; user \u0026lt;- \u0026quot;blas\u0026quot; #list of machines, user names, and cores spec \u0026lt;- list( list( host = director, user = user, ncore = 7 ), list( host = nuc1, user = user, ncore = 4 ), list( host = nuc2, user = user, ncore = 4 ) ) #generating nodes from the list of machines spec \u0026lt;- lapply( spec, function(spec.i) rep( list( list( host = spec.i$host, user = spec.i$user) ), spec.i$ncore ) ) #formating into a list of lists spec \u0026lt;- unlist( spec, recursive = FALSE ) Generating the spec definition is a bit easier with the function below.\n#function to generate cluster specifications from a vector of IPs, a vector with the number of cores to use on each IP, and a user name cluster_spec \u0026lt;- function( ips, cores, user ){ #creating initial list spec \u0026lt;- list() for(i in 1:length(ips)){ spec[[i]] \u0026lt;- list() spec[[i]]$host \u0026lt;- ips[i] spec[[i]]$user \u0026lt;- user spec[[i]]$ncore \u0026lt;- cores[i] } #generating nodes from the list of machines spec \u0026lt;- lapply( spec, function(spec.i) rep( list( list( host = spec.i$host, user = spec.i$user) ), spec.i$ncore ) ) #formating into a list of lists spec \u0026lt;- unlist( spec, recursive = FALSE ) return(spec) } This function is also available in this GitHub Gist, so you can load it into your R environment by executing:\nsource(\u0026quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R\u0026quot;) Below I use it to generate the input to the spec argument to start the cluster with parallel::makeCluster(). Notice that I have added several arguments.\n The argument outfile determines where the workers write a log. In this case it is set to nowhere with the double quotes, but the path to a text file in the director could be provided here. The argument homogeneous = TRUE indicates that all machines have the Rscript in the same location. In this case all three machines have it at “/usr/lib/R/bin/Rscript”. Otherwise, set it up to FALSE.  #generate cluster specification spec \u0026lt;- cluster_spec( ips = c(\u0026#39;10.42.0.1\u0026#39;, \u0026#39;10.42.0.34\u0026#39;, \u0026#39;10.42.0.104\u0026#39;), cores = c(7, 4, 4), user = \u0026quot;blas\u0026quot; ) #setting up cluster my.cluster \u0026lt;- parallel::makeCluster( master = \u0026#39;10.42.0.1\u0026#39;, spec = spec, port = Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;), outfile = \u0026quot;\u0026quot;, homogeneous = TRUE ) #check cluster definition (optional) print(my.cluster) #register cluster doParallel::registerDoParallel(cl = my.cluster) #how many workers are available? (optional) foreach::getDoParWorkers() Now we can use the cluster to execute a dummy operation in parallel using all machines in the network.\nx \u0026lt;- foreach( i = 1:20, .combine = \u0026#39;c\u0026#39; ) %dopar% { sqrt(i) } x Once everything is done, remember to close the cluster.\nparallel::stopCluster(cl = my.cluster)    Practical examples In this section I cover two examples on how to use parallelized loops to explore model outputs:\n Tuning random forest hyperparameters to maximize classification accuracy. Obtain a confidence interval for the importance score of each predictor from a set random forest models fitted with ranger().  In the examples I use the penguins data from the palmerpenguins package to fit classification models with random forest using species as a response, and bill_length_mm, bill_depth_mm, flipper_length_mm, and body_mass_g as predictors.\n#removing NA and subsetting columns penguins \u0026lt;- as.data.frame( na.omit( penguins[, c( \u0026quot;species\u0026quot;, \u0026quot;bill_length_mm\u0026quot;, \u0026quot;bill_depth_mm\u0026quot;, \u0026quot;flipper_length_mm\u0026quot;, \u0026quot;body_mass_g\u0026quot; )] ) )    species  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g      Adelie  39.1  18.7  181  3750    Adelie  39.5  17.4  186  3800    Adelie  40.3  18.0  195  3250    Adelie  36.7  19.3  193  3450    Adelie  39.3  20.6  190  3650    Adelie  38.9  17.8  181  3625    Adelie  39.2  19.6  195  4675    Adelie  34.1  18.1  193  3475    Adelie  42.0  20.2  190  4250    Adelie  37.8  17.1  186  3300    Adelie  37.8  17.3  180  3700    Adelie  41.1  17.6  182  3200    Adelie  38.6  21.2  191  3800    Adelie  34.6  21.1  198  4400    Adelie  36.6  17.8  185  3700    Adelie  38.7  19.0  195  3450    Adelie  42.5  20.7  197  4500    Adelie  34.4  18.4  184  3325    Adelie  46.0  21.5  194  4200    Adelie  37.8  18.3  174  3400     We’ll fit random forest models with the ranger package, which works as follows:\n#fitting classification model m \u0026lt;- ranger::ranger( data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, importance = \u0026quot;permutation\u0026quot; ) #summary m ## Ranger result ## ## Call: ## ranger::ranger(data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, importance = \u0026quot;permutation\u0026quot;) ## ## Type: Classification ## Number of trees: 500 ## Sample size: 342 ## Number of independent variables: 4 ## Mtry: 2 ## Target node size: 1 ## Variable importance mode: permutation ## Splitrule: gini ## OOB prediction error: 2.34 % #variable importance m$variable.importance ## bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## 0.31452816 0.16948700 0.21670253 0.07988962 The output shows that the percentage of misclassified cases is 2.34, and that bill_length_mm is the variable that contributes the most to the accuracy of the classification.\nIf you are not familiar with random forest, this post and the video below do a pretty good job in explaining the basics:\n Tuning random forest hyperparameters Random forest has several hyperparameters that influence model fit:\n num.trees is the total number of trees to fit. The default value is 500. mtry is the number of variables selected by chance (from the total pool of variables) as candidates for a tree split. The minimum is 2, and the maximum is the total number of predictors. min.node.size is the minimum number of cases that shall go together in the terminal nodes of each tree. For classification models as the ones we are going to fit, 1 is the minimum.  Here we are going to explore how combinations of these values increase or decrease the prediction error of the model (percentage of misclassified cases) on the out-of-bag data (not used to train each decision tree). This operation is usually named grid search for hyperparameter optimization.\nTo create these combinations of hyperparameters we use expand.grid().\nsensitivity.df \u0026lt;- expand.grid( num.trees = c(500, 1000, 1500), mtry = 2:4, min.node.size = c(1, 10, 20) )    num.trees  mtry  min.node.size      500  2  1    1000  2  1    1500  2  1    500  3  1    1000  3  1    1500  3  1    500  4  1    1000  4  1    1500  4  1    500  2  10    1000  2  10    1500  2  10    500  3  10    1000  3  10    1500  3  10    500  4  10    1000  4  10    1500  4  10    500  2  20    1000  2  20    1500  2  20    500  3  20    1000  3  20    1500  3  20    500  4  20    1000  4  20    1500  4  20     Each row in sensitivity.df corresponds to a combination of parameters to test, so there are 27 models to fit. The code below prepares the cluster, and uses the ability of foreach to work with several iterators at once to easily introduce the right set of hyperparameters to each fitted model.\nNotice how in the foreach definition I use the .packages argument to export the ranger package to the environments of the workers.\n#create and register cluster my.cluster \u0026lt;- parallel::makeCluster(n.cores) doParallel::registerDoParallel(cl = my.cluster) #fitting each rf model with different hyperparameters prediction.error \u0026lt;- foreach( num.trees = sensitivity.df$num.trees, mtry = sensitivity.df$mtry, min.node.size = sensitivity.df$min.node.size, .combine = \u0026#39;c\u0026#39;, .packages = \u0026quot;ranger\u0026quot; ) %dopar% { #fit model m.i \u0026lt;- ranger::ranger( data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, num.trees = num.trees, mtry = mtry, min.node.size = min.node.size ) #returning prediction error as percentage return(m.i$prediction.error * 100) } #adding the prediction error column sensitivity.df$prediction.error \u0026lt;- prediction.error To plot the results:\nggplot2::ggplot(data = sensitivity.df) + ggplot2::aes( x = mtry, y = as.factor(min.node.size), fill = prediction.error ) + ggplot2::facet_wrap(as.factor(num.trees)) + ggplot2::geom_tile() + ggplot2::scale_y_discrete(breaks = c(1, 10, 20)) + ggplot2::scale_fill_viridis_c() + ggplot2::ylab(\u0026quot;min.node.size\u0026quot;) The figure shows that combinations of lower values of min.node.size and mtry generally lead to models with a lower prediction error across different numbers of trees. Retrieving the first line of sensitivity.df ordered by ascending prediction.error will give us the values of the hyperparameters we need to use to reduce the prediction error as much as possible.\nbest.hyperparameters \u0026lt;- sensitivity.df %\u0026gt;% dplyr::arrange(prediction.error) %\u0026gt;% dplyr::slice(1)    num.trees  mtry  min.node.size  prediction.error      500  2  1  2.339181      Confidence intervals of variable importance scores Random forest has an important stochastic component during model fitting, and as consequence, the same model will return slightly different results in different runs (unless set.seed() or the seed argument of ranger are used). This variability also affects the importance scores of the predictors, and can be use to our advantage to assess whether the importance scores of different variables do really overlap or not.\nI have written a little function to transform the vector of importance scores returned by ranger into a data frame (of one row). It helps arranging the importance scores of different runs into a long format, which helps a lot to plot a boxplot with ggplot2 right away. This function could have been just some code thrown inside the foreach loop, but I want to illustrate how foreach automatically transfers functions available in the R environment into the environments of the workers when required, without the intervention of the user. The same will happen with the best.hyperparameters tiny data frame we created in the previous section.\nimportance_to_df \u0026lt;- function(model){ x \u0026lt;- as.data.frame(model$variable.importance) x$variable \u0026lt;- rownames(x) colnames(x)[1] \u0026lt;- \u0026quot;importance\u0026quot; rownames(x) \u0026lt;- NULL return(x) } The code chunk below setups the cluster and runs 1000 random forest models in parallel (using the best hyperparameters computed in the previous section) while using system.time() to assess running time.\n#we don\u0026#39;t need to create the cluster, it is still up print(my.cluster) ## socket cluster with 7 nodes on host \u0026#39;localhost\u0026#39; #assessing execution time system.time( #performing 1000 iterations in parallel importance.scores \u0026lt;- foreach( i = 1:1000, .combine = \u0026#39;rbind\u0026#39;, .packages = \u0026quot;ranger\u0026quot; ) %dopar% { #fit model m.i \u0026lt;- ranger::ranger( data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, importance = \u0026quot;permutation\u0026quot;, mtry = best.hyperparameters$mtry, num.trees = best.hyperparameters$num.trees, min.node.size = best.hyperparameters$min.node.size ) #format importance m.importance.i \u0026lt;- importance_to_df(model = m.i) #returning output return(m.importance.i) } ) ## user system elapsed ## 0.308 0.028 6.382 The output of system.time() goes as follows:\n user: seconds the R session has been using the CPU. system: seconds the operating system has been using the CPU. elapsed: the total execution time experienced by the user.  This will make sense in a minute. In the meantime, let’s plot our results!\nggplot2::ggplot(data = importance.scores) + ggplot2::aes( y = reorder(variable, importance), x = importance ) + ggplot2::geom_boxplot() + ggplot2::ylab(\u0026quot;\u0026quot;) The figure shows that the variable bill_length_mm is the most important in helping the model classifying penguin species, with no overlap with any other variable. In this particular case, since the distributions of the importance scores do not overlap, this analysis isn’t truly helpful, but now you know how to do it!\nI assessed the running time with system.time() because ranger() can run in parallel by itself just by setting the num.threads argument to the number of cores available in the machine. This capability cannot be used when executing ranger() inside a parallelized foreach loop though, and it is only useful inside classic for loops.\nWhat option is more efficient then? The code below executes a regular for loop running the function sequentially to evaluate whether it is more efficient to run ranger() in parallel using one core per model, as we did above, or sequentially while using several cores per model on each iteration.\n#list to save results importance.scores.list \u0026lt;- list() #performing 1000 iterations sequentially system.time( for(i in 1:1000){ #fit model m.i \u0026lt;- ranger::ranger( data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, importance = \u0026quot;permutation\u0026quot;, seed = i, num.threads = parallel::detectCores() - 1 ) #format importance importance.scores.list[[i]] \u0026lt;- importance_to_df(model = m.i) } ) ## user system elapsed ## 42.878 2.914 13.336 As you can see, ranger() takes longer to execute in a regular for loop using several cores at once than in a parallel foreach loop using one core at once. That’s a win for the parallelized loop!\nWe can stop our cluster now, we are done with it.\nparallel::stopCluster(cl = my.cluster)    A few things to take in mind As I have shown in this post, using parallelized foreach loops can accelerate long computing processes, even when some functions have the ability to run in parallel on their own. However, there are things to take in mind, that might vary depending on whether we are executing the parallelized task on a single computer or on a small cluster.\nIn a single computer, the communication between workers and the director is usually pretty fast, so there are no obvious bottlenecks to take into account here. The only limitation that might arise comes from the availability of RAM memory. For example, if a computer has 8 cores and 8GB of RAM, less than 1GB of RAM will be available for each worker. So, if you need to repeat a process that consumes a significant amount of RAM, the ideal number of cores running in parallel might be lower than the total number of cores available in your system. Don’t be greedy, and try to understand the capabilities of your machine while designing a parallelized task.\nWhen running foreach loops as in x \u0026lt;- foreach(...){...}, the variable x is receiving whatever results the workers are producing. For example, if you are only returning the prediction error of a model, or its importance scores, x will have a very manageable size. But if you are returning heavy objects such as complete random forest models, the size of x is going to grow VERY FAST, and at the end it will be competing for RAM resources with the workers, which might even crash your R session. Again, don’t be greedy, and size your outputs carefully.\nClusters spanning several computers are a different beast, since the workers and the director communicate through a switch and network wires and interfaces. If the amount of data going to and coming from the workers is large, the network can get clogged easily, reducing the cluster’s efficiency drastically. In general, if the amount of data produced by a worker on each iteration takes longer to arrive to the director than the time it takes the worker to produce it, then a cluster is not going to be more efficient than a single machine. But this is not important if you don’t care about efficiency.\nOther issues you might come across while parallelizing tasks in R are thoroughly commented in this post, by Imre Gera.\nThat’s all for now folks, happy parallelization!\n ","date":1608940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608940800,"objectID":"258c4372b0dc8ca13f2e28fc8d34faa5","permalink":"/post/02_parallelizing_loops_with_r/","publishdate":"2020-12-26T00:00:00Z","relpermalink":"/post/02_parallelizing_loops_with_r/","section":"post","summary":"Note: to better follow this tutorial you can download the .Rmd file from here.\nIn a previous post I explained how to set up a small home cluster.","tags":null,"title":"Parallelized loops with R","type":"post"},{"authors":null,"categories":null,"content":"   \nThe package distantia allows to measure the dissimilarity between multivariate time-series. The package assumes that the target sequences are ordered along a given dimension, being depth and time the most common ones, but others such as latitude or elevation are also possible. Furthermore, the target time-series can be regular or irregular, and have their samples aligned (same age/time/depth) or unaligned (different age/time/depth). The only requirement is that the sequences must have at least two (but ideally more) columns with the same name and units representing different variables relevant to the dynamics of a system of interest.\nThe GitHub page of the project contains a thorough explanation of the statistics behind the method. The paper published in the Ecography journal describes the method, the package, and a couple of practical examples. The code and data used to develop the examples can be found in GitHub and Zenodo.\nPlease, if you find this package useful, please cite it as:\nBenito, B.M. and Birks, H.J.B. (2020), distantia: an open‐source toolset to quantify dissimilarity between multivariate ecological time‐series. Ecography, 43: 660-667. https://doi.org/10.1111/ecog.04895\n","date":1608336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608336000,"objectID":"a64ca32d94825ca0e1d3efdede85f5d0","permalink":"/project/distantia/","publishdate":"2020-12-19T00:00:00Z","relpermalink":"/project/distantia/","section":"project","summary":"R package to compare multivariate time-series.","tags":["R packages","Time Series Analysis"],"title":"R package \"distantia\"","type":"project"},{"authors":null,"categories":null,"content":"   \nThe goal of memoria is to provide the tools to quantify ecological memory in long time-series involving environmental drivers and biotic responses, including palaeoecological datasets.\nEcological memory has two main components: the endogenous component, which represents the effect of antecedent values of the response on itself, and endogenous component, which represents the effect of antecedent values of the driver or drivers on the current state of the biotic response. Additionally, the concurrent effect, which represents the synchronic effect of the environmental drivers over the response is measured. The functions in the package allow the user\nThe package memoria uses the fast implementation of Random Forest available in the ranger package to fit a model of the form shown in Equation 1:\nEquation 1 (simplified from the one in the paper): $$p_{t} = p_{t-1} +\u0026hellip;+ p_{t-n} + d_{t} + d_{t-1} +\u0026hellip;+ d_{t-n}$$\nWhere:\n $p$ is the response variable, Pollen counts were used in this particular case.. $d$ is an environmental Driver influencing the response variable. $t$ is the time of any given value of the response $p$. $t-1$ is the lag 1. $p_{t-1} +\u0026hellip;+ p_{t-n}$ represents the endogenous component of ecological memory. $d_{t-1} +\u0026hellip;+ d_{t-n}$ represents the exogenous component of ecological memory. $d_{t}$ represents the concurrent effect of the driver over the response.  Random Forest returns an importance score for each model term, and the functions in memoria let the user to plot the importance scores across time lags for each ecological memory components, and to compute different features of each memory component (length, strength, and dominance).\nThe GitHub page of the package features complete examples on how to use the package. The paper published in the Ecography journal describes ecological memory concepts and the method based on Random Forest used to assess ecological memory components. The code used to generate the supplementary materials can be found in GitHub and Zenodo.\nIf you ever use the package, please, cite it as:\nBenito, B.M., Gil‐Romera, G. and Birks, H.J.B. (2020), Ecological memory at millennial time‐scales: the importance of data constraints, species longevity and niche features. Ecography, 43: 1-10. https://doi.org/10.1111/ecog.04772\n","date":1608336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608336000,"objectID":"b2480e8e20be7bd9954b85358f24acc6","permalink":"/project/memoria/","publishdate":"2020-12-19T00:00:00Z","relpermalink":"/project/memoria/","section":"project","summary":"R package to assess ecological memory in multivariate time-series.","tags":["R packages","Ecological Memory","Time Series Analysis","Machine Learning","Random Forest"],"title":"R package \"memoria\"","type":"project"},{"authors":null,"categories":null,"content":"  \nThe goal of virtualPollen is to provide the tools to simulate pollen curves over millenial time-scales generated by virtual taxa with different life traits (life-span, fecundity, growth-rate) and niche features (niche position and breadth) as a response to virtual environmental drivers with a given temporal autocorrelation. It furthers allow to simulate specific data properties of fossil pollen datasets, such as sediment accumulation rate, and depth intervals between consecutive pollen samples. The simulation outcomes are useful to better understand the role of plant traits, niche properties, and climatic variability in defining the shape of pollen curves.\nThe GitHub page of the package offers a complete tutorial on how to use the package. The paper published in the Ecography journal describes the foundations of the model in brief.\nIf you ever use the package, please, cite it as:\nBenito, B.M., Gil‐Romera, G. and Birks, H.J.B. (2020), Ecological memory at millennial time‐scales: the importance of data constraints, species longevity and niche features. Ecography, 43: 1-10. https://doi.org/10.1111/ecog.04772\n","date":1608336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608336000,"objectID":"f3afdae9d91edee9e3abcdfb020c263f","permalink":"/project/virtualpollen/","publishdate":"2020-12-19T00:00:00Z","relpermalink":"/project/virtualpollen/","section":"project","summary":"R package to simulate pollen production of mono-specific tree populations over millennia.","tags":["R packages","Time Series Analysis","Palaeoecology","Mechanistic simulation"],"title":"R package \"vitualPollen\"","type":"project"},{"authors":null,"categories":null,"content":"In this post I explain how to setup a small Beowulf cluster with a personal PC running Ubuntu 20.04 and a couple of Intel NUCs running Ubuntu Server 20.04, with the end-goal of parallelizing R tasks.\nThe topics I cover here are:\n Required material Network setting Installing the secure shell protocol Installing Ubuntu server in the NUCs Installing R in the NUCs Managing the cluster\u0026rsquo;s network   Preamble I have a little but nice HP ENVY model TE01-0008ns with 32 GB RAM, 8 CPUs, and 3TB of hard disk running Ubuntu 20.04 that I use to do all my computational work (and most of my tweeting). A few months ago I connected it with my two laptops (one of them deceased now, RIP my dear skynet) to create a little cluster to run parallel tasks in R.\nToday I made a home cluster to spread parallel R tasks across all my computers. That was fun! #rstats pic.twitter.com/ic4plvO3Y6\n\u0026mdash; Blas M. Benito (@BlasBenito) April 27, 2020  It was just a draft cluster running on a wireless network, but it served me to think about getting a more permanent solution not requiring two additional laptops in my desk.\nThat\u0026rsquo;s were the nice INTEL NUCs (from Next Unit of Computing) come into play. NUCs are full-fledged computers fitted in small boxes usually sold without RAM memory sticks and no hard disk (hence the term barebone). Since they have a low energy consumption footprint, I thought these would be ideal units for my soon-to-be home cluster.\n Material I gifted myself with:\n 2 Intel Barebone BOXNUC6CAYH, each with 4 cores, and a maximum RAM memory of 32GB (you might read they only accept 8GB, but that\u0026rsquo;s not the case anymore). Notice that these NUCs aren\u0026rsquo;t state-of-the-art now, they were released by the end of 2016. 2 Hard disks SSD 2.5\u0026rdquo; Western Digital WDS250G2B0A WD Blue (250GB) 4 Crucial CT102464BF186D DDR3 SODIMM (204 pins) RAM sticks with 8GB each. 1 ethernet switch Netgear GS308-300PES with 8 ports. 3 ethernet wires NanoCable 10.20.0400-BL of cat 6 quality.  The whole set came to cost around 530€, but please notice that I had a clear goal in mind: \u0026ldquo;duplicating\u0026rdquo; my computing power with the minimum number of NUCs, while preserving a share of 4GB of RAM memory per CPU throughout the cluster (based on the features of my desk computer). A more basic setting with more modest NUCs and smaller RAM would cost half of that.\nThis instructive video by David Harry shows how to install the SSD and the RAM sticks in an Intel NUC. It really takes 5 minutes tops, one only has to be a bit careful with the RAM sticks, the pins need to go all the way in into their slots before securing the sticks in place.\n   Network settings Before starting to install an operating system in the NUCS, the network setup goes as follows:\n My desktop PC is connected to a router via WIFI and dynamic IP (DHCP). The PC and each NUC are connected to the switch with cat6 ethernet wires.  To share my PC\u0026rsquo;s WIFI connection with the NUCs I have to prepare a new connection profile with the command line tool of Ubuntu\u0026rsquo;s NetworkManager, named nmcli, as follows.\nFirst, I need to find the name of my ethernet interface by checking the status of my network devices with the command line.\nnmcli device status DEVICE TYPE STATE CONNECTION wlp3s0 wifi connected my_wifi enp2s0 ethernet unavailable -- lo loopback unmanaged --  There I can see that my ethernet interface is named enp2s0.\nSecond, I have to configure the shared connection.\nnmcli connection add type ethernet ifname enp2s0 ipv4.method shared con-name cluster  Were ifname enp2s0 is the name of the interface I want to use for the new connection, ipv4.method shared is the type of connection, and con-name cluster is the name I want the connection to have. This operation adds firewall rules to manage traffic within the cluster network, starts a DHCP server in the computer that serves IPs to the NUCS, and a DNS server that allows the NUCs to translate internet addresses.\nAfter turning on the switch, I can check the connection status again with\nnmcli device status DEVICE TYPE STATE CONNECTION enp2s0 ethernet connected cluster wlp3s0 wifi connected my_wifi lo loopback unmanaged --  When checking the IP of the device with bash ifconfig it should yield 10.42.0.1. Any other computer in the cluster network will have a dynamic IP in the range 10.42.0.1/24.\nFurther details about how to set a shared connection with NetworkManager can be found in this nice post by Beniamino Galvani.\n SSH setup My PC, as the director of the cluster, needs an SSH client running, while the NUCs need an SSH server. SSH (Secure Shell) is a remote authentication protocol that allows secure connections to remote servers that I will be using all the time to manage the cluster. To install, run, and check its status I just have to run these lines in the console:\nsudo apt install ssh sudo systemctl enable --now ssh sudo systemctl status ssh  Now, a secure certificate of the identity of a given computer, named ssh-key, that grants access to remote ssh servers and services needs to be generated.\nssh-keygen \u0026quot;label\u0026quot;  Here, substitute \u0026ldquo;label\u0026rdquo; by the name of the computer to be used as cluster\u0026rsquo;s \u0026ldquo;director\u0026rdquo;. The system will ask for a file name and a passphrase that will be used to encrypt the ssh-key.\nThe ssh-key needs to be added to the ssh-agent.\nssh-add ~/.ssh/id_rsa  To copy the ssh-key to my GitHub account, I have to copy the contents of the file ~/.ssh/id_rsa.pub (can be done just opening it with gedit ~/.ssh/id_rsa.pub + Ctrl + a + Ctrl + c), and paste it on GitHub account \u0026gt; Settings \u0026gt; SSH and GPG keys \u0026gt; New SSH Key (green button in the upper right part of the window).\nNote: If you don\u0026rsquo;t use GitHub, you\u0026rsquo;ll need to copy your ssh-key to the NUCs once they are up and running with ssh-copy-id -i ~/.ssh/id_rsa.pub user_name@nuc_IP.\n Installing and preparing ubuntu server in each NUC The NUCs don\u0026rsquo;t need to waste resources in a user graphical interface I won\u0026rsquo;t be using whatsoever. Since they will work in a headless configuration once the cluster is ready, a Linux distro without graphical user interface such as Ubuntu server is the way to go.\n Installing Ubuntu server First it is important to connect a display, a keyboard, and a mouse to the NUC in preparation, and turn it on while pushing F2 to start the visual BIOS. These BIOS parameters need to be modified:\n Advanced (upper right) \u0026gt; Boot \u0026gt; Boot Configuration \u0026gt; UEFI Boot \u0026gt; OS Selection: Linux Advanced \u0026gt; Boot \u0026gt; Boot Configuration \u0026gt; UEFI Boot \u0026gt; OS Selection: mark \u0026ldquo;Boot USB Devices First\u0026rdquo;. [optional] Advanced \u0026gt; Power \u0026gt; Secondary Power Settings \u0026gt; After Power Failure: \u0026ldquo;Power On\u0026rdquo;. I have the switch and nucs connected to an outlet plug extender with an interrupter. When I switch it on, the NUCs (and the switch) boot automatically after this option is enabled, so I only need to push one button to power up the cluster. F10 to save, and shutdown.  To prepare the USB boot device with Ubuntu server 20.04 I first download the .iso from here, by choosing \u0026ldquo;Option 3\u0026rdquo;, which leads to the manual install. Once the .iso file is downloaded, I use Ubuntu\u0026rsquo;s Startup Disk Creator to prepare a bootable USB stick. Now I just have to plug the stick in the NUC and reboot it.\nThe Ubuntu server install is pretty straightforward, and only a few things need to be decided along the way:\n As user name I choose the same I have in my personal computer. As name for the NUCs I choose \u0026ldquo;nuc1\u0026rdquo; and \u0026ldquo;nuc2\u0026rdquo;, but any other option will work well. As password, for comfort I use the same I have in my personal computer. During the network setup, choose DHCP. If the network is properly configured and the switch is powered on, after a few seconds the NUC will acquire an IP in the range 10.42.0.1/24, as any other machine within the cluster network. When asked, mark the option \u0026ldquo;Install in the whole disk\u0026rdquo;, unless you have other plans for your NUC. Mark \u0026ldquo;Install OpenSSH\u0026rdquo;. Provide it with your GitHub user name if you have your ssh-key there, and it will download it right away, facilitating a lot the ssh setup.  Reboot once the install is completed. Now I keep configuring the NUC\u0026rsquo;s operating system from my PC through ssh.\n Configuring a NUC First, to learn the IP of the NUC:\nsudo arp-scan 10.42.0.1/24  Other alternatives to this command are arp -a and sudo arp-scan -I enp2s0 --localnet. Once I learn the IP of the NUC, I add it to the file etc/hosts of my personal computer as follows.\nFirst I open the file as root.\nsudo gedit /etc/hosts  Add a new line there: 10.42.0.XXX nuc1 and save the file.\nNow I access the NUC trough ssh to keep preparing it without a keyboard and a display. I do it from Tilix, that allows to open different command line tabs in the same window, which is quite handy to manage several NUCs at once.\nAnother great option to manage the NUCs through ssh is terminator, that allows to broadcast the same commands to several ssh sessions at once. I have been trying it, and it is much better for cluster management purposes than Tilix. Actually, using it would simplify this workflow a lot, because once Ubuntu server is installed on each NUC, the rest of the configuration commands can be broadcasted at once to both NUCs. It\u0026rsquo;s a bummer I discovered this possibility way too late!\nssh blas@10.42.0.XXX  The NUC\u0026rsquo;s operating system probably has a bunch of pending software updates. To install these:\nsudo apt-get upgrade  Now I have to install a set of software packages that will facilitate managing the cluster\u0026rsquo;s network and the NUC itself.\nsudo apt install net-tools arp-scan lm-sensors dirmngr gnupg apt-transport-https ca-certificates software-properties-common samba libopenmpi3 libopenmpi-dev openmpi-bin openmpi-common htop   Setting the system time To set the system time of the NUC to the same you have in your computer, just repeat these steps in every computer in the cluster network.\n#list time zones: timedatectl list-timezones #set time zone sudo timedatectl set-timezone Europe/Madrid #enable timesyncd sudo timedatectl set-ntp on   Setting the locale The operating systems of the NUCs and the PC need to have the same locale. It can be set by editing the file /etc/default/locale with either nano (in the NUCS) or gedit (in the PC) and adding these lines, just replacing en_US.UTF-8 with your preferred locale.\nLANG=\u0026quot;en_US.UTF-8\u0026rdquo;\nLANGUAGE=\u0026quot;en_US:en\u0026rdquo;\nLC_NUMERIC=\u0026quot;en_US.UTF-8\u0026rdquo;\nLC_TIME=\u0026quot;en_US.UTF-8\u0026rdquo;\nLC_MONETARY=\u0026quot;en_US.UTF-8\u0026rdquo;\nLC_PAPER=\u0026quot;en_US.UTF-8\u0026rdquo;\nLC_IDENTIFICATION=\u0026quot;en_US.UTF-8\u0026rdquo;\nLC_NAME=\u0026quot;en_US.UTF-8\u0026rdquo;\nLC_ADDRESS=\u0026quot;en_US.UTF-8\u0026rdquo;\nLC_TELEPHONE=\u0026quot;en_US.UTF-8\u0026rdquo;\nLC_MEASUREMENT=\u0026quot;en_US.UTF-8\u0026rdquo;\n Temperature monitoring NUCs are prone to overheating when under heavy loads for prolonged times. Therefore, monitoring the temperature of the NUCs CPUs is kinda important. In a step before I installed lm-sensors in the NUC, which provides the tools to do so. To setup the sensors from an ssh session in the NUC:\nsudo sensors-detect  The program will request permission to find sensors in the NUC. I answered \u0026ldquo;yes\u0026rdquo; to every request. Once all sensors are identified, to check them\nsensors iwlwifi_1-virtual-0 Adapter: Virtual device temp1: N/A acpitz-acpi-0 Adapter: ACPI interface temp1: +32.0°C (crit = +100.0°C) coretemp-isa-0000 Adapter: ISA adapter Package id 0: +30.0°C (high = +105.0°C, crit = +105.0°C) Core 0: +30.0°C (high = +105.0°C, crit = +105.0°C) Core 1: +30.0°C (high = +105.0°C, crit = +105.0°C) Core 2: +29.0°C (high = +105.0°C, crit = +105.0°C) Core 3: +30.0°C (high = +105.0°C, crit = +105.0°C)  which gives the cpu temperatures at the moment the command was executed. The command watch sensors gives continuous temperature readings instead.\nTo control overheating in my NUCs I removed their top lids, and installed them into a custom LEGO \u0026ldquo;rack\u0026rdquo; with external USB fans with velocity control, as shown in the picture at the beginning of the post.\n Installing R To install R in the NUCs I just proceed as I would when installing it in my personal computer. There is a thorough guide here.\nIn a step above I installed all the pre-required software packages. Now I only have to add the security key of the R repository, add the repository itself, update the information on the packages available in the new repository, and finally install R.\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 sudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/' sudo apt update sudo apt install r-base  Note: If R has issues to recognize the system locale\nnano ~/.profile  add the following lines, replacing en_US.UTF-8 with your preferred locale\nexport LANG=en_US.UTF-8 export LC_ALL=en_US.UTF-8\nsave, and execute the file to export the locale so R can read it.\n. ~/.profile   Finalizing the network configuration Each NUC needs firewall rules to grant access from other computers withinn the cluster network. To activate the NUC\u0026rsquo;s firewall and check what ports are open:\nsudo ufw enable sudo ufw status  To grant access from the PC to the NUC through ssh, and later through R for parallel computing, the ports 22 and 11000 must be open for the IP of the PC (10.42.0.1).\nsudo ufw allow ssh sudo ufw allow from 10.42.0.1 to any port 11000 sudo ufw allow from 10.42.0.1 to any port 22  Finally, the other members of the cluster network must be declared in the /etc/hosts file of each computer.\nIn each NUC edit the file through ssh with bash sudo nano /etc/hosts and add the lines\n10.42.0.1 pc_name\n10.42.0.XXX name_of_the_other_nuc\nIn the PC, add the lines\n10.42.0.XXX name_of_one_nuc\n10.42.0.XXX name_of_the_other_nuc\nAt this point, after rebooting every machine, the NUCs must be accessible through ssh by using their names (ssh username@nuc_name) instead of their IPs (ssh username@n10.42.0.XXX). Just take in mind that, since the cluster network works with dynamic IPs (and such setting cannot be changed in a shared connection), the IPs of the NUCs might change if a new device is added to the network. That\u0026rsquo;s something you need to check from the PC with sudo arp-scan 10.42.0.1/24, to update every /etc/hosts file accordingly.\nI think that\u0026rsquo;s all folks. Good luck setting your home cluster! Next time I will describe how to use it for parallel computing in R.\n","date":1607299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607299200,"objectID":"ead52a439ba25afd3dd271401b5f5734","permalink":"/post/01_home_cluster/","publishdate":"2020-12-07T00:00:00Z","relpermalink":"/post/01_home_cluster/","section":"post","summary":"In this post I explain how to setup a small Beowulf cluster with a personal PC running Ubuntu 20.04 and a couple of Intel NUCs running Ubuntu Server 20.04, with the end-goal of parallelizing R tasks.","tags":null,"title":"Setting up a home cluster","type":"post"},{"authors":["Masahiro Ryo","Boyan Angelov","Stefano Mammola","Jamie M. Kass","Blas M. Benito","Florian Hartig"],"categories":null,"content":"","date":1605571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605571200,"objectID":"a92328360ad8f6c7395efe736f4f34e3","permalink":"/publication/2020_ryo_ecography/","publishdate":"2020-11-17T00:00:00Z","relpermalink":"/publication/2020_ryo_ecography/","section":"publication","summary":"Here we draw attention to an emerging subdiscipline of artificial intelligence, explainable AI (xAI), as a toolbox for better interpreting SDMs. xAI aims at deciphering the behavior of complex statistical or machine learning models (e.g. neural networks, random forests, boosted regression trees), and can produce more transparent and understandable SDM predictions.","tags":["Species Distribution Models","Explainable Artificial Intelligence (xAI)"],"title":"Explainable artificial intelligence enhances the ecological interpretability of black‐box species distribution models","type":"publication"},{"authors":["Andrea Contina","Scott W. Yanco","Allison K. Pierce","Michelle DePrenger-Levin","Michael B. Wunder","Andreas M. Neophytou","C. Phoebe Lostroh","Richard J. Telford","Blas M. Benito","Joseph Chipperfield","Robert B. O'Hara","Colin J. Carlson"],"categories":null,"content":"","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600646400,"objectID":"70fd84519cc7f0520ad6720d1a1a28e9","permalink":"/publication/2020_contina_ecological_modelling/","publishdate":"2020-09-21T00:00:00Z","relpermalink":"/publication/2020_contina_ecological_modelling/","section":"publication","summary":"In this letter we present comments on the article “A global-scale ecological niche model to predict SARS-CoV-2 coronavirus” by Coro published in 2020.","tags":["Irresponsible Covid19 modelling","Species Distribution Models"],"title":"Comment on “A global-scale ecological niche model to predict SARS-CoV-2 coronavirus infection rate”, author Coro","type":"publication"},{"authors":["Colin J. Carlson","Joseph D. Chipperfield","Blas M. Benito","Richard J. Telford","Robert B. O'Hara"],"categories":null,"content":"","date":1595980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595980800,"objectID":"0e2abe2a1f21b5de5d214beb5a8e28a3","permalink":"/publication/2020_carlson_nature_ecology_and_evolution/","publishdate":"2020-07-29T00:00:00Z","relpermalink":"/publication/2020_carlson_nature_ecology_and_evolution/","section":"publication","summary":"Araújo et al. have published a response to our piece ‘Species distribution models are inappropriate for COVID-19’1 entitled ‘Ecological and epidemiological models are both useful for SARS-CoV-2’2, in which they defend the idea that ecological models are likely to identify the signature of climate drivers in the R0 of COVID-19 transmission.","tags":["Irresponsible Covid19 modelling","Species Distribution Models"],"title":"Don’t gamble the COVID-19 response on ecological hypotheses","type":"publication"},{"authors":["Quai.Yu Cui","Marie-José Gaillard","Boris Vannière","Daniele Colombaroli","Geoffrey Lemdahl","Fredrik Olsson","Blas M. Benito","Yan Zhao"],"categories":null,"content":"","date":1594684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594684800,"objectID":"b0ae2a004a1861145db6167d8ae1d7f1","permalink":"/publication/2020_cui_the_holocene/","publishdate":"2020-07-14T00:00:00Z","relpermalink":"/publication/2020_cui_the_holocene/","section":"publication","summary":"In this study, we assess how representative a single charcoal record from a peat profile in small bogs (1.5–2 ha in area) is for the reconstruction of Holocene fire history.","tags":["Palaeoecology","Time Series Analysis"],"title":"Evaluating fossil charcoal representation in small peat bogs: Detailed Holocene fire records from southern Sweden","type":"publication"},{"authors":["Constantin M. Zohner","Lidong Mo","Susanne S. Renner","Jens-Christian Svenning","Yann Vitasse","Blas M. Benito","Alejandro Ordonez","Frederik Baumgarten","Jean-François Bastin","Veronica Sebald","Peter B. Reich","Jingjing Liang","Gert-Jan Nabuurs","Sergio de-Miguel","Giorgio Alberti","Clara Antón-Fernández","Radomir Balazy","Urs-Beat Brändli","Han Y. H. Chen","Chelsea Chisholm","Emil Cienciala","Selvadurai Dayanandan","Tom M. Fayle","Lorenzo Frizzera","Damiano Gianelle","Andrzej M. Jagodzinski","Bogdan Jaroszewicz","Tommaso Jucker","Sebastian Kepfer-Rojas","Mohammed Latif Khan","Hyun Seok Kim","Henn Korjus","Vivian Kvist Johannsen","Diana Laarmann","Mait Lang","Tomasz Zawila-Niedzwiecki","Pascal A. Niklaus","Alain Paquette","Hans Pretzsch","Purabi Saikia","Peter Schall","Vladimír Šebeň","Miroslav Svoboda","Elena Tikhonova","Helder Viana","Chunyu Zhang","Xiuhai Zhao","Thomas W. Crowther"],"categories":null,"content":"","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"a03c5f954b0fdb1d789e83f7b7821801","permalink":"/publication/2020_zohner_pnas/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/publication/2020_zohner_pnas/","section":"publication","summary":"Frost in late spring causes severe ecosystem damage in temperate and boreal regions. We here analyze late-spring frost occurrences between 1959 and 2017 and woody species’ resistance strategies to forecast forest vulnerability under climate change. Leaf-out phenology and leaf-freezing resistance data come from up to 1,500 species cultivated in common gardens. The greatest increase in leaf-damaging spring frost has occurred in Europe and East Asia, where species are more vulnerable to spring frost than in North America. The data imply that 35 and 26% of Europe’s and Asia’s forests are increasingly threatened by frost damage, while this is only true for 10% of North America. Phenological strategies that helped trees tolerate past frost frequencies will thus be increasingly mismatched to future conditions.","tags":["Phenology","Biogeography","Climate Change","Plant Ecology"],"title":"Late-spring frost risk between 1959 and 2017 decreased in North America but increased in Europe and Asia","type":"publication"},{"authors":["Colin J. Carlson","Joseph D. Chipperfield","Blas M. Benito","Richard J. Telford","Robert B. O'Hara"],"categories":null,"content":"","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588723200,"objectID":"bb8b41d7db5a51c063f7a5e33c256d3f","permalink":"/publication/2020_carlson_nature_ecology_and_evolution_b/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/publication/2020_carlson_nature_ecology_and_evolution_b/","section":"publication","summary":"Species distribution models are a powerful tool for ecological inference, but not every use is biologically justified. Applying these tools to the COVID-19 pandemic is unlikely to yield new insights, and could mislead policymakers at a critical moment.","tags":["Irresponsible Covid19 modelling","Species Distribution Models"],"title":"Species distribution models are inappropriate for COVID-19","type":"publication"},{"authors":["María Leunda","Graciela Gil-Romera","Anne-Laure Daniau","Blas M. Benito","Penélope González-Sampériz"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"48d1b269836f2685996b5ccae758bec5","permalink":"/publication/2020_leunda_catena/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/2020_leunda_catena/","section":"publication","summary":"In this paper we aim to (1) reconstruct the Holocene fire history at high altitudes of the southern Central Pyrenees, (2) add evidence to the debate on fire origin, naturally or anthropogenically produced, (3) determine the importance of fire as a disturbance agent for sub-alpine and alpine vegetation, in comparison with the plant community internal dynamics.","tags":["Palaeoecology","Fire dynamics","Ecological Memory","Generalized Least Squares","Plant Ecology"],"title":"Holocene fire and vegetation dynamics in the Central Pyrenees (Spain)","type":"publication"},{"authors":["Felde, V. A.","...","Blas M. Benito","et al."],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"0945150d072317bd056f82234adf52f5","permalink":"/publication/2020_felde_vegetation_history_and_archaeobotany/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/2020_felde_vegetation_history_and_archaeobotany/","section":"publication","summary":"The Eemian interglacial represents a natural experiment on how past vegetation with negligible human impact responded to amplified temperature changes compared to the Holocene. Here, we assemble 47 carefully selected Eemian pollen sequences from Europe to explore geographical patterns of (1) total compositional turnover and total variation for each sequence and (2) stratigraphical turnover between samples within each sequence using detrended canonical correspondence analysis, multivariate regression trees, and principal curves. Our synthesis shows that turnover and variation are highest in central Europe (47–55°N), low in southern Europe (south of 45°N), and lowest in the north (above 60°N). These results provide a basis for developing hypotheses about causes of vegetation change during the Eemian and their possible drivers.","tags":["Palaeoecology"],"title":"Compositional turnover and variation in Eemian pollen sequences in Europe","type":"publication"},{"authors":["Joseph D. Chipperfield","Robert B. O'Hara","Blas M. Benito","Richard J. Telford","Colin J. Carlson"],"categories":null,"content":"","date":1585353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585353600,"objectID":"33ce384276fda207d92a8cf37bd61e01","permalink":"/publication/2020_chipperfield_ecoevorxiv/","publishdate":"2020-03-28T00:00:00Z","relpermalink":"/publication/2020_chipperfield_ecoevorxiv/","section":"publication","summary":"The ongoing pandemic of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is causing significant damage to public health and economic livelihoods, and is putting significant strains on healthcare services globally. This unfolding emergency has prompted the preparation and dissemination of the article “Spread of SARS-CoV-2 Coronavirus likely to be constrained by climate” by Araújo and Naimi (2020). The authors present the results of an ensemble forecast made from a suite of species distribution models (SDMs), where they attempt to predict the suitability of the climate for the spread of SARS-CoV-2 over the coming months. They argue that climate is likely to be a primary regulator for the spread of the infection and that people in warm-temperate and cold climates are more vulnerable than those in tropical and arid climates. A central finding of their study is that the possibility of a synchronous global pandemic of SARS-CoV-2 is unlikely. Whilst we understand that the motivations behind producing such work are grounded in trying to be helpful, we demonstrate here that there are clear conceptual and methodological deficiencies with their study that render their results and conclusions invalid.","tags":["Irresponsible Covid19 modelling","Species Distribution Models"],"title":"On the inadequacy of species distribution models for modelling the spread of SARS-CoV-2: response to Araújo and Naimi","type":"publication"},{"authors":["Blas M. Benito"],"categories":null,"content":"","date":1579737600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579737600,"objectID":"80c7b3f2edb958819795bfdb5c63cf9b","permalink":"/publication/2020_benito_ecography_distantia/","publishdate":"2020-01-23T00:00:00Z","relpermalink":"/publication/2020_benito_ecography_distantia/","section":"publication","summary":"We introduce distantia (v1.0.1), an R package providing general toolset to quantify dissimilarity between ecological time‐series, independently of their regularity and number of samples. The functions in distantia provide the means to compute dissimilarity scores by time and by shape and assess their significance, evaluate the partial contribution of each variable to dissimilarity, and align or combine sequences by similarity.","tags":["Time Series Analysis","R packages"],"title":"distantia: an open‐source toolset to quantify dissimilarity between multivariate ecological time‐series","type":"publication"},{"authors":["Blas M. Benito","Graciela Gil-Romera"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"b54d3a6bfb2beb4f27f5c5b2a798cd55","permalink":"/publication/2020_benito_ecography_memoria/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/2020_benito_ecography_memoria/","section":"publication","summary":"Paper published in the section \"Editor's Choice\" of the *Ecography* journal. It received [an award](https://www.dropbox.com/s/oacsy1xqx4omv1b/2019_BMB_Ecography_b_top_downloaded.png?dl=1) for the number of downloads during the 12 months after its publication.","tags":["Quantitative methods","R packages","Palaeoecology","Ecological Memory","Plant Ecology","Machine Learning","Random Forest"],"title":"Ecological memory at millennial time‐scales: the importance of data constraints, species longevity and niche features","type":"publication"},{"authors":["B.L Valero-Garcés","Penélope González-Sampériz","Graciela Gil-Romera","Blas M. Benito","A. Moreno","B. Oliva-Urcia","J. Aranbarri","E. García-Prieto","M. Frugone","M. Morellón","L.J. Arnold","M. Demuro","M. Hardiman","S.P.E. Blockey","C.S. Lane"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"a61e896d35882a74ae0ea224990d2ae3","permalink":"/publication/2019_valero-garces_quaternary_geochronology/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/2019_valero-garces_quaternary_geochronology/","section":"publication","summary":"We present a multidisciplinary dating approach - including radiocarbon, Uranium/Thorium series (U/Th), paleomagnetism, single-grain optically stimulated luminescence (OSL), polymineral fine-grain infrared stimulated luminescence (IRSL) and tephrochronology - used for the development of an age model for the Cañizar de Villarquemado sequence (VIL) for the last ca. 135 ka.","tags":["Palaeoecology","Age-depth modelling","Bayesian models"],"title":"A multi-dating approach to age-modelling long continental records: The 135 ka El Cañizar de Villarquemado sequence (NE Spain)","type":"publication"},{"authors":["Graciela Gil-Romera","Carole Adolf","Blas M. Benito","Lucas Bittner","Maria U. Johansson","David A. Grady","Henry F. Lamb","Bruk Lemma","Mekbib Fekadu","Bruno Glaser","Betelhem Mekonnen","Miguel Sevilla-Callejo","Michael Zech","Wolfgang Zech","Georg Miaha"],"categories":null,"content":"","date":1563926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563926400,"objectID":"691382af0f3f50780ae78edc146f99e0","permalink":"/publication/2019_gil_romera_biology_letters/","publishdate":"2019-07-24T00:00:00Z","relpermalink":"/publication/2019_gil_romera_biology_letters/","section":"publication","summary":"We hypothesize that fire has influenced Erica communities in the Bale Mountains at millennial time-scales. To test this, we (1) identify the fire history of the Bale Mountains through a pollen and charcoal record from Garba Guracha, a lake at 3950 m.a.s.l., and (2) describe the long-term bidirectional feedback between wildfire and Erica, which may control the ecosystem's resilience.","tags":["Palaeoecology","Fire dynamics","Ecological Memory","Time Series Analysis","Generalized Least Squares","Plant Ecology"],"title":"Long-term fire resilience of the Ericaceous Belt, Bale Mountains, Ethiopia","type":"publication"},{"authors":["Blas M. Benito"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab  Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata ( front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"1f184bf285911dee5df13e26218d60b2","permalink":"/post_examples/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post_examples/jupyter/","section":"post_examples","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post_examples"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Albuquerque F.","Blas M. Benito","Rodríguez MÁM.","Gray C."],"categories":null,"content":"","date":1537315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537315200,"objectID":"0dd555719b21269ebb41f0603a2e3704","permalink":"/publication/2018_albuquerque_peerj/","publishdate":"2018-09-19T00:00:00Z","relpermalink":"/publication/2018_albuquerque_peerj/","section":"publication","summary":"The goals of this study are to provide a map of actual habitat suitability (1), describe the relationships between abiotic predictors and the saguaro distribution at regional extents (2), and describe the potential effect of climate change on the spatial distribution of the saguaro (3).","tags":["Biogeography","Climate Change","Species Distribution Models","Machine Learning","Gradient Boosting","Plant Ecology"],"title":"Potential changes in the distribution of Carnegiea gigantea under future scenarios","type":"publication"},{"authors":["Radoslav Kozma","Mette Lillie","Blas M. Benito","Jens-Christian Svenning","Jacob Höglund"],"categories":null,"content":"","date":1527552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527552000,"objectID":"2a60bd7aa28feceb59407d600d738438","permalink":"/publication/2018_kozma_ecology_and_evolution/","publishdate":"2018-05-29T00:00:00Z","relpermalink":"/publication/2018_kozma_ecology_and_evolution/","section":"publication","summary":"Here we investigated the demographic history of the willow grouse (Lagopus lagopus), rock ptarmigan (Lagopus muta), and black grouse (Tetrao tetrix) through the Late Pleistocene using two complementary methods and whole genome data. Species distribution modeling (SDM) allowed us to estimate the total range size during the Last Interglacial (LIG) and Last Glacial Maximum (LGM) as well as to indicate potential population subdivisions.","tags":["Biogeography","Climate Change","Species Distribution Models","Generalized Linear Models"],"title":"Past and potential future population dynamics of three grouse species using ecological and whole genome coalescent modeling","type":"publication"},{"authors":["Gang Feng","Ziyu Ma","Blas M. Benito","Signe Normand","Alejandro Ordoñez","Yi Jin","Lingfeng Mao","Jens-Christian Svenning"],"categories":null,"content":"","date":1500249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500249600,"objectID":"fc5bc772c0898cc6442de19eb3810539","permalink":"/publication/2017_feng_global_ecology_and_biogeography/","publishdate":"2017-07-17T00:00:00Z","relpermalink":"/publication/2017_feng_global_ecology_and_biogeography/","section":"publication","summary":"Our results show that phylogenetically diverse assemblages with large phylogenetic age differences among species are associated with relatively high long‐term climate stability, with intra‐regional links between long‐term climate variability and phylogenetic composition especially strong in the more unstable regions. These findings point to future climate change as a key risk to the preservation of the phylogenetically diverse assemblages in regions characterized by relatively high paleoclimate stability, with China as a key example.","tags":["Biogeography","Plant Ecology"],"title":"Phylogenetic age differences in tree assemblages across the Northern Hemisphere increase with long-term climate stability in unstable regions","type":"publication"},{"authors":["Gang Feng","Lingfeng Mao","Blas M. Benito","Nathan G. Swenson","Jens-Christian Svenning"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"02c9b25a1d6d387966133e1a58983917","permalink":"/publication/2017_feng_biological_conservation/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/2017_feng_biological_conservation/","section":"publication","summary":"In this study, for the first time, we linked the distribution of threatened species across China to current and historical changes in human population densities, cropland area, and pasture area since 1700 (at a 100 km × 100 km resolution). We find that variables describing historical changes in human impacts were consistently more strongly associated with proportions of threatened plants than variables describing current changes in human impacts. Notably, threatened plant species in China tend to be concentrated where historical anthropogenic impacts were relatively small, but anthropogenic activities have intensified relatively strongly since 1700.","tags":["Biogeography","Random Forest","Machine Learning","Biodiversity Conservation"],"title":"Historical anthropogenic footprints in the distribution of threatened plants in China","type":"publication"},{"authors":["Blas M. Benito","Jens-Christian Svenning","Trine Kellberg Nielsen","Felix Riede","Graciela Gil-Romera","Thomas Mailund","Brody Sandel"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"3fffb042779562c60fe874bc323131de","permalink":"/publication/2017_benito_journal_of_biogeography/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/2017_benito_journal_of_biogeography/","section":"publication","summary":"This paper [was highlighted in the *Editor's Picks* section of the Science Journal](https://www.dropbox.com/s/6k308eczv7i6kbj/2017_BMB_Journal_of_Biogeography_editors_choice.pdf?dl=1), and was among the [top downloaded articles](https://www.dropbox.com/s/sowq1h4bdngmipy/2017_BMB_Journal_of_Biogeography.png?dl=1) from the *Journal of Biogeography* during the 12 months after its publication.","tags":["Biogeography","Species Distribution Models","Biogeography of Neanderthals","Generalized Linear Models","Ensemble models"],"title":"The ecological niche and distribution of Neanderthals during the Last Interglacial","type":"publication"},{"authors":["Trine Kellberg Nielsen","Blas M. Benito","Jens-Christian Svenning","Brody Sandel","Luseadra McKerracher","Felix Riede"],"categories":null,"content":"","date":1488240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488240000,"objectID":"70212c1a73a4ebfe55db225953cd1f5d","permalink":"/publication/2017_kellberg-nielsen_quaternary_international/","publishdate":"2017-02-28T00:00:00Z","relpermalink":"/publication/2017_kellberg-nielsen_quaternary_international/","section":"publication","summary":"Our results are inconsistent with the claim that climatic constraint and/or a lack of suitable habitats can fully explain the absence of Neanderthals in Southern Scandinavia during the Eemian Interglacial and Early Weichselian Glaciation. We do, however, find evidence that a geographic barrier may have impeded northerly migrations during the Eemian.","tags":["Biogeography","Biogeography of Neanderthals","Species distribution models"],"title":"Investigating Neanderthal dispersal above 55°N in Europe during the Last Interglacial Complex","type":"publication"},{"authors":["Constantin M. Zohner","Blas M. Benito","Jason D. Fridley","Jens-Christian Svenning","Susanne S. Renner"],"categories":null,"content":"","date":1487030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487030400,"objectID":"1bf8e70c8f4c03b6d059e49a3fd0bf9b","permalink":"/publication/2017_zohner_ecology_letters/","publishdate":"2017-02-14T00:00:00Z","relpermalink":"/publication/2017_zohner_ecology_letters/","section":"publication","summary":"Intuitively, interannual spring temperature variability (STV) should influence the leaf‐out strategies of temperate zone woody species, with high winter chilling requirements in species from regions where spring warming varies greatly among years. We tested this hypothesis using experiments in 215 species and leaf‐out monitoring in 1585 species from East Asia (EA), Europe (EU) and North America (NA). The results reveal that species from regions with high STV indeed have higher winter chilling requirements, and, when grown under the same conditions, leaf out later than related species from regions with lower STV. Since 1900, STV has been consistently higher in NA than in EU and EA, and under experimentally short winter conditions NA species required 84% more spring warming for bud break, EU ones 49% and EA ones only 1%. These previously unknown continental‐scale differences in phenological strategies underscore the need for considering regional climate histories in global change models.","tags":["Phenology","Biogeography","Plant Ecology"],"title":"Spring predictability explains different leaf‐out strategies in the woody floras of North America, Europe and East Asia","type":"publication"},{"authors":["Constantin M. Zohner","Blas M. Benito","Jens-Christian Svenning","Susanne S. Renner"],"categories":null,"content":"","date":1476662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476662400,"objectID":"8bb3d0dd4e4d13b48c7b50f18f1b7e44","permalink":"/publication/2016_zohner_nature_climate_change/","publishdate":"2016-10-17T00:00:00Z","relpermalink":"/publication/2016_zohner_nature_climate_change/","section":"publication","summary":"Our results do not support previous ideas about phenological strategies in temperate woody species (the ‘high temperature variability’ hypothesis; the ‘oceanic climate’ hypothesis; the ‘high latitude’ hypothesis). In regions with long winters, trees appear to rely on cues other than day length, such as winter chilling and spring warming. By contrast, in regions with short winters, some species—mostly from lineages with a warm-temperate or subtropical background, for example, Fagus additionally rely on photoperiodism. Therefore, photoperiod may be expected to constrain climate-driven shifts in spring leaf unfolding only at lower latitudes.","tags":["Phenology","Biogeography","Plant Ecology"],"title":"Day length unlikely to constrain climate-driven shifts in leaf-out times of northern woody plants","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d337bc1d87a46134727f7fb46c0d4efc","permalink":"/project_examples/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project_examples/external-project/","section":"project_examples","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project_examples"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"875a64698311258ca5954edf3adc2327","permalink":"/project_examples/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project_examples/internal-project/","section":"project_examples","summary":"An example of using the in-built project page.","tags":[""],"title":"Internal Project","type":"project_examples"},{"authors":["Blas M. Benito","吳恩達"],"categories":["Demo","教程"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\n Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n 👉 Get Started 📚 View the documentation 💬 Ask a question on the forum 👥 Chat with the community 🐦 Twitter: @source_themes @GeorgeCushen #MadeWithAcademic 💡 Request a feature or report a bug ⬆️ Updating? View the Update Guide and Release Notes ❤️ Support development of Academic:  ☕️ Donate a coffee 💵 Become a backer on Patreon 🖼️ Decorate your laptop or journal with an Academic sticker 👕 Wear the T-shirt 👩‍💻 Contribute      Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\n Choose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem   Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site  Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n  one-click install using your web browser (recommended)  install on your computer using Git with the Command Prompt/Terminal app  install on your computer by downloading the ZIP files  install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating  View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"a25eb5ef2134a076531687948275d21e","permalink":"/post_examples/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post_examples/getting-started/","section":"post_examples","summary":"Create a beautifully simple website in under 10 minutes.","tags":null,"title":"Academic: the website builder for Hugo","type":"post_examples"},{"authors":null,"categories":["R"],"content":" R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932  Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA )  Figure 1: A fancy pie chart.   ","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"9554380237eec0879443a7c1a234a75f","permalink":"/post_examples/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post_examples/2015-07-23-r-rmarkdown/","section":"post_examples","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post_examples"},{"authors":["Jacquelyn L. Gill","Jessica L. Blois","Blas M. Benito","Solomon Dobrowski","Malcolm L. Hunter Jr.","Jenny L. McGuire"],"categories":null,"content":"","date":1430179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430179200,"objectID":"aebebdc92cbf3072b7bf6d150d269b6c","permalink":"/publication/2015_gill_conservation_biology/","publishdate":"2015-04-28T00:00:00Z","relpermalink":"/publication/2015_gill_conservation_biology/","section":"publication","summary":"Paleoecology provides a valuable perspective on coarse‐filter strategies by marshaling the natural experiments of the past to contextualize extinction risk due to the emerging impacts of climate change and anthropogenic threats. We reviewed examples from the paleoecological record that highlight the strengths, opportunities, and caveats of a CNS approach. We focused on the near‐time geological past of the Quaternary, during which species were subjected to widespread changes in climate and concomitant changes in the physical environment in general.","tags":["Palaeoecology","Biodiversity conservation"],"title":"A 2.5‐million‐year perspective on coarse‐filter strategies for conserving nature's stage","type":"publication"},{"authors":["Albuquerque F.","Blas M. Benito","Beier, P.","Assunção-Albuquerque, M.J.","Cayuela, L."],"categories":null,"content":"","date":1425340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1425340800,"objectID":"d1d254ad844854fa63cf4af98860d93d","permalink":"/publication/2015_albuquerque_naturaleza_and_conservacao/","publishdate":"2015-03-03T00:00:00Z","relpermalink":"/publication/2015_albuquerque_naturaleza_and_conservacao/","section":"publication","summary":"We had three key findings. First, dry forest is the least protected biome in Mesoamerica (4.5% protected), indicating that further action to safeguard this biome is warranted. Secondly, the poor overlap between protected areas and high-value forest conservation areas found herein may provide evidence that the establishment of protected areas may not be fully accounting for tree priority rank map. Third, high percentages of forest cover and high-value forest conservation areas still need to be represented by the protected areas network. Because deforestation rates are still increasing in this region, Mesoamerica needs funding and coordinated action by policy makers, national and local governmental and non-governmental organizations, conservationists and other stakeholders.","tags":["Biodiversity Conservation","Forests","Species Distribution Models","Random Forest","Machine Learning"],"title":"Supporting underrepresented forests in Mesoamerica","type":"publication"},{"authors":["A.J. Mendoza-Fernández","F. Martínez-Hernández","F.J. Pérez-García","J.A. Garrido Becerra","Blas M. Benito","E. Salmerón Sánchez","J. Guirado","M.E. Merlo","J.F. Mota"],"categories":null,"content":"","date":1421107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1421107200,"objectID":"65c5f86bbc46066e9c0c0c783d5667b8","permalink":"/publication/2015_mendoza-fernandez_plant_biosystems/","publishdate":"2015-01-13T00:00:00Z","relpermalink":"/publication/2015_mendoza-fernandez_plant_biosystems/","section":"publication","summary":"*Maytenus senegalensis* subsp. *europaea* communities are unique vegetal formations in Europe. In fact, they are considered Priority Habitat by Directive 92/43/EEC. These are ecologically valuable plant communities found in the southeast of Spain. By combining modeling methods of environmental variables, historical photo-interpretation, and fieldwork, a chronosequence of the evolution of their extent of occurrence (EOO) has been reconstructed in 1957 and 2011. Results showed a strong regression range of *Maytenus senegalensis* subsp. *europaea* populations. More than 26,000 ha of EOO for this species have been lost in the province of Almería. Considering the final number of polygons, this area has been fragmented 18 times since the 1950s. These results reinforce the idea that the alteration and fragmentation of habitat due to human activities is one of the most important drivers of biodiversity loss and global change. These activities are mostly intensive greenhouse agriculture and urbanization without sustainable land planning. Knowledge about the distribution of M. senegalensis subsp. europaea is of great interest for future habitat restoration. Therefore, this would be the key species to recover these damaged ecosystems.","tags":["Species distribution models","Biodiversity conservation","Ecoinformatics","Habitat loss"],"title":"Extreme habitat loss in a Mediterranean habitat: Maytenus senegalensis subsp. europaea","type":"publication"},{"authors":["José Miguel Barea-Azcón","Blas M. Benito (shared first coauthorship)","Francisco J. Olivares","Helena Ruiz","Javier Martín","Antonio L. García","Rogelio López"],"categories":null,"content":"","date":1392854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1392854400,"objectID":"24408a853074612aafa67c0a6df22901","permalink":"/publication/2014_barea-azcon_and_benito_biodiversity_and_conservation/","publishdate":"2014-02-20T00:00:00Z","relpermalink":"/publication/2014_barea-azcon_and_benito_biodiversity_and_conservation/","section":"publication","summary":"Herein we investigate the distribution and conservation problems of a relict interaction in the Sierra Nevada mountains (southern Europe) between the butterfly *Agriades zullichi* —a rare and threatened butterfly— and its larval foodplant *Androsace vitaliana* subsp. *nevadensis*. We designed an intensive field survey to obtain a comprehensive presence dataset. This was used to calibrate species distribution models with absences taken at local and regional extents, analyze the potential distribution, evaluate the influence of environmental factors in different geographical contexts, and evaluate conservation threats for both organisms.","tags":["Biodiversity Conservation","Species Distribution Models","Ecoinformatics","Biogeography","Random Forest","Machine Learning"],"title":"Distribution and conservation of the relict interaction between the butterfly Agriades zullichi and its larval foodplant (Androsace vitaliana nevadensis)","type":"publication"},{"authors":["Francisco J. Bonet","Ramón Pérez-Pérez","Blas M. Benito","Fabio Suzart de Albuquerque","Regino Zamora"],"categories":null,"content":"","date":1391212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1391212800,"objectID":"bf6fe82e9413557dd6e0d799c6787ba1","permalink":"/publication/2014_bonet_environmental_modelling_and_software/","publishdate":"2014-02-01T00:00:00Z","relpermalink":"/publication/2014_bonet_environmental_modelling_and_software/","section":"publication","summary":"Many of the best practices concerning the development of ecological models or analytic techniques published in the scientific literature are not fully available to modelers but rather are stored in scientists' digital or biological memories. We propose that it is time to address the problem of storing, documenting, and executing ecological models and analytical procedures. In this paper, we propose a conceptual framework to design and implement a web application that will help to meet this challenge. This tool will foster cooperation among scientists, enhancing the creation of relevant knowledge that could be transferred to environmental managers. We have implemented this conceptual framework in a tool called ModeleR. This is being used to document, share, and execute more than 200 models and analytical processes associated with a global change monitoring program that is being undertaken in the Sierra Nevada Mountains (south Spain). ModeleR uses the concept of scientific workflow to connect and execute different types of models and analytical processes. Finally, we have envisioned the creation of a federation of model repositories where models documented within a local repository could be linked and even executed by other researchers.","tags":["Ecoinformatics"],"title":"Documenting, storing, and executing models in Ecology: A conceptual framework and real implementation in a global change monitoring program","type":"publication"},{"authors":["Blas M. Benito","Juan Lorite","Ramón Pérez-Pérez","Lorena Gómez-Aparicio","Julio Peñas"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"c1e357c5f2a52ae592bcf91f774e7627","permalink":"/publication/2014_benito_diversity_and_distributions/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/2014_benito_diversity_and_distributions/","section":"publication","summary":"The Mediterranean Basin is threatened by climate change, and there is an urgent need for studies to determine the risk of plant range shift and potential extinction. In this study, we simulate potential range shifts of 176 plant species to perform a detailed prognosis of critical range decline and extinction in a transformed mediterranean landscape. Particularly, we seek to answer two pivotal questions: (1) what are the general plant‐extinction patterns we should expect in mediterranean landscapes during the 21st century? and (2) does dispersal ability prevent extinction under climate change?.","tags":["Climate Change","Species Distribution Models","Ecoinformatics","Mechanistic Simulation","Random Forest","Ensemble models","Conditional Inference Trees","Plant Ecology"],"title":"Forecasting plant range collapse in a mediterranean hotspot: when dispersal uncertainties matter","type":"publication"},{"authors":["Elise S. Gornish","Jill A. Hamilton","Albert Barberán","Blas M. Benito","Amrei Binzer","Julie E. DeMeester","Robert Gruwez","Bruno Moreira","Shirin Taheri","Sara Tomiolo","Catarina Vinagre","Pauline Vurain","Jennifer Weaver"],"categories":null,"content":"","date":1366070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1366070400,"objectID":"5ee0e5600c367d4720aa53a935a5c256","permalink":"/publication/2013_gornish_eos/","publishdate":"2013-04-16T00:00:00Z","relpermalink":"/publication/2013_gornish_eos/","section":"publication","summary":"Climate change research is an interdisciplinary field, and understanding its social, political, and environmental implications requires integration across fields of research where different tools may be used to address common concerns. One of the many advantages of interdisciplinary approaches is that they open communication between complementary fields, filling knowledge gaps and facilitating progression within both individual fields and the broader field of climate change research.","tags":["Climate Change"],"title":"Interdisciplinary Climate Change Collaborations Are Essential for Early‐Career Scientists","type":"publication"},{"authors":["Mireia Valle","Marieke M. van Katwijk","Dick J. de Jong","Tjeerd J. Bouma","Aafke M. Schipper","Guillem Chust","Blas M. Benito","Joxe M. Garmendia","Ángel Borja"],"categories":null,"content":"","date":1363305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1363305600,"objectID":"77359446d7cb7b0af132b5de61eb93b1","permalink":"/publication/2013_valle_journal_of_sea_research/","publishdate":"2013-03-15T00:00:00Z","relpermalink":"/publication/2013_valle_journal_of_sea_research/","section":"publication","summary":"A time series of 14-year distribution data of Zostera marina in the Ems estuary (The Netherlands) was used to build different data subsets: (1) total presence area; (2) a conservative estimate of the total presence area, defined as the area which had been occupied during at least 4 years; (3) core area, defined as the area which had been occupied during at least 2/3 of the total period; and (4–6) three random selections of monitoring years. On average, colonized and disappeared areas of the species in the Ems estuary showed remarkably similar transition probabilities of 12.7% and 12.9%, respectively. SDMs based upon machine-learning methods (Boosted Regression Trees and Random Forest) outperformed regression-based methods. Current velocity and wave exposure were the most important variables predicting the species presence for widely distributed data. Depth and sea floor slope were relevant to predict conservative presence area and core area.","tags":["Biodiversity Conservation","Species Distribution Models","Machine learning","Random Forest","Gradient Boosting"],"title":"Comparing the performance of species distribution models of Zostera marina: Implications for conservation","type":"publication"},{"authors":["Blas M. Benito","Luis Cayuela","Fabio S. Albuquerque"],"categories":null,"content":"","date":1362528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1362528000,"objectID":"69d5141556741b6f7698c5bfd3796a74","permalink":"/publication/2013_benito_methods_in_ecology_and_evolution/","publishdate":"2013-03-06T00:00:00Z","relpermalink":"/publication/2013_benito_methods_in_ecology_and_evolution/","section":"publication","summary":"We generated 380 S‐SDMs of 1224 tree species in Mesoamerica by combining 19 distribution modelling methods with 20 different thresholds using presence‐only data from the Global Biodiversity Information Facility. We compared the predicted richness and composition with inventory data obtained from the BIOTREE‐NET forest plot database. We designed two indicators of predictive performance that were based on the diversity factors used to measure species turnover: a (shared species between the observed and predicted compositions), b and c (the exclusive species of the predicted and observed compositions respectively) and compared them with the Sorensen and Beta‐Simpson turnover measures. Some modelling methods – especially machine learning and ensemble model forecasting methods performed significantly better than others in minimizing the error in predicted richness and composition. Our results also indicate that restrictive thresholds (with high omission errors) lead to more accurate S‐SDMs in terms of species richness and composition. Here, we demonstrate that particular combinations of modelling methods and thresholds provide results with higher predictive performance.","tags":["Species Distribution Models","Ecoinformatics","Random Forest","Ensemble models","Machine Learning"],"title":"The impact of modelling choices in the predictive performance of richness maps derived from species‐distribution models: guidelines to build better diversity models","type":"publication"},{"authors":["Albuquerque, F.","Assunção-Albuquerque, M.J.","Cayuela, L.","Zamora, R.","Blas M. Benito"],"categories":null,"content":"","date":1358726400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1358726400,"objectID":"bb2936f49fe5e9950d89f99bc84a0d90","permalink":"/publication/2013_albuquerque_biological_conservation/","publishdate":"2013-01-21T00:00:00Z","relpermalink":"/publication/2013_albuquerque_biological_conservation/","section":"publication","summary":"Our assessments showed little association between bird richness patterns and the cover of protected areas (PAs) across EU countries. The congruence between high-value richness areas of all bird species and IBS with PAs cover was moderate, suggesting that different conservation planning targets should be taken into account to safeguard IBS, or the composition of bird species. Our results also showed that 16 (3.9%) threatened species were present in gaps of PAs. The poor relationship between PAs cover and bird richness pattern found herein may provide evidence that the establishment of SPAs across Europe may not be fully accounting for richness patterns to enhance the performance of the current network.","tags":["Biodiversity Conservation"],"title":"European Bird distribution is “well” represented by Special Protected Areas: Mission accomplished?","type":"publication"},{"authors":["Ramón Pérez-Pérez","Blas M. Benito","Francisco J. Bonet"],"categories":null,"content":"","date":1328313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1328313600,"objectID":"29fc5074a7e132bb1510f42cecbde2aa","permalink":"/publication/2012_perez-perez_expert_systems_with_applications/","publishdate":"2012-02-04T00:00:00Z","relpermalink":"/publication/2012_perez-perez_expert_systems_with_applications/","section":"publication","summary":"In this paper, we present the development of ModeleR, a repository of models accessible from the web, which enables the user to design, document, manage, and execute environmental models.","tags":["Ecoinformatics"],"title":"ModeleR: An enviromental model repository as knowledge base for experts","type":"publication"},{"authors":["Julio Peñas","Blas M. Benito","Juan Lorite","Miguel Ballesteros","Eva María Cañadas","Montserrat Martínez-Ortega"],"categories":null,"content":"","date":1301097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1301097600,"objectID":"f7cb87fcde920214c11f820e82fdc695","permalink":"/publication/2011_penas_environmental_management/","publishdate":"2011-03-26T00:00:00Z","relpermalink":"/publication/2011_penas_environmental_management/","section":"publication","summary":"The results indicate that greenhouses and construction activities (mainly for tourist purposes) exert a strong impact on the populations of this endangered species. The habitat depletion showed peaks that constitute the destruction of 85% of the initial area in only 20 years for some populations of L. nigricans. According to the forecast established by the model, a rapid extinction could take place and some populations may disappear as early as the year 2030. Fragmentation-cadence analysis can help identify population units of primary concern for its conservation, by means of the adoption of improved management and regulatory measures.","tags":["Habitat Fragmentation","Drylands","Endangered Plants","Biodiversity Conservation"],"title":"Habitat Fragmentation in Arid Zones: A Case Study of Linaria nigricans Under Land Use Changes (SE Spain)","type":"publication"},{"authors":["Blas M. Benito","Juan Lorite","Julio Peñas"],"categories":null,"content":"","date":1296604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1296604800,"objectID":"d94d778916672e3a2b7a6e74a2278246","permalink":"/publication/2011_benito_climate_change/","publishdate":"2011-02-02T00:00:00Z","relpermalink":"/publication/2011_benito_climate_change/","section":"publication","summary":"According to the simulations, the suitable habitat for the key species inhabiting the summit area, where most of the endemic and/or rare species are located, may disappear before the middle of the century. The other key species considered show moderate to drastic suitable habitat loss depending on the considered scenario. Climate warming should provoke a strong substitution dynamics between species, increasing spatial competition between both of them. In this study, we introduce the application of differential suitability concept into the analysis of potential impact of climate change, forest management and environmental monitoring, and discuss the limitations and uncertainties of these simulations.","tags":["Climate Change","Species Distribution Models","Ecoinformatics","Plant Ecology"],"title":"Simulating potential effects of climatic warming on altitudinal patterns of key species in Mediterranean-alpine ecosystems","type":"publication"},{"authors":["Juan Lorite","Julio Peñas","Blas M. Benito","Eva Cañadas","Francisco Valle"],"categories":null,"content":"","date":1267401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1267401600,"objectID":"3d86a9f5c5dcbb080231f4e1d7755fe0","permalink":"/publication/2010_lorite_annales_botanici_fennici/","publishdate":"2010-03-01T00:00:00Z","relpermalink":"/publication/2010_lorite_annales_botanici_fennici/","section":"publication","summary":"We studied the natural history as well as the conservation status of the first-known population of Polygala balansae in Europe (Granada, SE Spain). In the study area, we located only one population occupying a small patch of 1920 m2, between 120 and 160 m a.s.l., with 246 mature individuals. The species is classified as Critically Endangered (CR), under the following criteria: severely fragmented, inferred continuous decline, small population size, and continuing decline inferred from the number mature individuals.","tags":["Botany","Endangered Plants","Biogeography","Biodiversity Conservation"],"title":"Conservation Status of the First Known Population of Polygala balansae in Europe","type":"publication"},{"authors":["Francisca Alba-Sánchez","José A. López-Sáez","Blas M. Benito","Juan C. Linares","Diego Nieto-Lugilde","Lourdes López-Merino"],"categories":null,"content":"","date":1266710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1266710400,"objectID":"3bd0e1e66942974aaf0e65818e475037","permalink":"/publication/2010_alba-sanchez_diversity_and_distributions/","publishdate":"2010-02-21T00:00:00Z","relpermalink":"/publication/2010_alba-sanchez_diversity_and_distributions/","section":"publication","summary":"Quaternary palaeopalynological records collected throughout the Iberian Peninsula and species distribution models (SDMs) were integrated to gain a better understanding of the historical biogeography of the Iberian Abies species (i.e. Abies pinsapo and Abies alba). We hypothesize that SDMs and Abies palaeorecords are closely correlated, assuming a certain stasis in climatic and topographic ecological niche dimensions. In addition, the modelling results were used to assign the fossil records to A. alba or A. pinsapo, to identify environmental variables affecting their distribution, and to evaluate the ecological segregation between the two taxa.","tags":["Palaeoecology","Species Distribution Models","Biogeography"],"title":"Past and present potential distribution of the Iberian Abies species: a phytogeographic approach using fossil pollen data and species distribution models ","type":"publication"},{"authors":["Blas M. Benito","Montserrat Martínez-Ortega","Luz M. Muñoz","Juan Lorite","Julio Peñas"],"categories":null,"content":"","date":1235779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1235779200,"objectID":"6039de1ce8c3c53184e8f0a191151510","permalink":"/publication/2009_benito_biodiversity_and_conservation/","publishdate":"2009-02-28T00:00:00Z","relpermalink":"/publication/2009_benito_biodiversity_and_conservation/","section":"publication","summary":"In this paper, we propose the application of SDMs to assess the extinction-risk of plant species in relation to the spread of greenhouses in a Mediterranean landscape, where habitat depletion is one of the main causes of biodiversity loss.","tags":["Endangered Plants","Biodiversity Conservation","Species Distribution Models","Drylands"],"title":"Assessing extinction-risk of endangered plants using species distribution models: a case study of habitat depletion caused by the spread of greenhouses","type":"publication"},{"authors":["Blas M. Benito"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"5dbb5dd8f81f0274ffb682d6d27968b6","permalink":"/publication/2008_penas_phyton/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/2008_penas_phyton/","section":"publication","summary":"We we develop a methodology predicting the expansion of greenhouses by combining a species distribution model (MaxEnt) and a simulator of land use change (Geomod).","tags":["Biodiversity Conservation","Species Distribution Models","Drylands"],"title":"Greenhouses, land use change, and predictive models: MaxEnt and Geomod working together","type":"publication"}]