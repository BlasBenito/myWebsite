[{"authors":["admin"],"categories":null,"content":"Hello there!\nMy name is Blas, and I am a spatial data scientist and engineer in AgTech, holding a PhD in computational ecology and an MSc in geographic information systems.\nMy expertise lies at the intersection of spatial and temporal modeling, soil and plant ecology, remote sensing, machine learning, and environmental dynamics and monitoring.\nMy work I\u0026rsquo;m deeply passionate about crafting automated data and modeling pipelines to tackle complex environmental challenges.\nCurrently, I lead the Environmental Data Team at Biome Makers Inc. In this role, I research and develop cutting-edge smart farming technologies, create essential R packages to enhance our Data Science Department\u0026rsquo;s capabilities, and oversee the design and maintenance of an environmental data infrastructure to further empower our flagship product, BeCrop®.\nMy Tech Stack My tech stack is built entirely on open-source tools: I rely on R and git+ GitHub for collaborative software development and version control. For pipeline design, I harness the power of targets, and to encapsulate code I employ renv and docker.\nFor GIS tasks, I turn to industry-standard tools like GRASS GIS, Quantum GIS, and PostGIS.\nMy data management and processing are handled by PostgreSQL, DuckDB, Apache Arrow, and Apache Spark.\nComputationally-intensive pipelines find their home in my tiny home-cluster managed by slurm.\nFor developing and deploying REST APIs, I turn to plumber, while interactive apps are crafted with Shiny. My interactive reports come to life using either Rmarkdown or Quarto.\nMy Academic Journey Before delving into AgTech, I honed my research and technical skills during a successful academic career in Computational Ecology. I worked in world-class labs in Spain ( IISTA and Maestre Lab), Denmark ( Jens-Christian Svenning Lab), and Norway ( EECRG).\nMy research primarily focused on unveiling the environmental drivers shaping the distribution of biological diversity in space and time. During this journey, I developed scientific R packages for various purposes, such as time-series comparison and analysis of lagged effects, spatial modeling with Random Forest, and ecological simulation.\nThroughout this journey, I collaborated with 210 esteemed coauthors from 22 countries to publish 49 research papers in reputable peer-reviewed journals. To date, our collective work has garnered over 1600 citations. Notably, three of these papers have received recognition as \u0026lsquo;most downloaded papers\u0026rsquo; in prestigious journals, and two have been honored as \u0026rsquo;editor\u0026rsquo;s picks\u0026rsquo;.\nBeyond Work In my leisure time, I cherish moments with my family, tinker on the piano with enthusiasm (regardless of the results!), embrace the serenity of the sea on my stand-up paddle board, and continue my passion for developing R packages.\nConnect with Me I\u0026rsquo;m always eager to connect with fellow data enthusiasts, researchers, and professionals. Feel free to connect with me on LinkedIn to explore potential collaborations and discussions within our shared field.\n","date":1642636800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1642636800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://blasbenito.com/author/blas-m.-benito/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/blas-m.-benito/","section":"authors","summary":"Hello there!\nMy name is Blas, and I am a spatial data scientist and engineer in AgTech, holding a PhD in computational ecology and an MSc in geographic information systems.","tags":null,"title":"Blas M. Benito","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\nOnline courses Project or software documentation Tutorials The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://blasbenito.com/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://blasbenito.com/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://blasbenito.com/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://blasbenito.com/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Joaquín Moreno","Sergio Asensio","Miguel Berdugo","Beatriz Gozalo","Victoria Ochoa","David S. Pescador","Blas M. Benito","Fernando T. Maestre"],"categories":null,"content":"","date":1642636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642636800,"objectID":"2ba9c1e53b4467221a4e54e5594ecfd8","permalink":"https://blasbenito.com/publication/2021_moreno_scientific_data/","publishdate":"2022-01-20T00:00:00Z","relpermalink":"/publication/2021_moreno_scientific_data/","section":"publication","summary":"Drylands cover ~41% of the terrestrial surface. In these water-limited ecosystems, soil moisture contributes to multiple hydrological processes and is a crucial determinant of the activity and performance of above- and belowground organisms and of the ecosystem processes that rely on them. Thus, an accurate characterisation of the temporal dynamics of soil moisture is critical to improve our understanding of how dryland ecosystems function and are responding to ongoing climate change. Furthermore, it may help improve climatic forecasts and drought monitoring. Here we present the MOISCRUST dataset, a long-term (2006–2020) soil moisture dataset at a sub-daily resolution from five different microsites (vascular plants and biocrusts) in a Mediterranean semiarid dryland located in Central Spain. MOISCRUST is a unique dataset for improving our understanding on how both vascular plants and biocrusts determine soil water dynamics in drylands, and thus to better assess their hydrological impacts and responses to ongoing climate change.","tags":["Environmental Data","Soil Ecology","Soil Sensors","Time Series","Plant Ecology"],"title":" Fourteen years of continuous soil moisture records from plant and biocrust-dominated microsites.","type":"publication"},{"authors":["Fernando T. Maestre","Blas M. Benito","Miguel Berdugo","Laura Concostrina-Zubiri","Manuel Delgado-Baquerizo","David J. Eldridge","Emilio Guirado","Nicolas Gross","Sonia Kéfi","Yoann Le Bagousse-Pinguet","Raúl Ochoa-Hueso","Santiago Soliveres"],"categories":null,"content":"","date":1618617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618617600,"objectID":"4f98b73b5572de5d985bf1ec380a2c19","permalink":"https://blasbenito.com/publication/2021_maestre_new_phytologist/","publishdate":"2021-04-17T00:00:00Z","relpermalink":"/publication/2021_maestre_new_phytologist/","section":"publication","summary":"Here we synthesize the biogeography of key organisms (vascular and non‐vascular vegetation and soil microorganisms), attributes (functional traits, spatial patterns, plant‐plant and plant‐soil interactions) and processes (productivity and land cover) across global drylands. We finish our review discussing major research gaps, which include: i) studying regular vegetation spatial patterns, ii) establishing large‐scale plant and biocrust field surveys assessing individual‐level trait measurements, iii) knowing whether plant‐plant and plant‐soil interactions impacts on biodiversity are predictable and iv) assessing how elevated CO2 modulates future aridity conditions and plant productivity.","tags":["Biogeography","Plant Ecology","Dryland ecology"],"title":"Biogeography of global drylands","type":"publication"},{"authors":null,"categories":null,"content":" This is a tutorial written for R users needing to compute betadiversity indices from species lists rather than from presence-absence matrices, and for R beginners or intermediate users that want to start using their own functions. If you are an advanced R user, this post will likely waste your time. We ecologists like to measure all things in nature, and compositional changes in biological communities over time or space, a.k.a betadiversity, is one of these things. I am not going to explain what betadiversity is because others that know better than me have done it already. Good examples are this post published the blog of Methods in Ecology and Evolution by Andres Baselga, and this lecture by Tim Seipel.\nWhat I am actually going to do in this post is to explain how to write functions to compute betadiversity indices in R from species lists rather than from presence-absence matrices. For the latter there are a few packages such as vegan, BAT, MBI, or betapart, but for the former I was unable to find anything suitable. To make this post useful for R beginners, I will go step by step on the rationale behind the design of the functions to compute betadiversity indices, and by the end of the post I will explain how to organize them to achieve a clean R workflow.\nLet’s go!\nBetadiversity indices There are a few betadiversity indices out there, and I totally recommend you to start with Koleff et al. (2003) as a primer. They review the literature and analyze the properties of 24 different indices to provide guidance on how to use them.\nBetadiversity components a, b, and c Betadiversity indices are designed to compare the taxa pools of two sites at a time, and require the computation of three components:\na: number of common taxa of both sites. b: number of exclusive taxa of one site. c: number of exclusive taxa of the other site. Let’s see how can we use these diversity components to compute betadiversity indices.\nSørensen’s Beta Let’s start with the Sørensen’s Beta (\\(\\beta_{sor}\\) hereafter), as presented in Koleff et al. (2003).\n\\[\\beta_{sor} = \\frac{2a}{2a + b + c}\\]\n\\(\\beta_{sor}\\) is a similarity index in the range [0, 1] (the closer to one, the more similar the taxa pools of both sites are) that puts a lot of weight in the \\(a\\) component, and is therefore a measure of continuity, as it focuses the most in the common taxa among sites.\nSimpson’s Beta Another popular betadiversity index is the Simpson’s Beta (\\(\\beta_{sim}\\) hereafter).\n\\[\\beta_{sim} = \\frac{min(b, c)}{min(b, c) + a}\\] where \\(min()\\) is a function that takes the minimum value among the diversity components within the parenthesis. \\(\\beta_{sim}\\) is a dissimilarity measure that focuses on compositional turnover among sites because it focuses the most on the values of \\(b\\) and \\(c\\). It has its lower bound in zero, and an open upper value.\nTo bring these ideas into R, first we have to load a few R packages, and generate some fake data to help us develop the functions.\nlibrary(magrittr) library(foreach) library(doParallel) ## Loading required package: iterators ## Loading required package: parallel The code chunk below generates 15 fake taxa names, from taxon_1 to taxon_15.\ntaxa \u0026lt;- paste0(\u0026quot;taxon_\u0026quot;, 1:15) With these fake taxa we are going to generate taxa lists for four hypothetical sites named site1, site2, site3, and site4. Two of the sites will have identical taxa lists, two will have non-overlapping taxa lists, and two of them will have some overlap.\nsite1 \u0026lt;- site2 \u0026lt;- taxa[1:7] site3 \u0026lt;- taxa[8:12] site4 \u0026lt;- taxa[10:15] So now we have these taxa lists:\nsite1 #and site2 ## [1] \u0026quot;taxon_1\u0026quot; \u0026quot;taxon_2\u0026quot; \u0026quot;taxon_3\u0026quot; \u0026quot;taxon_4\u0026quot; \u0026quot;taxon_5\u0026quot; \u0026quot;taxon_6\u0026quot; \u0026quot;taxon_7\u0026quot; site3 ## [1] \u0026quot;taxon_8\u0026quot; \u0026quot;taxon_9\u0026quot; \u0026quot;taxon_10\u0026quot; \u0026quot;taxon_11\u0026quot; \u0026quot;taxon_12\u0026quot; site4 ## [1] \u0026quot;taxon_10\u0026quot; \u0026quot;taxon_11\u0026quot; \u0026quot;taxon_12\u0026quot; \u0026quot;taxon_13\u0026quot; \u0026quot;taxon_14\u0026quot; \u0026quot;taxon_15\u0026quot; Step-by-step computation of betadiversity indices with R For a given pair of sites, how can we compute the diversity components a, b, and c?\nLooking at it from an R perspective, each site is a character vector, so a can be found by counting the number of common elements between two vectors. These common elements can be found with the function intersect(), and the number of elements can be computed by applying length() on the result of intersect().\na \u0026lt;- length(intersect(site3, site4)) a ## [1] 3 To compute b and c we can use the function setdiff(), that finds the exclusive elements of one character vector when comparing it with another. In this case, b is computed for the first vector introduced in the function, site3 in this case…\nb \u0026lt;- length(setdiff(site3, site4)) b ## [1] 2 … so to compute the c component we only need to switch the sites.\nc \u0026lt;- length(setdiff(site4, site3)) c ## [1] 3 Now that we know a, b, and c, we can compute \\(\\beta_{sor}\\) and \\(\\beta_{sim}\\).\nBsor \u0026lt;- 2 * a / (2 * a + b + c) Bsor ## [1] 0.5454545 Bsim\u0026lt;- min(b, c) / (min(b, c) + a) Bsim ## [1] 0.4 Of course, if we have a long list of sites, computing betadiversity indices like this can get quite boring quite fast. Let’s put everything in a set of functions to make it easier to work with.\nWriting functions to compute betadiversity indices The basic structure of a function definition in R looks as follows:\nfunction_name \u0026lt;- function(x, y, ...){ output \u0026lt;- [body] output #also return(output) } Where:\nfunction_name is the name of your function. Ideally, a verb, or otherwise, something indicating somehow what the function will do with the input data and arguments. function() is a function to define functions, there isn’t much more to it… x is the first argument of the function, and ideally, represents the input data. If that is the case, you can later use pipes (%\u0026gt;%) to chain functions together. y (it could have any other name) is another function argument, an can be either another input dataset, or an argument defining how the function has to behave. ... refers to other arguments the function may require. body is the code that operates with the data and function arguments. This can be one line of code, or a thousand, it all comes down to the function’s objective. In any case, the body must return an object (or an error if something went wrong) that will be the function’s output. output is the object ultimately produced by the function. It can have any name, and can be any kind of structure, such a number, a vector, a data frame, a list, etc. R functions return one output object only. Since R functions return the last evaluated value, it is good practice to put the output object at the end of the function as an explicit way to state what the actual output of the function is. Let’s start writing a function to compute a, b, and c from a pair of sites.\n#x: taxa list of one site #y: taxa list of another site abc \u0026lt;- function(x, y){ #list to store output out \u0026lt;- list() #filling the list out$a \u0026lt;- length(intersect(x, y)) out$b \u0026lt;- length(setdiff(x, y)) out$c \u0026lt;- length(setdiff(y, x)) #returning the output out } Notice that to to return the three values I am wrapping them in a list. Let’s run a little test.\nx \u0026lt;- abc( x = site3, y = site4 ) x ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 So far so good! From here we build the functions sorensen_beta() and simpson_beta() making sure they can accept the output of abc(), and return it with an added slot.\nsorensen_beta \u0026lt;- function(x){ x$bsor \u0026lt;- round(2 * x$a / (2 * x$a + x$b + x$c), 3) x } simpson_beta \u0026lt;- function(x){ x$bsim \u0026lt;- round(min(x$b, x$c) / (min(x$b, x$c) + x$a), 3) x } Notice that both functions are returning the input x with an added slot named after the given betadiversity index. Let’s test them first, to later see why returning the input object gives these functions a lot of flexibility.\nsorensen_beta(x) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 ## ## $bsor ## [1] 0.545 simpson_beta(x) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 ## ## $bsim ## [1] 0.4 When I said that returning the input object with an added slot gave these functions a lot of flexibility I was talking about this:\nx \u0026lt;- abc( x = site3, y = site4 ) %\u0026gt;% sorensen_beta() %\u0026gt;% simpson_beta() x ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 ## ## $bsor ## [1] 0.545 ## ## $bsim ## [1] 0.4 Chaining the functions through the %\u0026gt;% pipe of the magrittr package now allows us to combine their results in a single output no matter whether we use sorensen_beta() or sorensen_beta() first, or whether we omit one of them. The only thing the pipe is doing here is moving the output of the first function into the next. There are a couple of very nice tutorials about the magrittr package and the %\u0026gt;% here and here.\nWe can put that idea right away into a function to compute both betadiversity indices at once from the taxa list of a pair of sites.\nbetadiversity \u0026lt;- function(x, y){ require(magrittr) abc(x, y) %\u0026gt;% sorensen_beta() %\u0026gt;% simpson_beta() } The function now works as follows.\nx \u0026lt;- betadiversity( x = site3, y = site4 ) x ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 3 ## ## $bsor ## [1] 0.545 ## ## $bsim ## [1] 0.4 So far we have four functions…\nabc() simpson_beta(), that requires abc(). sorensen_beta(), that requires abc(). betadiversity(), that requires abc(), simpson_beta(), and sorensen_beta(). … and one limitation: so far we can only return betadiversity indices for two sites at a time. So at the moment, to compute betadiversity indices for all combinations of sites we have to do a pretty ridiculous thing:\nx1 \u0026lt;- betadiversity(x = site1, y = site2) x2 \u0026lt;- betadiversity(x = site1, y = site3) x3 \u0026lt;- betadiversity(x = site1, y = site4) #... and so on If I see you doing this I’ll come to haunt you in your nightmares! Since a real analysis may involve hundreds of sites, the next step is to use the functions above to build a new one able to intake an arbitrary number of sites.\nWriting a function to compute betadiversity indices for an arbitrary number of sites. First we have to organize our sites in a data frame with a long format.\nsites \u0026lt;- data.frame( site = c( rep(\u0026quot;site1\u0026quot;, length(site1)), rep(\u0026quot;site2\u0026quot;, length(site2)), rep(\u0026quot;site3\u0026quot;, length(site3)), rep(\u0026quot;site4\u0026quot;, length(site4)) ), taxon = c( site1, site2, site3, site4 ) ) site taxon site1 taxon_1 site1 taxon_2 site1 taxon_3 site1 taxon_4 site1 taxon_5 site1 taxon_6 site1 taxon_7 site2 taxon_1 site2 taxon_2 site2 taxon_3 site2 taxon_4 site2 taxon_5 site2 taxon_6 site2 taxon_7 site3 taxon_8 site3 taxon_9 site3 taxon_10 site3 taxon_11 site3 taxon_12 site4 taxon_10 site4 taxon_11 site4 taxon_12 site4 taxon_13 site4 taxon_14 site4 taxon_15 Our new function will need to do several things:\nGenerate combinations of the unique values of the column site two by two without repetition. Iterate through these combinations of two sites to compute betadiversity components and indices. Return a dataframe with the results to facilitate further analyses. The combinations of site pairs are done with utils::combn() as follows:\nsite.combinations \u0026lt;- utils::combn( x = unique(sites$site), m = 2 ) site.combinations ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] \u0026quot;site1\u0026quot; \u0026quot;site1\u0026quot; \u0026quot;site1\u0026quot; \u0026quot;site2\u0026quot; \u0026quot;site2\u0026quot; \u0026quot;site3\u0026quot; ## [2,] \u0026quot;site2\u0026quot; \u0026quot;site3\u0026quot; \u0026quot;site4\u0026quot; \u0026quot;site3\u0026quot; \u0026quot;site4\u0026quot; \u0026quot;site4\u0026quot; The result is a matrix, and each pair of rows in a column contain a pair of sites. The idea now is to iterate over the matrix columns, obtain the set of taxa from each site from the taxon column of the sites data frame, and use these taxa lists to compute the betadiversity components and indices.\nTo easily generate the output data frame, I use the foreach::foreach() function to iterate through pairs instead of a more traditional for loop. You can read more about foreach() in a previous post.\nbetadiversity.df \u0026lt;- foreach::foreach( i = 1:ncol(site.combinations), #iterates through columns of site.combinations .combine = \u0026#39;rbind\u0026#39; #to produce a data frame ) %do% { #site names site.one \u0026lt;- site.combinations[1, i] #from column i, row 1 site.two \u0026lt;- site.combinations[2, i] #from column i, row 2 #getting taxa lists taxa.list.one \u0026lt;- sites[sites$site %in% site.one, \u0026quot;taxon\u0026quot;] taxa.list.two \u0026lt;- sites[sites$site %in% site.two, \u0026quot;taxon\u0026quot;] #betadiversity beta \u0026lt;- betadiversity( x = taxa.list.one, y = taxa.list.two ) #adding site names beta$site.one \u0026lt;- site.one beta$site.two \u0026lt;- site.two #returning output beta } a b c bsor bsim site.one site.two 7 0 0 1 0 site1 site2 0 7 5 0 1 site1 site3 0 7 6 0 1 site1 site4 0 7 5 0 1 site2 site3 0 7 6 0 1 site2 site4 3 2 3 0.545 0.4 site3 site4 Now that we know it works, we can put everything together in a function. Notice that to make the function more general, I have added arguments requesting the names of the columns with the site and the taxa names.\nbetadiversity_multisite \u0026lt;- function( x, site.column, #column with site names taxa.column #column with taxa names ){ #get site combinations site.combinations \u0026lt;- utils::combn( x = unique(x[, site.column]), m = 2 ) #iterating through site pairs betadiversity.df \u0026lt;- foreach::foreach( i = 1:ncol(site.combinations), .combine = \u0026#39;rbind\u0026#39; ) %do% { #site names site.one \u0026lt;- site.combinations[1, i] site.two \u0026lt;- site.combinations[2, i] #getting taxa lists taxa.list.one \u0026lt;- x[x[, site.column] %in% site.one, taxa.column] taxa.list.two \u0026lt;- x[x[, site.column] %in% site.two, taxa.column] #betadiversity beta \u0026lt;- betadiversity( x = taxa.list.one, y = taxa.list.two ) #adding site names beta$site.one \u0026lt;- site.one beta$site.two \u0026lt;- site.two #returning output beta } #remove bad rownames rownames(betadiversity.df) \u0026lt;- NULL #reordering columns betadiversity.df \u0026lt;- betadiversity.df[, c( \u0026quot;site.one\u0026quot;, \u0026quot;site.two\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;bsor\u0026quot;, \u0026quot;bsim\u0026quot; )] #returning output return(betadiversity.df) } And the test!\nsites.betadiversity \u0026lt;- betadiversity_multisite( x = sites, site.column = \u0026quot;site\u0026quot;, taxa.column = \u0026quot;taxon\u0026quot; ) site.one site.two a b c bsor bsim site1 site2 7 0 0 1 0 site1 site3 0 7 5 0 1 site1 site4 0 7 6 0 1 site2 site3 0 7 5 0 1 site2 site4 0 7 6 0 1 site3 site4 3 2 3 0.545 0.4 That went well!\nFinally, to have these functions available in my R session I always put them all in a single file in the same folder where my Rstudio project lives, name it something like functions_betadiversity.R, and source it at the beginning of my script or .Rmd file by running a line like the one below.\nsource(\u0026quot;functions_betadiversity.R\u0026quot;) I have placed the file functions_betadiversity.R in this GitHub Gist in case you want to give it a look. You can also source it right away to your R environment by executing the following line:\nsource(\u0026quot;https://gist.githubusercontent.com/BlasBenito/4c3740b056a0c9bb3602f33dfd35990c/raw/bbb40d868787fc5d10e391a2121045eb5d75f165/functions_betadiversity.R\u0026quot;) I hope this post helped you to better understand how to write and organize R functions!\n","date":1609891200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609891200,"objectID":"09127f8961ebfb017dfd29461698bb7c","permalink":"https://blasbenito.com/post/04_betadiversity/","publishdate":"2021-01-06T00:00:00Z","relpermalink":"/post/04_betadiversity/","section":"post","summary":"This is a tutorial written for R users needing to compute betadiversity indices from species lists rather than from presence-absence matrices, and for R beginners or intermediate users that want to start using their own functions.","tags":null,"title":"Designing R functions to compute betadiversity indices from species lists","type":"post"},{"authors":null,"categories":null,"content":" In the previous posts I have covered how to setup a home cluster, and how to run parallel processes with foreach in R. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.\nThis post (developed on Ubuntu and Ubuntu Server 20.04) assumes that the home cluster runs on Linux, and covers the following topics.\nBasics of the Network File System protocol (NFS). Setup of an NFS folder in a home cluster. Using an NFS folder in a parallelized loop. The Network File System protocol (NFS) The Network File System protocol offers the means for a host computer to allow other computers in the network (clients) to read and write the contents of a given folder. The clients access such folder like if it was a local one, while in fact it is a reference to the one in the host computer.\nThe image at the beginning of the post illustrates the concept. There is a host computer with a folder in the path /home/user/cluster_shared (were user is your user name) that is broadcasted to the network, and there are one or several clients that are mounting mounting (making accessible) the same folder in their local paths /home/user/cluster_shared.\nIf the host writes a file to the shared folder, it is available right away for the clients, and the other way around. At the end, the idea is to have a folder shared among all computers in the cluster, while having the same exact path on each one of them to write or read files from such shared folder.\nSetup of an NFS folder in a home cluster To setup the shared folder we’ll need to do some things in the host, and some things in the clients. Let’s start with the host.\nPreparing the host computer First we need to install the nfs-kernel-server.\nsudo apt update sudo apt install nfs-kernel-server Now we can create the shared folder. Remember to replace user with your user name, and cluster_shared with the actual folder name you want to use.\nmkdir /home/user/cluster_shared To broadcast it we need to open the file /etc/exports…\nsudo gedit /etc/exports … and add the following line\n/home/user/cluster_shared IP_CLIENT1(rw,no_subtree_check) IP_CLIENT2(rw,no_subtree_check) IP_CLIENT3(rw,no_subtree_check) where:\n/home/user/cluster_shared is the path of the shared folder. IP_CLIENTx are the IPs of each one of the clients. rw gives reading and writing permission on the shared folder to the given client. no_subtree_check prevents the host from checking the complete tree of shares before attending a request (read or write) by a client. For example, the last line of my /etc/exports file looks like this:\n/home/blas/cluster_shared 10.42.0.34(rw,async,no_subtree_check) 10.42.0.104(rw,async,no_subtree_check) Save the file, and to make the changes effective, execute:\nsudo exportfs -ra To end preparing the host we have to update the firewall rules to allow nfs connections from the clients. Notice that one rule per client needs to be defined, using the clients IPs to identify them.\nsudo ufw allow from IP_CLIENT1 to any port nfs sudo ufw allow from IP_CLIENT2 to any port nfs sudo ufw status Preparing the clients First we have to install the Linux package nfs-common on each client.\nsudo apt update sudp apt install nfs-common Now we can create a folder in the clients and use it to mount the NFS folder of the host.\nmkdir -p /home/user/cluster_shared sudo mount IP_HOST:/home/user/cluster_shared /home/user/cluster_shared The second line of code is mounting the folder /home/user/cluster_shared of the host in the folder /home/user/cluster_shared of the client.\nTo make the mount permanent, we have to open /etc/fstab with super-user privilege in the clients…\nsudo gedit /etc/fstab … and add the line\nIP_HOST:/home/user/cluster_shared /home/user/cluster_shared nfs defaults 0 0 Remember to replace IP_HOST and user with the right values!\nNow we can test that the shared folder works as intended by writing one file into it from each computer in the network using the code below in the command line.\ncd cluster_shared touch filename.txt Once the files are created, we can check they are visible from each computer using the ls command.\nls Using an NFS folder in a parallelized loop In a previous post I described how to run parallelized tasks with foreach in R. This section intends to complete that post by showing how a shared NFS folder can be used to store input and output files during the execution of a parallelized loop\nThe task In this hypothetical example we have a large number of data frames stored in /home/user/cluster_shared/input. Each data frame has the same predictors a, b, c, and d, and a different response variable, named y1 for the data frame y1, y2 for the data frame y2, and so on. In the example we will be using 100 data frames, but there could be thousands. This example would be valid as well for any other kind of object, such as raster files, time-series, etc etc.\nFirst we have to load the libraries we’ll be using.\n#automatic install of packages if they are not installed already list.of.packages \u0026lt;- c( \u0026quot;foreach\u0026quot;, \u0026quot;doParallel\u0026quot;, \u0026quot;ranger\u0026quot; ) new.packages \u0026lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\u0026quot;Package\u0026quot;])] if(length(new.packages) \u0026gt; 0){ install.packages(new.packages, dep=TRUE) } #loading packages for(package.i in list.of.packages){ suppressPackageStartupMessages( library( package.i, character.only = TRUE ) ) } The code chunk below generates the folder /home/user/cluster_shared/input and populates it with the dummy files.\n#creating the input folder input.folder \u0026lt;- \u0026quot;/home/blas/cluster_shared/input\u0026quot; dir.create(input.folder) #data frame names df.names \u0026lt;- paste0(\u0026quot;y\u0026quot;, 1:100) #filling it with files for(i in df.names){ #creating the df df.i \u0026lt;- data.frame( y = rnorm(1000), a = rnorm(1000), b = rnorm(1000), c = rnorm(1000), d = rnorm(1000) ) #changing name of the response variable colnames(df.i)[1] \u0026lt;- i #assign to a variable with name i assign(i, df.i) #saving the object save( list = i, file = paste0(input.folder, \u0026quot;/\u0026quot;, i, \u0026quot;.RData\u0026quot;) ) #removing the generated data frame form the environment rm(list = i, df.i, i) } Our target now will be to fit one ranger::ranger() model per data frame stored in /home/blas/cluster_shared/input, save the model result to a folder with the path /home/blas/cluster_shared/input, and write a small summary of the model to the output of foreach.\nSuch target is based on this rationale: When executing a foreach loop as in x \u0026lt;- foreach(...) %dopar% {...}, the variable x is going to grow in memory very fast, competing for RAM resources with the worker nodes. Furthermore, since x is being written on the fly, the results would be lost if the computer crashes. When the size of the input and the output of our parallelized operation is larger than memory, we can use an NFS folder to store inputs and outputs, while keeping the RAM memory free for computational tasks only, with the positive side effect of having our outputs already stored should our computer decide to crash.\nAlso, please notice that here I am focusing in a cluster setting, but using a folder to read and write data during a loop paralellized with foreach can indeed be done in a single computer without an NFS folder. Any folder in your system will do the trick as well!\nSo, from here, we are going to prepare the cluster, and execute a parallelized loop fitting one model per data frame that reads the inputs and writes the outputs to the shared folder.\nCluster setup We will also need the function I showed in the previous post to generate the cluster specification from a GitHub Gist.\nsource(\u0026quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R\u0026quot;) Below I use the function to create a cluster specification and initiate the cluster with parallel::makeCluster().\n#generate cluster specification spec \u0026lt;- cluster_spec( ips = c(\u0026#39;10.42.0.1\u0026#39;, \u0026#39;10.42.0.34\u0026#39;, \u0026#39;10.42.0.104\u0026#39;), cores = c(7, 4, 4), user = \u0026quot;blas\u0026quot; ) #define parallel port Sys.setenv(R_PARALLEL_PORT = 11000) Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;) #setting up cluster my.cluster \u0026lt;- parallel::makeCluster( master = \u0026#39;10.42.0.1\u0026#39;, spec = spec, port = Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;), outfile = \u0026quot;\u0026quot;, homogeneous = TRUE ) #check cluster definition (optional) print(my.cluster) #register cluster doParallel::registerDoParallel(cl = my.cluster) #check number of workers foreach::getDoParWorkers() Parallelized loop For everything to work as intended, we first need to create the output folder.\noutput.folder \u0026lt;- \u0026quot;/home/blas/cluster_shared/output\u0026quot; dir.create(output.folder) And now we are ready to execute the parallelized loop. Notice that I am using the output of list.files() to have a vector of file names the loop will iterate over. The steps inside of the loop go as follows:\n1. Remove the extension .RData from the file name. We’ll later use the result to use assign() on the fitted model to change its name to the same as the input file before saving it. 2. Read the input data frame and store in an object named df. 3. Fit the model with ranger, using the first column of df as respose variable. 4. Change the model name to the name of the input file without extension, resulting from the first step described above. 5. Save the model into the output folder with the extension .RData. 6. Return a data frame with one line with the name of the response variable, the r-squared, and the importance of each predictor. #list of input files as iterator input.files \u0026lt;- list.files( path = input.folder, full.names = FALSE ) modelling.summary \u0026lt;- foreach( input.file = input.files, .combine = \u0026#39;rbind\u0026#39;, .packages = \u0026quot;ranger\u0026quot; ) %dopar% { # 1. input file name without extension input.file.name \u0026lt;- tools::file_path_sans_ext(input.file) # 2. read input file df \u0026lt;- get(load(paste0(input.folder, \u0026quot;/\u0026quot;, input.file))) # 3. fit model m.i \u0026lt;- ranger::ranger( data = df, dependent.variable.name = colnames(df)[1], importance = \u0026quot;permutation\u0026quot; ) # 4. change name of the model to one of the response variable assign(input.file.name, m.i) # 5. save model save( list = input.file.name, file = paste0(output.folder, \u0026quot;/\u0026quot;, input.file) ) # 6. returning summary return( data.frame( response.variable = input.file.name, r.squared = m.i$r.squared, importance.a = m.i$variable.importance[\u0026quot;a\u0026quot;], importance.b = m.i$variable.importance[\u0026quot;b\u0026quot;], importance.c = m.i$variable.importance[\u0026quot;c\u0026quot;], importance.d = m.i$variable.importance[\u0026quot;d\u0026quot;] ) ) } Once this parallelized loop is executed, the folder /home/blas/cluster_shared/output should be filled with the results from the cluster workers, and the modelling.summary data frame contains the summary of each fitted model.\nNow that the work is done, we can stop the cluster.\nparallel::stopCluster(cl = my.cluster) Now you know how to work with data larger than memory in a parallelized loop!\n","date":1609632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609632000,"objectID":"bf28eeedb27403ed52da5fe9b61d9449","permalink":"https://blasbenito.com/post/03_shared_folder_in_cluster/","publishdate":"2021-01-03T00:00:00Z","relpermalink":"/post/03_shared_folder_in_cluster/","section":"post","summary":"In the previous posts I have covered how to setup a home cluster, and how to run parallel processes with foreach in R. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.","tags":null,"title":"Setup of a shared folder in a home cluster","type":"post"},{"authors":null,"categories":null,"content":" In the previous posts I have covered how to setup a home cluster, and how to run parallel processes with foreach in R. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.\nThis post (developed on Ubuntu and Ubuntu Server 20.04) assumes that the home cluster runs on Linux, and covers the following topics.\nBasics of the Network File System protocol (NFS). Setup of an NFS folder in a home cluster. Using an NFS folder in a parallelized loop. The Network File System protocol (NFS) The Network File System protocol offers the means for a host computer to allow other computers in the network (clients) to read and write the contents of a given folder. The clients access such folder like if it was a local one, while in fact it is a reference to the one in the host computer.\nThe image at the beginning of the post illustrates the concept. There is a host computer with a folder in the path /home/user/cluster_shared (were user is your user name) that is broadcasted to the network, and there are one or several clients that are mounting mounting (making accessible) the same folder in their local paths /home/user/cluster_shared.\nIf the host writes a file to the shared folder, it is available right away for the clients, and the other way around. At the end, the idea is to have a folder shared among all computers in the cluster, while having the same exact path on each one of them to write or read files from such shared folder.\nSetup of an NFS folder in a home cluster To setup the shared folder we’ll need to do some things in the host, and some things in the clients. Let’s start with the host.\nPreparing the host computer First we need to install the nfs-kernel-server.\nsudo apt update sudo apt install nfs-kernel-server Now we can create the shared folder. Remember to replace user with your user name, and cluster_shared with the actual folder name you want to use.\nmkdir /home/user/cluster_shared To broadcast it we need to open the file /etc/exports…\nsudo gedit /etc/exports … and add the following line\n/home/user/cluster_shared IP_CLIENT1(rw,no_subtree_check) IP_CLIENT2(rw,no_subtree_check) IP_CLIENT3(rw,no_subtree_check) where:\n/home/user/cluster_shared is the path of the shared folder. IP_CLIENTx are the IPs of each one of the clients. rw gives reading and writing permission on the shared folder to the given client. no_subtree_check prevents the host from checking the complete tree of shares before attending a request (read or write) by a client. For example, the last line of my /etc/exports file looks like this:\n/home/blas/cluster_shared 10.42.0.34(rw,async,no_subtree_check) 10.42.0.104(rw,async,no_subtree_check) Save the file, and to make the changes effective, execute:\nsudo exportfs -ra To end preparing the host we have to update the firewall rules to allow nfs connections from the clients. Notice that one rule per client needs to be defined, using the clients IPs to identify them.\nsudo ufw allow from IP_CLIENT1 to any port nfs sudo ufw allow from IP_CLIENT2 to any port nfs sudo ufw status Preparing the clients First we have to install the Linux package nfs-common on each client.\nsudo apt update sudp apt install nfs-common Now we can create a folder in the clients and use it to mount the NFS folder of the host.\nmkdir -p /home/user/cluster_shared sudo mount IP_HOST:/home/user/cluster_shared /home/user/cluster_shared The second line of code is mounting the folder /home/user/cluster_shared of the host in the folder /home/user/cluster_shared of the client.\nTo make the mount permanent, we have to open /etc/fstab with super-user privilege in the clients…\nsudo gedit /etc/fstab … and add the line\nIP_HOST:/home/user/cluster_shared /home/user/cluster_shared nfs defaults 0 0 Remember to replace IP_HOST and user with the right values!\nNow we can test that the shared folder works as intended by writing one file into it from each computer in the network using the code below in the command line.\ncd cluster_shared touch filename.txt Once the files are created, we can check they are visible from each computer using the ls command.\nls Using an NFS folder in a parallelized loop In a previous post I described how to run parallelized tasks with foreach in R. This section intends to complete that post by showing how a shared NFS folder can be used to store input and output files during the execution of a parallelized loop\nThe task In this hypothetical example we have a large number of data frames stored in /home/user/cluster_shared/input. Each data frame has the same predictors a, b, c, and d, and a different response variable, named y1 for the data frame y1, y2 for the data frame y2, and so on. In the example we will be using 100 data frames, but there could be thousands. This example would be valid as well for any other kind of object, such as raster files, time-series, etc etc.\nFirst we have to load the libraries we’ll be using.\n#automatic install of packages if they are not installed already list.of.packages \u0026lt;- c( \u0026quot;foreach\u0026quot;, \u0026quot;doParallel\u0026quot;, \u0026quot;ranger\u0026quot; ) new.packages \u0026lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\u0026quot;Package\u0026quot;])] if(length(new.packages) \u0026gt; 0){ install.packages(new.packages, dep=TRUE) } #loading packages for(package.i in list.of.packages){ suppressPackageStartupMessages( library( package.i, character.only = TRUE ) ) } The code chunk below generates the folder /home/user/cluster_shared/input and populates it with the dummy files.\n#creating the input folder input.folder \u0026lt;- \u0026quot;/home/blas/cluster_shared/input\u0026quot; dir.create(input.folder) #data frame names df.names \u0026lt;- paste0(\u0026quot;y\u0026quot;, 1:100) #filling it with files for(i in df.names){ #creating the df df.i \u0026lt;- data.frame( y = rnorm(1000), a = rnorm(1000), b = rnorm(1000), c = rnorm(1000), d = rnorm(1000) ) #changing name of the response variable colnames(df.i)[1] \u0026lt;- i #assign to a variable with name i assign(i, df.i) #saving the object save( list = i, file = paste0(input.folder, \u0026quot;/\u0026quot;, i, \u0026quot;.RData\u0026quot;) ) #removing the generated data frame form the environment rm(list = i, df.i, i) } Our target now will be to fit one ranger::ranger() model per data frame stored in /home/blas/cluster_shared/input, save the model result to a folder with the path /home/blas/cluster_shared/input, and write a small summary of the model to the output of foreach.\nSuch target is based on this rationale: When executing a foreach loop as in x \u0026lt;- foreach(...) %dopar% {...}, the variable x is going to grow in memory very fast, competing for RAM resources with the worker nodes. Furthermore, since x is being written on the fly, the results would be lost if the computer crashes. When the size of the input and the output of our parallelized operation is larger than memory, we can use an NFS folder to store inputs and outputs, while keeping the RAM memory free for computational tasks only, with the positive side effect of having our outputs already stored should our computer decide to crash.\nAlso, please notice that here I am focusing in a cluster setting, but using a folder to read and write data during a loop paralellized with foreach can indeed be done in a single computer without an NFS folder. Any folder in your system will do the trick as well!\nSo, from here, we are going to prepare the cluster, and execute a parallelized loop fitting one model per data frame that reads the inputs and writes the outputs to the shared folder.\nCluster setup We will also need the function I showed in the previous post to generate the cluster specification from a GitHub Gist.\nsource(\u0026quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R\u0026quot;) Below I use the function to create a cluster specification and initiate the cluster with parallel::makeCluster().\n#generate cluster specification spec \u0026lt;- cluster_spec( ips = c(\u0026#39;10.42.0.1\u0026#39;, \u0026#39;10.42.0.34\u0026#39;, \u0026#39;10.42.0.104\u0026#39;), cores = c(7, 4, 4), user = \u0026quot;blas\u0026quot; ) #define parallel port Sys.setenv(R_PARALLEL_PORT = 11000) Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;) #setting up cluster my.cluster \u0026lt;- parallel::makeCluster( master = \u0026#39;10.42.0.1\u0026#39;, spec = spec, port = Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;), outfile = \u0026quot;\u0026quot;, homogeneous = TRUE ) #check cluster definition (optional) print(my.cluster) #register cluster doParallel::registerDoParallel(cl = my.cluster) #check number of workers foreach::getDoParWorkers() Parallelized loop For everything to work as intended, we first need to create the output folder.\noutput.folder \u0026lt;- \u0026quot;/home/blas/cluster_shared/output\u0026quot; dir.create(output.folder) And now we are ready to execute the parallelized loop. Notice that I am using the output of list.files() to have a vector of file names the loop will iterate over. The steps inside of the loop go as follows:\n1. Remove the extension .RData from the file name. We’ll later use the result to use assign() on the fitted model to change its name to the same as the input file before saving it. 2. Read the input data frame and store in an object named df. 3. Fit the model with ranger, using the first column of df as respose variable. 4. Change the model name to the name of the input file without extension, resulting from the first step described above. 5. Save the model into the output folder with the extension .RData. 6. Return a data frame with one line with the name of the response variable, the r-squared, and the importance of each predictor. #list of input files as iterator input.files \u0026lt;- list.files( path = input.folder, full.names = FALSE ) modelling.summary \u0026lt;- foreach( input.file = input.files, .combine = \u0026#39;rbind\u0026#39;, .packages = \u0026quot;ranger\u0026quot; ) %dopar% { # 1. input file name without extension input.file.name \u0026lt;- tools::file_path_sans_ext(input.file) # 2. read input file df \u0026lt;- get(load(paste0(input.folder, \u0026quot;/\u0026quot;, input.file))) # 3. fit model m.i \u0026lt;- ranger::ranger( data = df, dependent.variable.name = colnames(df)[1], importance = \u0026quot;permutation\u0026quot; ) # 4. change name of the model to one of the response variable assign(input.file.name, m.i) # 5. save model save( list = input.file.name, file = paste0(output.folder, \u0026quot;/\u0026quot;, input.file) ) # 6. returning summary return( data.frame( response.variable = input.file.name, r.squared = m.i$r.squared, importance.a = m.i$variable.importance[\u0026quot;a\u0026quot;], importance.b = m.i$variable.importance[\u0026quot;b\u0026quot;], importance.c = m.i$variable.importance[\u0026quot;c\u0026quot;], importance.d = m.i$variable.importance[\u0026quot;d\u0026quot;] ) ) } Once this parallelized loop is executed, the folder /home/blas/cluster_shared/output should be filled with the results from the cluster workers, and the modelling.summary data frame contains the summary of each fitted model.\nNow that the work is done, we can stop the cluster.\nparallel::stopCluster(cl = my.cluster) Now you know how to work with data larger than memory in a parallelized loop!\n","date":1609632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609632000,"objectID":"4eb8b71dc831df7557610c90552ef52b","permalink":"https://blasbenito.com/post_projects/template/","publishdate":"2021-01-03T00:00:00Z","relpermalink":"/post_projects/template/","section":"post_projects","summary":"In the previous posts I have covered how to setup a home cluster, and how to run parallel processes with foreach in R. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.","tags":null,"title":"Setup of a shared folder in a home cluster","type":"post_projects"},{"authors":null,"categories":null,"content":"This is a spatio-temporal simulation of the effect of fire regimes on the population dynamics of five forest species (Pinus sylvestris, Pinus uncinata, Betula pendula, Corylus avellana, and Quercus petraea) during the Lateglacial-Holocene transition (15-7 cal Kyr BP) at El Portalet, a subalpine bog located in the central Pyrenees region (1802m asl, Spain), that has served for palaeoenvironmental studies (González-Smapériz et al. 2006; Gil-Romera et al., 2014). This model is described in the paper in prep. titled Forest - fire interactions in the Central Pyrenees: a data-model comparison for the Lateglacial-Holocene transition, and authored by Graciela Gil-Romera, Blas M. Benito, Juli G. Pausas, Penélope González-Sampériz, J. Julio. Camarero, Jens-Christian Svenning, and Blas Valero-Garcés.\nHOW DOES IT WORK Abiotic component The abiotic layer of the model is represented by three main environmental factors:\nTopography derived from a digital elevation model at 200 x 200 meters resolution. Slope (along temperature) is used to impose restrictions to species distributions. Northness (in the range [0, 1]) is used to restrict fire spread. Aspect is used to draw a shaded relief map (at the user\u0026rsquo;s request). Elevation is used to compute a lapse rate map (see below).\nTemperature (average of montly minimum temperatures) time series for the study area computed from palaeoclimatic data at annual resolution provided by the TraCe simulation, a transient model for the global climate evolution of the last 21K years with an annual resolution. The single temperature value of every year is converted into a temperature map (200 x 200 m resolution) using a lapse rate map based on the elevation map. Temperature, along with slope, is used to compute habitat suitability by using a logistic equation. Habitat suitability affects plant growth and survival.\nFire: The charcoal accumulation rate record (CHAR) from El Portalet palaeoenvironmental sequence (Gil-Romera et al., 2014) is used as input to simulate forest fires. A value of this time series is read each year, and a random number in the range [0, 1] is generated. If the random number is lower than the Fire-probability-per-year (FPY) parameter defined by the user, the value from the charcoal time series is multiplied by the parameter Number-ignitions-per-fire-event (NIF) (defined by the user) to compute the number of ignitions for the given year. As many adult tree as ignitions are selected to start spreading fire. Fire spreads to a neighbor patch if there is an adult tree in there, and a random number in the range [0, 1] is higher than the northness value of the patch.\nBiotic component The biotic layer of the model is composed by five tree species. We have introduced the following elements to represent their ecological dynamics:\nTopoclimatic niche, inferred from their present day distributions and high resolution temperature maps (presence data taken from GBIF, temperature maps taken from Worldclim and the Digital Climatic Atlas of the Iberian Peninsula). The ecological niche is represented by a logistic equation (see below). The results of this equation plus the dispersal dynamics of each species defines changes in distribution over time.\nPopulation dynamics, driven by species traits such as dispersal distance, longevity, fecundity, mortality, growth rate, post-fire response to fire, and heliophity (competition for light). The data is based on the literature and/or expert opinion from forest and fire ecologists, and it is used to simulate growth (using logistic equations), competition for light and space, decay due to senescence, and mortality due to climate, fire, or plagues.\nThe model doesn\u0026rsquo;t simulate the entire populations of the target species. Instead, on each 200 x 200 meters patch it simulates the dynamics of an small forest plot (around 10 x 10 meters) where a maximum of one individual per species can exist.\nModel dynamics Let\u0026rsquo;s burn it! Simulating fire-vegetation dynamics at millennial timescales in the central Pyrenees. from blas benito on Vimeo.\nThe life of an individual\nDuring the model setup seeds of every species are created on every patch. From there, every seed will go through the following steps every simulated year:\nIts age increases by one year, and its life-stage is changed to \u0026ldquo;seedling\u0026rdquo;.\nThe minimum average temperature of its patch is updated.\nThe individual computes its habitat suitability using the logistic equation 1 / ( 1 + exp( -(intercept + coefficient * patch-temperature))), where the intercept and the coefficient are user defined. These parameters are hardcoded to save space in the GUI, and have been computed beforehand by using current presence data and temperature maps.\nIf habitat suitability is higher than a random number in the range [0, 1], the habitat is considered suitable (NOTE: this random number is defined for the patch, and it changes every ~10 years following a random walk drawn from a normal distribution with the average set to the previous value, and a standard deviation of 0.001).\nIf it is lower, the habitat is considered unsuitable, and the number of years under unsuitable habitat is increased by 1.\nIf the number of years unders unsuitable habitat becomes higher than seedling-tolerance, the seedling dies, and another seed from the seed bank takes its place. Otherwise it stays alive. Mortality: If a random number in the range [0, 1] is lower than the seedling mortality of the species the plant dies, and it is replaced by a seed from the seed bank. Otherwise it stays alive.\nCompetition and growth:\nIf the patch total biomass of the individuals in the patch equals Max-biomass-per-patch, the individual loses an amount of biomass between 0 and the 20% of its current biomass. This number is randomly selected.\nIf Max-biomass-per-patch has not been reached yet:\nAn interaction term is computed as (1 - (biomass of other individuals in the patch / Max-biomass-per-patch)) * (1 - heliophilia)).\nThe interaction term is introduced in the growth equation max-biomass / (1 + max-biomass * exp(- growth-rate * interaction-term * habitat-suitability * age)) to compute the current biomass of the individual. The lower the interaction term and habitat suitability are, the lower the growth becomes.\nIf a fire reaches the patch and there are adult individuals of other species on it, the plant dies, and it is replaced by a seed (this seed inherites the traits of the parent).\nThese steps continue while the individual is still a seedling, but once it reaches its maturity some steps become slightly different:\nIf a random number in the range [0, 1] is lower than the adults mortality of the species, or the maximum age of the species is reached, the individual is marked for decay. The current biomass of decaying individuals is computed as previous-biomass - years-of-decay. To add the effect of climatic variability to this decreasing function, its result is multiplied by 1 - habitat-suitability x random[0, 10]. If the biomass is higher than zero, pollen productivity is computed as current-biomass x species-pollen-productivity. The individual dies and is replaced by a seed when the biomass is below 1.\nDispersal: If the individual is in suitable habitat, a seed from it is placed in one of the neighboring patches within a radius given by the dispersal distance of the species (which is measured in \u0026ldquo;number of patches\u0026rdquo; and hardcoded) with no individuals of the same species.\nIf the individual starts a fire, or if fire spreads in from neighboring patches, it is marked as \u0026ldquo;burned\u0026rdquo;, spreads fire to its neighbors, dies, and is replaced by a seed. If the individual belongs to an species with post-fire resprouting, the growth-rate of the seed is multiplied by 2 to boost growth after fire.\nSimulating pollen and charcoal deposition\nThe user defines the radius of a catchment area round the core location (10 km by default, that is 50 patches). All patches within this radius define the RSAP (relevant source area of pollen).\nAt the end of every simulated year the pollen productivity of every adult of each species within the RSAP is summed, and this value is used to compose the simulated pollen curves. The same is done with the biomass of the burned individuals to compose the virtual charcoal curve.\nOutput In GUI\nThe simulation GUI shows the following results in real time:\nPlots of the input values:\nMinimum Temperature of the coldest month. Real charcoal data. Simulated pollen curves for the target taxa.\nSimulated charcoal curve.\nMap showing the distribution of every species and the forest fires.\nWritten to disk\nThe simulated pollen counts and charcoal is exported to the path defined by the user as a table in csv format named output_table.csv. It contains one row per simulated year and the following columns:\nage: simulated year. temperature_minimum_average: average minimum winter temperature of the study area. pollen_Psylvestris: pollen sum for Pinus sylvestris. pollen_Puncinata pollen_Bpendula pollen_Cavellana pollen_Qpetraea real_charcoal: real charcoal values from El Portalet core. ignitions: number of fire ignitions. charcoal_sum: biomass sum of all burned individuals. charcoal_Psylvestris: sum of the biomass of burned individuals of Pinus sylvestris. charcoal_Puncinata charcoal_Bpendula charcoal_Cavellana charcoal_Qpetraea Snapshots of the simulation map taken at 1 or 10 years intervals are stored in the output folder is requested by the user. These snapshots are useful to compose a video of the simulation.\nHOW TO USE IT Input files Input files are stored in a folder named \u0026ldquo;data\u0026rdquo;. These are:\nage: text file with no extension and a single column with no header containing age values from -15000 to -5701 fire: text file with no extension and a single column with no header containing actual charcoal counts expresed in the range [0, 1]. There are as many rows as in the age file t_minimum_average: text file with same features as the ones above containing minimum winter temperatures for the study area extracted from the TraCe simualtion. correct_t_minimum_average.asc: Map at 200m resolution containing the minimum winter temperature difference (period 1970-2000) between the TraCe simulation and the Digital Climatic Atlas of the Iberian Peninsula. It is used to transform the values of t_minimum_average into a high resolution temperature map. elevation.asc: digital elevation model of the study area at 200m resolution, coordinate system with EPSG code 23030. slope.asc: topographic slope. topography.asc: shaded relief map. It is used for plotting purposes only. Input parameters General configuration of the simulation\nThe user can set-up the following parameters throught the GUI controls.\nOutput-path: Character. Path of the output folder. This parameter cannot be empty, and the output folder must exist. Snapshots?: Boolean. If on, creates snapshots of the GUI to make videos. Snapshots-frequency: Character. Defines the frequency of snapshots. Only two options: \u0026ldquo;every year\u0026rdquo; and \u0026ldquo;every 10 years\u0026rdquo;. Draw-topography?: Boolean. If on, plots a shaded relief map (stored in topography.asc). RSAP-radius: Numeric[5, 50]. Radius of the RSAP in number of patches. Each patch is 200 x 200 m, so an RSAP-radius of 10 equals 2 kilometres. Randommness-settings: Character. Allows to choose between \u0026ldquo;fixed seed\u0026rdquo; to obtain deterministic results, or \u0026ldquo;free seed\u0026rdquo; to obtain different results on each run. Max-biomass-per-patch: Numeric, integer. Maximum charge capacity of a patch. Fire?: Boolean. If on, fires are produced whenever the data fire triggers a fire event. If off, fires are not produced (control simulation). Fire-probability-per-year: Numeric [0, 1]. Whenever the fire file provides a number higher than 0, if a random number in the range [0, 1] is lower than Fire-probability-per-year, a number of ignitions is computed (see below) and fires are triggered. Fire-ignitions-amplification-factor: Numeric The fire file provides values in the range [0, 1], and this multiplication factor converts these values in an integer number of ignitions. If fire equals one, and Fire-ignitions-amplification-factor equals 10, the number of ignitions will be 10 for the given year. Mortality?: Boolean. If on, mortality due to predation, plagues and other unpredictable sources is active (see Xx-seedling-mortality and Xx-adult-mortality parameters below). Burn-in-iterations: Numeric, integer. Number of years to run the model at a constant temperature (the initial one in the t_minimum_average file) and no fires to allow the population model to reach an equilibrium before to start the actual simulation. P.sylvestris?, P.uncinata?, B.pendula?, Q.petraea?, and C.avellana?: Boolean. If off, the given species is removed from the simulation. Used for testing purposes. Species traits\nEach species has a set of traits to be filled by the user. Note that a particular species can be removed from the simulation by switching it to \u0026ldquo;off\u0026rdquo;.\nXx-max-age: Numeric, integer. Maximum longevity. Every individual reaching this age is marked for decay. Xx-maturity-age: Numeric, integer. Age of sexual maturity. Individuals reaching this age are considered adults. Xx-pollen-productivity: Numeric. Multiplier of biomass to obtain a relative measure of pollen productivity among species. Xx-growth-rate: Numeric. Growth rate of the given species. Xx-max-biomass: Numeric, integer. Maximum biomass reachable by the given species. Xx-heliophilia: Numeric, [0, 1]. Dependance of the species on solar light to grow. It is used to compute the effect of competence in plant growth. Xx-seedling-tolerance: Numeric, integer. Numer of years a seedling can tolerate unsuitable climate. Xx-adult-tolerance: Numeric, integer. Numer of years an adult can tolerate unsuitable climate. Xx-seedling-mortality: Numeric, [0, 1]. Proportion of seedlings dying due to predation. Xx-adult-mortality: Numeric, [0, 1]. Proportion of adults dying due to plagues or other mortality sources. Xx-resprout-after-fire: Boolean. If 0 the species doesn\u0026rsquo;t show a post-fire response. If 1, growth-rate is multiplied by two in the resprouted individual to increase growth rate. Xx-min-temperature: Numeric. Minimum temperature at which the species has been found using GBIF presence data. Xx-max-temperature: Numeric. Maximum temperature at which the species has been found using GBIF presence data. Xx-min-slope: Numeric. Minimum topographic slope at which the species has been found. Xx-max-slope: Numeric. Maximum topographic slope at which the species has been found. Xx-intercept: Numeric. Intercept of the logistic equation to compute habitat suitability fitted to presence data and minimum temperature maps. Xx-coefficient: Numeric. Coefficient of the logistic equation to compute habitat suitability. ","date":1609545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609545600,"objectID":"ccb2f0c0f3fc235a1a29cfdb9e5f5669","permalink":"https://blasbenito.com/project/palaeo_fire_modeling/","publishdate":"2021-01-02T00:00:00Z","relpermalink":"/project/palaeo_fire_modeling/","section":"project","summary":"This is a spatio-temporal simulation of the effect of fire regimes on the population dynamics of five forest species during the Lateglacial-Holocene transition (15-7 cal Kyr BP) at El Portalet, a subalpine bog located in the central Pyrenees region (1802m asl, Spain)","tags":["ABM","Netlogo","Fire dynamics","Mechanistic simulation","Palaeoecology","Agent-based models"],"title":"Palaeo fire modeling","type":"project"},{"authors":["Antonio J. Pérez-Luque","Blas M. Benito","Francisco J. Bonet","Regino Zamora"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"34e3f18f11e96f8c35049f582c26df75","permalink":"https://blasbenito.com/publication/2020_perez-luque_forests/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/2020_perez-luque_forests/","section":"publication","summary":"Understanding the ecology of populations located in the rear edge of their distribution is key to assessing the response of the species to changing environmental conditions. Here, we focus on rear-edge populations of Quercus pyrenaica in Sierra Nevada (southern Iberian Peninsula) to analyze their ecological and floristic diversity. We perform multivariate analyses using high-resolution environmental information and forest inventories to determine how environmental variables differ among oak populations, and to identify population groups based on environmental and floristic composition.","tags":["Biogeography","Plant Ecology"],"title":"Ecological Diversity within Rear-Edge: A Case Study from Mediterranean Quercus pyrenaica Willd.","type":"publication"},{"authors":null,"categories":null,"content":"This Netlogo model simulates the dispersal of Quercus pyrenaica populations in Sierra Nevada (Spain) at a yearly resolution until 2100 while considering different levels of model complexity, from random dispersal and seedling establishment, to realistic dispersal based on the dispersal behavior of the Eurasian Jay.\nThe data required to run the model can be downloaded from here. It must be decompressed in the same folder containing the netlogo code of the model.\nThe video below shows the model in action for one population of Quercus pyrenaica. On the left, it shows the effect of a random dispersal model, and on the right, a realistic dispersal model based on observations of the dispersal behavior of the Eurasian Jay.\n","date":1609372800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609372800,"objectID":"e58a80cc3e1a80acfc33794a87e4e941","permalink":"https://blasbenito.com/project/quercus_range_shift/","publishdate":"2020-12-31T00:00:00Z","relpermalink":"/project/quercus_range_shift/","section":"project","summary":"Agent-based model coded with Netlogo to simulate range shift of *Quercus pyrenaica* populations in Sierra Nevada (Spain) using a realistic dispersal model with different levels of complexity.","tags":["ABM","Netlogo","Range-shift simulation","Mechanistic simulation","Dispersal","Agent-based models"],"title":"Range-shift simulation","type":"project"},{"authors":null,"categories":null,"content":" Note: to better follow this tutorial you can download the .Rmd file from here.\nIn a previous post I explained how to set up a small home cluster. Many things can be done with a cluster, and parallelizing loops is one of them. But there is no need of a cluster to parallelize loops and improve the efficiency of your coding!\nI believe that coding parallelized loops is an important asset for anyone working with R. That’s why this post covers the following topics:\nBeyond for: building loops with foreach. What is a parallel backend? Setup of a parallel backend for a single computer. Setup for a Beowulf cluster. Practical examples. Tuning of random forest hyperparameters. Confidence intervals of the importance scores of the predictors in random forest models. for loops are fine, but… Many experienced R users frequently say that nobody should write loops with R because they are tacky or whatever. However, I find loops easy to write, read, and debug, and are therefore my workhorse whenever I need to repeat a task and I don’t feel like using apply() and the likes. However, regular for loops in R are highly inefficient, because they only use one of your computer cores to perform the iterations.\nFor example, the for loop below sorts vectors of random numbers a given number of times, and will only work on one of your computer cores for a few seconds, while the others are there, procrastinating with no shame.\nWot Cpu GIF from Wot GIFs (gif kindly suggested by Andreas Angourakis)\nfor(i in 1:10000){ sort(runif(10000)) } If every i could run in a different core, the operation would indeed run a bit faster, and we would get rid of lazy cores. This is were packages like foreach and doParallel come into play. Let’s start installing these packages and a few others that will be useful throughout this tutorial.\n#automatic install of packages if they are not installed already list.of.packages \u0026lt;- c( \u0026quot;foreach\u0026quot;, \u0026quot;doParallel\u0026quot;, \u0026quot;ranger\u0026quot;, \u0026quot;palmerpenguins\u0026quot;, \u0026quot;tidyverse\u0026quot;, \u0026quot;kableExtra\u0026quot; ) new.packages \u0026lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\u0026quot;Package\u0026quot;])] if(length(new.packages) \u0026gt; 0){ install.packages(new.packages, dep=TRUE) } #loading packages for(package.i in list.of.packages){ suppressPackageStartupMessages( library( package.i, character.only = TRUE ) ) } #loading example data data(\u0026quot;penguins\u0026quot;) Beyond for: building loops with foreach The foreach package (the vignette is here) provides a way to build loops that support parallel execution, and easily gather the results provided by each iteration in the loop.\nFor example, this classic for loop computes the square root of the numbers 1 to 5 with sqrt() (the function is vectorized, but let’s conveniently forget that for a moment). Notice that I have to create a vector x to gather the results before executing the loop.\nx \u0026lt;- vector() for(i in 1:10){ x[i] \u0026lt;- sqrt(i) } x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 The foreach version returns a list with the results automatically. Notice that %do% operator after the loop definition, I’ll talk more about it later.\nx \u0026lt;- foreach(i = 1:10) %do% { sqrt(i) } x ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 1.414214 ## ## [[3]] ## [1] 1.732051 ## ## [[4]] ## [1] 2 ## ## [[5]] ## [1] 2.236068 ## ## [[6]] ## [1] 2.44949 ## ## [[7]] ## [1] 2.645751 ## ## [[8]] ## [1] 2.828427 ## ## [[9]] ## [1] 3 ## ## [[10]] ## [1] 3.162278 We can use the .combine argument of foreach to arrange the list as a vector. Other options such as cbind, rbind, or even custom functions can be used as well, only depending on the structure of the output of each iteration.\nx \u0026lt;- foreach( i = 1:10, .combine = \u0026#39;c\u0026#39; ) %do% { sqrt(i) } x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 Another interesting capability of foreach is that it supports several iterators of the same length at once. Notice that the values of the iterators are not combined. When the first value of one iterator is being used, the first value of the other iterators will be used as well.\nx \u0026lt;- foreach( i = 1:3, j = 1:3, k = 1:3, .combine = \u0026#39;c\u0026#39; ) %do% { i + j + k } x ## [1] 3 6 9 Running foreach loops in parallel The foreach loops shown above use the operator %do%, that processes the tasks sequentially. To run tasks in parallel, foreach uses the operator %dopar%, that has to be supported by a parallel backend. If there is no parallel backend, %dopar% warns the user that it is being run sequentially, as shown below. But what the heck is a parallel backend?\nx \u0026lt;- foreach( i = 1:10, .combine = \u0026#39;c\u0026#39; ) %dopar% { sqrt(i) } ## Warning: executing %dopar% sequentially: no parallel backend registered x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 What is a parallel backend? When running tasks in parallel, there should be a director node that tells a group of workers what to do with a given set of data and functions. The workers execute the iterations, and the director manages execution and gathers the results provided by the workers. A parallel backend provides the means for the director and workers to communicate, while allocating and managing the required computing resources (processors, RAM memory, and network bandwidth among others).\nThere are two types of parallel backends that can be used with foreach, FORK and PSOCK.\nFORK FORK backends are only available on UNIX machines (Linux, Mac, and the likes), and do not work in clusters [sad face], so only single-machine environments are appropriate for this backend. In a FORK backend, the workers share the same environment (data, loaded packages, and functions) as the director. This setup is highly efficient because the main environment doesn’t have to be copied, and only worker outputs need to be sent back to the director.\nPSOCK PSOCK backends (Parallel Socket Cluster) are available for both UNIX and WINDOWS systems, and are the default option provided with foreach. As their main disadvantage, the environment of the director needs to be copied to the environment of each worker, which increases network overhead while decreasing the overall efficiency of the cluster. By default, all the functions available in base R are copied to each worker, and if a particular set of R packages are needed in the workers, they need to be copied to the respective environments of the workers as well.\nThis post compares both backends and concludes that FORK is about a 40% faster than PSOCK.\nSetup of a parallel backend Here I explain how to setup the parallel backend for a simple computer and for a Beowulf cluster as the one I described in a previous post.\nSetup for a single computer Setting up a cluster in a single computer requires first to find out how many cores we want to use from the ones we have available. It is recommended to leave one free core for other tasks.\nparallel::detectCores() ## [1] 8 n.cores \u0026lt;- parallel::detectCores() - 1 Now we need to define the cluster with parallel::makeCluster() and register it so it can be used by %dopar% with doParallel::registerDoParallel(my.cluster). The type argument of parallel::makeCluster() accepts the strings “PSOCK” and “FORK” to define the type of parallel backend to be used.\n#create the cluster my.cluster \u0026lt;- parallel::makeCluster( n.cores, type = \u0026quot;PSOCK\u0026quot; ) #check cluster definition (optional) print(my.cluster) ## socket cluster with 7 nodes on host \u0026#39;localhost\u0026#39; #register it to be used by %dopar% doParallel::registerDoParallel(cl = my.cluster) #check if it is registered (optional) foreach::getDoParRegistered() ## [1] TRUE #how many workers are available? (optional) foreach::getDoParWorkers() ## [1] 7 Now we can run a set of tasks in parallel!\nx \u0026lt;- foreach( i = 1:10, .combine = \u0026#39;c\u0026#39; ) %dopar% { sqrt(i) } x ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 If everything went well, now %dopar% should not be throwing the warning executing %dopar% sequentially: no parallel backend registered, meaning that the parallel execution is working as it should. In this little example there is no gain in execution speed, because the operation being executed is extremely fast, but this will change when the operations running inside of the loop take longer times to run.\nFinally, it is always recommendable to stop the cluster when we are done working with it.\nparallel::stopCluster(cl = my.cluster) Setup for a Beowulf cluster This setup is a bit more complex, because it requires to open a port in every computer of the cluster. Ports are virtual communication channels, and are identified by a number.\nFirst, lets tell R what port we want to use:\n#define port Sys.setenv(R_PARALLEL_PORT = 11000) #check that it Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;) Now, we need to open the selected port in every computer of the network. In Linux we need to setup the firewall to allow connections from the network 10.42.1.0/24 (replace this with your network range if different!) to the port 11000 by splitting the window of the Terminator console in as many computers available in your network (the figure below shows three, one for my PC and two for my Intel NUCs), opening an ssh session on each remote machine, and setting Terminator with Grouping equal to Broadcast all so we only need to type the commands once.\nOpening port 11000 in three computers at once with Terminator\nNow we have to create an object defining the IPs of the computers in the network, the number of cores to use from each computer, the user name, and the identity of the director. This will be the spec argument required by parallel::makeCluster() to create the cluster throughtout the machines in the network. It is a list of lists, with as many lists as nodes are defined. Each sub-list has a slot named host with the IP of the computer where the given node is, and user, with the name of the user in each computer.\nThe code below shows how this would be done, step by step. Yes, this is CUMBERSOME.\n#main parameters director \u0026lt;- \u0026#39;10.42.0.1\u0026#39; nuc2 \u0026lt;- \u0026#39;10.42.0.34\u0026#39; nuc1 \u0026lt;- \u0026#39;10.42.0.104\u0026#39; user \u0026lt;- \u0026quot;blas\u0026quot; #list of machines, user names, and cores spec \u0026lt;- list( list( host = director, user = user, ncore = 7 ), list( host = nuc1, user = user, ncore = 4 ), list( host = nuc2, user = user, ncore = 4 ) ) #generating nodes from the list of machines spec \u0026lt;- lapply( spec, function(spec.i) rep( list( list( host = spec.i$host, user = spec.i$user) ), spec.i$ncore ) ) #formating into a list of lists spec \u0026lt;- unlist( spec, recursive = FALSE ) Generating the spec definition is a bit easier with the function below.\n#function to generate cluster specifications from a vector of IPs, a vector with the number of cores to use on each IP, and a user name cluster_spec \u0026lt;- function( ips, cores, user ){ #creating initial list spec \u0026lt;- list() for(i in 1:length(ips)){ spec[[i]] \u0026lt;- list() spec[[i]]$host \u0026lt;- ips[i] spec[[i]]$user \u0026lt;- user spec[[i]]$ncore \u0026lt;- cores[i] } #generating nodes from the list of machines spec \u0026lt;- lapply( spec, function(spec.i) rep( list( list( host = spec.i$host, user = spec.i$user) ), spec.i$ncore ) ) #formating into a list of lists spec \u0026lt;- unlist( spec, recursive = FALSE ) return(spec) } This function is also available in this GitHub Gist, so you can load it into your R environment by executing:\nsource(\u0026quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R\u0026quot;) Below I use it to generate the input to the spec argument to start the cluster with parallel::makeCluster(). Notice that I have added several arguments.\nThe argument outfile determines where the workers write a log. In this case it is set to nowhere with the double quotes, but the path to a text file in the director could be provided here. The argument homogeneous = TRUE indicates that all machines have the Rscript in the same location. In this case all three machines have it at “/usr/lib/R/bin/Rscript”. Otherwise, set it up to FALSE. #generate cluster specification spec \u0026lt;- cluster_spec( ips = c(\u0026#39;10.42.0.1\u0026#39;, \u0026#39;10.42.0.34\u0026#39;, \u0026#39;10.42.0.104\u0026#39;), cores = c(7, 4, 4), user = \u0026quot;blas\u0026quot; ) #setting up cluster my.cluster \u0026lt;- parallel::makeCluster( master = \u0026#39;10.42.0.1\u0026#39;, spec = spec, port = Sys.getenv(\u0026quot;R_PARALLEL_PORT\u0026quot;), outfile = \u0026quot;\u0026quot;, homogeneous = TRUE ) #check cluster definition (optional) print(my.cluster) #register cluster doParallel::registerDoParallel(cl = my.cluster) #how many workers are available? (optional) foreach::getDoParWorkers() Now we can use the cluster to execute a dummy operation in parallel using all machines in the network.\nx \u0026lt;- foreach( i = 1:20, .combine = \u0026#39;c\u0026#39; ) %dopar% { sqrt(i) } x Once everything is done, remember to close the cluster.\nparallel::stopCluster(cl = my.cluster) Practical examples In this section I cover two examples on how to use parallelized loops to explore model outputs:\nTuning random forest hyperparameters to maximize classification accuracy. Obtain a confidence interval for the importance score of each predictor from a set random forest models fitted with ranger(). In the examples I use the penguins data from the palmerpenguins package to fit classification models with random forest using species as a response, and bill_length_mm, bill_depth_mm, flipper_length_mm, and body_mass_g as predictors.\n#removing NA and subsetting columns penguins \u0026lt;- as.data.frame( na.omit( penguins[, c( \u0026quot;species\u0026quot;, \u0026quot;bill_length_mm\u0026quot;, \u0026quot;bill_depth_mm\u0026quot;, \u0026quot;flipper_length_mm\u0026quot;, \u0026quot;body_mass_g\u0026quot; )] ) ) species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g Adelie 39.1 18.7 181 3750 Adelie 39.5 17.4 186 3800 Adelie 40.3 18.0 195 3250 Adelie 36.7 19.3 193 3450 Adelie 39.3 20.6 190 3650 Adelie 38.9 17.8 181 3625 Adelie 39.2 19.6 195 4675 Adelie 34.1 18.1 193 3475 Adelie 42.0 20.2 190 4250 Adelie 37.8 17.1 186 3300 Adelie 37.8 17.3 180 3700 Adelie 41.1 17.6 182 3200 Adelie 38.6 21.2 191 3800 Adelie 34.6 21.1 198 4400 Adelie 36.6 17.8 185 3700 Adelie 38.7 19.0 195 3450 Adelie 42.5 20.7 197 4500 Adelie 34.4 18.4 184 3325 Adelie 46.0 21.5 194 4200 Adelie 37.8 18.3 174 3400 We’ll fit random forest models with the ranger package, which works as follows:\n#fitting classification model m \u0026lt;- ranger::ranger( data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, importance = \u0026quot;permutation\u0026quot; ) #summary m ## Ranger result ## ## Call: ## ranger::ranger(data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, importance = \u0026quot;permutation\u0026quot;) ## ## Type: Classification ## Number of trees: 500 ## Sample size: 342 ## Number of independent variables: 4 ## Mtry: 2 ## Target node size: 1 ## Variable importance mode: permutation ## Splitrule: gini ## OOB prediction error: 2.34 % #variable importance m$variable.importance ## bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## 0.30464149 0.16554689 0.22329574 0.07775624 The output shows that the percentage of misclassified cases is 2.34, and that bill_length_mm is the variable that contributes the most to the accuracy of the classification.\nIf you are not familiar with random forest, this post and the video below do a pretty good job in explaining the basics:\nTuning random forest hyperparameters Random forest has several hyperparameters that influence model fit:\nnum.trees is the total number of trees to fit. The default value is 500. mtry is the number of variables selected by chance (from the total pool of variables) as candidates for a tree split. The minimum is 2, and the maximum is the total number of predictors. min.node.size is the minimum number of cases that shall go together in the terminal nodes of each tree. For classification models as the ones we are going to fit, 1 is the minimum. Here we are going to explore how combinations of these values increase or decrease the prediction error of the model (percentage of misclassified cases) on the out-of-bag data (not used to train each decision tree). This operation is usually named grid search for hyperparameter optimization.\nTo create these combinations of hyperparameters we use expand.grid().\nsensitivity.df \u0026lt;- expand.grid( num.trees = c(500, 1000, 1500), mtry = 2:4, min.node.size = c(1, 10, 20) ) num.trees mtry min.node.size 500 2 1 1000 2 1 1500 2 1 500 3 1 1000 3 1 1500 3 1 500 4 1 1000 4 1 1500 4 1 500 2 10 1000 2 10 1500 2 10 500 3 10 1000 3 10 1500 3 10 500 4 10 1000 4 10 1500 4 10 500 2 20 1000 2 20 1500 2 20 500 3 20 1000 3 20 1500 3 20 500 4 20 1000 4 20 1500 4 20 Each row in sensitivity.df corresponds to a combination of parameters to test, so there are 27 models to fit. The code below prepares the cluster, and uses the ability of foreach to work with several iterators at once to easily introduce the right set of hyperparameters to each fitted model.\nNotice how in the foreach definition I use the .packages argument to export the ranger package to the environments of the workers.\n#create and register cluster my.cluster \u0026lt;- parallel::makeCluster(n.cores) doParallel::registerDoParallel(cl = my.cluster) #fitting each rf model with different hyperparameters prediction.error \u0026lt;- foreach( num.trees = sensitivity.df$num.trees, mtry = sensitivity.df$mtry, min.node.size = sensitivity.df$min.node.size, .combine = \u0026#39;c\u0026#39;, .packages = \u0026quot;ranger\u0026quot; ) %dopar% { #fit model m.i \u0026lt;- ranger::ranger( data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, num.trees = num.trees, mtry = mtry, min.node.size = min.node.size ) #returning prediction error as percentage return(m.i$prediction.error * 100) } #adding the prediction error column sensitivity.df$prediction.error \u0026lt;- prediction.error To plot the results:\nggplot2::ggplot(data = sensitivity.df) + ggplot2::aes( x = mtry, y = as.factor(min.node.size), fill = prediction.error ) + ggplot2::facet_wrap(as.factor(num.trees)) + ggplot2::geom_tile() + ggplot2::scale_y_discrete(breaks = c(1, 10, 20)) + ggplot2::scale_fill_viridis_c() + ggplot2::ylab(\u0026quot;min.node.size\u0026quot;) The figure shows that combinations of lower values of min.node.size and mtry generally lead to models with a lower prediction error across different numbers of trees. Retrieving the first line of sensitivity.df ordered by ascending prediction.error will give us the values of the hyperparameters we need to use to reduce the prediction error as much as possible.\nbest.hyperparameters \u0026lt;- sensitivity.df %\u0026gt;% dplyr::arrange(prediction.error) %\u0026gt;% dplyr::slice(1) num.trees mtry min.node.size prediction.error 500 2 1 2.339181 Confidence intervals of variable importance scores Random forest has an important stochastic component during model fitting, and as consequence, the same model will return slightly different results in different runs (unless set.seed() or the seed argument of ranger are used). This variability also affects the importance scores of the predictors, and can be use to our advantage to assess whether the importance scores of different variables do really overlap or not.\nI have written a little function to transform the vector of importance scores returned by ranger into a data frame (of one row). It helps arranging the importance scores of different runs into a long format, which helps a lot to plot a boxplot with ggplot2 right away. This function could have been just some code thrown inside the foreach loop, but I want to illustrate how foreach automatically transfers functions available in the R environment into the environments of the workers when required, without the intervention of the user. The same will happen with the best.hyperparameters tiny data frame we created in the previous section.\nimportance_to_df \u0026lt;- function(model){ x \u0026lt;- as.data.frame(model$variable.importance) x$variable \u0026lt;- rownames(x) colnames(x)[1] \u0026lt;- \u0026quot;importance\u0026quot; rownames(x) \u0026lt;- NULL return(x) } The code chunk below setups the cluster and runs 1000 random forest models in parallel (using the best hyperparameters computed in the previous section) while using system.time() to assess running time.\n#we don\u0026#39;t need to create the cluster, it is still up print(my.cluster) ## socket cluster with 7 nodes on host \u0026#39;localhost\u0026#39; #assessing execution time system.time( #performing 1000 iterations in parallel importance.scores \u0026lt;- foreach( i = 1:1000, .combine = \u0026#39;rbind\u0026#39;, .packages = \u0026quot;ranger\u0026quot; ) %dopar% { #fit model m.i \u0026lt;- ranger::ranger( data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, importance = \u0026quot;permutation\u0026quot;, mtry = best.hyperparameters$mtry, num.trees = best.hyperparameters$num.trees, min.node.size = best.hyperparameters$min.node.size ) #format importance m.importance.i \u0026lt;- importance_to_df(model = m.i) #returning output return(m.importance.i) } ) ## user system elapsed ## 0.267 0.027 6.556 The output of system.time() goes as follows:\nuser: seconds the R session has been using the CPU. system: seconds the operating system has been using the CPU. elapsed: the total execution time experienced by the user. This will make sense in a minute. In the meantime, let’s plot our results!\nggplot2::ggplot(data = importance.scores) + ggplot2::aes( y = reorder(variable, importance), x = importance ) + ggplot2::geom_boxplot() + ggplot2::ylab(\u0026quot;\u0026quot;) The figure shows that the variable bill_length_mm is the most important in helping the model classifying penguin species, with no overlap with any other variable. In this particular case, since the distributions of the importance scores do not overlap, this analysis isn’t truly helpful, but now you know how to do it!\nI assessed the running time with system.time() because ranger() can run in parallel by itself just by setting the num.threads argument to the number of cores available in the machine. This capability cannot be used when executing ranger() inside a parallelized foreach loop though, and it is only useful inside classic for loops.\nWhat option is more efficient then? The code below executes a regular for loop running the function sequentially to evaluate whether it is more efficient to run ranger() in parallel using one core per model, as we did above, or sequentially while using several cores per model on each iteration.\n#list to save results importance.scores.list \u0026lt;- list() #performing 1000 iterations sequentially system.time( for(i in 1:1000){ #fit model m.i \u0026lt;- ranger::ranger( data = penguins, dependent.variable.name = \u0026quot;species\u0026quot;, importance = \u0026quot;permutation\u0026quot;, seed = i, num.threads = parallel::detectCores() - 1 ) #format importance importance.scores.list[[i]] \u0026lt;- importance_to_df(model = m.i) } ) ## user system elapsed ## 43.663 2.815 12.948 As you can see, ranger() takes longer to execute in a regular for loop using several cores at once than in a parallel foreach loop using one core at once. That’s a win for the parallelized loop!\nWe can stop our cluster now, we are done with it.\nparallel::stopCluster(cl = my.cluster) A few things to take in mind As I have shown in this post, using parallelized foreach loops can accelerate long computing processes, even when some functions have the ability to run in parallel on their own. However, there are things to take in mind, that might vary depending on whether we are executing the parallelized task on a single computer or on a small cluster.\nIn a single computer, the communication between workers and the director is usually pretty fast, so there are no obvious bottlenecks to take into account here. The only limitation that might arise comes from the availability of RAM memory. For example, if a computer has 8 cores and 8GB of RAM, less than 1GB of RAM will be available for each worker. So, if you need to repeat a process that consumes a significant amount of RAM, the ideal number of cores running in parallel might be lower than the total number of cores available in your system. Don’t be greedy, and try to understand the capabilities of your machine while designing a parallelized task.\nWhen running foreach loops as in x \u0026lt;- foreach(...){...}, the variable x is receiving whatever results the workers are producing. For example, if you are only returning the prediction error of a model, or its importance scores, x will have a very manageable size. But if you are returning heavy objects such as complete random forest models, the size of x is going to grow VERY FAST, and at the end it will be competing for RAM resources with the workers, which might even crash your R session. Again, don’t be greedy, and size your outputs carefully.\nClusters spanning several computers are a different beast, since the workers and the director communicate through a switch and network wires and interfaces. If the amount of data going to and coming from the workers is large, the network can get clogged easily, reducing the cluster’s efficiency drastically. In general, if the amount of data produced by a worker on each iteration takes longer to arrive to the director than the time it takes the worker to produce it, then a cluster is not going to be more efficient than a single machine. But this is not important if you don’t care about efficiency.\nOther issues you might come across while parallelizing tasks in R are thoroughly commented in this post, by Imre Gera.\nThat’s all for now folks, happy parallelization!\n","date":1608940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608940800,"objectID":"258c4372b0dc8ca13f2e28fc8d34faa5","permalink":"https://blasbenito.com/post/02_parallelizing_loops_with_r/","publishdate":"2020-12-26T00:00:00Z","relpermalink":"/post/02_parallelizing_loops_with_r/","section":"post","summary":"Note: to better follow this tutorial you can download the .Rmd file from here.\nIn a previous post I explained how to set up a small home cluster. Many things can be done with a cluster, and parallelizing loops is one of them.","tags":null,"title":"Parallelized loops with R","type":"post"},{"authors":null,"categories":null,"content":" The package distantia allows to measure the dissimilarity between multivariate time-series. The package assumes that the target sequences are ordered along a given dimension, being depth and time the most common ones, but others such as latitude or elevation are also possible. Furthermore, the target time-series can be regular or irregular, and have their samples aligned (same age/time/depth) or unaligned (different age/time/depth). The only requirement is that the sequences must have at least two (but ideally more) columns with the same name and units representing different variables relevant to the dynamics of a system of interest.\nThe GitHub page of the project contains a thorough explanation of the statistics behind the method. The paper published in the Ecography journal describes the method, the package, and a couple of practical examples. The code and data used to develop the examples can be found in GitHub and Zenodo.\nPlease, if you find this package useful, please cite it as:\nBenito, B.M. and Birks, H.J.B. (2020), distantia: an open‐source toolset to quantify dissimilarity between multivariate ecological time‐series. Ecography, 43: 660-667. https://doi.org/10.1111/ecog.04895\n","date":1608336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608336000,"objectID":"a64ca32d94825ca0e1d3efdede85f5d0","permalink":"https://blasbenito.com/project/distantia/","publishdate":"2020-12-19T00:00:00Z","relpermalink":"/project/distantia/","section":"project","summary":"R package to compare multivariate time-series.","tags":["R packages","Time Series Analysis"],"title":"R package \"distantia\"","type":"project"},{"authors":null,"categories":null,"content":" The goal of memoria is to provide the tools to quantify ecological memory in long time-series involving environmental drivers and biotic responses, including palaeoecological datasets.\nEcological memory has two main components: the endogenous component, which represents the effect of antecedent values of the response on itself, and endogenous component, which represents the effect of antecedent values of the driver or drivers on the current state of the biotic response. Additionally, the concurrent effect, which represents the synchronic effect of the environmental drivers over the response is measured. The functions in the package allow the user\nThe package memoria uses the fast implementation of Random Forest available in the ranger package to fit a model of the form shown in Equation 1:\nEquation 1 (simplified from the one in the paper): $$p_{t} = p_{t-1} +\u0026hellip;+ p_{t-n} + d_{t} + d_{t-1} +\u0026hellip;+ d_{t-n}$$\nWhere:\n$p$ is the response variable, Pollen counts were used in this particular case.. $d$ is an environmental Driver influencing the response variable. $t$ is the time of any given value of the response $p$. $t-1$ is the lag 1. $p_{t-1} +\u0026hellip;+ p_{t-n}$ represents the endogenous component of ecological memory. $d_{t-1} +\u0026hellip;+ d_{t-n}$ represents the exogenous component of ecological memory. $d_{t}$ represents the concurrent effect of the driver over the response. Random Forest returns an importance score for each model term, and the functions in memoria let the user to plot the importance scores across time lags for each ecological memory components, and to compute different features of each memory component (length, strength, and dominance).\nThe GitHub page of the package features complete examples on how to use the package. The paper published in the Ecography journal describes ecological memory concepts and the method based on Random Forest used to assess ecological memory components. The code used to generate the supplementary materials can be found in GitHub and Zenodo.\nIf you ever use the package, please, cite it as:\nBenito, B.M., Gil‐Romera, G. and Birks, H.J.B. (2020), Ecological memory at millennial time‐scales: the importance of data constraints, species longevity and niche features. Ecography, 43: 1-10. https://doi.org/10.1111/ecog.04772\n","date":1608336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608336000,"objectID":"b2480e8e20be7bd9954b85358f24acc6","permalink":"https://blasbenito.com/project/memoria/","publishdate":"2020-12-19T00:00:00Z","relpermalink":"/project/memoria/","section":"project","summary":"R package to assess ecological memory in multivariate time-series.","tags":["R packages","Ecological Memory","Time Series Analysis","Machine Learning","Random Forest"],"title":"R package \"memoria\"","type":"project"},{"authors":null,"categories":null,"content":" Introduction The package spatialRF facilitates fitting spatial regression models on regular or irregular data with Random Forest by generating spatial predictors that allow the model to take into account the spatial structure of the training data. The end goal is minimizing the spatial autocorrelation of the model residuals as much as possible.\nTwo main methods to generate spatial predictors from the distance matrix of the data points are implemented in the package:\nMoran’s Eigenvector Maps (Dray, Legendre, and Peres-Neto 2006). Distance matrix columns as explanatory variables (Hengl et al. 2018). The package is designed to minimize the amount of code required to fit a spatial model from a training dataset, the names of the response and the predictors, and a distance matrix, as the example below shows.\nspatial.model \u0026lt;- rf_spatial( data = your_dataframe, dependent.variable.name = \u0026quot;your_response_variable\u0026quot;, predictor.variable.names = c(\u0026quot;predictor1\u0026quot;, \u0026quot;predictor2\u0026quot;, ..., \u0026quot;predictorN\u0026quot;), distance.matrix = your_distance_matrix ) The package, that uses the ranger package under the hood (Wright and Ziegler 2017), also provides tools to identify potentially interesting variable interactions, tune random forest hyperparameters, assess model performance on spatially independent data folds, and examine the resulting models via importance plots, response curves, and response surfaces.\nDevelopment This package is reaching its final form, and big changes are not expected at this stage. However, it has many functions, and even though all them have been tested, only one dataset has been used for those tests. You will find bugs, and something will go wrong almost surely. If you have time to report bugs, please, do so in any of the following ways:\nOpen a new issue in the Issues GitHub page of the package. Send me an email explaining the issue and the error messages with enough detail at blasbenito at gmail dot com. Send a direct message to my twitter account explaining the issue. I will do my best to solve any issues ASAP!\nApplications The goal of spatialRF is to help fitting explanatory spatial regression, where the target is to understand how a set of predictors and the spatial structure of the data influences response variable. Therefore, the spatial analyses implemented in the package can be applied to any spatial dataset, regular or irregular, with a sample size between ~100 and ~5000 cases (the higher end will depend on the RAM memory available), a quantitative or binary (values 0 and 1) response variable, and a more or less large set of predictive variables.\nAll functions but rf_spatial() work with non-spatial data as well if the arguments distance.matrix and distance.thresholds are ignored. In such case, the number of cases is no longer limited by the size of the distance matrix, and models can be trained with hundreds of thousands of rows.\nHowever, when the focus is on fitting spatial models, and due to the nature of the spatial predictors used to represent the spatial structure of the training data, there are many things this package cannot do:\nPredict model results over raster data.\nPredict a model result over another region with a different spatial structure.\nWork with “big data”, whatever that means.\nImputation or extrapolation (it can be done, but models based on spatial predictors are hardly transferable).\nTake temporal autocorrelation into account (but this is something that might be implemented later on).\nIf after considering these limitations you are still interested, follow me, I will show you how it works.\nInstall The package is not yet in the CRAN repositories, so at the moment it must be installed from GitHub as follows.\nremotes::install_github( repo = \u0026quot;blasbenito/spatialRF\u0026quot;, ref = \u0026quot;main\u0026quot;, force = TRUE, quiet = TRUE ) library(spatialRF) There are a few other libraries that will be useful during this tutorial.\nlibrary(kableExtra) library(rnaturalearth) library(rnaturalearthdata) library(tidyverse) Data requirements The data required to fit random forest models with spatialRF must fulfill several conditions:\nThe input format is data.frame. At the moment, tibbles are not fully supported. The number of rows must be somewhere between 100 and ~5000, but that will depend on the RAM available in your system. However, this limitation only affects spatial analyses performed with rf_spatial(), while all other modeling and plotting functions should work without a distance matrix (if they don’t tell me, that’d be a bug!), and therefore analyses in large datasets can still be done with the package. The number of predictors should be larger than 3, fitting a Random Forest model is moot otherwise. Factors in the response or the predictors are not explicitly supported in the package, they may work, or they won’t, but in any case, I designed this package for quantitative data alone. However, binary data with values 0 and 1 in the response variable are supported. Must be free of NA. You can check if there are NA records with sum(apply(df, 2, is.na)). If the result is larger than 0, then just execute df \u0026lt;- na.omit(df) to remove rows with empty cells. Columns cannot have zero variance. This condition can be checked with apply(df, 2, var) == 0. Columns yielding TRUE should be removed. Columns must not yield NaN or Inf when scaled. You can check each condition with sum(apply(scale(df), 2, is.nan)) and sum(apply(scale(df), 2, is.infinite)). If higher than 0, you can find what columns are giving issues with sapply(as.data.frame(scale(df)), function(x)any(is.nan(x))) and sapply(as.data.frame(scale(df)), function(x)any(is.infinite(x))). Any column yielding TRUE will generate issues while trying to fit models with spatialRF. Example data The package includes an example dataset that fulfills the conditions mentioned above, named plant_richness_df. It is a data frame with plant species richness and predictors for 227 ecoregions in the Americas, and a distance matrix among the ecoregion edges named, well, distance_matrix.\ndata(plant_richness_df) data(distance_matrix) #names of the response variable and the predictors dependent.variable.name \u0026lt;- \u0026quot;richness_species_vascular\u0026quot; predictor.variable.names \u0026lt;- colnames(plant_richness_df)[5:21] The response variable of plant_richness_df is “richness_species_vascular”, with the total count of vascular plant species found on each ecoregion. The figure below shows the centroids of each ecoregion along with their associated value of the response variable.\nThe predictors (columns 5 to 21) represent diverse factors that may influence plant richness such as sampling bias, the area of the ecoregion, climatic variables, human presence and impact, topography, geographical fragmentation, and features of the neighbors of each ecoregion. The figure below shows the scatterplots of the response variable (y axis) against each predictor (x axis).\nThe function plot_training_df_moran() helps to check the spatial autocorrelation of the response variable and the predictors.\nFinding promising variable interactions Random Forests already takes into account variable interactions of the form “variable a becomes important when b is higher than x”. However, Random Forest can also take advantage of variable interactions of the form a * b, as they are commonly defined in regression models.\nThe function rf_interactions() tests all possible interactions among predictors by using each one of them in a separate model, and suggesting the ones with the higher potential contribution to the model’s R squared and the higher relative importance (presented as a percentage of the maximum importance of a variable in the model).\ninteractions \u0026lt;- rf_interactions( data = plant_richness_df, dependent.variable.name = dependent.variable.name, predictor.variable.names = predictor.variable.names ) ## Testing 10 candidate interactions. ## 2 potential interactions identified. ## ┌─────────────────────────┬───────────────────────┬────────────────┐ ## │ Interaction │ Importance (% of max) │ R2 improvement │ ## ├─────────────────────────┼───────────────────────┼────────────────┤ ## │ human_population_X_bias │ 100.0 │ 0.002 │ ## │ _area_km2 │ │ │ ## ├─────────────────────────┼───────────────────────┼────────────────┤ ## │ climate_bio1_average_X_ │ 81.7 │ 0 │ ## │ bias_area_km2 │ │ │ ## └─────────────────────────┴───────────────────────┴────────────────┘ Here rf_interactions() suggests several candidate interactions ordered by their impact on the model. The function cannot say whether an interaction makes sense, and it is up to the user to choose wisely whether to select an interaction or not.\nFor the sake of the example, I will choose climate_bio1_average_X_bias_area_km2, hypothesizing that ecoregions with higher area (bias_area_km2) and energy (represented by the annual temperature, climate_bio1_average) will have more species of vascular plants (this is just an example, many other rationales are possible when choosing between candidate interactions). The data required to add the interaction to the training data is in the output of rf_interactions().\n#adding interaction column to the training data plant_richness_df[, \u0026quot;climate_bio1_average_X_bias_area_km2\u0026quot;] \u0026lt;- interactions$columns[, \u0026quot;climate_bio1_average_X_bias_area_km2\u0026quot;] #adding interaction name to predictor.variable.names predictor.variable.names \u0026lt;- c(predictor.variable.names, \u0026quot;climate_bio1_average_X_bias_area_km2\u0026quot;) Reducing multicollinearity in the predictors The functions auto_cor() and auto_vif() help reduce redundancy in the predictors by using different criteria (bivariate R squared vs. variance inflation factor), while allowing the user to define an order of preference, which can be based either on domain expertise or on a quantitative assessment. The preference order is defined as a character vector in the preference.order argument of both functions, and does not need to include the names of all predictors, but just the ones the user would like to keep in the analysis.\nIn the example below I give preference to the interaction suggested by rf_interactions() over it’s two components, and prioritize climate over other types of predictors (any other choice would be valid, it just depends on the scope of the study). These rules are applied to both auto_cor() and auto_vif(), that are executed sequentially by using the %\u0026gt;% pipe from the magrittr package.\nNotice that I have set cor.threshold and vif.threshold to low values because the predictors in plant_richness_df already have little multicollinearity,. The default values (cor.threshold = 0.75 and vif.threshold = 5) should work well when combined together for any other set of predictors.\npreference.order \u0026lt;- c( \u0026quot;climate_bio1_average_X_bias_area_km2\u0026quot;, \u0026quot;climate_aridity_index_average\u0026quot;, \u0026quot;climate_hypervolume\u0026quot;, \u0026quot;climate_bio1_average\u0026quot;, \u0026quot;climate_bio15_minimum\u0026quot;, \u0026quot;bias_area_km2\u0026quot; ) predictor.variable.names \u0026lt;- auto_cor( x = plant_richness_df[, predictor.variable.names], cor.threshold = 0.6, preference.order = preference.order ) %\u0026gt;% auto_vif( vif.threshold = 2.5, preference.order = preference.order ) ## [auto_cor()]: Removed variables: bias_area_km2, human_footprint_average ## [auto_vif()]: Removed variables: human_population The output of auto_cor() or auto_vif() is of the class “variable_selection”, that can be used as input for the argument predictor.variable.names of any modeling function within the package. An example is shown in the next section.\nFitting a non-spatial Random Forest model To fit basic Random Forest models spatialRF provides the rf() function. It takes the training data, the names of the response and the predictors, and optionally (to assess the spatial autocorrelation of the residuals), the distance matrix, and a vector of distance thresholds (in the same units as the distances in distance_matrix).\nThese distance thresholds are the neighborhoods at which the model will check the spatial autocorrelation of the residuals. Their values may depend on the spatial scale of the data, and the ecological system under study.\nNotice that here I plug the object predictor.variable.names, output of auto_cor() and auto_vif(), directly into the predictor.variable.names argument.\nmodel.non.spatial \u0026lt;- rf( data = plant_richness_df, dependent.variable.name = dependent.variable.name, predictor.variable.names = predictor.variable.names, distance.matrix = distance_matrix, distance.thresholds = c(0, 1500, 3000), seed = 100, #for reproducibility verbose = FALSE ) The model output can be printed or plotted with a plethora of functions such as print(), print_importance(), print_performance(), plot_importance(), print_moran(), plot_moran(), plot_response_curves(), or plot_response_surfaces), among many others.\nplot_response_curves(model.non.spatial) In the response curves above, the other predictors are set to their quantiles 0.1, 0.5, and 0.8, but the user can change this behavior by modifying the values of the quantiles argument.\nplot_response_surfaces( x = model.non.spatial, a = \u0026quot;climate_bio1_average\u0026quot;, b = \u0026quot;neighbors_count\u0026quot; ) In this response surface, the predictors that are not shown are set to their medians (but other quantiles are possible).\nplot_importance(model.non.spatial, verbose = FALSE) Predicting onto new data\nModels fitted with rf() and other rf_X() functions within the package can be predicted onto new data just as it is done with ranger() models:\npredicted \u0026lt;- stats::predict( object = model.non.spatial, data = plant_richness_df, type = \u0026quot;response\u0026quot; )$predictions Repeating a model execution\nRandom Forest is an stochastic algorithm that yields slightly different results on each run unless a random seed is set. This particularity has implications for the interpretation of variable importance scores. For example, in the plot above, the difference in importance between the predictors climate_hypervolume and climate_bio1_average_X_bias_area_km2 could be just the result of chance. The function rf_repeat() repeats a model execution and yields the distribution of importance scores of the predictors across executions.\nmodel.non.spatial.repeat \u0026lt;- rf_repeat( model = model.non.spatial, repetitions = 30, verbose = FALSE ) plot_importance(model.non.spatial.repeat, verbose = FALSE) After 30 model repetitions it is clear that the difference in importance between climate_hypervolume and climate_bio1_average_X_bias_area_km2 is not the result of chance.\nThe response curves of models fitted with rf_repeat() can be plotted with plot_response_curves() as well. The median prediction is shown with a thicker line.\nplot_response_curves( model.non.spatial.repeat, quantiles = 0.5 ) The function get_response_curves() returns a data frame with the data required to make custom plots of the response curves.\nplot.df \u0026lt;- get_response_curves(model.non.spatial.repeat) response predictor quantile model predictor.name response.name 1347.937 -183.8091 0.1 1 climate_bio1_average richness_species_vascular 1347.937 -181.5008 0.1 1 climate_bio1_average richness_species_vascular 1347.937 -179.1924 0.1 1 climate_bio1_average richness_species_vascular 1347.937 -176.8841 0.1 1 climate_bio1_average richness_species_vascular 1347.937 -174.5758 0.1 1 climate_bio1_average richness_species_vascular 1347.937 -172.2675 0.1 1 climate_bio1_average richness_species_vascular 1347.937 -169.9592 0.1 1 climate_bio1_average richness_species_vascular 1347.937 -167.6509 0.1 1 climate_bio1_average richness_species_vascular 1347.937 -165.3426 0.1 1 climate_bio1_average richness_species_vascular 1347.937 -163.0343 0.1 1 climate_bio1_average richness_species_vascular Tuning Random Forest hyperparameters The model fitted above was based on the default hyperparameter values provided by ranger(), and those might not be the most adequate ones for a given dataset. The function rf_tuning() helps the user to choose sensible values for three Random Forest hyperparameters that are critical to model performance:\nnum.trees: number of regression trees in the forest. mtry: number of variables to choose from on each tree split. min.node.size: minimum number of cases on a terminal node. Model tuning can be done on out-of-bag (method = \"oob\") or spatial cross-validation (method = \"spatial.cv\") R squared values. The example below shows the out-of-bag approach because I will explain spatial cross-validation with rf_evaluate() later in this document.\nmodel.non.spatial.tuned \u0026lt;- rf_tuning( model = model.non.spatial, method = \u0026quot;spatial.cv\u0026quot;, xy = plant_richness_df[, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)], repetitions = 30, num.trees = c(500, 1000), mtry = seq( 2, 14, #equal or lower than the number of predictors by = 3 ), min.node.size = c(5, 10, 20) ) ## Exploring 30 combinations of hyperparameters. ## Best hyperparameters: ## - num.trees: 1000 ## - mtry: 14 ## - min.node.size: 5 ## R squared gain: 0.029 The function rf_tuning() returns a model fitted with the same data as the original model, but using the best hyperparameters found during tuning. Model tuning has helped to a very small improvement in performance measures (+ 0.029 R squared), so from here, we can keep working with model.non.spatial.tuned.\nFitting a spatial model The spatial autocorrelation of the residuals of model.non.spatial, measured with Moran’s I, can be plotted with plot_moran().\nplot_moran(model.non.spatial.tuned, verbose = FALSE) According to the plot, the spatial autocorrelation of the residuals is highly positive for a neighborhood of 0 km, while it becomes non-significant (p-value \u0026gt; 0.05, whatever that means) at 1500 and 3000 km. To reduce the spatial autocorrelation of the residuals as much as possible, the non-spatial tuned model fitted above can be converted into a spatial model easily with rf_spatial(), that by default uses the Moran’s Eigenvector Maps method.\nmodel.spatial \u0026lt;- rf_spatial( model = model.non.spatial.tuned, method = \u0026quot;mem.moran.sequential\u0026quot;, #default method verbose = FALSE, seed = 100 ) The plot below shows the Moran’s I of the residuals of the spatial model. It shows that rf_spatial() has managed to remove the spatial autocorrelation (p-values of the Moran’s I estimates for each neighborhood distance are higher than 0.05) of the model residuals for every neighborhood distance.\nplot_moran(model.spatial, verbose = FALSE) When comparing the variable importance plots of both models, we can see that the spatial model has an additional set of dots under the name “spatial_predictors”, and that the maximum importance of a few of these spatial predictors matches the importance of the most relevant non-spatial predictors.\np1 \u0026lt;- plot_importance( model.non.spatial, verbose = FALSE) + ggplot2::ggtitle(\u0026quot;Non-spatial model\u0026quot;) p2 \u0026lt;- plot_importance( model.spatial, verbose = FALSE) + ggplot2::ggtitle(\u0026quot;Spatial model\u0026quot;) p1 | p2 A few of the ten most important variables in model.spatial are spatial predictors.\nvariable importance climate_bio1_average_X_bias_area_km2 0.151 spatial_predictor_0_2 0.147 climate_hypervolume 0.140 climate_bio1_average 0.132 bias_species_per_record 0.080 spatial_predictor_0_1 0.064 spatial_predictor_3000_1 0.057 spatial_predictor_0_6 0.053 spatial_predictor_0_5 0.045 human_population_density 0.041 Spatial predictors are named spatial_predictor_X_Y, where X is the neighborhood distance at which the predictor has been generated, and Y is the index of the predictor.\nSpatial predictors, as shown below, are smooth surfaces representing neighborhood among records at different spatial scales.\nThe spatial predictors in the spatial model have been generated using the method “mem.moran.sequential” (function’s default), that mimics the Moran’s Eigenvector Maps method described in (Dray, Legendre, and Peres-Neto 2006).\nIn brief, the method consist on transforming the distance matrix into a double-centered matrix of normalized weights, to then compute the positive eigenvectors of the weights matrix (a.k.a, Moran’s Eigenvector Maps, or MEMs).\nThe MEMs are included in the model one by one in the order of their Moran’s I, and the subset of MEMs maximizing the model’s R squared and minimizing the Moran’s I of the residuals and the number of MEMs added to the model are selected, as shown in the optimization plot below (dots linked by lines represent the selected spatial predictors). The selection procedure is performed by the function select_spatial_predictors_sequential().\nTuning spatial models\nSpatial models fitted with rf_spatial() can be tuned as well with rf_tuning(). However, tuning may in some cases increase the spatial autocorrelation of the model residuals. In that case, the function will return a message explaining the situation, and the original model without any sort of tuning applied\nmodel.spatial.tuned \u0026lt;- rf_tuning( model = model.spatial, method = \u0026quot;spatial.cv\u0026quot;, xy = plant_richness_df[, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)], repetitions = 30, num.trees = c(500, 1000), mtry = seq( 2, length(model.spatial$ranger.arguments$predictor.variable.names), by = 9), min.node.size = c(5, 20) ) ## Exploring 24 combinations of hyperparameters. ## Best hyperparameters: ## - num.trees: 1000 ## - mtry: 47 ## - min.node.size: 5 ## R squared gain: 0.016 Assessing model performance on spatially independent folds The function rf_evaluate() separates the training data into a number of spatially independent training and testing folds, fits a model on each training fold, predicts over each testing fold, and computes performance measures, to finally aggregate them across model repetitions. Let’s see how it works.\nmodel.spatial.tuned \u0026lt;- rf_evaluate( model = model.spatial.tuned, xy = plant_richness_df[, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)], #data coordinates repetitions = 30, #number of folds training.fraction = 0.8, #training data fraction metrics = c(\u0026quot;r.squared\u0026quot;, \u0026quot;rmse\u0026quot;), verbose = FALSE ) The function generates a new slot in the model named “evaluation” with several objects that summarize the spatial cross-validation results.\nnames(model.spatial.tuned$evaluation) ## [1] \u0026quot;metrics\u0026quot; \u0026quot;training.fraction\u0026quot; \u0026quot;spatial.folds\u0026quot; ## [4] \u0026quot;per.fold\u0026quot; \u0026quot;per.fold.long\u0026quot; \u0026quot;per.model\u0026quot; ## [7] \u0026quot;aggregated\u0026quot; The slot “spatial.folds”, produced by make_spatial_folds(), contains the indices of the training and testing cases for each cross-validation repetition. The maps below show two sets of training and testing spatial folds.\nThe functions plot_evaluation() and print_evaluation() allow to check the evaluation results as a plot or as a table.\nprint_evaluation(model.spatial.tuned) ## ## Spatial evaluation ## - Training fraction: 0.8 ## - Spatial folds: 25 ## ## Metric Mean Standard deviation Minimum Maximum ## r.squared 0.250 0.159 0.080 0.601 ## rmse 3223.499 810.886 2285.215 4748.118 The low R squared yielded by the model evaluation shows that the spatial model is hard to transfer outside of the training space. Models based on a spatial structure like the ones fitted with rf_spatial() do not work well when transferred to a different place (that is what rf_compare() does), because spatial structures are not transferable when the data is irregularly distributed, as it is the case with plant_richness_df. The comparison below shows how non-spatial models may show better (not bad, not great) evaluation scores on independent spatial folds.\nComparing several models The function rf_evaluate() only assesses the predictive performance on unseen data of one model at a time. If the goal is to compare two models, rf_evaluate() can be indeed ran twice, but spatialRF offers a more convenient option named rf_compare(). It takes as input a named list with as many models as the user needs to compare.\ncomparison \u0026lt;- rf_compare( models = list( `Non-spatial` = model.non.spatial, `Non-spatial tuned` = model.non.spatial.tuned, `Spatial` = model.spatial, `Spatial tuned` = model.spatial.tuned ), xy = plant_richness_df[, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)], repetitions = 30, training.fraction = 0.8, metrics = c(\u0026quot;r.squared\u0026quot;, \u0026quot;rmse\u0026quot;), notch = TRUE ) Model Metric Mean Non-spatial r.squared 0.336 Non-spatial tuned r.squared 0.412 Spatial r.squared 0.167 Spatial tuned r.squared 0.217 Non-spatial rmse 2817.225 Non-spatial tuned rmse 2329.933 Spatial rmse 3086.170 Spatial tuned rmse 2930.469 Generating spatial predictors for other models You might not love Random Forest, but spatialRF loves you, and as such, it gives you tools to generate spatial predictors for other models anyway.\nThe first step requires generating Moran’s Eigenvector Maps (MEMs) from the distance matrix. Here there are two options, computing MEMs for a single neighborhood distance with mem(), and computing MEMs for several neighborhood distances at once with mem_multithreshold().\n#single distance (0km by default) mems \u0026lt;- mem(x = distance_matrix) #several distances mems \u0026lt;- mem_multithreshold( x = distance_matrix, distance.thresholds = c(0, 1000, 2000) ) In either case the result is a data frame with Moran’s Eigenvector Maps (“just” the positive eigenvectors of the double-centered distance matrix).\nspatial_predictor_0_1 spatial_predictor_0_2 spatial_predictor_0_3 spatial_predictor_0_4 0.0259217 0.0052203 0.0416969 -0.0363324 0.0996679 0.0539713 0.1324480 0.3826928 0.0010477 -0.0143046 -0.0443602 -0.0031386 0.0165695 0.0047991 0.0307457 0.0005170 0.0225761 0.0019595 0.0230368 -0.0524239 0.0155252 0.0023742 0.0197953 -0.0338956 0.0229197 0.0039860 0.0312561 -0.0416697 -0.2436009 -0.1155295 0.0791452 0.0189996 0.0150725 -0.0158684 -0.1010284 0.0095590 -0.1187381 -0.0471879 0.0359881 0.0065211 But not all MEMs are made equal, and you will need to rank them by their Moran’s I. The function rank_spatial_predictors() will help you do so.\nmem.rank \u0026lt;- rank_spatial_predictors( distance.matrix = distance_matrix, spatial.predictors.df = mems ) The output of rank_spatial_predictors() is a list with three slots: “method”, a character string with the name of the ranking method; “criteria”, an ordered data frame with the criteria used to rank the spatial predictors; and “ranking”, a character vector with the names of the spatial predictors in the order of their ranking (it is just the first column of the “criteria” data frame). We can use this “ranking” object to reorder or mems data frame.\nmems \u0026lt;- mems[, mem.rank$ranking] #also: #mems \u0026lt;- mem.rank$spatial.predictors.df From here, spatial predictors can be included in any model one by one, in the order of the ranking, until the spatial autocorrelation of the residuals is gone, or our model gets totally defaced. A little example with a linear model follows.\n#model definition predictors \u0026lt;- c( \u0026quot;climate_aridity_index_average \u0026quot;, \u0026quot;climate_bio1_average\u0026quot;, \u0026quot;bias_species_per_record\u0026quot;, \u0026quot;human_population_density\u0026quot;, \u0026quot;topography_elevation_average\u0026quot;, \u0026quot;fragmentation_division\u0026quot; ) model.formula \u0026lt;- as.formula( paste( dependent.variable.name, \u0026quot; ~ \u0026quot;, paste( predictors, collapse = \u0026quot; + \u0026quot; ) ) ) #scaling the data model.data \u0026lt;- scale(plant_richness_df) %\u0026gt;% as.data.frame() #fitting the model m \u0026lt;- lm(model.formula, data = plant_richness_df) #Moran\u0026#39;s I test of the residuals moran.test \u0026lt;- moran( x = residuals(m), distance.matrix = distance_matrix, ) moran.test ## moran.i p.value interpretation ## 1 0.21 0 Positive spatial correlation According to the Moran’s I test, the model residuals show spatial autocorrelation. Let’s introduce MEMs one by one until the problem is solved.\n#add mems to the data and applies scale() model.data \u0026lt;- data.frame( plant_richness_df, mems ) %\u0026gt;% scale() %\u0026gt;% as.data.frame() #initialize predictors.i predictors.i \u0026lt;- predictors #iterating through MEMs for(mem.i in colnames(mems)){ #add mem name to model definintion predictors.i \u0026lt;- c(predictors.i, mem.i) #generate model formula with the new spatial predictor model.formula.i \u0026lt;- as.formula( paste( dependent.variable.name, \u0026quot; ~ \u0026quot;, paste( predictors.i, collapse = \u0026quot; + \u0026quot; ) ) ) #fit model m.i \u0026lt;- lm(model.formula.i, data = model.data) #Moran\u0026#39;s I test moran.test.i \u0026lt;- moran( x = residuals(m.i), distance.matrix = distance_matrix, ) #stop if no autocorrelation if(moran.test.i$interpretation != \u0026quot;Positive spatial correlation\u0026quot;){ break } }#end of loop Now we can compare the model without spatial predictors m and the model with spatial predictors m.i.\nModel Predictors R_squared AIC BIC Moran.I Non-spatial 6 0.38 4238 4266 0.21 Spatial 21 0.50 530 608 0.06 According to the model comparison, it can be concluded that the addition of spatial predictors, in spite of the increase in complexity, has improved the model. In any case, this is just a simple demonstration of how spatial predictors generated with functions of the spatialRF package can still help you fit spatial models with other modeling methods.\n","date":1608336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608336000,"objectID":"ed56ca7b8f4312cdaf9dea013d8dea1c","permalink":"https://blasbenito.com/project/spatialrf/","publishdate":"2020-12-19T00:00:00Z","relpermalink":"/project/spatialrf/","section":"project","summary":"R package for spatial regression with Random Forest","tags":["R packages","Spatial Regression","Time Series Analysis","Machine Learning","Random Forest"],"title":"R package \"spatialRF\"","type":"project"},{"authors":null,"categories":null,"content":" The goal of virtualPollen is to provide the tools to simulate pollen curves over millenial time-scales generated by virtual taxa with different life traits (life-span, fecundity, growth-rate) and niche features (niche position and breadth) as a response to virtual environmental drivers with a given temporal autocorrelation. It furthers allow to simulate specific data properties of fossil pollen datasets, such as sediment accumulation rate, and depth intervals between consecutive pollen samples. The simulation outcomes are useful to better understand the role of plant traits, niche properties, and climatic variability in defining the shape of pollen curves.\nThe GitHub page of the package offers a complete tutorial on how to use the package. The paper published in the Ecography journal describes the foundations of the model in brief.\nIf you ever use the package, please, cite it as:\nBenito, B.M., Gil‐Romera, G. and Birks, H.J.B. (2020), Ecological memory at millennial time‐scales: the importance of data constraints, species longevity and niche features. Ecography, 43: 1-10. https://doi.org/10.1111/ecog.04772\n","date":1608336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608336000,"objectID":"f3afdae9d91edee9e3abcdfb020c263f","permalink":"https://blasbenito.com/project/virtualpollen/","publishdate":"2020-12-19T00:00:00Z","relpermalink":"/project/virtualpollen/","section":"project","summary":"R package to simulate pollen production of mono-specific tree populations over millennia.","tags":["R packages","Time Series Analysis","Palaeoecology","Mechanistic simulation"],"title":"R package \"vitualPollen\"","type":"project"},{"authors":null,"categories":null,"content":"In this post I explain how to setup a small Beowulf cluster with a personal PC running Ubuntu 20.04 and a couple of Intel NUCs running Ubuntu Server 20.04, with the end-goal of parallelizing R tasks.\nThe topics I cover here are:\nRequired material Network setting Installing the secure shell protocol Installing Ubuntu server in the NUCs Installing R in the NUCs Managing the cluster\u0026rsquo;s network Preamble I have a little but nice HP ENVY model TE01-0008ns with 32 GB RAM, 8 CPUs, and 3TB of hard disk running Ubuntu 20.04 that I use to do all my computational work (and most of my tweeting). A few months ago I connected it with my two laptops (one of them deceased now, RIP my dear skynet) to create a little cluster to run parallel tasks in R.\nIt was just a draft cluster running on a wireless network, but it served me to think about getting a more permanent solution not requiring two additional laptops in my desk.\nThat\u0026rsquo;s were the nice INTEL NUCs (from Next Unit of Computing) come into play. NUCs are full-fledged computers fitted in small boxes usually sold without RAM memory sticks and no hard disk (hence the term barebone). Since they have a low energy consumption footprint, I thought these would be ideal units for my soon-to-be home cluster.\nMaterial I gifted myself with:\n2 Intel Barebone BOXNUC6CAYH, each with 4 cores, and a maximum RAM memory of 32GB (you might read they only accept 8GB, but that\u0026rsquo;s not the case anymore). Notice that these NUCs aren\u0026rsquo;t state-of-the-art now, they were released by the end of 2016. 2 Hard disks SSD 2.5\u0026quot; Western Digital WDS250G2B0A WD Blue (250GB) 4 Crucial CT102464BF186D DDR3 SODIMM (204 pins) RAM sticks with 8GB each. 1 ethernet switch Netgear GS308-300PES with 8 ports. 3 ethernet wires NanoCable 10.20.0400-BL of cat 6 quality. The whole set came to cost around 530€, but please notice that I had a clear goal in mind: \u0026ldquo;duplicating\u0026rdquo; my computing power with the minimum number of NUCs, while preserving a share of 4GB of RAM memory per CPU throughout the cluster (based on the features of my desk computer). A more basic setting with more modest NUCs and smaller RAM would cost half of that.\nThis instructive video by David Harry shows how to install the SSD and the RAM sticks in an Intel NUC. It really takes 5 minutes tops, one only has to be a bit careful with the RAM sticks, the pins need to go all the way in into their slots before securing the sticks in place.\nNetwork settings Before starting to install an operating system in the NUCS, the network setup goes as follows:\nMy desktop PC is connected to a router via WIFI and dynamic IP (DHCP). The PC and each NUC are connected to the switch with cat6 ethernet wires. To share my PC\u0026rsquo;s WIFI connection with the NUCs I have to prepare a new connection profile with the command line tool of Ubuntu\u0026rsquo;s NetworkManager, named nmcli, as follows.\nFirst, I need to find the name of my ethernet interface by checking the status of my network devices with the command line.\nnmcli device status DEVICE TYPE STATE CONNECTION wlp3s0 wifi connected my_wifi enp2s0 ethernet unavailable -- lo loopback unmanaged -- There I can see that my ethernet interface is named enp2s0.\nSecond, I have to configure the shared connection.\nnmcli connection add type ethernet ifname enp2s0 ipv4.method shared con-name cluster Were ifname enp2s0 is the name of the interface I want to use for the new connection, ipv4.method shared is the type of connection, and con-name cluster is the name I want the connection to have. This operation adds firewall rules to manage traffic within the cluster network, starts a DHCP server in the computer that serves IPs to the NUCS, and a DNS server that allows the NUCs to translate internet addresses.\nAfter turning on the switch, I can check the connection status again with\nnmcli device status DEVICE TYPE STATE CONNECTION enp2s0 ethernet connected cluster wlp3s0 wifi connected my_wifi lo loopback unmanaged -- When checking the IP of the device with bash ifconfig it should yield 10.42.0.1. Any other computer in the cluster network will have a dynamic IP in the range 10.42.0.1/24.\nFurther details about how to set a shared connection with NetworkManager can be found in this nice post by Beniamino Galvani.\nSSH setup My PC, as the director of the cluster, needs an SSH client running, while the NUCs need an SSH server. SSH (Secure Shell) is a remote authentication protocol that allows secure connections to remote servers that I will be using all the time to manage the cluster. To install, run, and check its status I just have to run these lines in the console:\nsudo apt install ssh sudo systemctl enable --now ssh sudo systemctl status ssh Now, a secure certificate of the identity of a given computer, named ssh-key, that grants access to remote ssh servers and services needs to be generated.\nssh-keygen \u0026quot;label\u0026quot; Here, substitute \u0026ldquo;label\u0026rdquo; by the name of the computer to be used as cluster\u0026rsquo;s \u0026ldquo;director\u0026rdquo;. The system will ask for a file name and a passphrase that will be used to encrypt the ssh-key.\nThe ssh-key needs to be added to the ssh-agent.\nssh-add ~/.ssh/id_rsa To copy the ssh-key to my GitHub account, I have to copy the contents of the file ~/.ssh/id_rsa.pub (can be done just opening it with gedit ~/.ssh/id_rsa.pub + Ctrl + a + Ctrl + c), and paste it on GitHub account \u0026gt; Settings \u0026gt; SSH and GPG keys \u0026gt; New SSH Key (green button in the upper right part of the window).\nNote: If you don\u0026rsquo;t use GitHub, you\u0026rsquo;ll need to copy your ssh-key to the NUCs once they are up and running with ssh-copy-id -i ~/.ssh/id_rsa.pub user_name@nuc_IP.\nInstalling and preparing ubuntu server in each NUC The NUCs don\u0026rsquo;t need to waste resources in a user graphical interface I won\u0026rsquo;t be using whatsoever. Since they will work in a headless configuration once the cluster is ready, a Linux distro without graphical user interface such as Ubuntu server is the way to go.\nInstalling Ubuntu server First it is important to connect a display, a keyboard, and a mouse to the NUC in preparation, and turn it on while pushing F2 to start the visual BIOS. These BIOS parameters need to be modified:\nAdvanced (upper right) \u0026gt; Boot \u0026gt; Boot Configuration \u0026gt; UEFI Boot \u0026gt; OS Selection: Linux Advanced \u0026gt; Boot \u0026gt; Boot Configuration \u0026gt; UEFI Boot \u0026gt; OS Selection: mark \u0026ldquo;Boot USB Devices First\u0026rdquo;. [optional] Advanced \u0026gt; Power \u0026gt; Secondary Power Settings \u0026gt; After Power Failure: \u0026ldquo;Power On\u0026rdquo;. I have the switch and nucs connected to an outlet plug extender with an interrupter. When I switch it on, the NUCs (and the switch) boot automatically after this option is enabled, so I only need to push one button to power up the cluster. F10 to save, and shutdown. To prepare the USB boot device with Ubuntu server 20.04 I first download the .iso from here, by choosing \u0026ldquo;Option 3\u0026rdquo;, which leads to the manual install. Once the .iso file is downloaded, I use Ubuntu\u0026rsquo;s Startup Disk Creator to prepare a bootable USB stick. Now I just have to plug the stick in the NUC and reboot it.\nThe Ubuntu server install is pretty straightforward, and only a few things need to be decided along the way:\nAs user name I choose the same I have in my personal computer. As name for the NUCs I choose \u0026ldquo;nuc1\u0026rdquo; and \u0026ldquo;nuc2\u0026rdquo;, but any other option will work well. As password, for comfort I use the same I have in my personal computer. During the network setup, choose DHCP. If the network is properly configured and the switch is powered on, after a few seconds the NUC will acquire an IP in the range 10.42.0.1/24, as any other machine within the cluster network. When asked, mark the option \u0026ldquo;Install in the whole disk\u0026rdquo;, unless you have other plans for your NUC. Mark \u0026ldquo;Install OpenSSH\u0026rdquo;. Provide it with your GitHub user name if you have your ssh-key there, and it will download it right away, facilitating a lot the ssh setup. Reboot once the install is completed. Now I keep configuring the NUC\u0026rsquo;s operating system from my PC through ssh.\nConfiguring a NUC First, to learn the IP of the NUC:\nsudo arp-scan 10.42.0.1/24 Other alternatives to this command are arp -a and sudo arp-scan -I enp2s0 --localnet. Once I learn the IP of the NUC, I add it to the file etc/hosts of my personal computer as follows.\nFirst I open the file as root.\nsudo gedit /etc/hosts Add a new line there: 10.42.0.XXX nuc1 and save the file.\nNow I access the NUC trough ssh to keep preparing it without a keyboard and a display. I do it from Tilix, that allows to open different command line tabs in the same window, which is quite handy to manage several NUCs at once.\nAnother great option to manage the NUCs through ssh is terminator, that allows to broadcast the same commands to several ssh sessions at once. I have been trying it, and it is much better for cluster management purposes than Tilix. Actually, using it would simplify this workflow a lot, because once Ubuntu server is installed on each NUC, the rest of the configuration commands can be broadcasted at once to both NUCs. It\u0026rsquo;s a bummer I discovered this possibility way too late!\nssh blas@10.42.0.XXX The NUC\u0026rsquo;s operating system probably has a bunch of pending software updates. To install these:\nsudo apt-get upgrade Now I have to install a set of software packages that will facilitate managing the cluster\u0026rsquo;s network and the NUC itself.\nsudo apt install net-tools arp-scan lm-sensors dirmngr gnupg apt-transport-https ca-certificates software-properties-common samba libopenmpi3 libopenmpi-dev openmpi-bin openmpi-common htop Setting the system time To set the system time of the NUC to the same you have in your computer, just repeat these steps in every computer in the cluster network.\n#list time zones: timedatectl list-timezones #set time zone sudo timedatectl set-timezone Europe/Madrid #enable timesyncd sudo timedatectl set-ntp on Setting the locale The operating systems of the NUCs and the PC need to have the same locale. It can be set by editing the file /etc/default/locale with either nano (in the NUCS) or gedit (in the PC) and adding these lines, just replacing en_US.UTF-8 with your preferred locale.\nLANG=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLANGUAGE=\u0026ldquo;en_US:en\u0026rdquo;\nLC_NUMERIC=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_TIME=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_MONETARY=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_PAPER=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_IDENTIFICATION=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_NAME=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_ADDRESS=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_TELEPHONE=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nLC_MEASUREMENT=\u0026ldquo;en_US.UTF-8\u0026rdquo;\nTemperature monitoring NUCs are prone to overheating when under heavy loads for prolonged times. Therefore, monitoring the temperature of the NUCs CPUs is kinda important. In a step before I installed lm-sensors in the NUC, which provides the tools to do so. To setup the sensors from an ssh session in the NUC:\nsudo sensors-detect The program will request permission to find sensors in the NUC. I answered \u0026ldquo;yes\u0026rdquo; to every request. Once all sensors are identified, to check them\nsensors iwlwifi_1-virtual-0 Adapter: Virtual device temp1: N/A acpitz-acpi-0 Adapter: ACPI interface temp1: +32.0°C (crit = +100.0°C) coretemp-isa-0000 Adapter: ISA adapter Package id 0: +30.0°C (high = +105.0°C, crit = +105.0°C) Core 0: +30.0°C (high = +105.0°C, crit = +105.0°C) Core 1: +30.0°C (high = +105.0°C, crit = +105.0°C) Core 2: +29.0°C (high = +105.0°C, crit = +105.0°C) Core 3: +30.0°C (high = +105.0°C, crit = +105.0°C) which gives the cpu temperatures at the moment the command was executed. The command watch sensors gives continuous temperature readings instead.\nTo control overheating in my NUCs I removed their top lids, and installed them into a custom LEGO \u0026ldquo;rack\u0026rdquo; with external USB fans with velocity control, as shown in the picture at the beginning of the post.\nInstalling R To install R in the NUCs I just proceed as I would when installing it in my personal computer. There is a thorough guide here.\nIn a step above I installed all the pre-required software packages. Now I only have to add the security key of the R repository, add the repository itself, update the information on the packages available in the new repository, and finally install R.\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 sudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/' sudo apt update sudo apt install r-base Note: If R has issues to recognize the system locale\nnano ~/.profile add the following lines, replacing en_US.UTF-8 with your preferred locale\nexport LANG=en_US.UTF-8 export LC_ALL=en_US.UTF-8\nsave, and execute the file to export the locale so R can read it.\n. ~/.profile Finalizing the network configuration Each NUC needs firewall rules to grant access from other computers withinn the cluster network. To activate the NUC\u0026rsquo;s firewall and check what ports are open:\nsudo ufw enable sudo ufw status To grant access from the PC to the NUC through ssh, and later through R for parallel computing, the ports 22 and 11000 must be open for the IP of the PC (10.42.0.1).\nsudo ufw allow ssh sudo ufw allow from 10.42.0.1 to any port 11000 sudo ufw allow from 10.42.0.1 to any port 22 Finally, the other members of the cluster network must be declared in the /etc/hosts file of each computer.\nIn each NUC edit the file through ssh with bash sudo nano /etc/hosts and add the lines\n10.42.0.1 pc_name\n10.42.0.XXX name_of_the_other_nuc\nIn the PC, add the lines\n10.42.0.XXX name_of_one_nuc\n10.42.0.XXX name_of_the_other_nuc\nAt this point, after rebooting every machine, the NUCs must be accessible through ssh by using their names (ssh username@nuc_name) instead of their IPs (ssh username@n10.42.0.XXX). Just take in mind that, since the cluster network works with dynamic IPs (and such setting cannot be changed in a shared connection), the IPs of the NUCs might change if a new device is added to the network. That\u0026rsquo;s something you need to check from the PC with sudo arp-scan 10.42.0.1/24, to update every /etc/hosts file accordingly.\nI think that\u0026rsquo;s all folks. Good luck setting your home cluster! Next time I will describe how to use it for parallel computing in R.\n","date":1607299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607299200,"objectID":"ead52a439ba25afd3dd271401b5f5734","permalink":"https://blasbenito.com/post/01_home_cluster/","publishdate":"2020-12-07T00:00:00Z","relpermalink":"/post/01_home_cluster/","section":"post","summary":"In this post I explain how to setup a small Beowulf cluster with a personal PC running Ubuntu 20.04 and a couple of Intel NUCs running Ubuntu Server 20.04, with the end-goal of parallelizing R tasks.","tags":null,"title":"Setting up a home cluster","type":"post"},{"authors":["Masahiro Ryo","Boyan Angelov","Stefano Mammola","Jamie M. Kass","Blas M. Benito","Florian Hartig"],"categories":null,"content":"","date":1605571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605571200,"objectID":"a92328360ad8f6c7395efe736f4f34e3","permalink":"https://blasbenito.com/publication/2020_ryo_ecography/","publishdate":"2020-11-17T00:00:00Z","relpermalink":"/publication/2020_ryo_ecography/","section":"publication","summary":"Here we draw attention to an emerging subdiscipline of artificial intelligence, explainable AI (xAI), as a toolbox for better interpreting SDMs. xAI aims at deciphering the behavior of complex statistical or machine learning models (e.g. neural networks, random forests, boosted regression trees), and can produce more transparent and understandable SDM predictions.","tags":["Species Distribution Models","Explainable Artificial Intelligence (xAI)"],"title":"Explainable artificial intelligence enhances the ecological interpretability of black‐box species distribution models","type":"publication"},{"authors":["Andrea Contina","Scott W. Yanco","Allison K. Pierce","Michelle DePrenger-Levin","Michael B. Wunder","Andreas M. Neophytou","C. Phoebe Lostroh","Richard J. Telford","Blas M. Benito","Joseph Chipperfield","Robert B. O'Hara","Colin J. Carlson"],"categories":null,"content":"","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600646400,"objectID":"70fd84519cc7f0520ad6720d1a1a28e9","permalink":"https://blasbenito.com/publication/2020_contina_ecological_modelling/","publishdate":"2020-09-21T00:00:00Z","relpermalink":"/publication/2020_contina_ecological_modelling/","section":"publication","summary":"In this letter we present comments on the article “A global-scale ecological niche model to predict SARS-CoV-2 coronavirus” by Coro published in 2020.","tags":["Irresponsible Covid19 modelling","Species Distribution Models"],"title":"Comment on “A global-scale ecological niche model to predict SARS-CoV-2 coronavirus infection rate”, author Coro","type":"publication"},{"authors":["Colin J. Carlson","Joseph D. Chipperfield","Blas M. Benito","Richard J. Telford","Robert B. O'Hara"],"categories":null,"content":"","date":1595980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595980800,"objectID":"0e2abe2a1f21b5de5d214beb5a8e28a3","permalink":"https://blasbenito.com/publication/2020_carlson_nature_ecology_and_evolution/","publishdate":"2020-07-29T00:00:00Z","relpermalink":"/publication/2020_carlson_nature_ecology_and_evolution/","section":"publication","summary":"Araújo et al. have published a response to our piece ‘Species distribution models are inappropriate for COVID-19’1 entitled ‘Ecological and epidemiological models are both useful for SARS-CoV-2’2, in which they defend the idea that ecological models are likely to identify the signature of climate drivers in the R0 of COVID-19 transmission.","tags":["Irresponsible Covid19 modelling","Species Distribution Models"],"title":"Don’t gamble the COVID-19 response on ecological hypotheses","type":"publication"},{"authors":["Quai.Yu Cui","Marie-José Gaillard","Boris Vannière","Daniele Colombaroli","Geoffrey Lemdahl","Fredrik Olsson","Blas M. Benito","Yan Zhao"],"categories":null,"content":"","date":1594684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594684800,"objectID":"b0ae2a004a1861145db6167d8ae1d7f1","permalink":"https://blasbenito.com/publication/2020_cui_the_holocene/","publishdate":"2020-07-14T00:00:00Z","relpermalink":"/publication/2020_cui_the_holocene/","section":"publication","summary":"In this study, we assess how representative a single charcoal record from a peat profile in small bogs (1.5–2 ha in area) is for the reconstruction of Holocene fire history.","tags":["Palaeoecology","Time Series Analysis"],"title":"Evaluating fossil charcoal representation in small peat bogs: Detailed Holocene fire records from southern Sweden","type":"publication"},{"authors":["Constantin M. Zohner","Lidong Mo","Susanne S. Renner","Jens-Christian Svenning","Yann Vitasse","Blas M. Benito","Alejandro Ordonez","Frederik Baumgarten","Jean-François Bastin","Veronica Sebald","Peter B. Reich","Jingjing Liang","Gert-Jan Nabuurs","Sergio de-Miguel","Giorgio Alberti","Clara Antón-Fernández","Radomir Balazy","Urs-Beat Brändli","Han Y. H. Chen","Chelsea Chisholm","Emil Cienciala","Selvadurai Dayanandan","Tom M. Fayle","Lorenzo Frizzera","Damiano Gianelle","Andrzej M. Jagodzinski","Bogdan Jaroszewicz","Tommaso Jucker","Sebastian Kepfer-Rojas","Mohammed Latif Khan","Hyun Seok Kim","Henn Korjus","Vivian Kvist Johannsen","Diana Laarmann","Mait Lang","Tomasz Zawila-Niedzwiecki","Pascal A. Niklaus","Alain Paquette","Hans Pretzsch","Purabi Saikia","Peter Schall","Vladimír Šebeň","Miroslav Svoboda","Elena Tikhonova","Helder Viana","Chunyu Zhang","Xiuhai Zhao","Thomas W. Crowther"],"categories":null,"content":"","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"a03c5f954b0fdb1d789e83f7b7821801","permalink":"https://blasbenito.com/publication/2020_zohner_pnas/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/publication/2020_zohner_pnas/","section":"publication","summary":"Frost in late spring causes severe ecosystem damage in temperate and boreal regions. We here analyze late-spring frost occurrences between 1959 and 2017 and woody species’ resistance strategies to forecast forest vulnerability under climate change. Leaf-out phenology and leaf-freezing resistance data come from up to 1,500 species cultivated in common gardens. The greatest increase in leaf-damaging spring frost has occurred in Europe and East Asia, where species are more vulnerable to spring frost than in North America. The data imply that 35 and 26% of Europe’s and Asia’s forests are increasingly threatened by frost damage, while this is only true for 10% of North America. Phenological strategies that helped trees tolerate past frost frequencies will thus be increasingly mismatched to future conditions.","tags":["Phenology","Biogeography","Climate Change","Plant Ecology"],"title":"Late-spring frost risk between 1959 and 2017 decreased in North America but increased in Europe and Asia","type":"publication"},{"authors":["Colin J. Carlson","Joseph D. Chipperfield","Blas M. Benito","Richard J. Telford","Robert B. O'Hara"],"categories":null,"content":"","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588723200,"objectID":"bb8b41d7db5a51c063f7a5e33c256d3f","permalink":"https://blasbenito.com/publication/2020_carlson_nature_ecology_and_evolution_b/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/publication/2020_carlson_nature_ecology_and_evolution_b/","section":"publication","summary":"Species distribution models are a powerful tool for ecological inference, but not every use is biologically justified. Applying these tools to the COVID-19 pandemic is unlikely to yield new insights, and could mislead policymakers at a critical moment.","tags":["Irresponsible Covid19 modelling","Species Distribution Models"],"title":"Species distribution models are inappropriate for COVID-19","type":"publication"},{"authors":["María Leunda","Graciela Gil-Romera","Anne-Laure Daniau","Blas M. Benito","Penélope González-Sampériz"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"48d1b269836f2685996b5ccae758bec5","permalink":"https://blasbenito.com/publication/2020_leunda_catena/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/2020_leunda_catena/","section":"publication","summary":"In this paper we aim to (1) reconstruct the Holocene fire history at high altitudes of the southern Central Pyrenees, (2) add evidence to the debate on fire origin, naturally or anthropogenically produced, (3) determine the importance of fire as a disturbance agent for sub-alpine and alpine vegetation, in comparison with the plant community internal dynamics.","tags":["Palaeoecology","Fire dynamics","Ecological Memory","Generalized Least Squares","Plant Ecology"],"title":"Holocene fire and vegetation dynamics in the Central Pyrenees (Spain)","type":"publication"},{"authors":["Felde, V. A.","...","Blas M. Benito","et al."],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"0945150d072317bd056f82234adf52f5","permalink":"https://blasbenito.com/publication/2020_felde_vegetation_history_and_archaeobotany/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/2020_felde_vegetation_history_and_archaeobotany/","section":"publication","summary":"The Eemian interglacial represents a natural experiment on how past vegetation with negligible human impact responded to amplified temperature changes compared to the Holocene. Here, we assemble 47 carefully selected Eemian pollen sequences from Europe to explore geographical patterns of (1) total compositional turnover and total variation for each sequence and (2) stratigraphical turnover between samples within each sequence using detrended canonical correspondence analysis, multivariate regression trees, and principal curves. Our synthesis shows that turnover and variation are highest in central Europe (47–55°N), low in southern Europe (south of 45°N), and lowest in the north (above 60°N). These results provide a basis for developing hypotheses about causes of vegetation change during the Eemian and their possible drivers.","tags":["Palaeoecology"],"title":"Compositional turnover and variation in Eemian pollen sequences in Europe","type":"publication"},{"authors":["Joseph D. Chipperfield","Robert B. O'Hara","Blas M. Benito","Richard J. Telford","Colin J. Carlson"],"categories":null,"content":"","date":1585353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585353600,"objectID":"33ce384276fda207d92a8cf37bd61e01","permalink":"https://blasbenito.com/publication/2020_chipperfield_ecoevorxiv/","publishdate":"2020-03-28T00:00:00Z","relpermalink":"/publication/2020_chipperfield_ecoevorxiv/","section":"publication","summary":"The ongoing pandemic of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is causing significant damage to public health and economic livelihoods, and is putting significant strains on healthcare services globally. This unfolding emergency has prompted the preparation and dissemination of the article “Spread of SARS-CoV-2 Coronavirus likely to be constrained by climate” by Araújo and Naimi (2020). The authors present the results of an ensemble forecast made from a suite of species distribution models (SDMs), where they attempt to predict the suitability of the climate for the spread of SARS-CoV-2 over the coming months. They argue that climate is likely to be a primary regulator for the spread of the infection and that people in warm-temperate and cold climates are more vulnerable than those in tropical and arid climates. A central finding of their study is that the possibility of a synchronous global pandemic of SARS-CoV-2 is unlikely. Whilst we understand that the motivations behind producing such work are grounded in trying to be helpful, we demonstrate here that there are clear conceptual and methodological deficiencies with their study that render their results and conclusions invalid.","tags":["Irresponsible Covid19 modelling","Species Distribution Models"],"title":"On the inadequacy of species distribution models for modelling the spread of SARS-CoV-2: response to Araújo and Naimi","type":"publication"},{"authors":["Blas M. Benito"],"categories":null,"content":"","date":1579737600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579737600,"objectID":"80c7b3f2edb958819795bfdb5c63cf9b","permalink":"https://blasbenito.com/publication/2020_benito_ecography_distantia/","publishdate":"2020-01-23T00:00:00Z","relpermalink":"/publication/2020_benito_ecography_distantia/","section":"publication","summary":"We introduce distantia (v1.0.1), an R package providing general toolset to quantify dissimilarity between ecological time‐series, independently of their regularity and number of samples. The functions in distantia provide the means to compute dissimilarity scores by time and by shape and assess their significance, evaluate the partial contribution of each variable to dissimilarity, and align or combine sequences by similarity.","tags":["Time Series Analysis","R packages"],"title":"distantia: an open‐source toolset to quantify dissimilarity between multivariate ecological time‐series","type":"publication"},{"authors":["Blas M. Benito","Graciela Gil-Romera"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"b54d3a6bfb2beb4f27f5c5b2a798cd55","permalink":"https://blasbenito.com/publication/2020_benito_ecography_memoria/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/2020_benito_ecography_memoria/","section":"publication","summary":"Paper published in the section \"Editor's Choice\" of the *Ecography* journal. It received [an award](https://www.dropbox.com/s/oacsy1xqx4omv1b/2019_BMB_Ecography_b_top_downloaded.png?dl=1) for the number of downloads during the 12 months after its publication.","tags":["Quantitative methods","R packages","Palaeoecology","Ecological Memory","Plant Ecology","Machine Learning","Random Forest"],"title":"Ecological memory at millennial time‐scales: the importance of data constraints, species longevity and niche features","type":"publication"},{"authors":["B.L Valero-Garcés","Penélope González-Sampériz","Graciela Gil-Romera","Blas M. Benito","A. Moreno","B. Oliva-Urcia","J. Aranbarri","E. García-Prieto","M. Frugone","M. Morellón","L.J. Arnold","M. Demuro","M. Hardiman","S.P.E. Blockey","C.S. Lane"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"a61e896d35882a74ae0ea224990d2ae3","permalink":"https://blasbenito.com/publication/2019_valero-garces_quaternary_geochronology/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/2019_valero-garces_quaternary_geochronology/","section":"publication","summary":"We present a multidisciplinary dating approach - including radiocarbon, Uranium/Thorium series (U/Th), paleomagnetism, single-grain optically stimulated luminescence (OSL), polymineral fine-grain infrared stimulated luminescence (IRSL) and tephrochronology - used for the development of an age model for the Cañizar de Villarquemado sequence (VIL) for the last ca. 135 ka.","tags":["Palaeoecology","Age-depth modelling","Bayesian models"],"title":"A multi-dating approach to age-modelling long continental records: The 135 ka El Cañizar de Villarquemado sequence (NE Spain)","type":"publication"},{"authors":["Graciela Gil-Romera","Carole Adolf","Blas M. Benito","Lucas Bittner","Maria U. Johansson","David A. Grady","Henry F. Lamb","Bruk Lemma","Mekbib Fekadu","Bruno Glaser","Betelhem Mekonnen","Miguel Sevilla-Callejo","Michael Zech","Wolfgang Zech","Georg Miaha"],"categories":null,"content":"","date":1563926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563926400,"objectID":"691382af0f3f50780ae78edc146f99e0","permalink":"https://blasbenito.com/publication/2019_gil_romera_biology_letters/","publishdate":"2019-07-24T00:00:00Z","relpermalink":"/publication/2019_gil_romera_biology_letters/","section":"publication","summary":"We hypothesize that fire has influenced Erica communities in the Bale Mountains at millennial time-scales. To test this, we (1) identify the fire history of the Bale Mountains through a pollen and charcoal record from Garba Guracha, a lake at 3950 m.a.s.l., and (2) describe the long-term bidirectional feedback between wildfire and Erica, which may control the ecosystem's resilience.","tags":["Palaeoecology","Fire dynamics","Ecological Memory","Time Series Analysis","Generalized Least Squares","Plant Ecology"],"title":"Long-term fire resilience of the Ericaceous Belt, Bale Mountains, Ethiopia","type":"publication"},{"authors":["Blas M. Benito"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png') print(\u0026quot;Welcome to Academic!\u0026quot;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata ( front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"1f184bf285911dee5df13e26218d60b2","permalink":"https://blasbenito.com/post_examples/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post_examples/jupyter/","section":"post_examples","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post_examples"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://blasbenito.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Albuquerque F.","Blas M. Benito","Rodríguez MÁM.","Gray C."],"categories":null,"content":"","date":1537315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537315200,"objectID":"0dd555719b21269ebb41f0603a2e3704","permalink":"https://blasbenito.com/publication/2018_albuquerque_peerj/","publishdate":"2018-09-19T00:00:00Z","relpermalink":"/publication/2018_albuquerque_peerj/","section":"publication","summary":"The goals of this study are to provide a map of actual habitat suitability (1), describe the relationships between abiotic predictors and the saguaro distribution at regional extents (2), and describe the potential effect of climate change on the spatial distribution of the saguaro (3).","tags":["Biogeography","Climate Change","Species Distribution Models","Machine Learning","Gradient Boosting","Plant Ecology"],"title":"Potential changes in the distribution of Carnegiea gigantea under future scenarios","type":"publication"},{"authors":["Radoslav Kozma","Mette Lillie","Blas M. Benito","Jens-Christian Svenning","Jacob Höglund"],"categories":null,"content":"","date":1527552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527552000,"objectID":"2a60bd7aa28feceb59407d600d738438","permalink":"https://blasbenito.com/publication/2018_kozma_ecology_and_evolution/","publishdate":"2018-05-29T00:00:00Z","relpermalink":"/publication/2018_kozma_ecology_and_evolution/","section":"publication","summary":"Here we investigated the demographic history of the willow grouse (Lagopus lagopus), rock ptarmigan (Lagopus muta), and black grouse (Tetrao tetrix) through the Late Pleistocene using two complementary methods and whole genome data. Species distribution modeling (SDM) allowed us to estimate the total range size during the Last Interglacial (LIG) and Last Glacial Maximum (LGM) as well as to indicate potential population subdivisions.","tags":["Biogeography","Climate Change","Species Distribution Models","Generalized Linear Models"],"title":"Past and potential future population dynamics of three grouse species using ecological and whole genome coalescent modeling","type":"publication"},{"authors":["Gang Feng","Ziyu Ma","Blas M. Benito","Signe Normand","Alejandro Ordoñez","Yi Jin","Lingfeng Mao","Jens-Christian Svenning"],"categories":null,"content":"","date":1500249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500249600,"objectID":"fc5bc772c0898cc6442de19eb3810539","permalink":"https://blasbenito.com/publication/2017_feng_global_ecology_and_biogeography/","publishdate":"2017-07-17T00:00:00Z","relpermalink":"/publication/2017_feng_global_ecology_and_biogeography/","section":"publication","summary":"Our results show that phylogenetically diverse assemblages with large phylogenetic age differences among species are associated with relatively high long‐term climate stability, with intra‐regional links between long‐term climate variability and phylogenetic composition especially strong in the more unstable regions. These findings point to future climate change as a key risk to the preservation of the phylogenetically diverse assemblages in regions characterized by relatively high paleoclimate stability, with China as a key example.","tags":["Biogeography","Plant Ecology"],"title":"Phylogenetic age differences in tree assemblages across the Northern Hemisphere increase with long-term climate stability in unstable regions","type":"publication"},{"authors":["Gang Feng","Lingfeng Mao","Blas M. Benito","Nathan G. Swenson","Jens-Christian Svenning"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"02c9b25a1d6d387966133e1a58983917","permalink":"https://blasbenito.com/publication/2017_feng_biological_conservation/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/2017_feng_biological_conservation/","section":"publication","summary":"In this study, for the first time, we linked the distribution of threatened species across China to current and historical changes in human population densities, cropland area, and pasture area since 1700 (at a 100 km × 100 km resolution). We find that variables describing historical changes in human impacts were consistently more strongly associated with proportions of threatened plants than variables describing current changes in human impacts. Notably, threatened plant species in China tend to be concentrated where historical anthropogenic impacts were relatively small, but anthropogenic activities have intensified relatively strongly since 1700.","tags":["Biogeography","Random Forest","Machine Learning","Biodiversity Conservation"],"title":"Historical anthropogenic footprints in the distribution of threatened plants in China","type":"publication"},{"authors":["Blas M. Benito","Jens-Christian Svenning","Trine Kellberg Nielsen","Felix Riede","Graciela Gil-Romera","Thomas Mailund","Brody Sandel"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"3fffb042779562c60fe874bc323131de","permalink":"https://blasbenito.com/publication/2017_benito_journal_of_biogeography/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/2017_benito_journal_of_biogeography/","section":"publication","summary":"This paper [was highlighted in the *Editor's Picks* section of the Science Journal](https://www.dropbox.com/s/6k308eczv7i6kbj/2017_BMB_Journal_of_Biogeography_editors_choice.pdf?dl=1), and was among the [top downloaded articles](https://www.dropbox.com/s/sowq1h4bdngmipy/2017_BMB_Journal_of_Biogeography.png?dl=1) from the *Journal of Biogeography* during the 12 months after its publication.","tags":["Biogeography","Species Distribution Models","Biogeography of Neanderthals","Generalized Linear Models","Ensemble models"],"title":"The ecological niche and distribution of Neanderthals during the Last Interglacial","type":"publication"},{"authors":["Trine Kellberg Nielsen","Blas M. Benito","Jens-Christian Svenning","Brody Sandel","Luseadra McKerracher","Felix Riede"],"categories":null,"content":"","date":1488240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488240000,"objectID":"70212c1a73a4ebfe55db225953cd1f5d","permalink":"https://blasbenito.com/publication/2017_kellberg-nielsen_quaternary_international/","publishdate":"2017-02-28T00:00:00Z","relpermalink":"/publication/2017_kellberg-nielsen_quaternary_international/","section":"publication","summary":"Our results are inconsistent with the claim that climatic constraint and/or a lack of suitable habitats can fully explain the absence of Neanderthals in Southern Scandinavia during the Eemian Interglacial and Early Weichselian Glaciation. We do, however, find evidence that a geographic barrier may have impeded northerly migrations during the Eemian.","tags":["Biogeography","Biogeography of Neanderthals","Species distribution models"],"title":"Investigating Neanderthal dispersal above 55°N in Europe during the Last Interglacial Complex","type":"publication"},{"authors":["Constantin M. Zohner","Blas M. Benito","Jason D. Fridley","Jens-Christian Svenning","Susanne S. Renner"],"categories":null,"content":"","date":1487030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487030400,"objectID":"1bf8e70c8f4c03b6d059e49a3fd0bf9b","permalink":"https://blasbenito.com/publication/2017_zohner_ecology_letters/","publishdate":"2017-02-14T00:00:00Z","relpermalink":"/publication/2017_zohner_ecology_letters/","section":"publication","summary":"Intuitively, interannual spring temperature variability (STV) should influence the leaf‐out strategies of temperate zone woody species, with high winter chilling requirements in species from regions where spring warming varies greatly among years. We tested this hypothesis using experiments in 215 species and leaf‐out monitoring in 1585 species from East Asia (EA), Europe (EU) and North America (NA). The results reveal that species from regions with high STV indeed have higher winter chilling requirements, and, when grown under the same conditions, leaf out later than related species from regions with lower STV. Since 1900, STV has been consistently higher in NA than in EU and EA, and under experimentally short winter conditions NA species required 84% more spring warming for bud break, EU ones 49% and EA ones only 1%. These previously unknown continental‐scale differences in phenological strategies underscore the need for considering regional climate histories in global change models.","tags":["Phenology","Biogeography","Plant Ecology"],"title":"Spring predictability explains different leaf‐out strategies in the woody floras of North America, Europe and East Asia","type":"publication"},{"authors":["Constantin M. Zohner","Blas M. Benito","Jens-Christian Svenning","Susanne S. Renner"],"categories":null,"content":"","date":1476662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476662400,"objectID":"8bb3d0dd4e4d13b48c7b50f18f1b7e44","permalink":"https://blasbenito.com/publication/2016_zohner_nature_climate_change/","publishdate":"2016-10-17T00:00:00Z","relpermalink":"/publication/2016_zohner_nature_climate_change/","section":"publication","summary":"Our results do not support previous ideas about phenological strategies in temperate woody species (the ‘high temperature variability’ hypothesis; the ‘oceanic climate’ hypothesis; the ‘high latitude’ hypothesis). In regions with long winters, trees appear to rely on cues other than day length, such as winter chilling and spring warming. By contrast, in regions with short winters, some species—mostly from lineages with a warm-temperate or subtropical background, for example, Fagus additionally rely on photoperiodism. Therefore, photoperiod may be expected to constrain climate-driven shifts in spring leaf unfolding only at lower latitudes.","tags":["Phenology","Biogeography","Plant Ecology"],"title":"Day length unlikely to constrain climate-driven shifts in leaf-out times of northern woody plants","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d337bc1d87a46134727f7fb46c0d4efc","permalink":"https://blasbenito.com/project_examples/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project_examples/external-project/","section":"project_examples","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project_examples"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"875a64698311258ca5954edf3adc2327","permalink":"https://blasbenito.com/project_examples/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project_examples/internal-project/","section":"project_examples","summary":"An example of using the in-built project page.","tags":[null],"title":"Internal Project","type":"project_examples"},{"authors":["Blas M. Benito","吳恩達"],"categories":["Demo","教程"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\nCheck out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n👉 Get Started 📚 View the documentation 💬 Ask a question on the forum 👥 Chat with the community 🐦 Twitter: @source_themes @GeorgeCushen #MadeWithAcademic 💡 Request a feature or report a bug ⬆️ Updating? View the Update Guide and Release Notes ❤️ Support development of Academic: ☕️ Donate a coffee 💵 Become a backer on Patreon 🖼️ Decorate your laptop or journal with an Academic sticker 👕 Wear the T-shirt 👩‍💻 Contribute Academic is mobile first with a responsive design to ensure that your site looks stunning on every device. Key features:\nPage builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic Install You can choose from one of the following four methods to install:\none-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio Then personalize and deploy your new site.\nUpdating View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"a25eb5ef2134a076531687948275d21e","permalink":"https://blasbenito.com/post_examples/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post_examples/getting-started/","section":"post_examples","summary":"Create a beautifully simple website in under 10 minutes.","tags":null,"title":"Academic: the website builder for Hugo","type":"post_examples"},{"authors":null,"categories":["R"],"content":" R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA ) Figure 1: A fancy pie chart. ","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"9554380237eec0879443a7c1a234a75f","permalink":"https://blasbenito.com/post_examples/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post_examples/2015-07-23-r-rmarkdown/","section":"post_examples","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post_examples"},{"authors":["Jacquelyn L. Gill","Jessica L. Blois","Blas M. Benito","Solomon Dobrowski","Malcolm L. Hunter Jr.","Jenny L. McGuire"],"categories":null,"content":"","date":1430179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430179200,"objectID":"aebebdc92cbf3072b7bf6d150d269b6c","permalink":"https://blasbenito.com/publication/2015_gill_conservation_biology/","publishdate":"2015-04-28T00:00:00Z","relpermalink":"/publication/2015_gill_conservation_biology/","section":"publication","summary":"Paleoecology provides a valuable perspective on coarse‐filter strategies by marshaling the natural experiments of the past to contextualize extinction risk due to the emerging impacts of climate change and anthropogenic threats. We reviewed examples from the paleoecological record that highlight the strengths, opportunities, and caveats of a CNS approach. We focused on the near‐time geological past of the Quaternary, during which species were subjected to widespread changes in climate and concomitant changes in the physical environment in general.","tags":["Palaeoecology","Biodiversity conservation"],"title":"A 2.5‐million‐year perspective on coarse‐filter strategies for conserving nature's stage","type":"publication"},{"authors":["Albuquerque F.","Blas M. Benito","Beier, P.","Assunção-Albuquerque, M.J.","Cayuela, L."],"categories":null,"content":"","date":1425340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1425340800,"objectID":"d1d254ad844854fa63cf4af98860d93d","permalink":"https://blasbenito.com/publication/2015_albuquerque_naturaleza_and_conservacao/","publishdate":"2015-03-03T00:00:00Z","relpermalink":"/publication/2015_albuquerque_naturaleza_and_conservacao/","section":"publication","summary":"We had three key findings. First, dry forest is the least protected biome in Mesoamerica (4.5% protected), indicating that further action to safeguard this biome is warranted. Secondly, the poor overlap between protected areas and high-value forest conservation areas found herein may provide evidence that the establishment of protected areas may not be fully accounting for tree priority rank map. Third, high percentages of forest cover and high-value forest conservation areas still need to be represented by the protected areas network. Because deforestation rates are still increasing in this region, Mesoamerica needs funding and coordinated action by policy makers, national and local governmental and non-governmental organizations, conservationists and other stakeholders.","tags":["Biodiversity Conservation","Forests","Species Distribution Models","Random Forest","Machine Learning"],"title":"Supporting underrepresented forests in Mesoamerica","type":"publication"},{"authors":["A.J. Mendoza-Fernández","F. Martínez-Hernández","F.J. Pérez-García","J.A. Garrido Becerra","Blas M. Benito","E. Salmerón Sánchez","J. Guirado","M.E. Merlo","J.F. Mota"],"categories":null,"content":"","date":1421107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1421107200,"objectID":"65c5f86bbc46066e9c0c0c783d5667b8","permalink":"https://blasbenito.com/publication/2015_mendoza-fernandez_plant_biosystems/","publishdate":"2015-01-13T00:00:00Z","relpermalink":"/publication/2015_mendoza-fernandez_plant_biosystems/","section":"publication","summary":"*Maytenus senegalensis* subsp. *europaea* communities are unique vegetal formations in Europe. In fact, they are considered Priority Habitat by Directive 92/43/EEC. These are ecologically valuable plant communities found in the southeast of Spain. By combining modeling methods of environmental variables, historical photo-interpretation, and fieldwork, a chronosequence of the evolution of their extent of occurrence (EOO) has been reconstructed in 1957 and 2011. Results showed a strong regression range of *Maytenus senegalensis* subsp. *europaea* populations. More than 26,000 ha of EOO for this species have been lost in the province of Almería. Considering the final number of polygons, this area has been fragmented 18 times since the 1950s. These results reinforce the idea that the alteration and fragmentation of habitat due to human activities is one of the most important drivers of biodiversity loss and global change. These activities are mostly intensive greenhouse agriculture and urbanization without sustainable land planning. Knowledge about the distribution of M. senegalensis subsp. europaea is of great interest for future habitat restoration. Therefore, this would be the key species to recover these damaged ecosystems.","tags":["Species distribution models","Biodiversity conservation","Ecoinformatics","Habitat loss"],"title":"Extreme habitat loss in a Mediterranean habitat: Maytenus senegalensis subsp. europaea","type":"publication"},{"authors":["José Miguel Barea-Azcón","Blas M. Benito (shared first coauthorship)","Francisco J. Olivares","Helena Ruiz","Javier Martín","Antonio L. García","Rogelio López"],"categories":null,"content":"","date":1392854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1392854400,"objectID":"24408a853074612aafa67c0a6df22901","permalink":"https://blasbenito.com/publication/2014_barea-azcon_and_benito_biodiversity_and_conservation/","publishdate":"2014-02-20T00:00:00Z","relpermalink":"/publication/2014_barea-azcon_and_benito_biodiversity_and_conservation/","section":"publication","summary":"Herein we investigate the distribution and conservation problems of a relict interaction in the Sierra Nevada mountains (southern Europe) between the butterfly *Agriades zullichi* —a rare and threatened butterfly— and its larval foodplant *Androsace vitaliana* subsp. *nevadensis*. We designed an intensive field survey to obtain a comprehensive presence dataset. This was used to calibrate species distribution models with absences taken at local and regional extents, analyze the potential distribution, evaluate the influence of environmental factors in different geographical contexts, and evaluate conservation threats for both organisms.","tags":["Biodiversity Conservation","Species Distribution Models","Ecoinformatics","Biogeography","Random Forest","Machine Learning"],"title":"Distribution and conservation of the relict interaction between the butterfly Agriades zullichi and its larval foodplant (Androsace vitaliana nevadensis)","type":"publication"},{"authors":["Francisco J. Bonet","Ramón Pérez-Pérez","Blas M. Benito","Fabio Suzart de Albuquerque","Regino Zamora"],"categories":null,"content":"","date":1391212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1391212800,"objectID":"bf6fe82e9413557dd6e0d799c6787ba1","permalink":"https://blasbenito.com/publication/2014_bonet_environmental_modelling_and_software/","publishdate":"2014-02-01T00:00:00Z","relpermalink":"/publication/2014_bonet_environmental_modelling_and_software/","section":"publication","summary":"Many of the best practices concerning the development of ecological models or analytic techniques published in the scientific literature are not fully available to modelers but rather are stored in scientists' digital or biological memories. We propose that it is time to address the problem of storing, documenting, and executing ecological models and analytical procedures. In this paper, we propose a conceptual framework to design and implement a web application that will help to meet this challenge. This tool will foster cooperation among scientists, enhancing the creation of relevant knowledge that could be transferred to environmental managers. We have implemented this conceptual framework in a tool called ModeleR. This is being used to document, share, and execute more than 200 models and analytical processes associated with a global change monitoring program that is being undertaken in the Sierra Nevada Mountains (south Spain). ModeleR uses the concept of scientific workflow to connect and execute different types of models and analytical processes. Finally, we have envisioned the creation of a federation of model repositories where models documented within a local repository could be linked and even executed by other researchers.","tags":["Ecoinformatics"],"title":"Documenting, storing, and executing models in Ecology: A conceptual framework and real implementation in a global change monitoring program","type":"publication"},{"authors":["Blas M. Benito","Juan Lorite","Ramón Pérez-Pérez","Lorena Gómez-Aparicio","Julio Peñas"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"c1e357c5f2a52ae592bcf91f774e7627","permalink":"https://blasbenito.com/publication/2014_benito_diversity_and_distributions/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/2014_benito_diversity_and_distributions/","section":"publication","summary":"The Mediterranean Basin is threatened by climate change, and there is an urgent need for studies to determine the risk of plant range shift and potential extinction. In this study, we simulate potential range shifts of 176 plant species to perform a detailed prognosis of critical range decline and extinction in a transformed mediterranean landscape. Particularly, we seek to answer two pivotal questions: (1) what are the general plant‐extinction patterns we should expect in mediterranean landscapes during the 21st century? and (2) does dispersal ability prevent extinction under climate change?.","tags":["Climate Change","Species Distribution Models","Ecoinformatics","Mechanistic Simulation","Random Forest","Ensemble models","Conditional Inference Trees","Plant Ecology"],"title":"Forecasting plant range collapse in a mediterranean hotspot: when dispersal uncertainties matter","type":"publication"},{"authors":["Elise S. Gornish","Jill A. Hamilton","Albert Barberán","Blas M. Benito","Amrei Binzer","Julie E. DeMeester","Robert Gruwez","Bruno Moreira","Shirin Taheri","Sara Tomiolo","Catarina Vinagre","Pauline Vurain","Jennifer Weaver"],"categories":null,"content":"","date":1366070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1366070400,"objectID":"5ee0e5600c367d4720aa53a935a5c256","permalink":"https://blasbenito.com/publication/2013_gornish_eos/","publishdate":"2013-04-16T00:00:00Z","relpermalink":"/publication/2013_gornish_eos/","section":"publication","summary":"Climate change research is an interdisciplinary field, and understanding its social, political, and environmental implications requires integration across fields of research where different tools may be used to address common concerns. One of the many advantages of interdisciplinary approaches is that they open communication between complementary fields, filling knowledge gaps and facilitating progression within both individual fields and the broader field of climate change research.","tags":["Climate Change"],"title":"Interdisciplinary Climate Change Collaborations Are Essential for Early‐Career Scientists","type":"publication"},{"authors":["Mireia Valle","Marieke M. van Katwijk","Dick J. de Jong","Tjeerd J. Bouma","Aafke M. Schipper","Guillem Chust","Blas M. Benito","Joxe M. Garmendia","Ángel Borja"],"categories":null,"content":"","date":1363305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1363305600,"objectID":"77359446d7cb7b0af132b5de61eb93b1","permalink":"https://blasbenito.com/publication/2013_valle_journal_of_sea_research/","publishdate":"2013-03-15T00:00:00Z","relpermalink":"/publication/2013_valle_journal_of_sea_research/","section":"publication","summary":"A time series of 14-year distribution data of Zostera marina in the Ems estuary (The Netherlands) was used to build different data subsets: (1) total presence area; (2) a conservative estimate of the total presence area, defined as the area which had been occupied during at least 4 years; (3) core area, defined as the area which had been occupied during at least 2/3 of the total period; and (4–6) three random selections of monitoring years. On average, colonized and disappeared areas of the species in the Ems estuary showed remarkably similar transition probabilities of 12.7% and 12.9%, respectively. SDMs based upon machine-learning methods (Boosted Regression Trees and Random Forest) outperformed regression-based methods. Current velocity and wave exposure were the most important variables predicting the species presence for widely distributed data. Depth and sea floor slope were relevant to predict conservative presence area and core area.","tags":["Biodiversity Conservation","Species Distribution Models","Machine learning","Random Forest","Gradient Boosting"],"title":"Comparing the performance of species distribution models of Zostera marina: Implications for conservation","type":"publication"},{"authors":["Blas M. Benito","Luis Cayuela","Fabio S. Albuquerque"],"categories":null,"content":"","date":1362528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1362528000,"objectID":"69d5141556741b6f7698c5bfd3796a74","permalink":"https://blasbenito.com/publication/2013_benito_methods_in_ecology_and_evolution/","publishdate":"2013-03-06T00:00:00Z","relpermalink":"/publication/2013_benito_methods_in_ecology_and_evolution/","section":"publication","summary":"We generated 380 S‐SDMs of 1224 tree species in Mesoamerica by combining 19 distribution modelling methods with 20 different thresholds using presence‐only data from the Global Biodiversity Information Facility. We compared the predicted richness and composition with inventory data obtained from the BIOTREE‐NET forest plot database. We designed two indicators of predictive performance that were based on the diversity factors used to measure species turnover: a (shared species between the observed and predicted compositions), b and c (the exclusive species of the predicted and observed compositions respectively) and compared them with the Sorensen and Beta‐Simpson turnover measures. Some modelling methods – especially machine learning and ensemble model forecasting methods performed significantly better than others in minimizing the error in predicted richness and composition. Our results also indicate that restrictive thresholds (with high omission errors) lead to more accurate S‐SDMs in terms of species richness and composition. Here, we demonstrate that particular combinations of modelling methods and thresholds provide results with higher predictive performance.","tags":["Species Distribution Models","Ecoinformatics","Random Forest","Ensemble models","Machine Learning"],"title":"The impact of modelling choices in the predictive performance of richness maps derived from species‐distribution models: guidelines to build better diversity models","type":"publication"},{"authors":["Albuquerque, F.","Assunção-Albuquerque, M.J.","Cayuela, L.","Zamora, R.","Blas M. Benito"],"categories":null,"content":"","date":1358726400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1358726400,"objectID":"bb2936f49fe5e9950d89f99bc84a0d90","permalink":"https://blasbenito.com/publication/2013_albuquerque_biological_conservation/","publishdate":"2013-01-21T00:00:00Z","relpermalink":"/publication/2013_albuquerque_biological_conservation/","section":"publication","summary":"Our assessments showed little association between bird richness patterns and the cover of protected areas (PAs) across EU countries. The congruence between high-value richness areas of all bird species and IBS with PAs cover was moderate, suggesting that different conservation planning targets should be taken into account to safeguard IBS, or the composition of bird species. Our results also showed that 16 (3.9%) threatened species were present in gaps of PAs. The poor relationship between PAs cover and bird richness pattern found herein may provide evidence that the establishment of SPAs across Europe may not be fully accounting for richness patterns to enhance the performance of the current network.","tags":["Biodiversity Conservation"],"title":"European Bird distribution is “well” represented by Special Protected Areas: Mission accomplished?","type":"publication"},{"authors":["Ramón Pérez-Pérez","Blas M. Benito","Francisco J. Bonet"],"categories":null,"content":"","date":1328313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1328313600,"objectID":"29fc5074a7e132bb1510f42cecbde2aa","permalink":"https://blasbenito.com/publication/2012_perez-perez_expert_systems_with_applications/","publishdate":"2012-02-04T00:00:00Z","relpermalink":"/publication/2012_perez-perez_expert_systems_with_applications/","section":"publication","summary":"In this paper, we present the development of ModeleR, a repository of models accessible from the web, which enables the user to design, document, manage, and execute environmental models.","tags":["Ecoinformatics"],"title":"ModeleR: An enviromental model repository as knowledge base for experts","type":"publication"},{"authors":["Julio Peñas","Blas M. Benito","Juan Lorite","Miguel Ballesteros","Eva María Cañadas","Montserrat Martínez-Ortega"],"categories":null,"content":"","date":1301097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1301097600,"objectID":"f7cb87fcde920214c11f820e82fdc695","permalink":"https://blasbenito.com/publication/2011_penas_environmental_management/","publishdate":"2011-03-26T00:00:00Z","relpermalink":"/publication/2011_penas_environmental_management/","section":"publication","summary":"The results indicate that greenhouses and construction activities (mainly for tourist purposes) exert a strong impact on the populations of this endangered species. The habitat depletion showed peaks that constitute the destruction of 85% of the initial area in only 20 years for some populations of L. nigricans. According to the forecast established by the model, a rapid extinction could take place and some populations may disappear as early as the year 2030. Fragmentation-cadence analysis can help identify population units of primary concern for its conservation, by means of the adoption of improved management and regulatory measures.","tags":["Habitat Fragmentation","Drylands","Endangered Plants","Biodiversity Conservation"],"title":"Habitat Fragmentation in Arid Zones: A Case Study of Linaria nigricans Under Land Use Changes (SE Spain)","type":"publication"},{"authors":["Blas M. Benito","Juan Lorite","Julio Peñas"],"categories":null,"content":"","date":1296604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1296604800,"objectID":"d94d778916672e3a2b7a6e74a2278246","permalink":"https://blasbenito.com/publication/2011_benito_climate_change/","publishdate":"2011-02-02T00:00:00Z","relpermalink":"/publication/2011_benito_climate_change/","section":"publication","summary":"According to the simulations, the suitable habitat for the key species inhabiting the summit area, where most of the endemic and/or rare species are located, may disappear before the middle of the century. The other key species considered show moderate to drastic suitable habitat loss depending on the considered scenario. Climate warming should provoke a strong substitution dynamics between species, increasing spatial competition between both of them. In this study, we introduce the application of differential suitability concept into the analysis of potential impact of climate change, forest management and environmental monitoring, and discuss the limitations and uncertainties of these simulations.","tags":["Climate Change","Species Distribution Models","Ecoinformatics","Plant Ecology"],"title":"Simulating potential effects of climatic warming on altitudinal patterns of key species in Mediterranean-alpine ecosystems","type":"publication"},{"authors":["Juan Lorite","Julio Peñas","Blas M. Benito","Eva Cañadas","Francisco Valle"],"categories":null,"content":"","date":1267401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1267401600,"objectID":"3d86a9f5c5dcbb080231f4e1d7755fe0","permalink":"https://blasbenito.com/publication/2010_lorite_annales_botanici_fennici/","publishdate":"2010-03-01T00:00:00Z","relpermalink":"/publication/2010_lorite_annales_botanici_fennici/","section":"publication","summary":"We studied the natural history as well as the conservation status of the first-known population of Polygala balansae in Europe (Granada, SE Spain). In the study area, we located only one population occupying a small patch of 1920 m2, between 120 and 160 m a.s.l., with 246 mature individuals. The species is classified as Critically Endangered (CR), under the following criteria: severely fragmented, inferred continuous decline, small population size, and continuing decline inferred from the number mature individuals.","tags":["Botany","Endangered Plants","Biogeography","Biodiversity Conservation"],"title":"Conservation Status of the First Known Population of Polygala balansae in Europe","type":"publication"},{"authors":["Francisca Alba-Sánchez","José A. López-Sáez","Blas M. Benito","Juan C. Linares","Diego Nieto-Lugilde","Lourdes López-Merino"],"categories":null,"content":"","date":1266710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1266710400,"objectID":"3bd0e1e66942974aaf0e65818e475037","permalink":"https://blasbenito.com/publication/2010_alba-sanchez_diversity_and_distributions/","publishdate":"2010-02-21T00:00:00Z","relpermalink":"/publication/2010_alba-sanchez_diversity_and_distributions/","section":"publication","summary":"Quaternary palaeopalynological records collected throughout the Iberian Peninsula and species distribution models (SDMs) were integrated to gain a better understanding of the historical biogeography of the Iberian Abies species (i.e. Abies pinsapo and Abies alba). We hypothesize that SDMs and Abies palaeorecords are closely correlated, assuming a certain stasis in climatic and topographic ecological niche dimensions. In addition, the modelling results were used to assign the fossil records to A. alba or A. pinsapo, to identify environmental variables affecting their distribution, and to evaluate the ecological segregation between the two taxa.","tags":["Palaeoecology","Species Distribution Models","Biogeography"],"title":"Past and present potential distribution of the Iberian Abies species: a phytogeographic approach using fossil pollen data and species distribution models ","type":"publication"},{"authors":["Blas M. Benito","Montserrat Martínez-Ortega","Luz M. Muñoz","Juan Lorite","Julio Peñas"],"categories":null,"content":"","date":1235779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1235779200,"objectID":"6039de1ce8c3c53184e8f0a191151510","permalink":"https://blasbenito.com/publication/2009_benito_biodiversity_and_conservation/","publishdate":"2009-02-28T00:00:00Z","relpermalink":"/publication/2009_benito_biodiversity_and_conservation/","section":"publication","summary":"In this paper, we propose the application of SDMs to assess the extinction-risk of plant species in relation to the spread of greenhouses in a Mediterranean landscape, where habitat depletion is one of the main causes of biodiversity loss.","tags":["Endangered Plants","Biodiversity Conservation","Species Distribution Models","Drylands"],"title":"Assessing extinction-risk of endangered plants using species distribution models: a case study of habitat depletion caused by the spread of greenhouses","type":"publication"},{"authors":["Blas M. Benito"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"5dbb5dd8f81f0274ffb682d6d27968b6","permalink":"https://blasbenito.com/publication/2008_penas_phyton/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/2008_penas_phyton/","section":"publication","summary":"We we develop a methodology predicting the expansion of greenhouses by combining a species distribution model (MaxEnt) and a simulator of land use change (Geomod).","tags":["Biodiversity Conservation","Species Distribution Models","Drylands"],"title":"Greenhouses, land use change, and predictive models: MaxEnt and Geomod working together","type":"publication"}]