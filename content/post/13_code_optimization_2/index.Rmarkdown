---
title: "R Code Optimization II: The Toolbox"
author: ""
date: '2025-03-15'
slug: R-code-optimization-toolbox
categories: []
tags:
- Rstats
- Data Science
- Tutorial
- Code Optimization
subtitle: ''
summary: "Post focused on the R toolbox required for code optimization."
authors: [admin]
lastmod: '2025-03-15T07:28:01+01:00'
featured: no
draft: true
image:
  caption: ''
  focal_point: Smart
  margin: auto
projects: []
toc: true
---

<style>
.alert-warning {
  background-color: #f4f4f4;
  color: #333333;
  border-color: #333333;
}
</style>

```{r, include = FALSE}
knitr::opts_chunk$set(
  fig.width = 8, 
  fig.height = 6
)
```


## Profiling and Benchmarking

Profiling and benchmarking are methods to measure your code's performance and help guide optimization decisions. The former helps identify low-performance hot-spots in your code, while the latter helps make informed choices between alternative implementations.

### Profiling

Profiling helps analyze your code to understand where the execution time and RAM memory are spent, and will help you answer the question: *Where should I start optimizing?* Profilers break down code execution into function calls, highlighting potential bottlenecks

In R, the function `utils::Rprof()` is the default profiling tool. You can find a great explanation on this tool in the chapter [Profiling R Code](https://bookdown.org/rdpeng/rprogdatascience/profiling-r-code.html#profiling-r-code) of the book *R Programming for Data Science*, by [Roger D. Peng](https://rdpeng.org/).

The function [`profvis::profvis()`](https://profvis.r-lib.org/reference/profvis.html) is a modern alternative that still uses `Rprof()` underneath, but generates a neat HTML widget to facilitate the visualization and interpretation of profiling data. 

Let's check how it works on the silly function `f()` shown below. It runs a correlation test between two large vectors generated randomly from a normal and a uniform distribution.

```{r}
f <- function(n = 1e7){
  stats::cor.test(
    x = stats::rnorm(n = n), 
    y = stats::runif(n = n)
  )
}
```

To profile it with `profvis()` we just have to introduce our function into the argument `expr`. The argument `rerun = TRUE` is helpful to profile functions that might run too fast for the profiler.

```{r, eval = FALSE}
profvis::profvis(
  expr = f(), 
  rerun = TRUE
  )
```


```{r, echo = FALSE}
p <- profvis::profvis(
  expr = f(), 
  rerun = TRUE,
  width = "100%", 
  height = "300"
  )

htmlwidgets::saveWidget(
  p, 
  file = "profiling.html", 
  selfcontained = TRUE
  )

rm(p)
```

<iframe src="profiling.html" name="profiler"
  width="600" height="600" scrolling="auto" frameborder="0">
   <p>Profiling result.</p>
</iframe>

<br>

The **Flame Graph** shows the *call stack* of the profiled code. It indicates, from bottom to top, what functions are called by other functions, their run time, and memory allocated or deallocated. For example, on the lower right corner you can see how `cor.test.default` () calls the functions `complete.cases` and `cor`, and a gray block labelled `<GC>` wiht a negative memory number (indicating deallocated memory). These gray boxes show [garbage collection](https://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29) calls, used for memory housekeeping.

The **Data** tab summarizes the profiling from top to bottom in a very intuitive way that highlights computational hot-spots in our code right away. 

Even though `profvis()` is a very useful tool to guide software optimization, there are some limitations to be aware of: the memory usage values be unreliable due to R's lazy memory management and the automated application of garbage collection, which can obscure memory allocations and deallocations. 

If you wish to learn more, there are several `profvis` examples available in this [vignette](https://profvis.r-lib.org/articles/examples.html). 

Another alternative leveraging `Rprof()` is the package [`proftools`](https://cran.r-project.org/package=proftools). It does not produce a neat html widget, but has [extended plotting functionalities](https://cran.r-project.org/web/packages/proftools/vignettes/proftools.pdf).

### Benchmarking

Once we identify a worthy bottleneck in our code, it's time to start thinking about alternative implementations to mitigate the issue.

![https://xkcd.com/1445](https://imgs.xkcd.com/comics/efficiency.png)

For example, the profiling of `f()` shows that `stats::cor.test()` requires 33% of the run time and has the highest memory usage. Here we could replace it with `stats::cor()` to see if we can improve performance.

```{r}
f_alt <- function(n = 1e7){
  stats::cor(
    x = stats::rnorm(n = n), 
    y = stats::runif(n = n)
  )
}
```

Now, how do we compare `f()` and `f_alt()`? We could indeed profile each one of them and compare results.

benchmarking is the process of running alternative versions (original *vs.* optimized) of a piece of code to compare their performance. It's purpose is helping make data-driven decisions during code optimization.


The R package [`microbenchmark`](https://cran.r-project.org/package=microbenchmark) 

```{r}
library(microbenchmark)

df <- data.frame(
  x = stats::runif(n = 1e7)
)

microbenchmark::microbenchmark(
  df$x,
  df[["x"]],
  df[, "x"],
  dplyr::pull(df, x),
  times = 100
)
```

The R package [`bench`](https://bench.r-lib.org/) provides a 

```{r}
library(bench)

benchmark <- bench::mark(
  df$x,
  df[["x"]],
  df[, "x"],
  dplyr::pull(df, x),
  iterations = 100,
  check = FALSE
)
```


        Test real-world data: Benchmarks should reflect the actual data and usage patterns in your application.
        Use repeated trials: Small variations in timing can occur between trials, so repeat your benchmark several times and look for consistent results.

Putting It Together

    Profile your code first to identify the parts that need optimization.
    Implement optimizations carefully and gradually.
    Benchmark the old and new versions of your code to ensure that changes actually improve performance.

Remember: performance tuning should be a process of continuous improvement, not an all-at-once overhaul. Focus on high-impact changes that significantly improve execution time or memory usage.


Advanced R, by Hadley Whickham, has a [great section in code optimization]().

## The Optimization Loop

Optimizing R code isn’t a one-time task, it’s an iterative process that starts with 

![](flowchart.png)


2. Measure Performance (Profile Your Code!)

Instead of guessing where bottlenecks might be, use profiling tools to identify the actual slow parts of your code:

    Use profvis::profvis() or Rprof() for detailed profiling.
    For quick benchmarking, use bench::mark() or microbenchmark::microbenchmark().
    Log memory usage with lobstr::mem_used() if memory is a concern.

This step helps you find real inefficiencies, so you don’t waste time optimizing parts of the code that aren’t problematic.

3. Optimize the Low-Hanging Fruit

Once you know where the real slowdowns are, optimize only the parts that provide significant gains without compromising clarity. Some common low-hanging fruit:

    Eliminate unnecessary computations (e.g., avoid redundant loops, reuse calculations).
    Refactor bottleneck functions (e.g., replace slow operations with built-in vectorized alternatives).
    Simplify data handling (e.g., use appropriate data types, avoid excessive copies of large objects).

Avoid over-optimizing too early—focus only on fixes that are clear, easy to implement, and make a measurable difference.



4. Iterate Until Satisfied

After each round of optimization, re-profile your code and check if further improvements are needed. If performance is now acceptable, stop optimizing. If not, repeat the process:

    Profile again.
    Identify new bottlenecks.
    Optimize only where necessary.
    Repeat until additional optimization no longer justifies the cost in complexity.

The Golden Rule: Stop When It’s “Good Enough”

Optimization should be goal-driven, not an endless pursuit of perfection. If your code is fast enough for its intended use case, further optimization is unnecessary, especially if it would reduce readability or maintainability.

By following this loop, you ensure that your R code remains clean, efficient, and easy to maintain while optimizing only when it truly matters.







