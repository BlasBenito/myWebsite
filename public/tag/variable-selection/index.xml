<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Variable Selection | Blas M. Benito, PhD</title>
    <link>https://blasbenito.com/tag/variable-selection/</link>
      <atom:link href="https://blasbenito.com/tag/variable-selection/index.xml" rel="self" type="application/rss+xml" />
    <description>Variable Selection</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2023 Blas M. Benito. All Rights Reserved.</copyright><lastBuildDate>Wed, 15 Nov 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://blasbenito.com/media/avatar.jpg</url>
      <title>Variable Selection</title>
      <link>https://blasbenito.com/tag/variable-selection/</link>
    </image>
    
    <item>
      <title>Mapping Categorical Predictors to Numeric With Target Encoding</title>
      <link>https://blasbenito.com/post/target-encoding/</link>
      <pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/target-encoding/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;Categorical predictors are annoying stringy monsters that can turn any data analysis and modeling effort into a real annoyance. The post delves into the complexities of dealing with these types of predictors using methods such as one-hot encoding (please don&amp;rsquo;t) or target encoding, and provides insights into its mechanisms and quirks&lt;/p&gt;
&lt;h2 id=&#34;key-highlights&#34;&gt;Key Highlights:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Categorical Predictors are Kinda Annoying:&lt;/strong&gt; This section discusses the common issues encountered with categorical predictors during data analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;One-Hot Encoding Pitfalls:&lt;/strong&gt; While discussing one-hot encoding, the post focuses on its limitations, including dimensionality explosion, increased multicollinearity, and sparsity in tree-based models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro to Target Encoding:&lt;/strong&gt; Introducing target encoding as an alternative, the post explains its concept, illustrating the basic form with mean encoding and subsequent enhancements with additive smoothing, leave-one-out encoding, and more.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Handling Sparsity and Repetition:&lt;/strong&gt; It emphasizes the potential pitfalls of target encoding, such as repeated values within categories and their impact on model performance, prompting the exploration of strategies like white noise addition and random encoding to mitigate these issues.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Target Encoding Lab:&lt;/strong&gt; The post concludes with a detailed demonstration using the &lt;code&gt;collinear::target_encoding_lab()&lt;/code&gt; function, offering a hands-on exploration of various target encoding methods, parameter combinations, and their visual representations.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The post intends to serve as a useful resource for data scientists exploring alternative encoding techniques for categorical predictors.&lt;/p&gt;
&lt;h1 id=&#34;resources&#34;&gt;Resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/BlasBenito/notebooks/blob/main/target_encoding.Rmd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactive notebook of this post&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://doi.org/10.1145/507533.507538&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://towardsdatascience.com/extending-target-encoding-443aa9414cae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Extending Target Encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://maxhalford.github.io/blog/target-encoding/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Target encoding done the right way&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;r-packages&#34;&gt;R packages&lt;/h1&gt;
&lt;p&gt;This tutorial requires the development version (&amp;gt;= 1.0.3) of the newly released R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;, and a few more.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#required
install.packages(&amp;quot;remotes&amp;quot;)
remotes::install_github(
  repo = &amp;quot;blasbenito/collinear&amp;quot;, 
  ref = &amp;quot;development&amp;quot;,
  force = TRUE
  )
install.packages(&amp;quot;fastDummies&amp;quot;)
install.packages(&amp;quot;rpart&amp;quot;)
install.packages(&amp;quot;rpart.plot&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)
install.packages(&amp;quot;ggplot2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;categorical-predictors-can-be-annoying&#34;&gt;Categorical Predictors Can Be Annoying&lt;/h1&gt;
&lt;p&gt;I bet you have experienced it during an Exploratory Data Analysis (EDA) or a feature selection for model training and the likes. You likely had a nice bunch of numerical predictors, and then some things like &amp;ldquo;sampling_location&amp;rdquo;, &amp;ldquo;region_name&amp;rdquo;, &amp;ldquo;favorite_color&amp;rdquo;, or any other type of character or factor columns. And you had to branch your code to deal with numeric and categorical variables separately. Or maybe chose to ignore them, as I have done plenty of times.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s why many efforts have been made to convert them to numeric and kill the problem at once. And now we all have two problems to solve instead.&lt;/p&gt;
&lt;p&gt;Let me go ahead and illustrate the issue. There is a data frame in the &lt;code&gt;collinear&lt;/code&gt; R package named &lt;code&gt;vi&lt;/code&gt;, with one response variable named &lt;code&gt;vi_numeric&lt;/code&gt;, and several numeric and categorical predictors in the vector &lt;code&gt;vi_predictors&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(
  vi,
  vi_predictors
)

dplyr::glimpse(vi)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 30,000
## Columns: 68
## $ longitude                  &amp;lt;dbl&amp;gt; -114.254306, 114.845693, -122.145972, 108.3…
## $ latitude                   &amp;lt;dbl&amp;gt; 45.0540272, 26.2706940, 56.3790272, 29.9456…
## $ vi_numeric                 &amp;lt;dbl&amp;gt; 0.38, 0.53, 0.45, 0.69, 0.42, 0.68, 0.70, 0…
## $ vi_counts                  &amp;lt;int&amp;gt; 380, 530, 450, 690, 420, 680, 700, 260, 550…
## $ vi_binomial                &amp;lt;dbl&amp;gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0…
## $ vi_categorical             &amp;lt;chr&amp;gt; &amp;quot;medium&amp;quot;, &amp;quot;high&amp;quot;, &amp;quot;medium&amp;quot;, &amp;quot;very_high&amp;quot;, &amp;quot;m…
## $ vi_factor                  &amp;lt;fct&amp;gt; medium, high, medium, very_high, medium, ve…
## $ koppen_zone                &amp;lt;chr&amp;gt; &amp;quot;BSk&amp;quot;, &amp;quot;Cfa&amp;quot;, &amp;quot;Dfc&amp;quot;, &amp;quot;Cfb&amp;quot;, &amp;quot;Aw&amp;quot;, &amp;quot;Cfa&amp;quot;, &amp;quot;A…
## $ koppen_group               &amp;lt;chr&amp;gt; &amp;quot;Arid&amp;quot;, &amp;quot;Temperate&amp;quot;, &amp;quot;Cold&amp;quot;, &amp;quot;Temperate&amp;quot;, &amp;quot;…
## $ koppen_description         &amp;lt;chr&amp;gt; &amp;quot;steppe, cold&amp;quot;, &amp;quot;no dry season, hot summer&amp;quot;…
## $ soil_type                  &amp;lt;fct&amp;gt; Cambisols, Acrisols, Luvisols, Alisols, Gle…
## $ topo_slope                 &amp;lt;int&amp;gt; 6, 2, 0, 10, 0, 10, 6, 0, 2, 0, 0, 1, 0, 1,…
## $ topo_diversity             &amp;lt;int&amp;gt; 29, 24, 21, 25, 19, 30, 26, 20, 26, 22, 25,…
## $ topo_elevation             &amp;lt;int&amp;gt; 1821, 143, 765, 1474, 378, 485, 604, 1159, …
## $ swi_mean                   &amp;lt;dbl&amp;gt; 27.5, 56.1, 41.4, 59.3, 37.4, 56.3, 52.3, 2…
## $ swi_max                    &amp;lt;dbl&amp;gt; 62.9, 74.4, 81.9, 81.1, 83.2, 73.8, 55.8, 3…
## $ swi_min                    &amp;lt;dbl&amp;gt; 24.5, 33.3, 42.2, 31.3, 8.3, 28.8, 25.3, 11…
## $ swi_range                  &amp;lt;dbl&amp;gt; 38.4, 41.2, 39.7, 49.8, 74.9, 45.0, 30.5, 2…
## $ soil_temperature_mean      &amp;lt;dbl&amp;gt; 4.8, 19.9, 1.2, 13.0, 28.2, 18.1, 21.5, 23.…
## $ soil_temperature_max       &amp;lt;dbl&amp;gt; 29.9, 32.6, 20.4, 24.6, 41.6, 29.1, 26.4, 4…
## $ soil_temperature_min       &amp;lt;dbl&amp;gt; -12.4, 3.9, -16.0, -0.4, 16.8, 4.1, 17.3, 5…
## $ soil_temperature_range     &amp;lt;dbl&amp;gt; 42.3, 28.8, 36.4, 25.0, 24.8, 24.9, 9.1, 38…
## $ soil_sand                  &amp;lt;int&amp;gt; 41, 39, 27, 29, 48, 33, 30, 78, 23, 64, 54,…
## $ soil_clay                  &amp;lt;int&amp;gt; 20, 24, 28, 31, 27, 29, 40, 15, 26, 22, 23,…
## $ soil_silt                  &amp;lt;int&amp;gt; 38, 35, 43, 38, 23, 36, 29, 6, 49, 13, 22, …
## $ soil_ph                    &amp;lt;dbl&amp;gt; 6.5, 5.9, 5.6, 5.5, 6.5, 5.8, 5.2, 7.1, 7.3…
## $ soil_soc                   &amp;lt;dbl&amp;gt; 43.1, 14.6, 36.4, 34.9, 8.1, 20.8, 44.5, 4.…
## $ soil_nitrogen              &amp;lt;dbl&amp;gt; 2.8, 1.3, 2.9, 3.6, 1.2, 1.9, 2.8, 0.6, 3.1…
## $ solar_rad_mean             &amp;lt;dbl&amp;gt; 17.634, 19.198, 13.257, 14.163, 24.512, 17.…
## $ solar_rad_max              &amp;lt;dbl&amp;gt; 31.317, 24.498, 25.283, 17.237, 28.038, 22.…
## $ solar_rad_min              &amp;lt;dbl&amp;gt; 5.209, 13.311, 1.587, 9.642, 19.102, 12.196…
## $ solar_rad_range            &amp;lt;dbl&amp;gt; 26.108, 11.187, 23.696, 7.595, 8.936, 10.20…
## $ growing_season_length      &amp;lt;dbl&amp;gt; 139, 365, 164, 333, 228, 365, 365, 60, 365,…
## $ growing_season_temperature &amp;lt;dbl&amp;gt; 12.65, 19.35, 11.55, 12.45, 26.45, 17.75, 2…
## $ growing_season_rainfall    &amp;lt;dbl&amp;gt; 224.5, 1493.4, 345.4, 1765.5, 984.4, 1860.5…
## $ growing_degree_days        &amp;lt;dbl&amp;gt; 2140.5, 7080.9, 2053.2, 4162.9, 10036.7, 64…
## $ temperature_mean           &amp;lt;dbl&amp;gt; 3.65, 19.35, 1.45, 11.35, 27.55, 17.65, 22.…
## $ temperature_max            &amp;lt;dbl&amp;gt; 24.65, 33.35, 21.15, 23.75, 38.35, 30.55, 2…
## $ temperature_min            &amp;lt;dbl&amp;gt; -14.05, 3.05, -18.25, -3.55, 19.15, 2.45, 1…
## $ temperature_range          &amp;lt;dbl&amp;gt; 38.7, 30.3, 39.4, 27.3, 19.2, 28.1, 7.0, 29…
## $ temperature_seasonality    &amp;lt;dbl&amp;gt; 882.6, 786.6, 1070.9, 724.7, 219.3, 747.2, …
## $ rainfall_mean              &amp;lt;int&amp;gt; 446, 1493, 560, 1794, 990, 1860, 3150, 356,…
## $ rainfall_min               &amp;lt;int&amp;gt; 25, 37, 24, 29, 0, 60, 122, 1, 10, 12, 0, 0…
## $ rainfall_max               &amp;lt;int&amp;gt; 62, 209, 87, 293, 226, 275, 425, 62, 256, 3…
## $ rainfall_range             &amp;lt;int&amp;gt; 37, 172, 63, 264, 226, 215, 303, 61, 245, 2…
## $ evapotranspiration_mean    &amp;lt;dbl&amp;gt; 78.32, 105.88, 50.03, 64.65, 156.60, 108.50…
## $ evapotranspiration_max     &amp;lt;dbl&amp;gt; 164.70, 190.86, 117.53, 115.79, 187.71, 191…
## $ evapotranspiration_min     &amp;lt;dbl&amp;gt; 13.67, 50.44, 3.53, 28.01, 128.59, 51.39, 8…
## $ evapotranspiration_range   &amp;lt;dbl&amp;gt; 151.03, 140.42, 113.99, 87.79, 59.13, 139.9…
## $ cloud_cover_mean           &amp;lt;int&amp;gt; 31, 48, 42, 64, 38, 52, 60, 13, 53, 20, 11,…
## $ cloud_cover_max            &amp;lt;int&amp;gt; 39, 61, 49, 71, 58, 67, 77, 18, 60, 27, 23,…
## $ cloud_cover_min            &amp;lt;int&amp;gt; 16, 34, 33, 54, 19, 39, 45, 6, 45, 14, 2, 1…
## $ cloud_cover_range          &amp;lt;int&amp;gt; 23, 27, 15, 17, 38, 27, 32, 11, 15, 12, 21,…
## $ aridity_index              &amp;lt;dbl&amp;gt; 0.54, 1.27, 0.90, 2.08, 0.55, 1.67, 2.88, 0…
## $ humidity_mean              &amp;lt;dbl&amp;gt; 55.56, 62.14, 59.87, 69.32, 51.60, 62.76, 7…
## $ humidity_max               &amp;lt;dbl&amp;gt; 63.98, 65.00, 68.19, 71.90, 67.07, 65.68, 7…
## $ humidity_min               &amp;lt;dbl&amp;gt; 48.41, 58.97, 53.75, 67.21, 33.89, 59.92, 7…
## $ humidity_range             &amp;lt;dbl&amp;gt; 15.57, 6.03, 14.44, 4.69, 33.18, 5.76, 3.99…
## $ biogeo_ecoregion           &amp;lt;chr&amp;gt; &amp;quot;South Central Rockies forests&amp;quot;, &amp;quot;Jian Nan …
## $ biogeo_biome               &amp;lt;chr&amp;gt; &amp;quot;Temperate Conifer Forests&amp;quot;, &amp;quot;Tropical &amp;amp; Su…
## $ biogeo_realm               &amp;lt;chr&amp;gt; &amp;quot;Nearctic&amp;quot;, &amp;quot;Indomalayan&amp;quot;, &amp;quot;Nearctic&amp;quot;, &amp;quot;Pal…
## $ country_name               &amp;lt;chr&amp;gt; &amp;quot;United States of America&amp;quot;, &amp;quot;China&amp;quot;, &amp;quot;Canad…
## $ country_population         &amp;lt;dbl&amp;gt; 313973000, 1338612970, 33487208, 1338612970…
## $ country_gdp                &amp;lt;dbl&amp;gt; 15094000, 7973000, 1300000, 7973000, 15860,…
## $ country_income             &amp;lt;chr&amp;gt; &amp;quot;1. High income: OECD&amp;quot;, &amp;quot;3. Upper middle in…
## $ continent                  &amp;lt;chr&amp;gt; &amp;quot;North America&amp;quot;, &amp;quot;Asia&amp;quot;, &amp;quot;North America&amp;quot;, &amp;quot;…
## $ region                     &amp;lt;chr&amp;gt; &amp;quot;Americas&amp;quot;, &amp;quot;Asia&amp;quot;, &amp;quot;Americas&amp;quot;, &amp;quot;Asia&amp;quot;, &amp;quot;Af…
## $ subregion                  &amp;lt;chr&amp;gt; &amp;quot;Northern America&amp;quot;, &amp;quot;Eastern Asia&amp;quot;, &amp;quot;Northe…
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The categorical variables in this data frame are identified below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vi_categorical &amp;lt;- collinear::identify_predictors_categorical(
  df = vi,
  predictors = vi_predictors
)
vi_categorical
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;koppen_zone&amp;quot;        &amp;quot;koppen_group&amp;quot;       &amp;quot;koppen_description&amp;quot;
##  [4] &amp;quot;soil_type&amp;quot;          &amp;quot;biogeo_ecoregion&amp;quot;   &amp;quot;biogeo_biome&amp;quot;      
##  [7] &amp;quot;biogeo_realm&amp;quot;       &amp;quot;country_name&amp;quot;       &amp;quot;country_income&amp;quot;    
## [10] &amp;quot;continent&amp;quot;          &amp;quot;region&amp;quot;             &amp;quot;subregion&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And finally, their number of categories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data.frame(
  name = vi_categorical,
  categories = lapply(
  X = vi_categorical,
  FUN = function(x) length(unique(vi[[x]]))
) |&amp;gt; 
  unlist()
) |&amp;gt; 
  dplyr::arrange(
    dplyr::desc(categories)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  name categories
## 1    biogeo_ecoregion        604
## 2        country_name        176
## 3           soil_type         29
## 4         koppen_zone         25
## 5           subregion         21
## 6  koppen_description         19
## 7        biogeo_biome         13
## 8        biogeo_realm          7
## 9           continent          7
## 10     country_income          6
## 11             region          6
## 12       koppen_group          5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some of them, like &lt;code&gt;country_name&lt;/code&gt; and &lt;code&gt;biogeo_ecoregion&lt;/code&gt;, have a cardinality high enough to ruin our day, don&amp;rsquo;t they? But ok, let&amp;rsquo;s start with one with a moderate number of categories, like &lt;code&gt;koppen_zone&lt;/code&gt;. This variable has 25 categories representing climate zones.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sort(unique(vi$koppen_zone))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Af&amp;quot;  &amp;quot;Am&amp;quot;  &amp;quot;Aw&amp;quot;  &amp;quot;BSh&amp;quot; &amp;quot;BSk&amp;quot; &amp;quot;BWh&amp;quot; &amp;quot;BWk&amp;quot; &amp;quot;Cfa&amp;quot; &amp;quot;Cfb&amp;quot; &amp;quot;Cfc&amp;quot; &amp;quot;Csa&amp;quot; &amp;quot;Csb&amp;quot;
## [13] &amp;quot;Cwa&amp;quot; &amp;quot;Cwb&amp;quot; &amp;quot;Dfa&amp;quot; &amp;quot;Dfb&amp;quot; &amp;quot;Dfc&amp;quot; &amp;quot;Dfd&amp;quot; &amp;quot;Dsa&amp;quot; &amp;quot;Dsb&amp;quot; &amp;quot;Dsc&amp;quot; &amp;quot;Dwa&amp;quot; &amp;quot;Dwb&amp;quot; &amp;quot;Dwc&amp;quot;
## [25] &amp;quot;ET&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;one-hot-encoding-is-here&#34;&gt;One-hot Encoding is here&amp;hellip;&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s use it as predictor of &lt;code&gt;vi_numeric&lt;/code&gt; in a linear model and take a look at the summary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;lm(
  formula = vi_numeric ~ koppen_zone, 
  data = vi
  ) |&amp;gt; 
  summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = vi_numeric ~ koppen_zone, data = vi)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.57090 -0.05592 -0.00305  0.05695  0.49212 
## 
## Coefficients:
##                 Estimate Std. Error  t value Pr(&amp;gt;|t|)    
## (Intercept)     0.670899   0.002054  326.651  &amp;lt; 2e-16 ***
## koppen_zoneAm  -0.022807   0.003151   -7.239 4.64e-13 ***
## koppen_zoneAw  -0.143375   0.002434  -58.903  &amp;lt; 2e-16 ***
## koppen_zoneBSh -0.347894   0.002787 -124.839  &amp;lt; 2e-16 ***
## koppen_zoneBSk -0.422162   0.002823 -149.523  &amp;lt; 2e-16 ***
## koppen_zoneBWh -0.537854   0.002392 -224.859  &amp;lt; 2e-16 ***
## koppen_zoneBWk -0.543022   0.002906 -186.883  &amp;lt; 2e-16 ***
## koppen_zoneCfa -0.104730   0.003087  -33.928  &amp;lt; 2e-16 ***
## koppen_zoneCfb -0.081909   0.003949  -20.744  &amp;lt; 2e-16 ***
## koppen_zoneCfc -0.120899   0.017419   -6.941 3.99e-12 ***
## koppen_zoneCsa -0.274720   0.005145  -53.399  &amp;lt; 2e-16 ***
## koppen_zoneCsb -0.136575   0.006142  -22.237  &amp;lt; 2e-16 ***
## koppen_zoneCwa -0.149006   0.003318  -44.910  &amp;lt; 2e-16 ***
## koppen_zoneCwb -0.177753   0.004579  -38.817  &amp;lt; 2e-16 ***
## koppen_zoneDfa -0.214981   0.004437  -48.453  &amp;lt; 2e-16 ***
## koppen_zoneDfb -0.179080   0.003347  -53.499  &amp;lt; 2e-16 ***
## koppen_zoneDfc -0.237050   0.003937  -60.207  &amp;lt; 2e-16 ***
## koppen_zoneDfd -0.395899   0.065900   -6.008 1.91e-09 ***
## koppen_zoneDsa -0.462494   0.011401  -40.567  &amp;lt; 2e-16 ***
## koppen_zoneDsb -0.330969   0.008056  -41.084  &amp;lt; 2e-16 ***
## koppen_zoneDsc -0.327097   0.011244  -29.090  &amp;lt; 2e-16 ***
## koppen_zoneDwa -0.282620   0.005248  -53.850  &amp;lt; 2e-16 ***
## koppen_zoneDwb -0.254027   0.005981  -42.473  &amp;lt; 2e-16 ***
## koppen_zoneDwc -0.306156   0.005660  -54.096  &amp;lt; 2e-16 ***
## koppen_zoneET  -0.297869   0.011649  -25.571  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.09315 on 29975 degrees of freedom
## Multiple R-squared:  0.805,	Adjusted R-squared:  0.8049 
## F-statistic:  5157 on 24 and 29975 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look at that. What the hell happened there? Well, linear models cannot deal with categorical predictors, so they create numeric &lt;strong&gt;dummy variables&lt;/strong&gt; instead. The function &lt;code&gt;stats::model.matrix()&lt;/code&gt; does exactly that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dummy_variables &amp;lt;- stats::model.matrix( 
  ~ koppen_zone,
  data = vi
  )
ncol(dummy_variables)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 25
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dummy_variables[1:10, 1:10]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    (Intercept) koppen_zoneAm koppen_zoneAw koppen_zoneBSh koppen_zoneBSk
## 1            1             0             0              0              1
## 2            1             0             0              0              0
## 3            1             0             0              0              0
## 4            1             0             0              0              0
## 5            1             0             1              0              0
## 6            1             0             0              0              0
## 7            1             0             0              0              0
## 8            1             0             0              1              0
## 9            1             0             0              0              0
## 10           1             0             0              0              0
##    koppen_zoneBWh koppen_zoneBWk koppen_zoneCfa koppen_zoneCfb koppen_zoneCfc
## 1               0              0              0              0              0
## 2               0              0              1              0              0
## 3               0              0              0              0              0
## 4               0              0              0              1              0
## 5               0              0              0              0              0
## 6               0              0              1              0              0
## 7               0              0              0              0              0
## 8               0              0              0              0              0
## 9               0              0              0              0              0
## 10              1              0              0              0              0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function first creates an Intercept column with all ones. Then, for each original category except the first one (&amp;ldquo;Af&amp;rdquo;), a new column with value 1 in the cases where the given category was present and 0 otherwise is created. The category with no column (&amp;ldquo;Af&amp;rdquo;) is represented in these cases in the intercept where all other dummy columns are zero. This is, essentially, &lt;strong&gt;one-hot encoding&lt;/strong&gt; with a little twist. You will find most people use the terms &lt;em&gt;dummy variables&lt;/em&gt; and &lt;em&gt;one-hot encoding&lt;/em&gt; interchangeably, and that&amp;rsquo;s ok. But in the end, the little twist of omitting the first category is what differentiates them. Most functions performing one-hot encoding, no matter their name, are creating as many columns as categories. That is for example the case of &lt;code&gt;fastDummies::dummy_cols()&lt;/code&gt;, from the R package 
&lt;a href=&#34;https://jacobkap.github.io/fastDummies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;fastDummies&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- fastDummies::dummy_cols(
  .data = vi[, &amp;quot;koppen_zone&amp;quot;, drop = FALSE],
  select_columns = &amp;quot;koppen_zone&amp;quot;,
  remove_selected_columns = TRUE
)
dplyr::glimpse(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 30,000
## Columns: 25
## $ koppen_zone_Af  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Am  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Aw  &amp;lt;int&amp;gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_BSh &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …
## $ koppen_zone_BSk &amp;lt;int&amp;gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_BWh &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, …
## $ koppen_zone_BWk &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …
## $ koppen_zone_Cfa &amp;lt;int&amp;gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Cfb &amp;lt;int&amp;gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …
## $ koppen_zone_Cfc &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Csa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Csb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Cwa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Cwb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dfa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …
## $ koppen_zone_Dfb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dfc &amp;lt;int&amp;gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dfd &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dsa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dsb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dsc &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dwa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dwb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dwc &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_ET  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;to-mess-up-your-models&#34;&gt;&amp;hellip;to mess-up your models&lt;/h1&gt;
&lt;p&gt;As good as one-hot encoding is to fit linear models when predictors are categorical, it creates a couple of glaring issues that are hard to address when the number of encoded categories is high.&lt;/p&gt;
&lt;p&gt;The first issue can easily be named the &lt;strong&gt;dimensionality explosion&lt;/strong&gt;. If we created dummy variables for all categorical predictors in &lt;code&gt;vi&lt;/code&gt;, then we&amp;rsquo;d go from the original 61 predictors to a total of 967 new columns to handle. This alone can degrade the computational performance of a model due to increased data size.&lt;/p&gt;
&lt;p&gt;The second issue is &lt;strong&gt;increased multicollinearity&lt;/strong&gt;. One-hot encoded features are highly collinear, which makes obtaining accurate estimates for the coefficients of the encoded categories very hard. Look at the Variance Inflation Factors of the encoded Koppen zones, they have incredibly high values!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::vif_df(
  df = df,
  quiet = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          predictor          vif
## 6  koppen_zone_BWh 2.403991e+15
## 3   koppen_zone_Aw 2.177627e+15
## 4  koppen_zone_BSh 1.158438e+15
## 5  koppen_zone_BSk 1.100300e+15
## 1   koppen_zone_Af 9.879589e+14
## 7  koppen_zone_BWk 9.866240e+14
## 8  koppen_zone_Cfa 7.966760e+14
## 2   koppen_zone_Am 7.440723e+14
## 13 koppen_zone_Cwa 6.309241e+14
## 16 koppen_zone_Dfb 6.139201e+14
## 17 koppen_zone_Dfc 3.863684e+14
## 9  koppen_zone_Cfb 3.834325e+14
## 15 koppen_zone_Dfa 2.838687e+14
## 14 koppen_zone_Cwb 2.624933e+14
## 11 koppen_zone_Csa 1.984881e+14
## 22 koppen_zone_Dwa 1.894423e+14
## 24 koppen_zone_Dwc 1.592088e+14
## 23 koppen_zone_Dwb 1.405032e+14
## 12 koppen_zone_Csb 1.323997e+14
## 20 koppen_zone_Dsb 7.338609e+13
## 21 koppen_zone_Dsc 3.652432e+13
## 19 koppen_zone_Dsa 3.549784e+13
## 25  koppen_zone_ET 3.395786e+13
## 10 koppen_zone_Cfc 1.493932e+13
## 18 koppen_zone_Dfd 1.031226e+12
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On top of those issues, one-hot encoding also causes &lt;strong&gt;sparsity&lt;/strong&gt; in tree-based models. Let me show you an example. Below I train a recursive partition tree using &lt;code&gt;vi_numeric&lt;/code&gt; as response, and the one-hot encoded version of &lt;code&gt;koppen_zone&lt;/code&gt; we have in &lt;code&gt;df&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#add response variable to df
df$vi_numeric &amp;lt;- vi$vi_numeric

#fit model using all one-hot encoded variables
koppen_zone_one_hot &amp;lt;- rpart::rpart(
  formula = vi_numeric ~ .,
  data = df
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I do the same using the categorical version of &lt;code&gt;koppen_zone&lt;/code&gt; in &lt;code&gt;vi&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;koppen_zone_categorical &amp;lt;- rpart::rpart(
  formula = vi_numeric ~ koppen_zone,
  data = vi
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I am plotting the skeletons of these trees side by side (we don&amp;rsquo;t care about numbers here).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#plot tree skeleton
par(mfrow = c(1, 2))
plot(koppen_zone_one_hot, main = &amp;quot;One-hot encoding&amp;quot;)
plot(koppen_zone_categorical, main = &amp;quot;Categorical&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/target-encoding/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;576&#34; /&gt;
&lt;p&gt;Notice the stark differences in tree structure between both options. On the left, the tree trained on the one-hot encoded data only shows growth on one side! This is the &lt;em&gt;sparsity&lt;/em&gt; I was talking about before. On the right side, however, the tree based on the categorical variable shows a balanced and healthy structure. One-hot encoded data can easily mess up a single univariate regression tree, so imagine what it can do to your fancy random forest model with hundreds of these trees.&lt;/p&gt;
&lt;p&gt;In the end, the magic of one-hot encoding is in its inherent ability to create two or three problems for each one it promised to solve. We all know someone like that. Not so hot, if you ask me.&lt;/p&gt;
&lt;h1 id=&#34;target-encoding-mean-encoding-and-dummy-variables-all-the-same&#34;&gt;Target Encoding, Mean Encoding, and Dummy Variables (All The Same)&lt;/h1&gt;
&lt;p&gt;On a bright summer day of 2001, 
&lt;a href=&#34;https://www.aitimejournal.com/interview-with-daniele-micci-barreca-product-analytics-lead-data-science-google/30110/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniele Micci-Barreca&lt;/a&gt; finally got sick of the one-hot encoding wonders and decided to publish 
&lt;a href=&#34;https://doi.org/10.1145/507533.507538&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;his ideas on a suitable alternative&lt;/a&gt; others later named &lt;em&gt;mean encoding&lt;/em&gt; or &lt;em&gt;target encoding&lt;/em&gt;. He told the story himself 20 years later, in a nice blog post titled 
&lt;a href=&#34;https://towardsdatascience.com/extending-target-encoding-443aa9414cae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Extending Target Encoding&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But what is target encoding? Let&amp;rsquo;s start with a continuous response variable &lt;code&gt;y&lt;/code&gt; (a.k.a &lt;em&gt;the target&lt;/em&gt;) and a categorical predictor &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;mean-encoding&#34;&gt;Mean Encoding&lt;/h2&gt;
&lt;p&gt;In &lt;em&gt;it&amp;rsquo;s simplest form&lt;/em&gt;, target encoding replaces each category in &lt;code&gt;x&lt;/code&gt; with the mean of &lt;code&gt;y&lt;/code&gt; across the category cases. This results in a new numeric version of &lt;code&gt;x&lt;/code&gt; named &lt;code&gt;x_encoded&lt;/code&gt; in the example below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx |&amp;gt; 
  dplyr::group_by(x) |&amp;gt; 
  dplyr::mutate(
    x_encoded = mean(y)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a             2
## 2     2 a             2
## 3     3 a             2
## 4     4 b             5
## 5     5 b             5
## 6     6 b             5
## 7     7 c             7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simple is good, right? But sometimes it&amp;rsquo;s not. In our toy case, the category &amp;ldquo;c&amp;rdquo; has only one case that maps directly to an actual value of &lt;code&gt;y&lt;/code&gt;.Imagine the worst case scenario of &lt;code&gt;x&lt;/code&gt; having one different category per row, then &lt;code&gt;x_encoded&lt;/code&gt; would be identical to &lt;code&gt;y&lt;/code&gt;!&lt;/p&gt;
&lt;h2 id=&#34;mean-encoding-with-additive-smoothing&#34;&gt;Mean Encoding With Additive Smoothing&lt;/h2&gt;
&lt;p&gt;The issue can be solved by pushing the mean of &lt;code&gt;y&lt;/code&gt; for each category in &lt;code&gt;x&lt;/code&gt; towards the global mean of &lt;code&gt;y&lt;/code&gt; by the weighted sample size of the category, as suggested by the expression&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$x\_encoded_i = \frac{n_i \times \overline{y}_i + m \times \overline{y}}{n_i + m}$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;\(n_i\)&lt;/code&gt; is the size of the category &lt;code&gt;\(i\)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\(\overline{y}_i\)&lt;/code&gt; is the mean of the target over the category &lt;code&gt;\(i\)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\(m\)&lt;/code&gt; is the smoothing parameter.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\(\overline{y}\)&lt;/code&gt; is the global mean of the target.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y_mean &amp;lt;- mean(yx$y)

m &amp;lt;- 3

yx |&amp;gt; 
  dplyr::group_by(x) |&amp;gt; 
  dplyr::mutate(
    x_encoded = 
      (dplyr::n() * mean(y) + m * y_mean) / (dplyr::n() + m)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a          3   
## 2     2 a          3   
## 3     3 a          3   
## 4     4 b          4.5 
## 5     5 b          4.5 
## 6     6 b          4.5 
## 7     7 c          4.75
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far so good! But still, the simplest implementations of target encoding generate repeated values for all cases within a category. This can still mess-up tree-based models a bit, because splits may happen again and again in the same values of the predictor. However, there are several strategies to limit this issue as well.&lt;/p&gt;
&lt;h2 id=&#34;leave-one-out-target-encoding&#34;&gt;Leave-one-out Target Encoding&lt;/h2&gt;
&lt;p&gt;In this version of target encoding, the encoded value of one case within a category is the mean of all other cases within the same category. This results in a robust encoding that avoids direct reference to the target value of the sample being encoded, and does not generate repeated values.&lt;/p&gt;
&lt;p&gt;The code below implements the idea in a way so simple that it cannot even deal with one-case categories.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx |&amp;gt;
  dplyr::group_by(x) |&amp;gt;
  dplyr::mutate(
    x_encoded = (sum(y) - y) / (dplyr::n() - 1)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a           2.5
## 2     2 a           2  
## 3     3 a           1.5
## 4     4 b           5.5
## 5     5 b           5  
## 6     6 b           4.5
## 7     7 c         NaN
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;mean-encoding-with-white-noise&#34;&gt;Mean Encoding with White Noise&lt;/h2&gt;
&lt;p&gt;Another way to avoid repeated values while keeping the encoding as simple as possible consists of just adding a white noise to the encoded values. The code below adds noise generated by &lt;code&gt;stats::runif()&lt;/code&gt; to the mean-encoded values, but other options such as &lt;code&gt;stats::rnorm()&lt;/code&gt; (noise from a normal distribution) can be useful here. Since white noise is random, we need to set the seed of the pseudo-random number generator (with &lt;code&gt;set.seed()&lt;/code&gt;) to obtain constant results every time we run the code below.&lt;/p&gt;
&lt;p&gt;When using this method we have to be careful with the amount of noise we add. It should be a harmless fraction of target, small enough to not throw a model off the signal provided by the encoded variable. In our toy case &lt;code&gt;y&lt;/code&gt; is between 1 and 7, so something like &amp;ldquo;one percent of the maximum&amp;rdquo; could work well here.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#maximum noise to add
max_noise &amp;lt;- max(yx$y)/100

#set seed for reproducibility
set.seed(1)

yx |&amp;gt; 
  dplyr::group_by(x) |&amp;gt; 
  dplyr::mutate(
    x_encoded = mean(y) + runif(n = dplyr::n(), max = max_noise)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a          2.02
## 2     2 a          2.03
## 3     3 a          2.04
## 4     4 b          5.06
## 5     5 b          5.01
## 6     6 b          5.06
## 7     7 c          7.07
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This method can deal with one-case categories without issues, and does not generate repeated values, but in exchange, we have to be mindful of the amount of noise we add, and we have to set a random seed to ensure reproducibility.&lt;/p&gt;
&lt;h2 id=&#34;rank-encoding-plus-white-noise&#34;&gt;Rank Encoding plus White Noise&lt;/h2&gt;
&lt;p&gt;This is a little different from all the other methods, because it does not map the categories to values from the target, but to the rank/order of the target means per category. It basically converts the categorical variable into an ordinal one arranged along with the target, and then adds white noise on top to avoid value repetition.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#maximum noise as function of the number of categories
max_noise &amp;lt;- length(unique(yx$x))/100

yx |&amp;gt; 
  dplyr::arrange(y) |&amp;gt; 
  dplyr::group_by(x) |&amp;gt; 
  dplyr::mutate(
    x_encoded = dplyr::cur_group_id() + runif(n = dplyr::n(), max = max_noise)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a          1.02
## 2     2 a          1.02
## 3     3 a          1.00
## 4     4 b          2.01
## 5     5 b          2.01
## 6     6 b          2.02
## 7     7 c          3.01
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-target-encoding-lab&#34;&gt;The Target Encoding Lab&lt;/h2&gt;
&lt;p&gt;The function &lt;code&gt;collinear::target_encoding_lab()&lt;/code&gt; implements all these encoding methods, and allows defining different combinations of parameters. It was designed to help understand how they work, and maybe help make choices about what&amp;rsquo;s the right encoding for a given categorical predictor.&lt;/p&gt;
&lt;p&gt;In the example below, the methods rank, mean, and leave-one-out are computed with white noise of 0 and 0.1 (that&amp;rsquo;s the width of the uniform distribution the noise is extracted from), the mean is also with and without smoothing, and the rnorm is computed using two different multipliers of the standard deviation of the normal distribution computed for each group in the predictor, just to help control the data spread.&lt;/p&gt;
&lt;p&gt;The function also uses a random seed to generate the same noise across the encoded versions of the predictor to make them as comparable as possible. Every time you change the seed, results using white noise and the rnorm method should change as well.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx_encoded &amp;lt;- target_encoding_lab(
  df = yx,
  response = &amp;quot;y&amp;quot;,
  predictors = &amp;quot;x&amp;quot;,
  white_noise = c(0, 0.1),
  smoothing = c(0, 2),
  quiet = FALSE,
  seed = 1, #for reproducibility
  overwrite = FALSE #to overwrite or not the predictors with their encodings
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## collinear::target_encoding_lab(): using response &#39;y&#39; to encode categorical predictors:
##  - x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dplyr::glimpse(yx_encoded)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 7
## Columns: 14
## $ y                                               &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7
## $ x                                               &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;b…
## $ x__encoded_loo                                  &amp;lt;dbl&amp;gt; 2.5, 2.0, 1.5, 5.5, 5.…
## $ x__encoded_loo__noise_0.1__seed_1               &amp;lt;dbl&amp;gt; 2.554579, 1.564069, 2.…
## $ x__encoded_loo                                  &amp;lt;dbl&amp;gt; 2.5, 2.0, 1.5, 5.5, 5.…
## $ x__encoded_loo__noise_0.1__seed_1               &amp;lt;dbl&amp;gt; 2.554579, 1.564069, 2.…
## $ x__encoded_mean                                 &amp;lt;dbl&amp;gt; 2, 2, 2, 5, 5, 5, 7
## $ x__encoded_mean__noise_0.1__seed_1              &amp;lt;dbl&amp;gt; 2.054579, 1.564069, 2.…
## $ x__encoded_mean__smoothing_2                    &amp;lt;dbl&amp;gt; 2.8, 2.8, 2.8, 4.6, 4.…
## $ x__encoded_mean__smoothing_2__noise_0.1__seed_1 &amp;lt;dbl&amp;gt; 2.854579, 2.364069, 3.…
## $ x__encoded_rank                                 &amp;lt;int&amp;gt; 1, 1, 1, 2, 2, 2, 3
## $ x__encoded_rank__noise_0.1__seed_1              &amp;lt;dbl&amp;gt; 1.0545786, 0.5640694, …
## $ x__encoded_rank                                 &amp;lt;int&amp;gt; 1, 1, 1, 2, 2, 2, 3
## $ x__encoded_rank__noise_0.1__seed_1              &amp;lt;dbl&amp;gt; 1.0545786, 0.5640694, …
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx_encoded |&amp;gt; 
  tidyr::pivot_longer(
    cols = dplyr::contains(&amp;quot;__encoded&amp;quot;),
    values_to = &amp;quot;x_encoded&amp;quot;
  ) |&amp;gt; 
  ggplot() + 
  facet_wrap(&amp;quot;name&amp;quot;) +
  aes(
    x = x_encoded,
    y = y,
    color = x
  ) +
  geom_point(size = 3) + 
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/target-encoding/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1152&#34; /&gt;
The function also allows to replace a given predictor with their selected encoding.
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx_encoded &amp;lt;- collinear::target_encoding_lab(
  df = yx,
  response = &amp;quot;y&amp;quot;,
  predictors = &amp;quot;x&amp;quot;,
  methods = &amp;quot;mean&amp;quot;, #selected encoding method
  smoothing = 2,
  quiet = FALSE,
  overwrite = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## collinear::target_encoding_lab(): using response &#39;y&#39; to encode categorical predictors:
##  - x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dplyr::glimpse(yx_encoded)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 7
## Columns: 2
## $ y &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7
## $ x &amp;lt;dbl&amp;gt; 2.8, 2.8, 2.8, 4.6, 4.6, 4.6, 5.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s all about target encoding so far!&lt;/p&gt;
&lt;p&gt;I have a post in my TODO list with a little real experiment comparing target encoding with one-hot encoding in tree-based models. If you are interested, stay tuned!&lt;/p&gt;
&lt;p&gt;Cheers,&lt;/p&gt;
&lt;p&gt;Blas&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Everything You Don&#39;t Need to Know About Variance Inflation Factors</title>
      <link>https://blasbenito.com/post/variance-inflation-factor/</link>
      <pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/variance-inflation-factor/</guid>
      <description>&lt;h1 id=&#34;resources&#34;&gt;Resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/BlasBenito/notebooks/blob/main/variance_inflation_factors.Rmd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rmarkdown notebook used in this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://blasbenito.com/post/multicollinearity-model-interpretability/&#34;&gt;Multicollinearity Hinders Model Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R package &lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post, I dive deep into Variance Inflation Factors (VIF) and their crucial role in identifying multicollinearity within linear models.&lt;/p&gt;
&lt;p&gt;The post covers the following main points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VIF meaning and interpretation&lt;/strong&gt;: Through practical examples, I demonstrate how to compute VIF values and their significance in model design. Particularly, I try to shed light on their influence on coefficient estimates and their confidence intervals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Impact of High VIF&lt;/strong&gt;: I use a small simulation to show how having a model design with a high VIF hinders the identification of predictors with moderate effects, particularly in situations with limited data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Effective VIF Management&lt;/strong&gt;: I introduce how to use the &lt;code&gt;collinear&lt;/code&gt; package and its &lt;code&gt;vif_select()&lt;/code&gt; function. to aid in the selection of predictors with low VIF, thereby enhancing model stability and interpretability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ultimately, this post serves as a comprehensive resource for understanding, interpreting, and managing VIF in the context of linear modeling. It caters to those with a strong command of R and a keen interest in statistical modeling.&lt;/p&gt;
&lt;h1 id=&#34;r-packages&#34;&gt;R packages&lt;/h1&gt;
&lt;p&gt;This tutorial requires the development version (&amp;gt;= 1.0.3) of the newly released R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;, and a few more.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#required
install.packages(&amp;quot;remotes&amp;quot;)
remotes::install_github(
  repo = &amp;quot;blasbenito/collinear&amp;quot;, 
  ref = &amp;quot;development&amp;quot;
  )
install.packages(&amp;quot;ranger&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)
install.packages(&amp;quot;ggplot2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;example-data&#34;&gt;Example data&lt;/h1&gt;
&lt;p&gt;This post uses the &lt;code&gt;toy&lt;/code&gt; data set shipped with the version &amp;gt;= 1.0.3 of the R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;. It is a data frame of centered and scaled variables representing a model design of the form &lt;code&gt;y ~ a + b + c + d&lt;/code&gt;, where the predictors show varying degrees of relatedness. Let&amp;rsquo;s load and check it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
library(ggplot2)
library(collinear)

toy |&amp;gt; 
  round(3) |&amp;gt; 
  head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        y      a      b      c      d
## 1  0.655  0.342 -0.158  0.254  0.502
## 2  0.610  0.219  1.814  0.450  1.373
## 3  0.316  1.078 -0.643  0.580  0.673
## 4  0.202  0.956 -0.815  1.168 -0.147
## 5 -0.509 -0.149 -0.356 -0.456  0.187
## 6  0.675  0.465  1.292 -0.020  0.983
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The columns in &lt;code&gt;toy&lt;/code&gt; are related as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: response generated from &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; using the expression &lt;code&gt;y = a * 0.75 + b * 0.25 + noise&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt;: predictor of &lt;code&gt;y&lt;/code&gt; uncorrelated with &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt;: predictor of &lt;code&gt;y&lt;/code&gt; uncorrelated with &lt;code&gt;a&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: predictor generated as &lt;code&gt;c = a + noise&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d&lt;/code&gt;: predictor generated as &lt;code&gt;d = (a + b)/2 + noise&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The pairwise correlations between all predictors in &lt;code&gt;toy&lt;/code&gt; are shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::cor_df(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x y correlation
## 1 c a  0.96154984
## 2 d b  0.63903887
## 3 d a  0.63575882
## 4 d c  0.61480312
## 5 b a -0.04740881
## 6 c b -0.04218308
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep these pairwise correlations in mind for what comes next!&lt;/p&gt;
&lt;h1 id=&#34;the-meaning-of-variance-inflation-factors&#34;&gt;The Meaning of Variance Inflation Factors&lt;/h1&gt;
&lt;p&gt;There are two general cases of multicollinearity in model designs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When there are pairs of predictors highly correlated.&lt;/li&gt;
&lt;li&gt;When there are &lt;strong&gt;predictors that are linear combinations of other predictors&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The focus of this post is on the second one.&lt;/p&gt;
&lt;p&gt;We can say a predictor is a linear combination of other predictors when it can be reasonably predicted from a multiple regression model against all other predictors.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say we focus on &lt;code&gt;a&lt;/code&gt; and fit the multiple regression model &lt;code&gt;a ~ b + c + d&lt;/code&gt;. The higher the R-squared of this model, the more confident we are to say that &lt;code&gt;a&lt;/code&gt; is a linear combination of &lt;code&gt;b + c + d&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#model of a against all other predictors
abcd_model &amp;lt;- lm(
  formula = a ~ b + c + d,
  data = toy
)

#r-squared of the a_model
abcd_R2 &amp;lt;- summary(abcd_model)$r.squared
abcd_R2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9381214
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the R-squared of &lt;code&gt;a&lt;/code&gt; against all other predictors is pretty high, it definitely seems that &lt;code&gt;a&lt;/code&gt; is a linear combination of the other predictors, and we can conclude that there is multicollinearity in the model design.&lt;/p&gt;
&lt;p&gt;However, as informative as this R-squared is, it tells us nothing about the consequences of having multicollinearity in our model design. And this is where &lt;strong&gt;Variance Inflation Factors&lt;/strong&gt;, or &lt;strong&gt;VIF&lt;/strong&gt; for short, come into play.&lt;/p&gt;
&lt;h2 id=&#34;what-are-variance-inflation-factors&#34;&gt;What are Variance Inflation Factors?&lt;/h2&gt;
&lt;p&gt;The Variance Inflation Factor (VIF) of a predictor is computed as &lt;code&gt;\(1/(1 - R^2)\)&lt;/code&gt;, where &lt;code&gt;\(R^²\)&lt;/code&gt; is the R-squared of the multiple linear regression of the predictor against all other predictors.&lt;/p&gt;
&lt;p&gt;In the case of &lt;code&gt;a&lt;/code&gt;, we just have to apply the VIF expression to the R-squared of the regression model against all other predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;abcd_vif &amp;lt;- 1/(1-abcd_R2)
abcd_vif
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16.16067
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This VIF score is relative to the other predictors in the model design. If we change the model design, so does the VIF of all predictors! For example, if we remove &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt; from the model design, we are left with this VIF for &lt;code&gt;a&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ab_model &amp;lt;- lm(
  formula = a ~ b,
  data = toy
)

ab_vif &amp;lt;- 1/(1 - summary(ab_model)$r.squared)
ab_vif
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.002253
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An almost perfect VIF score!&lt;/p&gt;
&lt;p&gt;We can simplify the VIF computation using &lt;code&gt;collinear::vif_df()&lt;/code&gt;, which returns the VIF of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; at once.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::vif_df(
  df = toy[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   predictor    vif
## 1         a 1.0023
## 2         b 1.0023
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In plot below, the worst and best VIF scores of &lt;code&gt;a&lt;/code&gt; are shown in the context of the relationship between R-squared and VIF, and three VIF thresholds commonly mentioned in the literature. These thresholds are represented as vertical dashed lines at VIF 2.5, 5, and 10, and are used as criteria to control multicollinearity in model designs. I will revisit this topic later in the post.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/variance-inflation-factor/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;When the R-squared of the linear regression model is 0, then the VIF expression becomes &lt;code&gt;\(1/(1 - 0) = 1\)&lt;/code&gt; and returns the minimum possible VIF. On the other end, when R-squared is 1, then we get &lt;code&gt;\(1/(1 - 1) = Inf\)&lt;/code&gt;, the maximum VIF.&lt;/p&gt;
&lt;p&gt;So far, we have learned that to assess whether the predictor &lt;code&gt;a&lt;/code&gt; induces multicollinearity in the model design &lt;code&gt;y ~ a + b + c + d&lt;/code&gt; we can compute it&amp;rsquo;s Variance Inflation Factor from the R-squared of the model &lt;code&gt;a ~ b + c + d&lt;/code&gt;. We have also learned that if the model design changes, so does the VIF of &lt;code&gt;a&lt;/code&gt;. We also know that there are some magic numbers (the VIF thresholds) we can use as reference.&lt;/p&gt;
&lt;p&gt;But still, we have no indication of what these VIF values actually mean! I will try to fix that in the next section.&lt;/p&gt;
&lt;h2 id=&#34;but-really-what-are-variance-inflation-factors&#34;&gt;But really, what are Variance Inflation Factors?&lt;/h2&gt;
&lt;p&gt;Variance Inflation Factors are inherently linked to these fundamental linear modeling concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Coefficient Estimate&lt;/strong&gt; (&lt;code&gt;\(\hat{\beta}\)&lt;/code&gt;): The estimated slope of the relationship between a predictor and the response in a linear model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Standard Error&lt;/strong&gt; (&lt;code&gt;\(\text{SE}\)&lt;/code&gt;): Represents the uncertainty around the estimation of the coefficient due to data variability.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Significance level&lt;/strong&gt; (&lt;code&gt;\(1.96\)&lt;/code&gt;): The acceptable level of error when determining the significance of the &lt;em&gt;coefficient estimate&lt;/em&gt;. Here it is simplified to 1.96, the 97.5th percentile of a normal distribution, to approximate a significance level of 0.05.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Confidence Interval&lt;/strong&gt; (&lt;code&gt;\(CI\)&lt;/code&gt;): The range of values containing the true value of the &lt;em&gt;coefficient estimate&lt;/em&gt; withing a certain &lt;em&gt;significance level&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These terms are related by the expression to compute the confidence interval of the coefficient estimate:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$\text{CI} = \beta \pm 1.96 \cdot \text{SE}$$&lt;/code&gt;
Let me convert this equation into a small function to compute confidence intervals of coefficient estimates named &lt;code&gt;ci()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ci &amp;lt;- function(b, se){
  x &amp;lt;- se * 1.96
  as.numeric(c(b-x, b+x))
}
#note: stats::confint() which uses t-critical values to compute more precise confidence intervals. 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are going to look at the coefficient estimate and standard error of &lt;code&gt;a&lt;/code&gt; in the model &lt;code&gt;y ~ a + b&lt;/code&gt;. We know that &lt;code&gt;a&lt;/code&gt; in this model has a vif of 1.0022527.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yab_model &amp;lt;- lm(
  formula = y ~ a + b,
  data = toy
) |&amp;gt; 
  summary()

#coefficient estimate and standard error of a
a_coef &amp;lt;- yab_model$coefficients[2, 1:2]
a_coef
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Estimate  Std. Error 
## 0.747689326 0.006636511
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we plug them into our little function to compute the confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;a_ci &amp;lt;- ci(
  b = a_coef[1], 
  se = a_coef[2]
  )
a_ci
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7346818 0.7606969
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And, finally, we compute the width of the confidence interval for &lt;code&gt;a&lt;/code&gt; as the difference between the extremes of the confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;old_width &amp;lt;- diff(a_ci)
old_width
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02601512
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep this number in mind, it&amp;rsquo;s important.&lt;/p&gt;
&lt;p&gt;Now, let me tell you something weird: &lt;strong&gt;The confidence interval of a predictor is widened by a factor equal to the square root of its Variance Inflation Factor&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So, if the VIF of a predictor is, let&amp;rsquo;s say, 16, then this means that, in a linear model, multicollinearity is inflating the width of its confidence interval by a factor of 4.&lt;/p&gt;
&lt;p&gt;In case you don&amp;rsquo;t want to take my word for it, here goes a demonstration. Now we fit the model &lt;code&gt;y ~ a + b + c + d&lt;/code&gt;, where &lt;code&gt;a&lt;/code&gt; has a vif of 16.1606674. If we follow the definition above, we could now expect an inflation of the confidence interval for &lt;code&gt;a&lt;/code&gt; of about 4.0200333. Let&amp;rsquo;s find out if that&amp;rsquo;s the case!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#model y against all predictors and get summary
yabcd_model &amp;lt;- lm(
  formula = y ~ a + b + c + d,
  data = toy
) |&amp;gt; 
  summary()

#compute confidence interval of a
a_ci &amp;lt;- ci(
  b = yabcd_model$coefficients[&amp;quot;a&amp;quot;, &amp;quot;Estimate&amp;quot;], 
  se = yabcd_model$coefficients[&amp;quot;a&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )

#compute width of confidence interval of a
new_width &amp;lt;- diff(a_ci)
new_width
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1044793
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to find out the inflation factor of this new confidence interval, we divide it by the width of the old one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;new_width/old_width
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.016101
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the result is VERY CLOSE to the square root of the VIF of &lt;code&gt;a&lt;/code&gt; (4.0200333) in this model. &lt;strong&gt;Notice that this works because in the model &lt;code&gt;y ~ a + b&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt; has a perfect VIF of 1.0022527. This demonstration needs a model with a quasi-perfect VIF as reference.&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now we can confirm our experiment about the meaning of VIF by repeating the exercise with &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;First we compute the VIF of &lt;code&gt;b&lt;/code&gt; against &lt;code&gt;a&lt;/code&gt; alone, and against &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;, and the expected level of inflation of the confidence interval as the square root of the second VIF.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#vif of b vs a
ba_vif &amp;lt;- collinear::vif_df(
  df = toy[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]
) |&amp;gt; 
  dplyr::filter(predictor == &amp;quot;b&amp;quot;)

#vif of b vs a c d
bacd_vif &amp;lt;- collinear::vif_df(
  df = toy[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]
) |&amp;gt; 
  dplyr::filter(predictor == &amp;quot;b&amp;quot;)

#expeced inflation of the confidence interval
sqrt(bacd_vif$vif)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.015515
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, since &lt;code&gt;b&lt;/code&gt; is already in the models &lt;code&gt;y ~ a + b&lt;/code&gt; and &lt;code&gt;y ~ a + b + c + d&lt;/code&gt;, we just need to extract its coefficients, compute their confidence intervals, and divide one by the other to obtain the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#compute confidence interval of b in y ~ a + b
b_ci_old &amp;lt;- ci(
  b = yab_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;], 
  se = yab_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )

#compute confidence interval of b in y ~ a + b + c + d
b_ci_new &amp;lt;- ci(
  b = yabcd_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;], 
  se = yabcd_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )

#compute inflation
diff(b_ci_new)/diff(b_ci_old)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.013543
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, the square root of the VIF of &lt;code&gt;b&lt;/code&gt; in &lt;code&gt;y ~ a + b + c + d&lt;/code&gt; is a great indicator of how much the confidence interval of &lt;code&gt;b&lt;/code&gt; is inflated by multicollinearity in the model.&lt;/p&gt;
&lt;p&gt;And that, folks, is the meaning of VIF.&lt;/p&gt;
&lt;h1 id=&#34;when-the-vif-hurts&#34;&gt;When the VIF Hurts&lt;/h1&gt;
&lt;p&gt;In the previous sections we acquired an intuition of how Variance Inflation Factors measure the effect of multicollinearity in the precision of the coefficient estimates in a linear model. But there is more to that!&lt;/p&gt;
&lt;p&gt;A coefficient estimate divided by its standard error results in the &lt;strong&gt;T statistic&lt;/strong&gt;. This number is named &amp;ldquo;t value&amp;rdquo; in the table of coefficients shown below, and represents the distance (in number of standard errors) between the estimate and zero.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yabcd_model$coefficients[-1, ] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Estimate Std. Error t value Pr(&amp;gt;|t|)
## a   0.7184     0.0267 26.9552   0.0000
## b   0.2596     0.0134 19.4253   0.0000
## c   0.0273     0.0232  1.1757   0.2398
## d   0.0039     0.0230  0.1693   0.8656
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;p-value&lt;/strong&gt;, named &amp;ldquo;Pr(&amp;gt;|t|)&amp;rdquo; above, is the probability of getting the T statistic when there is &lt;em&gt;no effect of the predictor over the response&lt;/em&gt;. The part in italics is named the &lt;em&gt;null hypothesis&lt;/em&gt; (H0), and happens when the confidence interval of the estimate intersects with zero, as in &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ci(
  b = yabcd_model$coefficients[&amp;quot;c&amp;quot;, &amp;quot;Estimate&amp;quot;],
  se = yabcd_model$coefficients[&amp;quot;c&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.01819994  0.07276692
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ci(
  b = yabcd_model$coefficients[&amp;quot;d&amp;quot;, &amp;quot;Estimate&amp;quot;],
  se = yabcd_model$coefficients[&amp;quot;d&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.04111146  0.04888457
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value of any predictor in the coefficients table above is computed as:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#predictor
predictor &amp;lt;- &amp;quot;d&amp;quot;

#number of cases
n &amp;lt;- nrow(toy)

#number of model terms
p &amp;lt;- nrow(yabcd_model$coefficients)

#one-tailed p-value
#q = absolute t-value
#df = degrees of freedom
p_value_one_tailed &amp;lt;- stats::pt(
  q = abs(yabcd_model$coefficients[predictor, &amp;quot;t value&amp;quot;]), 
  df = n - p #degrees of freedom
  )

#two-tailed p-value
2 * (1 - p_value_one_tailed)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8655869
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This p-value is then compared to a &lt;strong&gt;significance level&lt;/strong&gt; (for example, 0.05 for a 95% confidence), which is just the lowest p-value acceptable as strong evidence to make a claim:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;p-value &amp;gt; significance&lt;/strong&gt;: Evidence to claim that the predictor has no effect on the response. If the claim is wrong (we&amp;rsquo;ll see whey we could be wrong), we fall into a &lt;em&gt;false negative&lt;/em&gt; (also &lt;em&gt;Type II Error&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;p-value &amp;lt;= significance&lt;/strong&gt;: Evidence to claim that the predictor has an effect on the response. If the claim is wrong, we fall into a &lt;em&gt;false positive&lt;/em&gt; (also &lt;em&gt;Type I Error&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, how does all this matter when talking about the Variance Inflation Factor? Because a high VIF triggers a cascade of effects that increases p-values that can mess up your claims about the importance of the predictors!&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    ↑ VIF ► ↑ Std. Error ► ↓ T statistic  ► ↑ p-value  ► ↑  false negatives (Type II Error)
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;This cascade becomes a problem when the predictor has a small effect on the response, and the number of cases is small.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see how this works with &lt;code&gt;b&lt;/code&gt;. This predictor has a solid effect on the response &lt;code&gt;y&lt;/code&gt; (nonetheless, &lt;code&gt;y&lt;/code&gt; was created as &lt;code&gt;a * 0.75 + b * 0.25 + noise&lt;/code&gt;). It has a coefficient around 0.25, and a p-value of 0, so there is little to no risk of falling into a false negative when claiming that it is important to explain &lt;code&gt;y&lt;/code&gt;, even when its confidence interval is inflated by a factor of two in the full model.&lt;/p&gt;
&lt;p&gt;But let&amp;rsquo;s try a little experiment. We are going to create many small versions of &lt;code&gt;toy&lt;/code&gt;, using only 30 cases selected by chance over a number of iterations, we are going to fit models in which &lt;code&gt;b&lt;/code&gt; has a lower and a higher VIF, to monitor its p-values and estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#number of repetitions
repetitions &amp;lt;- 1000

#number of cases to subset in toy
sample_size &amp;lt;- 30

#vectors to store results
lowvif_p_value &amp;lt;- 
  highvif_p_value &amp;lt;- 
  lowvif_estimate &amp;lt;-
  highvif_estimate &amp;lt;- 
  vector(length = repetitions)

#repetitions
for(i in 1:repetitions){
  
  #seed to make randomization reproducible
  set.seed(i)
  
  #toy subset
  toy.i &amp;lt;- toy[sample(x = 1:nrow(toy), size = sample_size), ]
  
  #high vif model
  highvif_model &amp;lt;- lm(
    formula =  y ~ a + b + c + d,
    data = toy.i
  ) |&amp;gt; 
    summary()
  
  #gather results of high vif model
  highvif_p_value[i] &amp;lt;- highvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
  highvif_estimate[i] &amp;lt;- highvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;]
  
  #low_vif_model
  lowvif_model &amp;lt;- lm(
    formula =  y ~ a + b,
    data = toy.i
  ) |&amp;gt; 
    summary()
  
  #gather results of lowvif
  lowvif_p_value[i] &amp;lt;- lowvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
  lowvif_estimate[i] &amp;lt;- lowvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;]
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below shows all p-values of the predictor &lt;code&gt;b&lt;/code&gt; for the high and low VIF models across the experiment repetitions.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/variance-inflation-factor/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;At a significance level of 0.05, the &lt;strong&gt;high VIF&lt;/strong&gt; model rejects &lt;code&gt;b&lt;/code&gt; as an important predictor of &lt;code&gt;y&lt;/code&gt; on 53.5% of the model repetitions, while the &lt;strong&gt;low VIf model&lt;/strong&gt; does the same on 2.2% of repetitions. This is a clear case of increase in Type II Error (false negatives) under multicollinearity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Under multicollinearity, the probability of overlooking predictors with moderate effects increases dramatically!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The plot below shifts the focus towards the coefficient estimates for &lt;code&gt;b&lt;/code&gt; across repetitions.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/variance-inflation-factor/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;The gray vertical line represents the real value of the slope of &lt;code&gt;b&lt;/code&gt;, and each dot represents a model repetition. The coefficients of the &lt;strong&gt;high VIF&lt;/strong&gt; model are all over the place when compared to the &lt;strong&gt;low VIF&lt;/strong&gt; one. Probably you have read somewhere that &amp;ldquo;multicollinearity induces model instability&amp;rdquo;, or something similar, and that is exactly what we are seeing here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finding the true effect of a predictor with a moderate effect becomes harder under multicollinearity.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;managing-vif-in-a-model-design&#34;&gt;Managing VIF in a Model Design&lt;/h1&gt;
&lt;p&gt;The second most common form of modeling self-sabotage is &lt;em&gt;having high VIF predictors in a model design&lt;/em&gt;, just right after &lt;em&gt;throwing deep learning at tabular problems to see what sticks&lt;/em&gt;. I don&amp;rsquo;t have solutions for the deep learning issue, but I have some pointers for the VIFs one: &lt;strong&gt;letting things go!&lt;/strong&gt;. And with &lt;em&gt;things&lt;/em&gt; I mean &lt;em&gt;predictors&lt;/em&gt;, not the pictures of your old love. There is no rule &lt;em&gt;the more predictors the better&lt;/em&gt; rule written anywhere relevant, and letting your model shed some fat is the best way to go here.&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt; package has something to help here. The function 
&lt;a href=&#34;https://blasbenito.github.io/collinear/reference/vif_select.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear::vif_select()&lt;/code&gt;&lt;/a&gt; is specifically designed to help reduce VIF in a model design. And it can do it in two ways: either using domain knowledge to guide the process, or applying quantitative criteria instead.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s follow the domain knowledge route first. Imagine you know a lot about &lt;code&gt;y&lt;/code&gt;, you have read that &lt;code&gt;a&lt;/code&gt; is very important to explain it, and you need to discuss this predictor in your results. But you are on the fence about the other predictors, so you don&amp;rsquo;t really care about what others are in the design. You can express such an idea using the argument &lt;code&gt;preference_order&lt;/code&gt;, as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = &amp;quot;a&amp;quot;,
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you have it, your new model design with a VIF below 2.5 is now &lt;code&gt;y ~ a + b&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;But what if you get new information and it turns out that &lt;code&gt;d&lt;/code&gt; is also a variable of interest? Then you should just modify &lt;code&gt;preference_order&lt;/code&gt; to include this new information.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = c(&amp;quot;a&amp;quot;, &amp;quot;d&amp;quot;),
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;d&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that if your favorite variables are highly correlated, some of them are going to be removed anyway. For example, if &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; are your faves, since they are highly correlated, &lt;code&gt;c&lt;/code&gt; is removed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = c(&amp;quot;a&amp;quot;, &amp;quot;c&amp;quot;),
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In either case, you can now build your model while being sure that the coefficients of these predictors are going to be stable and precise.&lt;/p&gt;
&lt;p&gt;Now, what if &lt;code&gt;y&lt;/code&gt; is totally new for you, and you have no idea about what to use? In this case, the function 
&lt;a href=&#34;https://blasbenito.github.io/collinear/reference/preference_order.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear::preference_order()&lt;/code&gt;&lt;/a&gt; helps you rank the predictors following a quantiative criteria, and after that, &lt;code&gt;collinear::vif_select()&lt;/code&gt; can use it to reduce your VIFs.&lt;/p&gt;
&lt;p&gt;By default, &lt;code&gt;collinear::preference_order()&lt;/code&gt; calls 
&lt;a href=&#34;https://blasbenito.github.io/collinear/reference/f_rsquared.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear::f_rsquared()&lt;/code&gt;&lt;/a&gt; to compute the R-squared between each predictor and the response variable (that&amp;rsquo;s why the argument &lt;code&gt;response&lt;/code&gt; is required here), to return a data frame with the variables ranked from &amp;ldquo;better&amp;rdquo; to &amp;ldquo;worse&amp;rdquo;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;preference &amp;lt;- collinear::preference_order(
  df = toy,
  response = &amp;quot;y&amp;quot;,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  f = collinear::f_r2_pearson,
  quiet = TRUE
)

preference
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   response predictor                       f preference
## 1        y         a collinear::f_r2_pearson 0.77600503
## 2        y         c collinear::f_r2_pearson 0.72364944
## 3        y         d collinear::f_r2_pearson 0.59345954
## 4        y         b collinear::f_r2_pearson 0.07343563
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can use this data frame as input for the argument &lt;code&gt;preference_order&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = preference,
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;d&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now at least you can be sure that the predictors in your model design have low VIF, and were selected taking their correlation with the response as criteria.&lt;/p&gt;
&lt;p&gt;Well, I think that&amp;rsquo;s enough for today. I hope you found this post helpful. Have a great time!&lt;/p&gt;
&lt;p&gt;Blas&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multicollinearity Hinders Model Interpretability</title>
      <link>https://blasbenito.com/post/multicollinearity-model-interpretability/</link>
      <pubDate>Sun, 29 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/multicollinearity-model-interpretability/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This post is written for beginner to intermediate R users wishing to learn what multicollinearity is and how it can turn model interpretation into a challenge.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post, I delve into the intricacies of model interpretation under the influence of multicollinearity, and use R and a toy data set to demonstrate how this phenomenon impacts both linear and machine learning models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The section &lt;em&gt;Multicollinearity Explained&lt;/em&gt; explains the origin of the word and the nature of the problem.&lt;/li&gt;
&lt;li&gt;The section &lt;em&gt;Model Interpretation Challenges&lt;/em&gt; describes how to create the toy data set, and applies it to &lt;em&gt;Linear Models&lt;/em&gt; and &lt;em&gt;Random Forest&lt;/em&gt; to explain how multicollinearity can make model interpretation a challenge.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;Appendix&lt;/em&gt; shows extra examples of linear and machine learning models affected by multicollinearity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I hope you&amp;rsquo;ll enjoy it!&lt;/p&gt;
&lt;h1 id=&#34;r-packages&#34;&gt;R packages&lt;/h1&gt;
&lt;p&gt;This tutorial requires the newly released R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;, and a few more listed below. The optional ones are used only in the &lt;em&gt;Appendix&lt;/em&gt; at the end of the post.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#required
install.packages(&amp;quot;collinear&amp;quot;)
install.packages(&amp;quot;ranger&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)

#optional
install.packages(&amp;quot;nlme&amp;quot;)
install.packages(&amp;quot;glmnet&amp;quot;)
install.packages(&amp;quot;xgboost&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;multicollinearity-explained&#34;&gt;Multicollinearity Explained&lt;/h1&gt;
&lt;p&gt;This cute word comes from the amalgamation of these three Latin terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;multus&lt;/em&gt;: adjective meaning &lt;em&gt;many&lt;/em&gt; or &lt;em&gt;multiple&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;con&lt;/em&gt;: preposition often converted to &lt;em&gt;co-&lt;/em&gt; (as in &lt;em&gt;co-worker&lt;/em&gt;) meaning &lt;em&gt;together&lt;/em&gt; or &lt;em&gt;mutually&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;linealis&lt;/em&gt; (later converted to &lt;em&gt;linearis&lt;/em&gt;): from &lt;em&gt;linea&lt;/em&gt; (line), adjective meaning &amp;ldquo;resembling a line&amp;rdquo; or &amp;ldquo;belonging to a line&amp;rdquo;, among others.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After looking at these serious words, we can come up with a (VERY) liberal translation: &amp;ldquo;several things together in the same line&amp;rdquo;. From here, we just have to replace the word &amp;ldquo;things&amp;rdquo; with &amp;ldquo;predictors&amp;rdquo; (or &amp;ldquo;features&amp;rdquo;, or &amp;ldquo;independent variables&amp;rdquo;, whatever rocks your boat) to build an intuition of the whole meaning of the word in the context of statistical and machine learning modeling.&lt;/p&gt;
&lt;p&gt;If I lost you there, we can move forward with this idea instead: &lt;strong&gt;multicollinearity happens when there are redundant predictors in a modeling dataset&lt;/strong&gt;. A predictor can be redundant because it shows a high pairwise correlation with other predictors, or because it is a linear combination of other predictors. For example, in a data frame with the columns &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt;, if the correlation between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; is high, we can say that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are mutually redundant and there is multicollinearity. But also, if &lt;code&gt;c&lt;/code&gt; is the result of a linear operation between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, like &lt;code&gt;c &amp;lt;- a + b&lt;/code&gt;, or &lt;code&gt;c &amp;lt;- a * 1 + b * 0.5&lt;/code&gt;, then we can also say that there is multicollinearity between &lt;code&gt;c&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt;, and &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Multicollinearity is a fact of life that lurks in most data sets. For example, in climate data, variables like temperature, humidity and air pressure are closely intertwined, leading to multicollinearity. That&amp;rsquo;s the case as well in medical research, where parameters like blood pressure, heart rate, and body mass index frequently display common patterns. Economic analysis is another good example, as variables such as Gross Domestic Product (GDP), unemployment rate, and consumer spending often exhibit multicollinearity.&lt;/p&gt;
&lt;h1 id=&#34;model-interpretation-challenges&#34;&gt;Model Interpretation Challenges&lt;/h1&gt;
&lt;p&gt;Multicollinearity isn&amp;rsquo;t inherently problematic, but it can be a real buzz kill when the goal is interpreting predictor importance in explanatory models. In the presence of highly correlated predictors, most modelling methods, from the veteran linear models to the fancy gradient boosting, attribute a large part of the importance to only one of the predictors and not the others. In such cases, neglecting multicollinearity will certainly lead to underestimate the relevance of certain predictors.&lt;/p&gt;
&lt;p&gt;Let me go ahead and develop a toy data set to showcase this issue. But let&amp;rsquo;s load the required libraries first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#load the collinear package and its example data
library(collinear)
data(vi)

#other required libraries
library(ranger)
library(dplyr)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;vi&lt;/code&gt; data frame shipped with the 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt; package, the variables &amp;ldquo;soil_clay&amp;rdquo; and &amp;ldquo;humidity_range&amp;rdquo; are not correlated at all (Pearson correlation = -0.06).&lt;/p&gt;
&lt;p&gt;In the code block below, the &lt;code&gt;dplyr::transmute()&lt;/code&gt; command selects and renames them as &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. After that, the two variables are scaled and centered, and &lt;code&gt;dplyr::mutate()&lt;/code&gt; generates a few new columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: response variable resulting from a linear model where &lt;code&gt;a&lt;/code&gt; has a slope of 0.75, &lt;code&gt;b&lt;/code&gt; has a slope of 0.25, plus a bit of white noise generated with &lt;code&gt;runif()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: a new predictor highly correlated with &lt;code&gt;a&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d&lt;/code&gt;: a new predictor resulting from a linear combination of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(1)
df &amp;lt;- vi |&amp;gt;
  dplyr::slice_sample(n = 2000) |&amp;gt;
  dplyr::transmute(
    a = soil_clay,
    b = humidity_range
  ) |&amp;gt;
  scale() |&amp;gt;
  as.data.frame() |&amp;gt; 
  dplyr::mutate(
    y = a * 0.75 + b * 0.25 + runif(n = dplyr::n(), min = -0.5, max = 0.5),
    c = a + runif(n = dplyr::n(), min = -0.5, max = 0.5),
    d = (a + b)/2 + runif(n = dplyr::n(), min = -0.5, max = 0.5)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Pearson correlation between all pairs of these predictors is shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::cor_df(
  df = df,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 3
##   x     y     correlation
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 c     a           0.962
## 2 d     b           0.639
## 3 d     a           0.636
## 4 d     c           0.615
## 5 b     a          -0.047
## 6 c     b          -0.042
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, we have are two groups of predictors useful to understand how multicollinearity muddles model interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictors with &lt;strong&gt;no&lt;/strong&gt; multicollinearity: &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Predictors with multicollinearity: &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the next two sections and the &lt;em&gt;Appendix&lt;/em&gt;, I show how and why model interpretation becomes challenging when multicollinearity is high. Let&amp;rsquo;s start with linear models.&lt;/p&gt;
&lt;h3 id=&#34;linear-models&#34;&gt;Linear Models&lt;/h3&gt;
&lt;p&gt;The code below fits &lt;em&gt;multiple linear regression models&lt;/em&gt; for both groups of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#non-collinear predictors
lm_ab &amp;lt;- lm(
  formula = y ~ a + b,
  data = df
  )

#collinear predictors
lm_abcd &amp;lt;- lm(
  formula = y ~ a + b + c + d,
  data = df
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I would like you to pay attention to the estimates of the predictors &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; for both models. The estimates are the slopes in the linear model, a direct indication of the effect of a predictor over the response.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coefficients(lm_ab)[2:3] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coefficients(lm_abcd)[2:5] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On one hand, the model with no multicollinearity (&lt;code&gt;lm_ab&lt;/code&gt;) achieved a pretty good solution for the coefficients of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. Remember that we created &lt;code&gt;y&lt;/code&gt; as &lt;code&gt;a * 0.75 + b * 0.25&lt;/code&gt; plus some noise, and that&amp;rsquo;s exactly what the model is telling us here, so the interpretation is pretty straightforward.&lt;/p&gt;
&lt;p&gt;On the other hand, the model with multicollinearity (&lt;code&gt;lm_abcd&lt;/code&gt;) did well with &lt;code&gt;b&lt;/code&gt;, but there are a few things in there that make the interpretation harder.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The coefficient of &lt;code&gt;a&lt;/code&gt; (0.7165) is slightly smaller than the true one (0.75), which could lead us to downplay its relationship with &lt;code&gt;y&lt;/code&gt; by a tiny bit. This is kinda OK though, as long as one is not using the model&amp;rsquo;s results to build nukes in the basement.&lt;/li&gt;
&lt;li&gt;The coefficient of &lt;code&gt;c&lt;/code&gt; is so small that it could led us to believe that this predictor not important at all to explain &lt;code&gt;y&lt;/code&gt;. But we know that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; are almost identical copies, so model interpretation here is being definitely muddled by multicollinearity.&lt;/li&gt;
&lt;li&gt;The coefficient of &lt;code&gt;d&lt;/code&gt; is tiny. Since &lt;code&gt;d&lt;/code&gt; results from the sum of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, we could expect this predictor to be important in explaining &lt;code&gt;y&lt;/code&gt;, but it got the shorter end of the stick in this case.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is not that the model it&amp;rsquo;s wrong though. This behavior of the linear model results from the &lt;em&gt;QR decomposition&lt;/em&gt; (also &lt;em&gt;QR factorization&lt;/em&gt;) applied by functions like &lt;code&gt;lm()&lt;/code&gt;, &lt;code&gt;glm()&lt;/code&gt;, &lt;code&gt;glmnet::glmnet()&lt;/code&gt;, and &lt;code&gt;nlme::gls()&lt;/code&gt; to improve numerical stability and computational efficiency, and to&amp;hellip; address multicollinearity in the model predictors.&lt;/p&gt;
&lt;p&gt;The QR decomposition transforms the original predictors into a set of orthogonal predictors with no multicollinearity. This is the &lt;em&gt;Q matrix&lt;/em&gt;, created in a fashion that resembles the way in which a Principal Components Analysis generates uncorrelated components from a set of correlated variables.&lt;/p&gt;
&lt;p&gt;The code below applies QR decomposition to our multicollinear predictors, extracts the Q matrix, and shows the correlation between the new versions of &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#predictors names
predictors &amp;lt;- c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)

#QR decomposition of predictors
df.qr &amp;lt;- qr(df[, predictors])

#extract Q matrix
df.q &amp;lt;- qr.Q(df.qr)
colnames(df.q) &amp;lt;- predictors

#correlation between transformed predictors
collinear::cor_df(df = df.q)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 3
##   x     y     correlation
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 d     c               0
## 2 c     b               0
## 3 d     b               0
## 4 d     a               0
## 5 c     a               0
## 6 b     a               0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new set of predictors we are left with after the QR decomposition have exactly zero correlation! And now they are not our original predictors anymore, and have a different interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt; is now &amp;ldquo;the part of &lt;code&gt;a&lt;/code&gt; not in &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt; is now &amp;ldquo;the part of &lt;code&gt;b&lt;/code&gt; not in &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&amp;hellip;and so on&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The result of the QR decomposition can be plugged into the &lt;code&gt;solve()&lt;/code&gt; function along with the response vector to estimate the coefficients of the linear model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solve(a = df.qr, b = df$y) |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7189 0.2595 0.0268 0.0040
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are almost exactly the ones we got for our model with multicollinearity. In the end, the coefficients resulting from a linear model are not those of the original predictors, but the ones of their uncorrelated versions generated by the QR decomposition.&lt;/p&gt;
&lt;p&gt;But this is not the only issue of model interpretability under multicollinearity. Let&amp;rsquo;s take a look at the standard errors of the estimates. These are a measure of the coefficient estimation uncertainty, and are used to compute the p-values of the estimates. As such, they are directly linked with the &amp;ldquo;statistical significance&amp;rdquo; (whatever that means) of the predictors within the model.&lt;/p&gt;
&lt;p&gt;The code below shows the standard errors of the model without and with multicollinearity.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(lm_ab)$coefficients[, &amp;quot;Std. Error&amp;quot;][2:3] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.0066 0.0066
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(lm_abcd)$coefficients[, &amp;quot;Std. Error&amp;quot;][2:5] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.0267 0.0134 0.0232 0.0230
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These standard errors of the model with multicollinearity are an order of magnitude higher than the ones of the model without multicollinearity.&lt;/p&gt;
&lt;p&gt;Since our toy dataset is relatively large (2000 cases) and the relationship between the response and a few of the predictors pretty robust, there are no real issues arising, as these differences in estimation precision are not enough to change the p-values of the estimates. However, in a small data set with high multicollinearity and a weaker relationship between the response and the predictors, standard errors of the estimate become wide, which increases p-values and reduces &amp;ldquo;significance&amp;rdquo;. Such a situation might lead us to believe that a predictor does not explain the response, when in fact it does. And this, again, is a model interpretability issue caused by multicollinearity.&lt;/p&gt;
&lt;p&gt;At the end of this post there is an appendix with code examples of other types of linear models that use QR decomposition and become challenging to interpret in the presence of multicollinearity. Play with them as you please!&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s take a look at how multicollinearity can also mess up the interpretation of a commonly used machine learning algorithm.&lt;/p&gt;
&lt;h3 id=&#34;random-forest&#34;&gt;Random Forest&lt;/h3&gt;
&lt;p&gt;It is not uncommon to hear something like &amp;ldquo;random forest is insensitive to multicollinearity&amp;rdquo;. Actually, I cannot confirm nor deny that I have said that before. Anyway, it is kind of true if one is focused on prediction problmes. However, when the aim is interpreting predictor importance scores, then one has to be mindful about multicollinearity as well.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see an example. The code below fits two random forest models with our two sets of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#non-collinear predictors
rf_ab &amp;lt;- ranger::ranger(
  formula = y ~ a + b,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1 #for reproducibility
)

#collinear predictors
rf_abcd &amp;lt;- ranger::ranger(
  formula = y ~ a + b + c + d,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at the prediction error the two models on the out-of-bag data. While building each regression tree, Random Forest leaves a random subset of the data out. Then, each case gets a prediction from all trees that had it in the out-of-bag data, and the prediction error is averaged across all cases to get the numbers below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_ab$prediction.error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1026779
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abcd$prediction.error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1035678
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to these numbers, these two models are basically equivalent in their ability to predict our response &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But now, you noticed that I set the argument &lt;code&gt;importance&lt;/code&gt; to &amp;ldquo;permutation&amp;rdquo;. Permutation importance quantifies how the out-of-bag error increases when a predictor is permuted across all trees where the predictor is used. It is pretty robust importance metric that bears no resemblance whatsoever with the coefficients of a linear model. Think of it as a very different way to answer the question &amp;ldquo;what variables are important in this model?&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The permutation importance scores of the two random forest models are show below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_ab$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 1.0702 0.1322
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abcd$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.5019 0.0561 0.1662 0.0815
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is one interesting detail here. The predictor &lt;code&gt;a&lt;/code&gt; has a permutation error three times higher than &lt;code&gt;c&lt;/code&gt; in the second model, even though we could expect them to be similar due to their very high correlation. There are two reasons for this mismatch:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Random Forest is much more sensitive to the white noise in &lt;code&gt;c&lt;/code&gt; than linear models, especially in the deep parts of the regression trees, due to local (within-split data) decoupling with the response &lt;code&gt;y&lt;/code&gt;. In consequence, it does not get selected as often as &lt;code&gt;a&lt;/code&gt; in these deeper areas of the trees, and has less overall importance.&lt;/li&gt;
&lt;li&gt;The predictor &lt;code&gt;c&lt;/code&gt; competes with &lt;code&gt;d&lt;/code&gt;, that has around 50% of the information in &lt;code&gt;c&lt;/code&gt; (and &lt;code&gt;a&lt;/code&gt;). If we remove &lt;code&gt;d&lt;/code&gt; from the model, then the permutation importance of &lt;code&gt;c&lt;/code&gt; doubles up. Then, with &lt;code&gt;d&lt;/code&gt; in the model, we underestimate the real importance of &lt;code&gt;c&lt;/code&gt; due to multicollinearity alone.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abc &amp;lt;- ranger::ranger(
  formula = y ~ a + b + c,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1
)
rf_abc$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c 
## 0.5037 0.1234 0.3133
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With all that in mind, we can conclude that interpreting importance scores in Random Forest models is challenging when multicollinearity is high. But Random Forest is not the only machine learning affected by this issue. In the Appendix below I have left an example with Extreme Gradient Boosting so you can play with it.&lt;/p&gt;
&lt;p&gt;And that&amp;rsquo;s all for now, folks, I hope you found this post useful!&lt;/p&gt;
&lt;h1 id=&#34;appendix&#34;&gt;Appendix&lt;/h1&gt;
&lt;p&gt;This section shows several extra examples of linear and machine learning models you can play with.&lt;/p&gt;
&lt;h2 id=&#34;other-linear-models-using-qr-decomposition&#34;&gt;Other linear models using QR decomposition&lt;/h2&gt;
&lt;p&gt;As I commented above, many linear modeling functions use QR decomposition, and you will have to be careful interpreting model coefficients in the presence of strong multicollinearity in the predictors.&lt;/p&gt;
&lt;p&gt;Here I show several examples with &lt;code&gt;glm()&lt;/code&gt; (Generalized Linear Models), &lt;code&gt;nlme::gls()&lt;/code&gt; (Generalized Least Squares), and &lt;code&gt;glmnet::cv.glmnet()&lt;/code&gt; (Elastic Net Regularization). In all them, no matter how fancy, the interpretation of coefficients becomes tricky when multicollinearity is high.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generalized Linear Models with glm()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Generalized Linear Models
#non-collinear predictors
glm_ab &amp;lt;- glm(
  formula = y ~ a + b,
  data = df
  )

round(coefficients(glm_ab), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
glm_abcd &amp;lt;- glm(
  formula = y ~ a + b + c + d,
  data = df
  )

round(coefficients(glm_abcd), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Generalized Least Squares with nlme::gls()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(nlme)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;nlme&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:dplyr&#39;:
## 
##     collapse
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Generalized Least Squares
#non-collinear predictors
gls_ab &amp;lt;- nlme::gls(
  model = y ~ a + b,
  data = df
  )

round(coefficients(gls_ab), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
gls_abcd &amp;lt;- nlme::gls(
  model = y ~ a + b + c + d,
  data = df
  )

round(coefficients(gls_abcd), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Elastic Net Regularization and Lasso penalty with glmnet::glmnet()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(glmnet)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loaded glmnet 4.1-8
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Elastic net regularization with Lasso penalty
#non-collinear predictors
glmnet_ab &amp;lt;- glmnet::cv.glmnet(
  x = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]),
  y = df$y,
  alpha = 1 #lasso penalty
)

round(coef(glmnet_ab$glmnet.fit, s = glmnet_ab$lambda.min), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7438 0.2578
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
glmnet_abcd &amp;lt;- glmnet::cv.glmnet(
  x = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  y = df$y,
  alpha = 1 
)

#notice that the lasso regularization nuked the coefficients of predictors b and c
round(coef(glmnet_abcd$glmnet.fit, s = glmnet_abcd$lambda.min), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7101 0.2507 0.0267 0.0149
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;extreme-gradient-boosting-under-multicollinearity&#34;&gt;Extreme Gradient Boosting under multicollinearity&lt;/h2&gt;
&lt;p&gt;Gradient Boosting models trained with multicollinear predictors behave in a way similar to linear models with QR decomposition. When two variables are highly correlated, one of them is going to have an importance much higher than the other.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(xgboost)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;xgboost&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:dplyr&#39;:
## 
##     slice
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#without multicollinearity
gb_ab &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
  )

#with multicollinearity
gb_abcd &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xgb.importance(model = gb_ab)[, c(1:2)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature      Gain
## 1:       a 0.8463005
## 2:       b 0.1536995
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xgb.importance(model = gb_abcd)[, c(1:2)] |&amp;gt; 
  dplyr::arrange(Feature)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature       Gain
## 1:       a 0.78129661
## 2:       b 0.07386393
## 3:       c 0.03595619
## 4:       d 0.10888327
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But there is a twist too. When two variables are perfectly correlated, one of them is removed right away from the model!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#replace c with perfect copy of a
df$c &amp;lt;- df$a

#with multicollinearity
gb_abcd &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
)

xgb.importance(model = gb_abcd)[, c(1:2)] |&amp;gt; 
  dplyr::arrange(Feature)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature       Gain
## 1:       a 0.79469959
## 2:       b 0.07857141
## 3:       d 0.12672900
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
