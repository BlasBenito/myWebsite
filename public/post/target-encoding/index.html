<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Blas M. Benito">

  
  
  
    
  
  <meta name="description" content="Target encoding is commonly used to map categorical variables to numeric with the objective of facilitating exploratory data analysis and machine learning modeling. This post covers the basics of this method, and explains how and when to use it.">

  
  <link rel="alternate" hreflang="en-us" href="https://blasbenito.com/post/target-encoding/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/github.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/github.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto+Mono&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu1c59cf3385effff74945fd18b5aeda91_399117_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu1c59cf3385effff74945fd18b5aeda91_399117_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="https://blasbenito.com/post/target-encoding/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Blas M. Benito, PhD">
  <meta property="og:url" content="https://blasbenito.com/post/target-encoding/">
  <meta property="og:title" content="Mapping Categorical Predictors to Numeric With Target Encoding | Blas M. Benito, PhD">
  <meta property="og:description" content="Target encoding is commonly used to map categorical variables to numeric with the objective of facilitating exploratory data analysis and machine learning modeling. This post covers the basics of this method, and explains how and when to use it."><meta property="og:image" content="https://blasbenito.com/post/target-encoding/featured.png">
  <meta property="twitter:image" content="https://blasbenito.com/post/target-encoding/featured.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2023-11-15T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2023-11-15T08:14:23&#43;02:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blasbenito.com/post/target-encoding/"
  },
  "headline": "Mapping Categorical Predictors to Numeric With Target Encoding",
  
  "image": [
    "https://blasbenito.com/post/target-encoding/featured.png"
  ],
  
  "datePublished": "2023-11-15T00:00:00Z",
  "dateModified": "2023-11-15T08:14:23+02:00",
  
  "author": {
    "@type": "Person",
    "name": "Blas M. Benito"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Blas M. Benito, PhD",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blasbenito.com/images/icon_hu1c59cf3385effff74945fd18b5aeda91_399117_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Target encoding is commonly used to map categorical variables to numeric with the objective of facilitating exploratory data analysis and machine learning modeling. This post covers the basics of this method, and explains how and when to use it."
}
</script>

  

  


  
  
  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js" integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css" integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#2962ff",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#2962ff"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "https://www.cookiesandyou.com"
      }
    })});
  </script>



  





  <title>Mapping Categorical Predictors to Numeric With Target Encoding | Blas M. Benito, PhD</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class=" ">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Blas M. Benito, PhD</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Blas M. Benito, PhD</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#resume"><span>Resume</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#experience"><span>Experience</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#skills"><span>Skills</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Software</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#tags"><span>Tags</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>



  <article class="article">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>Mapping Categorical Predictors to Numeric With Target Encoding</h1>

  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/blas-m.-benito/">Blas M. Benito</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Nov 15, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    22 min read
  </span>
  

  
  
  

  
  

</div>

  













<div class="btn-links mb-3">
  
  








  


















  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1" href="https://github.com/BlasBenito/collinear" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>
    GitHub
  </a>


</div>


</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 298px;">
  <div style="position: relative">
    <img src="/post/target-encoding/featured_hu343ee4eab00b648709cff6886e650206_54144_720x0_resize_lanczos_3.png" alt="" class="featured-image">
    <span class="article-header-caption">Target encoding of a toy data frame performed with collinear::target_encoding_lab()</span>
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <h1 id="summary">Summary</h1>
<p>Categorical predictors are annoying stringy monsters that can turn any data analysis and modeling effort into a real annoyance. The post delves into the complexities of dealing with these types of predictors using methods such as one-hot encoding (please don&rsquo;t) or target encoding, and provides insights into its mechanisms and quirks</p>
<h2 id="key-highlights">Key Highlights:</h2>
<ul>
<li>
<p><strong>Categorical Predictors are Kinda Annoying:</strong> This section discusses the common issues encountered with categorical predictors during data analysis.</p>
</li>
<li>
<p><strong>One-Hot Encoding Pitfalls:</strong> While discussing one-hot encoding, the post focuses on its limitations, including dimensionality explosion, increased multicollinearity, and sparsity in tree-based models.</p>
</li>
<li>
<p><strong>Intro to Target Encoding:</strong> Introducing target encoding as an alternative, the post explains its concept, illustrating the basic form with mean encoding and subsequent enhancements with additive smoothing, leave-one-out encoding, and more.</p>
</li>
<li>
<p><strong>Handling Sparsity and Repetition:</strong> It emphasizes the potential pitfalls of target encoding, such as repeated values within categories and their impact on model performance, prompting the exploration of strategies like white noise addition and random encoding to mitigate these issues.</p>
</li>
<li>
<p><strong>Target Encoding Lab:</strong> The post concludes with a detailed demonstration using the <code>collinear::target_encoding_lab()</code> function, offering a hands-on exploration of various target encoding methods, parameter combinations, and their visual representations.</p>
</li>
</ul>
<p>The post intends to serve as a useful resource for data scientists exploring alternative encoding techniques for categorical predictors.</p>
<h1 id="resources">Resources</h1>
<ul>
<li>
<a href="https://github.com/BlasBenito/notebooks/blob/main/target_encoding.Rmd" target="_blank" rel="noopener">Interactive notebook of this post</a>.</li>
<li>
<a href="https://doi.org/10.1145/507533.507538" target="_blank" rel="noopener">A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems</a></li>
<li>
<a href="https://towardsdatascience.com/extending-target-encoding-443aa9414cae" target="_blank" rel="noopener">Extending Target Encoding</a></li>
<li>
<a href="https://maxhalford.github.io/blog/target-encoding/" target="_blank" rel="noopener">Target encoding done the right way</a>.</li>
</ul>
<h1 id="r-packages">R packages</h1>
<p>This tutorial requires the development version (&gt;= 1.0.3) of the newly released R package 
<a href="https://blasbenito.github.io/collinear/" target="_blank" rel="noopener"><code>collinear</code></a>, and a few more.</p>
<pre><code class="language-r">#required
install.packages(&quot;remotes&quot;)
remotes::install_github(
  repo = &quot;blasbenito/collinear&quot;, 
  ref = &quot;development&quot;,
  force = TRUE
  )
install.packages(&quot;fastDummies&quot;)
install.packages(&quot;rpart&quot;)
install.packages(&quot;rpart.plot&quot;)
install.packages(&quot;dplyr&quot;)
install.packages(&quot;ggplot2&quot;)
</code></pre>
<h1 id="categorical-predictors-can-be-annoying">Categorical Predictors Can Be Annoying</h1>
<p>I bet you have experienced it during an Exploratory Data Analysis (EDA) or a feature selection for model training and the likes. You likely had a nice bunch of numerical predictors, and then some things like &ldquo;sampling_location&rdquo;, &ldquo;region_name&rdquo;, &ldquo;favorite_color&rdquo;, or any other type of character or factor columns. And you had to branch your code to deal with numeric and categorical variables separately. Or maybe chose to ignore them, as I have done plenty of times.</p>
<p>That&rsquo;s why many efforts have been made to convert them to numeric and kill the problem at once. And now we all have two problems to solve instead.</p>
<p>Let me go ahead and illustrate the issue. There is a data frame in the <code>collinear</code> R package named <code>vi</code>, with one response variable named <code>vi_numeric</code>, and several numeric and categorical predictors in the vector <code>vi_predictors</code>.</p>
<pre><code class="language-r">data(
  vi,
  vi_predictors
)

dplyr::glimpse(vi)
</code></pre>
<pre><code>## Rows: 30,000
## Columns: 68
## $ longitude                  &lt;dbl&gt; -114.254306, 114.845693, -122.145972, 108.3…
## $ latitude                   &lt;dbl&gt; 45.0540272, 26.2706940, 56.3790272, 29.9456…
## $ vi_numeric                 &lt;dbl&gt; 0.38, 0.53, 0.45, 0.69, 0.42, 0.68, 0.70, 0…
## $ vi_counts                  &lt;int&gt; 380, 530, 450, 690, 420, 680, 700, 260, 550…
## $ vi_binomial                &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0…
## $ vi_categorical             &lt;chr&gt; &quot;medium&quot;, &quot;high&quot;, &quot;medium&quot;, &quot;very_high&quot;, &quot;m…
## $ vi_factor                  &lt;fct&gt; medium, high, medium, very_high, medium, ve…
## $ koppen_zone                &lt;chr&gt; &quot;BSk&quot;, &quot;Cfa&quot;, &quot;Dfc&quot;, &quot;Cfb&quot;, &quot;Aw&quot;, &quot;Cfa&quot;, &quot;A…
## $ koppen_group               &lt;chr&gt; &quot;Arid&quot;, &quot;Temperate&quot;, &quot;Cold&quot;, &quot;Temperate&quot;, &quot;…
## $ koppen_description         &lt;chr&gt; &quot;steppe, cold&quot;, &quot;no dry season, hot summer&quot;…
## $ soil_type                  &lt;fct&gt; Cambisols, Acrisols, Luvisols, Alisols, Gle…
## $ topo_slope                 &lt;int&gt; 6, 2, 0, 10, 0, 10, 6, 0, 2, 0, 0, 1, 0, 1,…
## $ topo_diversity             &lt;int&gt; 29, 24, 21, 25, 19, 30, 26, 20, 26, 22, 25,…
## $ topo_elevation             &lt;int&gt; 1821, 143, 765, 1474, 378, 485, 604, 1159, …
## $ swi_mean                   &lt;dbl&gt; 27.5, 56.1, 41.4, 59.3, 37.4, 56.3, 52.3, 2…
## $ swi_max                    &lt;dbl&gt; 62.9, 74.4, 81.9, 81.1, 83.2, 73.8, 55.8, 3…
## $ swi_min                    &lt;dbl&gt; 24.5, 33.3, 42.2, 31.3, 8.3, 28.8, 25.3, 11…
## $ swi_range                  &lt;dbl&gt; 38.4, 41.2, 39.7, 49.8, 74.9, 45.0, 30.5, 2…
## $ soil_temperature_mean      &lt;dbl&gt; 4.8, 19.9, 1.2, 13.0, 28.2, 18.1, 21.5, 23.…
## $ soil_temperature_max       &lt;dbl&gt; 29.9, 32.6, 20.4, 24.6, 41.6, 29.1, 26.4, 4…
## $ soil_temperature_min       &lt;dbl&gt; -12.4, 3.9, -16.0, -0.4, 16.8, 4.1, 17.3, 5…
## $ soil_temperature_range     &lt;dbl&gt; 42.3, 28.8, 36.4, 25.0, 24.8, 24.9, 9.1, 38…
## $ soil_sand                  &lt;int&gt; 41, 39, 27, 29, 48, 33, 30, 78, 23, 64, 54,…
## $ soil_clay                  &lt;int&gt; 20, 24, 28, 31, 27, 29, 40, 15, 26, 22, 23,…
## $ soil_silt                  &lt;int&gt; 38, 35, 43, 38, 23, 36, 29, 6, 49, 13, 22, …
## $ soil_ph                    &lt;dbl&gt; 6.5, 5.9, 5.6, 5.5, 6.5, 5.8, 5.2, 7.1, 7.3…
## $ soil_soc                   &lt;dbl&gt; 43.1, 14.6, 36.4, 34.9, 8.1, 20.8, 44.5, 4.…
## $ soil_nitrogen              &lt;dbl&gt; 2.8, 1.3, 2.9, 3.6, 1.2, 1.9, 2.8, 0.6, 3.1…
## $ solar_rad_mean             &lt;dbl&gt; 17.634, 19.198, 13.257, 14.163, 24.512, 17.…
## $ solar_rad_max              &lt;dbl&gt; 31.317, 24.498, 25.283, 17.237, 28.038, 22.…
## $ solar_rad_min              &lt;dbl&gt; 5.209, 13.311, 1.587, 9.642, 19.102, 12.196…
## $ solar_rad_range            &lt;dbl&gt; 26.108, 11.187, 23.696, 7.595, 8.936, 10.20…
## $ growing_season_length      &lt;dbl&gt; 139, 365, 164, 333, 228, 365, 365, 60, 365,…
## $ growing_season_temperature &lt;dbl&gt; 12.65, 19.35, 11.55, 12.45, 26.45, 17.75, 2…
## $ growing_season_rainfall    &lt;dbl&gt; 224.5, 1493.4, 345.4, 1765.5, 984.4, 1860.5…
## $ growing_degree_days        &lt;dbl&gt; 2140.5, 7080.9, 2053.2, 4162.9, 10036.7, 64…
## $ temperature_mean           &lt;dbl&gt; 3.65, 19.35, 1.45, 11.35, 27.55, 17.65, 22.…
## $ temperature_max            &lt;dbl&gt; 24.65, 33.35, 21.15, 23.75, 38.35, 30.55, 2…
## $ temperature_min            &lt;dbl&gt; -14.05, 3.05, -18.25, -3.55, 19.15, 2.45, 1…
## $ temperature_range          &lt;dbl&gt; 38.7, 30.3, 39.4, 27.3, 19.2, 28.1, 7.0, 29…
## $ temperature_seasonality    &lt;dbl&gt; 882.6, 786.6, 1070.9, 724.7, 219.3, 747.2, …
## $ rainfall_mean              &lt;int&gt; 446, 1493, 560, 1794, 990, 1860, 3150, 356,…
## $ rainfall_min               &lt;int&gt; 25, 37, 24, 29, 0, 60, 122, 1, 10, 12, 0, 0…
## $ rainfall_max               &lt;int&gt; 62, 209, 87, 293, 226, 275, 425, 62, 256, 3…
## $ rainfall_range             &lt;int&gt; 37, 172, 63, 264, 226, 215, 303, 61, 245, 2…
## $ evapotranspiration_mean    &lt;dbl&gt; 78.32, 105.88, 50.03, 64.65, 156.60, 108.50…
## $ evapotranspiration_max     &lt;dbl&gt; 164.70, 190.86, 117.53, 115.79, 187.71, 191…
## $ evapotranspiration_min     &lt;dbl&gt; 13.67, 50.44, 3.53, 28.01, 128.59, 51.39, 8…
## $ evapotranspiration_range   &lt;dbl&gt; 151.03, 140.42, 113.99, 87.79, 59.13, 139.9…
## $ cloud_cover_mean           &lt;int&gt; 31, 48, 42, 64, 38, 52, 60, 13, 53, 20, 11,…
## $ cloud_cover_max            &lt;int&gt; 39, 61, 49, 71, 58, 67, 77, 18, 60, 27, 23,…
## $ cloud_cover_min            &lt;int&gt; 16, 34, 33, 54, 19, 39, 45, 6, 45, 14, 2, 1…
## $ cloud_cover_range          &lt;int&gt; 23, 27, 15, 17, 38, 27, 32, 11, 15, 12, 21,…
## $ aridity_index              &lt;dbl&gt; 0.54, 1.27, 0.90, 2.08, 0.55, 1.67, 2.88, 0…
## $ humidity_mean              &lt;dbl&gt; 55.56, 62.14, 59.87, 69.32, 51.60, 62.76, 7…
## $ humidity_max               &lt;dbl&gt; 63.98, 65.00, 68.19, 71.90, 67.07, 65.68, 7…
## $ humidity_min               &lt;dbl&gt; 48.41, 58.97, 53.75, 67.21, 33.89, 59.92, 7…
## $ humidity_range             &lt;dbl&gt; 15.57, 6.03, 14.44, 4.69, 33.18, 5.76, 3.99…
## $ biogeo_ecoregion           &lt;chr&gt; &quot;South Central Rockies forests&quot;, &quot;Jian Nan …
## $ biogeo_biome               &lt;chr&gt; &quot;Temperate Conifer Forests&quot;, &quot;Tropical &amp; Su…
## $ biogeo_realm               &lt;chr&gt; &quot;Nearctic&quot;, &quot;Indomalayan&quot;, &quot;Nearctic&quot;, &quot;Pal…
## $ country_name               &lt;chr&gt; &quot;United States of America&quot;, &quot;China&quot;, &quot;Canad…
## $ country_population         &lt;dbl&gt; 313973000, 1338612970, 33487208, 1338612970…
## $ country_gdp                &lt;dbl&gt; 15094000, 7973000, 1300000, 7973000, 15860,…
## $ country_income             &lt;chr&gt; &quot;1. High income: OECD&quot;, &quot;3. Upper middle in…
## $ continent                  &lt;chr&gt; &quot;North America&quot;, &quot;Asia&quot;, &quot;North America&quot;, &quot;…
## $ region                     &lt;chr&gt; &quot;Americas&quot;, &quot;Asia&quot;, &quot;Americas&quot;, &quot;Asia&quot;, &quot;Af…
## $ subregion                  &lt;chr&gt; &quot;Northern America&quot;, &quot;Eastern Asia&quot;, &quot;Northe…
</code></pre>
<p>The categorical variables in this data frame are identified below:</p>
<pre><code class="language-r">vi_categorical &lt;- collinear::identify_predictors_categorical(
  df = vi,
  predictors = vi_predictors
)
vi_categorical
</code></pre>
<pre><code>##  [1] &quot;koppen_zone&quot;        &quot;koppen_group&quot;       &quot;koppen_description&quot;
##  [4] &quot;soil_type&quot;          &quot;biogeo_ecoregion&quot;   &quot;biogeo_biome&quot;      
##  [7] &quot;biogeo_realm&quot;       &quot;country_name&quot;       &quot;country_income&quot;    
## [10] &quot;continent&quot;          &quot;region&quot;             &quot;subregion&quot;
</code></pre>
<p>And finally, their number of categories:</p>
<pre><code class="language-r">data.frame(
  name = vi_categorical,
  categories = lapply(
  X = vi_categorical,
  FUN = function(x) length(unique(vi[[x]]))
) |&gt; 
  unlist()
) |&gt; 
  dplyr::arrange(
    dplyr::desc(categories)
  )
</code></pre>
<pre><code>##                  name categories
## 1    biogeo_ecoregion        604
## 2        country_name        176
## 3           soil_type         29
## 4         koppen_zone         25
## 5           subregion         21
## 6  koppen_description         19
## 7        biogeo_biome         13
## 8        biogeo_realm          7
## 9           continent          7
## 10     country_income          6
## 11             region          6
## 12       koppen_group          5
</code></pre>
<p>Some of them, like <code>country_name</code> and <code>biogeo_ecoregion</code>, have a cardinality high enough to ruin our day, don&rsquo;t they? But ok, let&rsquo;s start with one with a moderate number of categories, like <code>koppen_zone</code>. This variable has 25 categories representing climate zones.</p>
<pre><code class="language-r">sort(unique(vi$koppen_zone))
</code></pre>
<pre><code>##  [1] &quot;Af&quot;  &quot;Am&quot;  &quot;Aw&quot;  &quot;BSh&quot; &quot;BSk&quot; &quot;BWh&quot; &quot;BWk&quot; &quot;Cfa&quot; &quot;Cfb&quot; &quot;Cfc&quot; &quot;Csa&quot; &quot;Csb&quot;
## [13] &quot;Cwa&quot; &quot;Cwb&quot; &quot;Dfa&quot; &quot;Dfb&quot; &quot;Dfc&quot; &quot;Dfd&quot; &quot;Dsa&quot; &quot;Dsb&quot; &quot;Dsc&quot; &quot;Dwa&quot; &quot;Dwb&quot; &quot;Dwc&quot;
## [25] &quot;ET&quot;
</code></pre>
<h1 id="one-hot-encoding-is-here">One-hot Encoding is here&hellip;</h1>
<p>Let&rsquo;s use it as predictor of <code>vi_numeric</code> in a linear model and take a look at the summary.</p>
<pre><code class="language-r">lm(
  formula = vi_numeric ~ koppen_zone, 
  data = vi
  ) |&gt; 
  summary()
</code></pre>
<pre><code>## 
## Call:
## lm(formula = vi_numeric ~ koppen_zone, data = vi)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.57090 -0.05592 -0.00305  0.05695  0.49212 
## 
## Coefficients:
##                 Estimate Std. Error  t value Pr(&gt;|t|)    
## (Intercept)     0.670899   0.002054  326.651  &lt; 2e-16 ***
## koppen_zoneAm  -0.022807   0.003151   -7.239 4.64e-13 ***
## koppen_zoneAw  -0.143375   0.002434  -58.903  &lt; 2e-16 ***
## koppen_zoneBSh -0.347894   0.002787 -124.839  &lt; 2e-16 ***
## koppen_zoneBSk -0.422162   0.002823 -149.523  &lt; 2e-16 ***
## koppen_zoneBWh -0.537854   0.002392 -224.859  &lt; 2e-16 ***
## koppen_zoneBWk -0.543022   0.002906 -186.883  &lt; 2e-16 ***
## koppen_zoneCfa -0.104730   0.003087  -33.928  &lt; 2e-16 ***
## koppen_zoneCfb -0.081909   0.003949  -20.744  &lt; 2e-16 ***
## koppen_zoneCfc -0.120899   0.017419   -6.941 3.99e-12 ***
## koppen_zoneCsa -0.274720   0.005145  -53.399  &lt; 2e-16 ***
## koppen_zoneCsb -0.136575   0.006142  -22.237  &lt; 2e-16 ***
## koppen_zoneCwa -0.149006   0.003318  -44.910  &lt; 2e-16 ***
## koppen_zoneCwb -0.177753   0.004579  -38.817  &lt; 2e-16 ***
## koppen_zoneDfa -0.214981   0.004437  -48.453  &lt; 2e-16 ***
## koppen_zoneDfb -0.179080   0.003347  -53.499  &lt; 2e-16 ***
## koppen_zoneDfc -0.237050   0.003937  -60.207  &lt; 2e-16 ***
## koppen_zoneDfd -0.395899   0.065900   -6.008 1.91e-09 ***
## koppen_zoneDsa -0.462494   0.011401  -40.567  &lt; 2e-16 ***
## koppen_zoneDsb -0.330969   0.008056  -41.084  &lt; 2e-16 ***
## koppen_zoneDsc -0.327097   0.011244  -29.090  &lt; 2e-16 ***
## koppen_zoneDwa -0.282620   0.005248  -53.850  &lt; 2e-16 ***
## koppen_zoneDwb -0.254027   0.005981  -42.473  &lt; 2e-16 ***
## koppen_zoneDwc -0.306156   0.005660  -54.096  &lt; 2e-16 ***
## koppen_zoneET  -0.297869   0.011649  -25.571  &lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.09315 on 29975 degrees of freedom
## Multiple R-squared:  0.805,	Adjusted R-squared:  0.8049 
## F-statistic:  5157 on 24 and 29975 DF,  p-value: &lt; 2.2e-16
</code></pre>
<p>Look at that. What the hell happened there? Well, linear models cannot deal with categorical predictors, so they create numeric <strong>dummy variables</strong> instead. The function <code>stats::model.matrix()</code> does exactly that:</p>
<pre><code class="language-r">dummy_variables &lt;- stats::model.matrix( 
  ~ koppen_zone,
  data = vi
  )
ncol(dummy_variables)
</code></pre>
<pre><code>## [1] 25
</code></pre>
<pre><code class="language-r">dummy_variables[1:10, 1:10]
</code></pre>
<pre><code>##    (Intercept) koppen_zoneAm koppen_zoneAw koppen_zoneBSh koppen_zoneBSk
## 1            1             0             0              0              1
## 2            1             0             0              0              0
## 3            1             0             0              0              0
## 4            1             0             0              0              0
## 5            1             0             1              0              0
## 6            1             0             0              0              0
## 7            1             0             0              0              0
## 8            1             0             0              1              0
## 9            1             0             0              0              0
## 10           1             0             0              0              0
##    koppen_zoneBWh koppen_zoneBWk koppen_zoneCfa koppen_zoneCfb koppen_zoneCfc
## 1               0              0              0              0              0
## 2               0              0              1              0              0
## 3               0              0              0              0              0
## 4               0              0              0              1              0
## 5               0              0              0              0              0
## 6               0              0              1              0              0
## 7               0              0              0              0              0
## 8               0              0              0              0              0
## 9               0              0              0              0              0
## 10              1              0              0              0              0
</code></pre>
<p>This function first creates an Intercept column with all ones. Then, for each original category except the first one (&ldquo;Af&rdquo;), a new column with value 1 in the cases where the given category was present and 0 otherwise is created. The category with no column (&ldquo;Af&rdquo;) is represented in these cases in the intercept where all other dummy columns are zero. This is, essentially, <strong>one-hot encoding</strong> with a little twist. You will find most people use the terms <em>dummy variables</em> and <em>one-hot encoding</em> interchangeably, and that&rsquo;s ok. But in the end, the little twist of omitting the first category is what differentiates them. Most functions performing one-hot encoding, no matter their name, are creating as many columns as categories. That is for example the case of <code>fastDummies::dummy_cols()</code>, from the R package 
<a href="https://jacobkap.github.io/fastDummies/" target="_blank" rel="noopener"><code>fastDummies</code></a>:</p>
<pre><code class="language-r">df &lt;- fastDummies::dummy_cols(
  .data = vi[, &quot;koppen_zone&quot;, drop = FALSE],
  select_columns = &quot;koppen_zone&quot;,
  remove_selected_columns = TRUE
)
dplyr::glimpse(df)
</code></pre>
<pre><code>## Rows: 30,000
## Columns: 25
## $ koppen_zone_Af  &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Am  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Aw  &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_BSh &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …
## $ koppen_zone_BSk &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_BWh &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, …
## $ koppen_zone_BWk &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …
## $ koppen_zone_Cfa &lt;int&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Cfb &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …
## $ koppen_zone_Cfc &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Csa &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Csb &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Cwa &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Cwb &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dfa &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …
## $ koppen_zone_Dfb &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dfc &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dfd &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dsa &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dsb &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dsc &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dwa &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dwb &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dwc &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_ET  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
</code></pre>
<h1 id="to-mess-up-your-models">&hellip;to mess-up your models</h1>
<p>As good as one-hot encoding is to fit linear models when predictors are categorical, it creates a couple of glaring issues that are hard to address when the number of encoded categories is high.</p>
<p>The first issue can easily be named the <strong>dimensionality explosion</strong>. If we created dummy variables for all categorical predictors in <code>vi</code>, then we&rsquo;d go from the original 61 predictors to a total of 967 new columns to handle. This alone can degrade the computational performance of a model due to increased data size.</p>
<p>The second issue is <strong>increased multicollinearity</strong>. One-hot encoded features are highly collinear, which makes obtaining accurate estimates for the coefficients of the encoded categories very hard. Look at the Variance Inflation Factors of the encoded Koppen zones, they have incredibly high values!</p>
<pre><code class="language-r">collinear::vif_df(
  df = df,
  quiet = TRUE
)
</code></pre>
<pre><code>##          predictor          vif
## 6  koppen_zone_BWh 2.403991e+15
## 3   koppen_zone_Aw 2.177627e+15
## 4  koppen_zone_BSh 1.158438e+15
## 5  koppen_zone_BSk 1.100300e+15
## 1   koppen_zone_Af 9.879589e+14
## 7  koppen_zone_BWk 9.866240e+14
## 8  koppen_zone_Cfa 7.966760e+14
## 2   koppen_zone_Am 7.440723e+14
## 13 koppen_zone_Cwa 6.309241e+14
## 16 koppen_zone_Dfb 6.139201e+14
## 17 koppen_zone_Dfc 3.863684e+14
## 9  koppen_zone_Cfb 3.834325e+14
## 15 koppen_zone_Dfa 2.838687e+14
## 14 koppen_zone_Cwb 2.624933e+14
## 11 koppen_zone_Csa 1.984881e+14
## 22 koppen_zone_Dwa 1.894423e+14
## 24 koppen_zone_Dwc 1.592088e+14
## 23 koppen_zone_Dwb 1.405032e+14
## 12 koppen_zone_Csb 1.323997e+14
## 20 koppen_zone_Dsb 7.338609e+13
## 21 koppen_zone_Dsc 3.652432e+13
## 19 koppen_zone_Dsa 3.549784e+13
## 25  koppen_zone_ET 3.395786e+13
## 10 koppen_zone_Cfc 1.493932e+13
## 18 koppen_zone_Dfd 1.031226e+12
</code></pre>
<p>On top of those issues, one-hot encoding also causes <strong>sparsity</strong> in tree-based models. Let me show you an example. Below I train a recursive partition tree using <code>vi_numeric</code> as response, and the one-hot encoded version of <code>koppen_zone</code> we have in <code>df</code>.</p>
<pre><code class="language-r">#add response variable to df
df$vi_numeric &lt;- vi$vi_numeric

#fit model using all one-hot encoded variables
koppen_zone_one_hot &lt;- rpart::rpart(
  formula = vi_numeric ~ .,
  data = df
)
</code></pre>
<p>Now I do the same using the categorical version of <code>koppen_zone</code> in <code>vi</code>.</p>
<pre><code class="language-r">koppen_zone_categorical &lt;- rpart::rpart(
  formula = vi_numeric ~ koppen_zone,
  data = vi
)
</code></pre>
<p>Finally, I am plotting the skeletons of these trees side by side (we don&rsquo;t care about numbers here).</p>
<pre><code class="language-r">#plot tree skeleton
par(mfrow = c(1, 2))
plot(koppen_zone_one_hot, main = &quot;One-hot encoding&quot;)
plot(koppen_zone_categorical, main = &quot;Categorical&quot;)
</code></pre>
<img src="https://blasbenito.com/post/target-encoding/index_files/figure-html/unnamed-chunk-13-1.png" width="576" />
<p>Notice the stark differences in tree structure between both options. On the left, the tree trained on the one-hot encoded data only shows growth on one side! This is the <em>sparsity</em> I was talking about before. On the right side, however, the tree based on the categorical variable shows a balanced and healthy structure. One-hot encoded data can easily mess up a single univariate regression tree, so imagine what it can do to your fancy random forest model with hundreds of these trees.</p>
<p>In the end, the magic of one-hot encoding is in its inherent ability to create two or three problems for each one it promised to solve. We all know someone like that. Not so hot, if you ask me.</p>
<h1 id="target-encoding-mean-encoding-and-dummy-variables-all-the-same">Target Encoding, Mean Encoding, and Dummy Variables (All The Same)</h1>
<p>On a bright summer day of 2001, 
<a href="https://www.aitimejournal.com/interview-with-daniele-micci-barreca-product-analytics-lead-data-science-google/30110/" target="_blank" rel="noopener">Daniele Micci-Barreca</a> finally got sick of the one-hot encoding wonders and decided to publish 
<a href="https://doi.org/10.1145/507533.507538" target="_blank" rel="noopener">his ideas on a suitable alternative</a> others later named <em>mean encoding</em> or <em>target encoding</em>. He told the story himself 20 years later, in a nice blog post titled 
<a href="https://towardsdatascience.com/extending-target-encoding-443aa9414cae" target="_blank" rel="noopener">Extending Target Encoding</a>.</p>
<p>But what is target encoding? Let&rsquo;s start with a continuous response variable <code>y</code> (a.k.a <em>the target</em>) and a categorical predictor <code>x</code>.</p>
<h2 id="mean-encoding">Mean Encoding</h2>
<p>In <em>it&rsquo;s simplest form</em>, target encoding replaces each category in <code>x</code> with the mean of <code>y</code> across the category cases. This results in a new numeric version of <code>x</code> named <code>x_encoded</code> in the example below.</p>
<pre><code class="language-r">yx |&gt; 
  dplyr::group_by(x) |&gt; 
  dplyr::mutate(
    x_encoded = mean(y)
  )
</code></pre>
<pre><code>## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;
## 1     1 a             2
## 2     2 a             2
## 3     3 a             2
## 4     4 b             5
## 5     5 b             5
## 6     6 b             5
## 7     7 c             7
</code></pre>
<p>Simple is good, right? But sometimes it&rsquo;s not. In our toy case, the category &ldquo;c&rdquo; has only one case that maps directly to an actual value of <code>y</code>.Imagine the worst case scenario of <code>x</code> having one different category per row, then <code>x_encoded</code> would be identical to <code>y</code>!</p>
<h2 id="mean-encoding-with-additive-smoothing">Mean Encoding With Additive Smoothing</h2>
<p>The issue can be solved by pushing the mean of <code>y</code> for each category in <code>x</code> towards the global mean of <code>y</code> by the weighted sample size of the category, as suggested by the expression</p>
<p><code>$$x\_encoded_i = \frac{n_i \times \overline{y}_i + m \times \overline{y}}{n_i + m}$$</code></p>
<p>where:</p>
<ul>
<li><code>\(n_i\)</code> is the size of the category <code>\(i\)</code>.</li>
<li><code>\(\overline{y}_i\)</code> is the mean of the target over the category <code>\(i\)</code>.</li>
<li><code>\(m\)</code> is the smoothing parameter.</li>
<li><code>\(\overline{y}\)</code> is the global mean of the target.</li>
</ul>
<pre><code class="language-r">y_mean &lt;- mean(yx$y)

m &lt;- 3

yx |&gt; 
  dplyr::group_by(x) |&gt; 
  dplyr::mutate(
    x_encoded = 
      (dplyr::n() * mean(y) + m * y_mean) / (dplyr::n() + m)
  )
</code></pre>
<pre><code>## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;
## 1     1 a          3   
## 2     2 a          3   
## 3     3 a          3   
## 4     4 b          4.5 
## 5     5 b          4.5 
## 6     6 b          4.5 
## 7     7 c          4.75
</code></pre>
<p>So far so good! But still, the simplest implementations of target encoding generate repeated values for all cases within a category. This can still mess-up tree-based models a bit, because splits may happen again and again in the same values of the predictor. However, there are several strategies to limit this issue as well.</p>
<h2 id="leave-one-out-target-encoding">Leave-one-out Target Encoding</h2>
<p>In this version of target encoding, the encoded value of one case within a category is the mean of all other cases within the same category. This results in a robust encoding that avoids direct reference to the target value of the sample being encoded, and does not generate repeated values.</p>
<p>The code below implements the idea in a way so simple that it cannot even deal with one-case categories.</p>
<pre><code class="language-r">yx |&gt;
  dplyr::group_by(x) |&gt;
  dplyr::mutate(
    x_encoded = (sum(y) - y) / (dplyr::n() - 1)
  )
</code></pre>
<pre><code>## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;
## 1     1 a           2.5
## 2     2 a           2  
## 3     3 a           1.5
## 4     4 b           5.5
## 5     5 b           5  
## 6     6 b           4.5
## 7     7 c         NaN
</code></pre>
<h2 id="mean-encoding-with-white-noise">Mean Encoding with White Noise</h2>
<p>Another way to avoid repeated values while keeping the encoding as simple as possible consists of just adding a white noise to the encoded values. The code below adds noise generated by <code>stats::runif()</code> to the mean-encoded values, but other options such as <code>stats::rnorm()</code> (noise from a normal distribution) can be useful here. Since white noise is random, we need to set the seed of the pseudo-random number generator (with <code>set.seed()</code>) to obtain constant results every time we run the code below.</p>
<p>When using this method we have to be careful with the amount of noise we add. It should be a harmless fraction of target, small enough to not throw a model off the signal provided by the encoded variable. In our toy case <code>y</code> is between 1 and 7, so something like &ldquo;one percent of the maximum&rdquo; could work well here.</p>
<pre><code class="language-r">#maximum noise to add
max_noise &lt;- max(yx$y)/100

#set seed for reproducibility
set.seed(1)

yx |&gt; 
  dplyr::group_by(x) |&gt; 
  dplyr::mutate(
    x_encoded = mean(y) + runif(n = dplyr::n(), max = max_noise)
  )
</code></pre>
<pre><code>## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;
## 1     1 a          2.02
## 2     2 a          2.03
## 3     3 a          2.04
## 4     4 b          5.06
## 5     5 b          5.01
## 6     6 b          5.06
## 7     7 c          7.07
</code></pre>
<p>This method can deal with one-case categories without issues, and does not generate repeated values, but in exchange, we have to be mindful of the amount of noise we add, and we have to set a random seed to ensure reproducibility.</p>
<h2 id="rank-encoding-plus-white-noise">Rank Encoding plus White Noise</h2>
<p>This is a little different from all the other methods, because it does not map the categories to values from the target, but to the rank/order of the target means per category. It basically converts the categorical variable into an ordinal one arranged along with the target, and then adds white noise on top to avoid value repetition.</p>
<pre><code class="language-r">#maximum noise as function of the number of categories
max_noise &lt;- length(unique(yx$x))/100

yx |&gt; 
  dplyr::arrange(y) |&gt; 
  dplyr::group_by(x) |&gt; 
  dplyr::mutate(
    x_encoded = dplyr::cur_group_id() + runif(n = dplyr::n(), max = max_noise)
  )
</code></pre>
<pre><code>## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;
## 1     1 a          1.02
## 2     2 a          1.02
## 3     3 a          1.00
## 4     4 b          2.01
## 5     5 b          2.01
## 6     6 b          2.02
## 7     7 c          3.01
</code></pre>
<h2 id="the-target-encoding-lab">The Target Encoding Lab</h2>
<p>The function <code>collinear::target_encoding_lab()</code> implements all these encoding methods, and allows defining different combinations of parameters. It was designed to help understand how they work, and maybe help make choices about what&rsquo;s the right encoding for a given categorical predictor.</p>
<p>In the example below, the methods rank, mean, and leave-one-out are computed with white noise of 0 and 0.1 (that&rsquo;s the width of the uniform distribution the noise is extracted from), the mean is also with and without smoothing, and the rnorm is computed using two different multipliers of the standard deviation of the normal distribution computed for each group in the predictor, just to help control the data spread.</p>
<p>The function also uses a random seed to generate the same noise across the encoded versions of the predictor to make them as comparable as possible. Every time you change the seed, results using white noise and the rnorm method should change as well.</p>
<pre><code class="language-r">yx_encoded &lt;- target_encoding_lab(
  df = yx,
  response = &quot;y&quot;,
  predictors = &quot;x&quot;,
  white_noise = c(0, 0.1),
  smoothing = c(0, 2),
  quiet = FALSE,
  seed = 1, #for reproducibility
  overwrite = FALSE #to overwrite or not the predictors with their encodings
)
</code></pre>
<pre><code>## 
## collinear::target_encoding_lab(): using response 'y' to encode categorical predictors:
##  - x
</code></pre>
<pre><code class="language-r">dplyr::glimpse(yx_encoded)
</code></pre>
<pre><code>## Rows: 7
## Columns: 14
## $ y                                               &lt;int&gt; 1, 2, 3, 4, 5, 6, 7
## $ x                                               &lt;chr&gt; &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b…
## $ x__encoded_loo                                  &lt;dbl&gt; 2.5, 2.0, 1.5, 5.5, 5.…
## $ x__encoded_loo__noise_0.1__seed_1               &lt;dbl&gt; 2.554579, 1.564069, 2.…
## $ x__encoded_loo                                  &lt;dbl&gt; 2.5, 2.0, 1.5, 5.5, 5.…
## $ x__encoded_loo__noise_0.1__seed_1               &lt;dbl&gt; 2.554579, 1.564069, 2.…
## $ x__encoded_mean                                 &lt;dbl&gt; 2, 2, 2, 5, 5, 5, 7
## $ x__encoded_mean__noise_0.1__seed_1              &lt;dbl&gt; 2.054579, 1.564069, 2.…
## $ x__encoded_mean__smoothing_2                    &lt;dbl&gt; 2.8, 2.8, 2.8, 4.6, 4.…
## $ x__encoded_mean__smoothing_2__noise_0.1__seed_1 &lt;dbl&gt; 2.854579, 2.364069, 3.…
## $ x__encoded_rank                                 &lt;int&gt; 1, 1, 1, 2, 2, 2, 3
## $ x__encoded_rank__noise_0.1__seed_1              &lt;dbl&gt; 1.0545786, 0.5640694, …
## $ x__encoded_rank                                 &lt;int&gt; 1, 1, 1, 2, 2, 2, 3
## $ x__encoded_rank__noise_0.1__seed_1              &lt;dbl&gt; 1.0545786, 0.5640694, …
</code></pre>
<pre><code class="language-r">yx_encoded |&gt; 
  tidyr::pivot_longer(
    cols = dplyr::contains(&quot;__encoded&quot;),
    values_to = &quot;x_encoded&quot;
  ) |&gt; 
  ggplot() + 
  facet_wrap(&quot;name&quot;) +
  aes(
    x = x_encoded,
    y = y,
    color = x
  ) +
  geom_point(size = 3) + 
  theme_bw()
</code></pre>
<img src="https://blasbenito.com/post/target-encoding/index_files/figure-html/unnamed-chunk-21-1.png" width="1152" />
The function also allows to replace a given predictor with their selected encoding.
<pre><code class="language-r">yx_encoded &lt;- collinear::target_encoding_lab(
  df = yx,
  response = &quot;y&quot;,
  predictors = &quot;x&quot;,
  methods = &quot;mean&quot;, #selected encoding method
  smoothing = 2,
  quiet = FALSE,
  overwrite = TRUE
)
</code></pre>
<pre><code>## 
## collinear::target_encoding_lab(): using response 'y' to encode categorical predictors:
##  - x
</code></pre>
<pre><code class="language-r">dplyr::glimpse(yx_encoded)
</code></pre>
<pre><code>## Rows: 7
## Columns: 2
## $ y &lt;int&gt; 1, 2, 3, 4, 5, 6, 7
## $ x &lt;dbl&gt; 2.8, 2.8, 2.8, 4.6, 4.6, 4.6, 5.0
</code></pre>
<p>And that&rsquo;s all about target encoding so far!</p>
<p>I have a post in my TODO list with a little real experiment comparing target encoding with one-hot encoding in tree-based models. If you are interested, stay tuned!</p>
<p>Cheers,</p>
<p>Blas</p>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/r-packages/">R packages</a>
  
  <a class="badge badge-light" href="/tag/multicollinearity/">Multicollinearity</a>
  
  <a class="badge badge-light" href="/tag/variable-selection/">Variable Selection</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://blasbenito.com/post/target-encoding/&amp;text=Mapping%20Categorical%20Predictors%20to%20Numeric%20With%20Target%20Encoding" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://blasbenito.com/post/target-encoding/&amp;t=Mapping%20Categorical%20Predictors%20to%20Numeric%20With%20Target%20Encoding" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Mapping%20Categorical%20Predictors%20to%20Numeric%20With%20Target%20Encoding&amp;body=https://blasbenito.com/post/target-encoding/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://blasbenito.com/post/target-encoding/&amp;title=Mapping%20Categorical%20Predictors%20to%20Numeric%20With%20Target%20Encoding" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Mapping%20Categorical%20Predictors%20to%20Numeric%20With%20Target%20Encoding%20https://blasbenito.com/post/target-encoding/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://blasbenito.com/post/target-encoding/&amp;title=Mapping%20Categorical%20Predictors%20to%20Numeric%20With%20Target%20Encoding" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
    
    





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/blas-m.-benito/avatar_hu540359786128d310fe40f64bfadab7c8_179471_270x270_fill_q90_lanczos_center.jpg" alt="Blas M. Benito">
      

      <div class="media-body">
        <h5 class="card-title"><a href="https://blasbenito.com/">Blas M. Benito</a></h5>
        <h6 class="card-subtitle">Data Scientist and Team Lead</h6>
        
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:blasbenito@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://es.linkedin.com/in/blas-m-benito-6174a643" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/BlasBenito" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=WBTp0McAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://orcid.org/0000-0001-5105-7232" target="_blank" rel="noopener">
        <i class="fab fa-orcid"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://fosstodon.org/@blasbenito" target="_blank" rel="noopener">
        <i class="fab fa-mastodon"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  


  












  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/multicollinearity-model-interpretability/">Multicollinearity Hinders Model Interpretability</a></li>
      
      <li><a href="/post/variance-inflation-factor/">Everything You Don&#39;t Need to Know About Variance Inflation Factors</a></li>
      
      <li><a href="/project/post-title/">R package collinear</a></li>
      
      <li><a href="/project/post-title/">R package spatialRF</a></li>
      
      <li><a href="/publication/2020_benito_ecography_distantia/">distantia: an open‐source toolset to quantify dissimilarity between multivariate ecological time‐series</a></li>
      
    </ul>
  </div>
  





  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/highlight.min.js" integrity="sha512-7t8APmYpzEsZP7CYoA7RfMPV9Bb+PJHa9x2WiUnDXZx3XHveuyWUtvNOexhkierl5flZ3tr92dP1mMS+SGlD+A==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/sql.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/bash.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.4182b289689108d47721d002a07cd804.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    © 2023 Blas M. Benito. All Rights Reserved.
  </p>

  
  






  <p class="powered-by">
    
    Published with
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">__ Academic Website Builder</a>
    <a rel="me" href="https://fosstodon.org/@blasbenito">__</a>
    

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
