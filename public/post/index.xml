<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Blas M. Benito</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 03 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/avatar.jpg</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Setup of a shared folder in a home cluster</title>
      <link>/post/03_shared_folder_in_cluster/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/post/03_shared_folder_in_cluster/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In the previous posts I have covered how to &lt;a href=&#34;https://www.blasbenito.com/post/01_home_cluster/&#34;&gt;setup a home cluster&lt;/a&gt;, and how to &lt;a href=&#34;https://www.blasbenito.com/post/02_parallelizing_loops_with_r/&#34;&gt;run parallel processes with &lt;code&gt;foreach&lt;/code&gt; in R&lt;/a&gt;. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.&lt;/p&gt;
&lt;p&gt;This post (developed on Ubuntu and Ubuntu Server 20.04) assumes that the home cluster runs on Linux, and covers the following topics.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basics of the Network File System protocol (NFS).&lt;/li&gt;
&lt;li&gt;Setup of an NFS folder in a home cluster.&lt;/li&gt;
&lt;li&gt;Using an NFS folder in a parallelized loop.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div id=&#34;the-network-file-system-protocol-nfs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Network File System protocol (NFS)&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Network_File_System&#34;&gt;Network File System&lt;/a&gt; protocol offers the means for a &lt;em&gt;host&lt;/em&gt; computer to allow other computers in the network (&lt;em&gt;clients&lt;/em&gt;) to read and write the contents of a given folder. The clients access such folder like if it was a local one, while in fact it is a &lt;em&gt;reference&lt;/em&gt; to the one in the host computer.&lt;/p&gt;
&lt;p&gt;The image at the beginning of the post illustrates the concept. There is a &lt;em&gt;host&lt;/em&gt; computer with a folder in the path &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; (were &lt;code&gt;user&lt;/code&gt; is your user name) that is broadcasted to the network, and there are one or several &lt;em&gt;clients&lt;/em&gt; that are mounting &lt;em&gt;mounting&lt;/em&gt; (making accessible) the same folder in their local paths &lt;code&gt;/home/user/cluster_shared&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If the host writes a file to the shared folder, it is available right away for the clients, and the other way around. At the end, the idea is to have a folder shared among all computers in the cluster, while having the same exact path on each one of them to write or read files from such shared folder.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setup-of-an-nfs-folder-in-a-home-cluster&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup of an NFS folder in a home cluster&lt;/h2&gt;
&lt;p&gt;To setup the shared folder we’ll need to do some things in the &lt;em&gt;host&lt;/em&gt;, and some things in the &lt;em&gt;clients&lt;/em&gt;. Let’s start with the host.&lt;/p&gt;
&lt;div id=&#34;preparing-the-host-computer&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Preparing the host computer&lt;/h4&gt;
&lt;p&gt;First we need to install the &lt;code&gt;nfs-kernel-server&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo apt update
sudo apt install nfs-kernel-server&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create the shared folder. Remember to replace &lt;code&gt;user&lt;/code&gt; with your user name, and &lt;code&gt;cluster_shared&lt;/code&gt; with the actual folder name you want to use.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;mkdir /home/user/cluster_shared&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To broadcast it we need to open the file &lt;code&gt;/etc/exports&lt;/code&gt;…&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo gedit /etc/exports&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and add the following line&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;/home/user/cluster_shared IP_CLIENT1(rw,no_subtree_check) IP_CLIENT2(rw,no_subtree_check) IP_CLIENT3(rw,no_subtree_check)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/home/user/cluster_shared&lt;/code&gt; is the path of the shared folder.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IP_CLIENTx&lt;/code&gt; are the IPs of each one of the clients.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rw&lt;/code&gt; gives reading and writing permission on the shared folder to the given client.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;no_subtree_check&lt;/code&gt; prevents the host from checking the complete tree of shares before attending a request (read or write) by a client.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, the last line of my &lt;code&gt;/etc/exports&lt;/code&gt; file looks like this:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;/home/blas/cluster_shared 10.42.0.34(rw,async,no_subtree_check) 10.42.0.104(rw,async,no_subtree_check)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Save the file, and to make the changes effective, execute:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo exportfs -ra&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To end preparing the host we have to update the firewall rules to allow nfs connections from the clients. Notice that one rule per client needs to be defined, using the clients IPs to identify them.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo ufw allow from IP_CLIENT1 to any port nfs
sudo ufw allow from IP_CLIENT2 to any port nfs
sudo ufw status&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;preparing-the-clients&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Preparing the clients&lt;/h4&gt;
&lt;p&gt;First we have to install the Linux package &lt;code&gt;nfs-common&lt;/code&gt; on each client.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo apt update
sudp apt install nfs-common&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create a folder in the clients and use it to mount the NFS folder of the host.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;mkdir -p /home/user/cluster_shared
sudo mount IP_HOST:/home/user/cluster_shared /home/user/cluster_shared&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second line of code is mounting the folder &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; of the host in the folder &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; of the client.&lt;/p&gt;
&lt;p&gt;To make the mount permanent, we have to open &lt;code&gt;/etc/fstab&lt;/code&gt; with super-user privilege in the clients…&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo gedit /etc/fstab&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and add the line&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;IP_HOST:/home/user/cluster_shared /home/user/cluster_shared   nfs     defaults 0 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember to replace &lt;code&gt;IP_HOST&lt;/code&gt; and &lt;code&gt;user&lt;/code&gt; with the right values!&lt;/p&gt;
&lt;p&gt;Now we can test that the shared folder works as intended by writing one file into it from each computer in the network using the code below in the command line.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd cluster_shared
touch filename.txt&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;terminator.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once the files are created, we can check they are visible from each computer using the &lt;code&gt;ls&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;ls&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;terminator2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-an-nfs-folder-in-a-parallelized-loop&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using an NFS folder in a parallelized loop&lt;/h2&gt;
&lt;p&gt;In a &lt;a href=&#34;https://www.blasbenito.com/post/02_parallelizing_loops_with_r/&#34;&gt;previous post&lt;/a&gt; I described how to run parallelized tasks with &lt;code&gt;foreach&lt;/code&gt; in R. This section intends to complete that post by showing how a shared NFS folder can be used to store input and output files during the execution of a parallelized loop&lt;/p&gt;
&lt;div id=&#34;the-task&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The task&lt;/h3&gt;
&lt;p&gt;In this hypothetical example we have a large number of data frames stored in &lt;code&gt;/home/user/cluster_shared/input&lt;/code&gt;. Each data frame has the same predictors &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;, and a different response variable, named &lt;code&gt;y1&lt;/code&gt; for the data frame &lt;code&gt;y1&lt;/code&gt;, &lt;code&gt;y2&lt;/code&gt; for the data frame &lt;code&gt;y2&lt;/code&gt;, and so on. In the example we will be using 100 data frames, but there could be thousands. This example would be valid as well for any other kind of object, such as raster files, time-series, etc etc.&lt;/p&gt;
&lt;p&gt;First we have to load the libraries we’ll be using.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#automatic install of packages if they are not installed already
list.of.packages &amp;lt;- c(
  &amp;quot;foreach&amp;quot;,
  &amp;quot;doParallel&amp;quot;,
  &amp;quot;ranger&amp;quot;
  )

new.packages &amp;lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&amp;quot;Package&amp;quot;])]

if(length(new.packages) &amp;gt; 0){
  install.packages(new.packages, dep=TRUE)
}

#loading packages
for(package.i in list.of.packages){
  suppressPackageStartupMessages(
    library(
      package.i, 
      character.only = TRUE
      )
    )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code chunk below generates the folder &lt;code&gt;/home/user/cluster_shared/input&lt;/code&gt; and populates it with the dummy files.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#creating the input folder
input.folder &amp;lt;- &amp;quot;/home/blas/cluster_shared/input&amp;quot;
dir.create(input.folder)

#data frame names
df.names &amp;lt;- paste0(&amp;quot;y&amp;quot;, 1:100)

#filling it with files
for(i in df.names){
  
  #creating the df
  df.i &amp;lt;- data.frame(
    y = rnorm(1000),
    a = rnorm(1000),
    b = rnorm(1000),
    c = rnorm(1000),
    d = rnorm(1000)
  )
  
  #changing name of the response variable
  colnames(df.i)[1] &amp;lt;- i
  
  #assign to a variable with name i
  assign(i, df.i)
  
  #saving the object
  save(
    list = i,
    file = paste0(input.folder, &amp;quot;/&amp;quot;, i, &amp;quot;.RData&amp;quot;)
  )
  
  #removing the generated data frame form the environment
  rm(list = i, df.i, i)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our target now will be to fit one &lt;code&gt;ranger::ranger()&lt;/code&gt; model per data frame stored in &lt;code&gt;/home/blas/cluster_shared/input&lt;/code&gt;, save the model result to a folder with the path &lt;code&gt;/home/blas/cluster_shared/input&lt;/code&gt;, and write a small summary of the model to the output of &lt;code&gt;foreach&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Such target is based on this rationale: When executing a &lt;code&gt;foreach&lt;/code&gt; loop as in &lt;code&gt;x &amp;lt;- foreach(...) %dopar% {...}&lt;/code&gt;, the variable &lt;code&gt;x&lt;/code&gt; is going to grow in memory very fast, competing for RAM resources with the worker nodes. Furthermore, since &lt;code&gt;x&lt;/code&gt; is being written on the fly, the results would be lost if the computer crashes. When the size of the input and the output of our parallelized operation is larger than memory, we can use an NFS folder to store inputs and outputs, while keeping the RAM memory free for computational tasks only, with the positive side effect of having our outputs already stored should our computer decide to crash.&lt;/p&gt;
&lt;p&gt;Also, please notice that here I am focusing in a cluster setting, but using a folder to read and write data during a loop paralellized with &lt;code&gt;foreach&lt;/code&gt; can indeed be done in a single computer without an NFS folder. Any folder in your system will do the trick as well!&lt;/p&gt;
&lt;p&gt;So, from here, we are going to prepare the cluster, and execute a parallelized loop fitting one model per data frame that reads the inputs and writes the outputs to the shared folder.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cluster-setup&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cluster setup&lt;/h3&gt;
&lt;p&gt;We will also need the function I showed in the previous post to generate the cluster specification from a &lt;a href=&#34;https://gist.github.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca&#34;&gt;GitHub Gist&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below I use the function to create a cluster specification and initiate the cluster with &lt;code&gt;parallel::makeCluster()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#generate cluster specification
spec &amp;lt;- cluster_spec(
  ips = c(&amp;#39;10.42.0.1&amp;#39;, &amp;#39;10.42.0.34&amp;#39;, &amp;#39;10.42.0.104&amp;#39;),
  cores = c(7, 4, 4),
  user = &amp;quot;blas&amp;quot;
)

#define parallel port
Sys.setenv(R_PARALLEL_PORT = 11000)
Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;)

#setting up cluster
my.cluster &amp;lt;- parallel::makeCluster(
  master = &amp;#39;10.42.0.1&amp;#39;, 
  spec = spec,
  port = Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;),
  outfile = &amp;quot;&amp;quot;,
  homogeneous = TRUE
)

#check cluster definition (optional)
print(my.cluster)

#register cluster
doParallel::registerDoParallel(cl = my.cluster)

#check number of workers
foreach::getDoParWorkers()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;parallelized-loop&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parallelized loop&lt;/h3&gt;
&lt;p&gt;For everything to work as intended, we first need to create the output folder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;output.folder &amp;lt;- &amp;quot;/home/blas/cluster_shared/output&amp;quot;
dir.create(output.folder)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we are ready to execute the parallelized loop. Notice that I am using the output of &lt;code&gt;list.files()&lt;/code&gt; to have a vector of file names the loop will iterate over. The steps inside of the loop go as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;1.&lt;/em&gt; Remove the extension &lt;code&gt;.RData&lt;/code&gt; from the file name. We’ll later use the result to use &lt;code&gt;assign()&lt;/code&gt; on the fitted model to change its name to the same as the input file before saving it.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2.&lt;/em&gt; Read the input data frame and store in an object named &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;3.&lt;/em&gt; Fit the model with ranger, using the first column of &lt;code&gt;df&lt;/code&gt; as respose variable.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;4.&lt;/em&gt; Change the model name to the name of the input file without extension, resulting from the first step described above.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;5.&lt;/em&gt; Save the model into the output folder with the extension &lt;code&gt;.RData&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;6.&lt;/em&gt; Return a data frame with one line with the name of the response variable, the r-squared, and the importance of each predictor.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#list of input files as iterator
input.files &amp;lt;- list.files(
  path = input.folder,
  full.names = FALSE
)

modelling.summary &amp;lt;- foreach(
  input.file = input.files,
  .combine = &amp;#39;rbind&amp;#39;, 
  .packages = &amp;quot;ranger&amp;quot;
) %dopar% {
  
  # 1. input file name without extension
  input.file.name &amp;lt;- tools::file_path_sans_ext(input.file)
  
  # 2. read input file
  df &amp;lt;- get(load(paste0(input.folder, &amp;quot;/&amp;quot;, input.file)))
  
  # 3. fit model
  m.i &amp;lt;- ranger::ranger(
    data = df,
    dependent.variable.name = colnames(df)[1],
    importance = &amp;quot;permutation&amp;quot;
  )
  
  # 4. change name of the model to one of the response variable
  assign(input.file.name, m.i)
  
  # 5. save model
  save(
    list = input.file.name,
    file = paste0(output.folder, &amp;quot;/&amp;quot;, input.file)
  )
  
  # 6. returning summary
  return(
    data.frame(
      response.variable = input.file.name,
      r.squared = m.i$r.squared,
      importance.a = m.i$variable.importance[&amp;quot;a&amp;quot;],
      importance.b = m.i$variable.importance[&amp;quot;b&amp;quot;],
      importance.c = m.i$variable.importance[&amp;quot;c&amp;quot;],
      importance.d = m.i$variable.importance[&amp;quot;d&amp;quot;]
    )
  )
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once this parallelized loop is executed, the folder &lt;code&gt;/home/blas/cluster_shared/output&lt;/code&gt; should be filled with the results from the cluster workers, and the &lt;code&gt;modelling.summary&lt;/code&gt; data frame contains the summary of each fitted model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;output.png&#34; alt=&#34;Listing outputs in the shared folder&#34; /&gt;
Now that the work is done, we can stop the cluster.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::stopCluster(cl = my.cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you know how to work with data larger than memory in a parallelized loop!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Parallelized loops with R</title>
      <link>/post/02_parallelizing_loops_with_r/</link>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/02_parallelizing_loops_with_r/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; to better follow this tutorial you can download the .Rmd file &lt;a href=&#34;https://www.dropbox.com/s/wsl2hcex3w0lr6u/parallelized_loops.Rmd?dl=1&#34;&gt;from here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://www.blasbenito.com/post/01_home_cluster/&#34;&gt;a previous post&lt;/a&gt; I explained how to set up a small home cluster. Many things can be done with a cluster, and parallelizing loops is one of them. But there is no need of a cluster to parallelize loops and improve the efficiency of your coding!&lt;/p&gt;
&lt;p&gt;I believe that coding parallelized loops is an important asset for anyone working with R. That’s why this post covers the following topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Beyond &lt;code&gt;for&lt;/code&gt;: building loops with &lt;code&gt;foreach&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;What is a parallel backend?&lt;/li&gt;
&lt;li&gt;Setup of a parallel backend for a single computer.&lt;/li&gt;
&lt;li&gt;Setup for a Beowulf cluster.&lt;/li&gt;
&lt;li&gt;Practical examples.
&lt;ul&gt;
&lt;li&gt;Tuning of random forest hyperparameters.&lt;/li&gt;
&lt;li&gt;Confidence intervals of the importance scores of the predictors in random forest models.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div id=&#34;for-loops-are-fine-but&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;for&lt;/code&gt; loops are fine, but…&lt;/h2&gt;
&lt;p&gt;Many experienced R users frequently say that nobody should write loops with R because they are tacky or whatever. However, I find loops easy to write, read, and debug, and are therefore my workhorse whenever I need to repeat a task and I don’t feel like using &lt;code&gt;apply()&lt;/code&gt; and the likes. However, regular &lt;code&gt;for&lt;/code&gt; loops in R are highly inefficient, because they only use one of your computer cores to perform the iterations.&lt;/p&gt;
&lt;p&gt;For example, the &lt;code&gt;for&lt;/code&gt; loop below sorts vectors of random numbers a given number of times, and will only work on one of your computer cores for a few seconds, while the others are there, procrastinating with no shame.&lt;/p&gt;
&lt;div class=&#34;tenor-gif-embed&#34; data-postid=&#34;16563810&#34; data-share-method=&#34;host&#34; data-width=&#34;60%&#34; data-aspect-ratio=&#34;1.7785714285714287&#34;&gt;
&lt;a href=&#34;https://tenor.com/view/wot-cpu-danceing-break-dancing-cool-gif-16563810&#34;&gt;Wot Cpu GIF&lt;/a&gt; from &lt;a href=&#34;https://tenor.com/search/wot-gifs&#34;&gt;Wot GIFs&lt;/a&gt;
&lt;/div&gt;
&lt;script type=&#34;text/javascript&#34; async src=&#34;https://tenor.com/embed.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;(gif kindly suggested by &lt;a href=&#34;https://twitter.com/AndrosSpica&#34;&gt;Andreas Angourakis&lt;/a&gt;)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:10000){
  sort(runif(10000))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If every &lt;code&gt;i&lt;/code&gt; could run in a different core, the operation would indeed run a bit faster, and we would get rid of lazy cores. This is were packages like &lt;a href=&#34;https://cran.r-project.org/web/packages/foreach&#34;&gt;&lt;code&gt;foreach&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/doParallel&#34;&gt;&lt;code&gt;doParallel&lt;/code&gt;&lt;/a&gt; come into play. Let’s start installing these packages and a few others that will be useful throughout this tutorial.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#automatic install of packages if they are not installed already
list.of.packages &amp;lt;- c(
  &amp;quot;foreach&amp;quot;,
  &amp;quot;doParallel&amp;quot;,
  &amp;quot;ranger&amp;quot;,
  &amp;quot;palmerpenguins&amp;quot;,
  &amp;quot;tidyverse&amp;quot;,
  &amp;quot;kableExtra&amp;quot;
  )

new.packages &amp;lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&amp;quot;Package&amp;quot;])]

if(length(new.packages) &amp;gt; 0){
  install.packages(new.packages, dep=TRUE)
}

#loading packages
for(package.i in list.of.packages){
  suppressPackageStartupMessages(
    library(
      package.i, 
      character.only = TRUE
      )
    )
}

#loading example data
data(&amp;quot;penguins&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;beyond-for-building-loops-with-foreach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beyond &lt;code&gt;for&lt;/code&gt;: building loops with &lt;code&gt;foreach&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;foreach&lt;/code&gt; package (the vignette is &lt;a href=&#34;https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html&#34;&gt;here&lt;/a&gt;) provides a way to build loops that support parallel execution, and easily gather the results provided by each iteration in the loop.&lt;/p&gt;
&lt;p&gt;For example, this classic &lt;code&gt;for&lt;/code&gt; loop computes the square root of the numbers 1 to 5 with &lt;code&gt;sqrt()&lt;/code&gt; (the function is vectorized, but let’s conveniently forget that for a moment). Notice that I have to create a vector &lt;code&gt;x&lt;/code&gt; to gather the results before executing the loop.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- vector()
for(i in 1:10){
  x[i] &amp;lt;- sqrt(i)
  }
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;foreach&lt;/code&gt; version returns a list with the results automatically. Notice that &lt;code&gt;%do%&lt;/code&gt; operator after the loop definition, I’ll talk more about it later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- foreach(i = 1:10) %do% {
  sqrt(i)
  }
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1.414214
## 
## [[3]]
## [1] 1.732051
## 
## [[4]]
## [1] 2
## 
## [[5]]
## [1] 2.236068
## 
## [[6]]
## [1] 2.44949
## 
## [[7]]
## [1] 2.645751
## 
## [[8]]
## [1] 2.828427
## 
## [[9]]
## [1] 3
## 
## [[10]]
## [1] 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the &lt;code&gt;.combine&lt;/code&gt; argument of &lt;code&gt;foreach&lt;/code&gt; to arrange the list as a vector. Other options such as &lt;code&gt;cbind&lt;/code&gt;, &lt;code&gt;rbind&lt;/code&gt;, or even custom functions can be used as well, only depending on the structure of the output of each iteration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- foreach(
  i = 1:10, 
  .combine = &amp;#39;c&amp;#39;
) %do% {
    sqrt(i)
  }
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another interesting capability of &lt;code&gt;foreach&lt;/code&gt; is that it supports several iterators of the same length at once. Notice that the values of the iterators are not combined. When the first value of one iterator is being used, the first value of the other iterators will be used as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- foreach(
  i = 1:3, 
  j = 1:3, 
  k = 1:3, 
  .combine = &amp;#39;c&amp;#39;
  ) %do% {
  i + j + k
  }
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 6 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-foreach-loops-in-parallel&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running &lt;code&gt;foreach&lt;/code&gt; loops in parallel&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;foreach&lt;/code&gt; loops shown above use the operator &lt;code&gt;%do%&lt;/code&gt;, that processes the tasks sequentially. To run tasks in parallel, &lt;code&gt;foreach&lt;/code&gt; uses the operator &lt;code&gt;%dopar%&lt;/code&gt;, that has to be supported by a parallel &lt;em&gt;backend&lt;/em&gt;. If there is no parallel backend, &lt;code&gt;%dopar%&lt;/code&gt; warns the user that it is being run sequentially, as shown below. But what the heck is a parallel backend?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- foreach(
  i = 1:10, 
  .combine = &amp;#39;c&amp;#39;
) %dopar% {
    sqrt(i)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: executing %dopar% sequentially: no parallel backend registered&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div id=&#34;what-is-a-parallel-backend&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is a parallel backend?&lt;/h3&gt;
&lt;p&gt;When running tasks in parallel, there should be a &lt;em&gt;director&lt;/em&gt; node that tells a group of &lt;em&gt;workers&lt;/em&gt; what to do with a given set of data and functions. The &lt;em&gt;workers&lt;/em&gt; execute the iterations, and the &lt;em&gt;director&lt;/em&gt; manages execution and gathers the results provided by the &lt;em&gt;workers&lt;/em&gt;. A parallel backend provides the means for the director and workers to communicate, while allocating and managing the required computing resources (processors, RAM memory, and network bandwidth among others).&lt;/p&gt;
&lt;p&gt;There are two types of parallel backends that can be used with &lt;code&gt;foreach&lt;/code&gt;, &lt;strong&gt;FORK&lt;/strong&gt; and &lt;strong&gt;PSOCK&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div id=&#34;fork&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;FORK&lt;/h4&gt;
&lt;p&gt;FORK backends are only available on UNIX machines (Linux, Mac, and the likes), and do not work in clusters [sad face], so only single-machine environments are appropriate for this backend. In a FORK backend, the workers share the same environment (data, loaded packages, and functions) as the director. This setup is highly efficient because the main environment doesn’t have to be copied, and only worker outputs need to be sent back to the director.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;FORK.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;psock&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;PSOCK&lt;/h4&gt;
&lt;p&gt;PSOCK backends (Parallel Socket Cluster) are available for both UNIX and WINDOWS systems, and are the default option provided with &lt;code&gt;foreach&lt;/code&gt;. As their main disadvantage, the environment of the director needs to be copied to the environment of each worker, which increases network overhead while decreasing the overall efficiency of the cluster. By default, all the functions available in base R are copied to each worker, and if a particular set of R packages are needed in the workers, they need to be copied to the respective environments of the workers as well.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.r-bloggers.com/2019/06/parallel-r-socket-or-fork/&#34;&gt;This post&lt;/a&gt; compares both backends and concludes that FORK is about a 40% faster than PSOCK.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;PSOCK.png&#34; /&gt;
 &lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;setup-of-a-parallel-backend&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup of a parallel backend&lt;/h2&gt;
&lt;p&gt;Here I explain how to setup the parallel backend for a simple computer and for a Beowulf cluster as &lt;a href=&#34;(https://www.blasbenito.com/post/01_home_cluster/)&#34;&gt;the one I described in a previous post&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;setup-for-a-single-computer&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setup for a single computer&lt;/h3&gt;
&lt;p&gt;Setting up a cluster in a single computer requires first to find out how many cores we want to use from the ones we have available. It is recommended to leave one free core for other tasks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::detectCores()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n.cores &amp;lt;- parallel::detectCores() - 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to define the cluster with &lt;code&gt;parallel::makeCluster()&lt;/code&gt; and register it so it can be used by &lt;code&gt;%dopar%&lt;/code&gt; with &lt;code&gt;doParallel::registerDoParallel(my.cluster)&lt;/code&gt;. The &lt;code&gt;type&lt;/code&gt; argument of &lt;code&gt;parallel::makeCluster()&lt;/code&gt; accepts the strings “PSOCK” and “FORK” to define the type of parallel backend to be used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create the cluster
my.cluster &amp;lt;- parallel::makeCluster(
  n.cores, 
  type = &amp;quot;PSOCK&amp;quot;
  )

#check cluster definition (optional)
print(my.cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## socket cluster with 7 nodes on host &amp;#39;localhost&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#register it to be used by %dopar%
doParallel::registerDoParallel(cl = my.cluster)

#check if it is registered (optional)
foreach::getDoParRegistered()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#how many workers are available? (optional)
foreach::getDoParWorkers()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can run a set of tasks in parallel!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- foreach(
  i = 1:10, 
  .combine = &amp;#39;c&amp;#39;
) %dopar% {
    sqrt(i)
  }
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If everything went well, now &lt;code&gt;%dopar%&lt;/code&gt; should not be throwing the warning &lt;code&gt;executing %dopar% sequentially: no parallel backend registered&lt;/code&gt;, meaning that the parallel execution is working as it should. In this little example there is no gain in execution speed, because the operation being executed is extremely fast, but this will change when the operations running inside of the loop take longer times to run.&lt;/p&gt;
&lt;p&gt;Finally, it is always recommendable to stop the cluster when we are done working with it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::stopCluster(cl = my.cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;setup-for-a-beowulf-cluster&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setup for a Beowulf cluster&lt;/h3&gt;
&lt;p&gt;This setup is a bit more complex, because it requires to open a &lt;em&gt;port&lt;/em&gt; in every computer of the cluster. Ports are virtual communication channels, and are identified by a number.&lt;/p&gt;
&lt;p&gt;First, lets tell R what port we want to use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#define port
Sys.setenv(R_PARALLEL_PORT = 11000)

#check that it
Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we need to open the selected port in every computer of the network. In Linux we need to setup the firewall to allow connections from the network &lt;code&gt;10.42.1.0/24&lt;/code&gt; (replace this with your network range if different!) to the port &lt;code&gt;11000&lt;/code&gt; by splitting the window of the &lt;a href=&#34;https://gnometerminator.blogspot.com/p/introduction.html&#34;&gt;Terminator console&lt;/a&gt; in as many computers available in your network (the figure below shows three, one for my PC and two for my Intel NUCs), opening an ssh session on each remote machine, and setting Terminator with &lt;em&gt;Grouping&lt;/em&gt; equal to &lt;em&gt;Broadcast all&lt;/em&gt; so we only need to type the commands once.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;terminator.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Opening port 11000 in three computers at once with Terminator&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now we have to create an object defining the IPs of the computers in the network, the number of cores to use from each computer, the user name, and the identity of the &lt;em&gt;director&lt;/em&gt;. This will be the &lt;code&gt;spec&lt;/code&gt; argument required by &lt;code&gt;parallel::makeCluster()&lt;/code&gt; to create the cluster throughtout the machines in the network. It is a list of lists, with as many lists as nodes are defined. Each &lt;em&gt;sub-list&lt;/em&gt; has a slot named &lt;em&gt;host&lt;/em&gt; with the IP of the computer where the given node is, and &lt;em&gt;user&lt;/em&gt;, with the name of the user in each computer.&lt;/p&gt;
&lt;p&gt;The code below shows how this would be done, step by step. Yes, this is CUMBERSOME.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#main parameters
director &amp;lt;- &amp;#39;10.42.0.1&amp;#39;
nuc2 &amp;lt;- &amp;#39;10.42.0.34&amp;#39;
nuc1 &amp;lt;- &amp;#39;10.42.0.104&amp;#39;
user &amp;lt;- &amp;quot;blas&amp;quot;

#list of machines, user names, and cores
spec &amp;lt;- list(
  list(
    host = director, 
    user = user,
    ncore = 7
  ), 
  list(
    host = nuc1, 
    user = user,
    ncore = 4
  ),
  list(
    host = nuc2, 
    user = user,
    ncore = 4
  )
)

#generating nodes from the list of machines
spec &amp;lt;- lapply(
  spec, 
  function(spec.i) rep(
    list(
      list(
        host = spec.i$host, 
        user = spec.i$user)
      ), 
    spec.i$ncore
    )
)

#formating into a list of lists
spec &amp;lt;- unlist(
  spec, 
  recursive = FALSE
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Generating the &lt;code&gt;spec&lt;/code&gt; definition is a bit easier with the function below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#function to generate cluster specifications from a vector of IPs, a vector with the number of cores to use on each IP, and a user name
cluster_spec &amp;lt;- function(
  ips,
  cores,
  user
){
  
  #creating initial list
  spec &amp;lt;- list()
  
  for(i in 1:length(ips)){
    spec[[i]] &amp;lt;- list()
    spec[[i]]$host &amp;lt;- ips[i]
    spec[[i]]$user &amp;lt;- user
    spec[[i]]$ncore &amp;lt;- cores[i]
  }

  #generating nodes from the list of machines
  spec &amp;lt;- lapply(
    spec, 
    function(spec.i) rep(
      list(
        list(
          host = spec.i$host, 
          user = spec.i$user)
        ), 
      spec.i$ncore
      )
  )

  #formating into a list of lists
  spec &amp;lt;- unlist(
    spec, 
    recursive = FALSE
  )
  
  return(spec)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function is also available in &lt;a href=&#34;https://gist.github.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca&#34;&gt;this GitHub Gist&lt;/a&gt;, so you can load it into your R environment by executing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below I use it to generate the input to the &lt;code&gt;spec&lt;/code&gt; argument to start the cluster with &lt;code&gt;parallel::makeCluster()&lt;/code&gt;. Notice that I have added several arguments.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The argument &lt;code&gt;outfile&lt;/code&gt; determines where the workers write a log. In this case it is set to &lt;em&gt;nowhere&lt;/em&gt; with the double quotes, but the path to a text file in the director could be provided here.&lt;/li&gt;
&lt;li&gt;The argument &lt;code&gt;homogeneous = TRUE&lt;/code&gt; indicates that all machines have the &lt;code&gt;Rscript&lt;/code&gt; in the same location. In this case all three machines have it at “/usr/lib/R/bin/Rscript”. Otherwise, set it up to &lt;code&gt;FALSE&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#generate cluster specification
spec &amp;lt;- cluster_spec(
  ips = c(&amp;#39;10.42.0.1&amp;#39;, &amp;#39;10.42.0.34&amp;#39;, &amp;#39;10.42.0.104&amp;#39;),
  cores = c(7, 4, 4),
  user = &amp;quot;blas&amp;quot;
)

#setting up cluster
my.cluster &amp;lt;- parallel::makeCluster(
  master = &amp;#39;10.42.0.1&amp;#39;, 
  spec = spec,
  port = Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;),
  outfile = &amp;quot;&amp;quot;,
  homogeneous = TRUE
)

#check cluster definition (optional)
print(my.cluster)

#register cluster
doParallel::registerDoParallel(cl = my.cluster)

#how many workers are available? (optional)
foreach::getDoParWorkers()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can use the cluster to execute a dummy operation in parallel using all machines in the network.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- foreach(
  i = 1:20, 
  .combine = &amp;#39;c&amp;#39;
) %dopar% {
    sqrt(i)
  }
x&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once everything is done, remember to close the cluster.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::stopCluster(cl = my.cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;practical-examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Practical examples&lt;/h2&gt;
&lt;p&gt;In this section I cover two examples on how to use parallelized loops to explore model outputs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tuning random forest &lt;a href=&#34;https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)&#34;&gt;&lt;em&gt;hyperparameters&lt;/em&gt;&lt;/a&gt; to maximize classification accuracy.&lt;/li&gt;
&lt;li&gt;Obtain a confidence interval for the importance score of each predictor from a set random forest models fitted with &lt;a href=&#34;https://github.com/imbs-hl/ranger&#34;&gt;&lt;code&gt;ranger()&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the examples I use the &lt;code&gt;penguins&lt;/code&gt; data from the &lt;a href=&#34;https://github.com/allisonhorst/palmerpenguins&#34;&gt;&lt;code&gt;palmerpenguins&lt;/code&gt;&lt;/a&gt; package to fit classification models with random forest using &lt;em&gt;species&lt;/em&gt; as a response, and &lt;em&gt;bill_length_mm&lt;/em&gt;, &lt;em&gt;bill_depth_mm&lt;/em&gt;, &lt;em&gt;flipper_length_mm&lt;/em&gt;, and &lt;em&gt;body_mass_g&lt;/em&gt; as predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#removing NA and subsetting columns
penguins &amp;lt;- as.data.frame(
  na.omit(
    penguins[, c(
      &amp;quot;species&amp;quot;,
      &amp;quot;bill_length_mm&amp;quot;,
      &amp;quot;bill_depth_mm&amp;quot;,
      &amp;quot;flipper_length_mm&amp;quot;,
      &amp;quot;body_mass_g&amp;quot;
    )]
    )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
species
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
bill_length_mm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
bill_depth_mm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
flipper_length_mm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
body_mass_g
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
181
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3750
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
186
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3800
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
195
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3250
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
36.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
193
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3450
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
190
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3650
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
181
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3625
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
195
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4675
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
193
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3475
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
190
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4250
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
186
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3300
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
180
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3700
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
41.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
182
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3200
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
38.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
191
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3800
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
198
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4400
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
36.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
185
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3700
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
38.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
195
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3450
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
197
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4500
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
184
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3325
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
194
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4200
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
174
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3400
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We’ll fit random forest models with the &lt;a href=&#34;https://cran.r-project.org/package=ranger&#34;&gt;&lt;code&gt;ranger&lt;/code&gt;&lt;/a&gt; package, which works as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#fitting classification model
m &amp;lt;- ranger::ranger(
  data = penguins,
  dependent.variable.name = &amp;quot;species&amp;quot;,
  importance = &amp;quot;permutation&amp;quot;
)

#summary
m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Ranger result
## 
## Call:
##  ranger::ranger(data = penguins, dependent.variable.name = &amp;quot;species&amp;quot;,      importance = &amp;quot;permutation&amp;quot;) 
## 
## Type:                             Classification 
## Number of trees:                  500 
## Sample size:                      342 
## Number of independent variables:  4 
## Mtry:                             2 
## Target node size:                 1 
## Variable importance mode:         permutation 
## Splitrule:                        gini 
## OOB prediction error:             2.34 %&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#variable importance
m$variable.importance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    bill_length_mm     bill_depth_mm flipper_length_mm       body_mass_g 
##        0.31452816        0.16948700        0.21670253        0.07988962&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output shows that the percentage of misclassified cases is 2.34, and that &lt;em&gt;bill_length_mm&lt;/em&gt; is the variable that contributes the most to the accuracy of the classification.&lt;/p&gt;
&lt;p&gt;If you are not familiar with random forest, &lt;a href=&#34;https://victorzhou.com/blog/intro-to-random-forests/&#34;&gt;this post&lt;/a&gt; and the video below do a pretty good job in explaining the basics:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/D_2LkhMJcfY&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;div id=&#34;tuning-random-forest-hyperparameters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tuning random forest hyperparameters&lt;/h3&gt;
&lt;p&gt;Random forest has several hyperparameters that influence model fit:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;num.trees&lt;/code&gt; is the total number of trees to fit. The default value is 500.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mtry&lt;/code&gt; is the number of variables selected by chance (from the total pool of variables) as candidates for a tree split. The minimum is 2, and the maximum is the total number of predictors.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min.node.size&lt;/code&gt; is the minimum number of cases that shall go together in the terminal nodes of each tree. For classification models as the ones we are going to fit, 1 is the minimum.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we are going to explore how combinations of these values increase or decrease the prediction error of the model (percentage of misclassified cases) on the out-of-bag data (not used to train each decision tree). This operation is usually named &lt;strong&gt;grid search for hyperparameter optimization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To create these combinations of hyperparameters we use &lt;code&gt;expand.grid()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sensitivity.df &amp;lt;- expand.grid(
  num.trees = c(500, 1000, 1500),
  mtry = 2:4,
  min.node.size = c(1, 10, 20)
)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
num.trees
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
mtry
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
min.node.size
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Each row in &lt;code&gt;sensitivity.df&lt;/code&gt; corresponds to a combination of parameters to test, so there are 27 models to fit. The code below prepares the cluster, and uses the ability of &lt;code&gt;foreach&lt;/code&gt; to work with several iterators at once to easily introduce the right set of hyperparameters to each fitted model.&lt;/p&gt;
&lt;p&gt;Notice how in the &lt;code&gt;foreach&lt;/code&gt; definition I use the &lt;code&gt;.packages&lt;/code&gt; argument to export the &lt;code&gt;ranger&lt;/code&gt; package to the environments of the workers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create and register cluster
my.cluster &amp;lt;- parallel::makeCluster(n.cores)
doParallel::registerDoParallel(cl = my.cluster)
  
#fitting each rf model with different hyperparameters
prediction.error &amp;lt;- foreach(
  num.trees = sensitivity.df$num.trees,
  mtry = sensitivity.df$mtry,
  min.node.size = sensitivity.df$min.node.size,
  .combine = &amp;#39;c&amp;#39;, 
  .packages = &amp;quot;ranger&amp;quot;
) %dopar% {
  
  #fit model
  m.i &amp;lt;- ranger::ranger(
    data = penguins,
    dependent.variable.name = &amp;quot;species&amp;quot;,
    num.trees = num.trees,
    mtry = mtry,
    min.node.size = min.node.size
  )
  
  #returning prediction error as percentage
  return(m.i$prediction.error * 100)
  
}

#adding the prediction error column
sensitivity.df$prediction.error &amp;lt;- prediction.error&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To plot the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot2::ggplot(data = sensitivity.df) + 
  ggplot2::aes(
    x = mtry,
    y = as.factor(min.node.size),
    fill = prediction.error
  ) + 
  ggplot2::facet_wrap(as.factor(num.trees)) +
  ggplot2::geom_tile() + 
  ggplot2::scale_y_discrete(breaks = c(1, 10, 20)) +
  ggplot2::scale_fill_viridis_c() + 
  ggplot2::ylab(&amp;quot;min.node.size&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;sensitivity.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The figure shows that combinations of lower values of &lt;code&gt;min.node.size&lt;/code&gt; and &lt;code&gt;mtry&lt;/code&gt; generally lead to models with a lower prediction error across different numbers of trees. Retrieving the first line of &lt;code&gt;sensitivity.df&lt;/code&gt; ordered by ascending &lt;code&gt;prediction.error&lt;/code&gt; will give us the values of the hyperparameters we need to use to reduce the prediction error as much as possible.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best.hyperparameters &amp;lt;- sensitivity.df %&amp;gt;% 
  dplyr::arrange(prediction.error) %&amp;gt;% 
  dplyr::slice(1)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
num.trees
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
mtry
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
min.node.size
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
prediction.error
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.339181
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-intervals-of-variable-importance-scores&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confidence intervals of variable importance scores&lt;/h3&gt;
&lt;p&gt;Random forest has an important stochastic component during model fitting, and as consequence, the same model will return slightly different results in different runs (unless &lt;code&gt;set.seed()&lt;/code&gt; or the &lt;code&gt;seed&lt;/code&gt; argument of &lt;code&gt;ranger&lt;/code&gt; are used). This variability also affects the importance scores of the predictors, and can be use to our advantage to assess whether the importance scores of different variables do really overlap or not.&lt;/p&gt;
&lt;p&gt;I have written a little function to transform the vector of importance scores returned by &lt;code&gt;ranger&lt;/code&gt; into a data frame (of one row). It helps arranging the importance scores of different runs into a long format, which helps a lot to plot a boxplot with &lt;code&gt;ggplot2&lt;/code&gt; right away. This function could have been just some code thrown inside the &lt;code&gt;foreach&lt;/code&gt; loop, but I want to illustrate how &lt;code&gt;foreach&lt;/code&gt; automatically transfers functions available in the R environment into the environments of the workers when required, without the intervention of the user. The same will happen with the &lt;code&gt;best.hyperparameters&lt;/code&gt; tiny data frame we created in the previous section.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;importance_to_df &amp;lt;- function(model){
  x &amp;lt;- as.data.frame(model$variable.importance)
  x$variable &amp;lt;- rownames(x)
  colnames(x)[1] &amp;lt;- &amp;quot;importance&amp;quot;
  rownames(x) &amp;lt;- NULL
  return(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code chunk below setups the cluster and runs 1000 random forest models in parallel (using the best hyperparameters computed in the previous section) while using &lt;code&gt;system.time()&lt;/code&gt; to assess running time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#we don&amp;#39;t need to create the cluster, it is still up
print(my.cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## socket cluster with 7 nodes on host &amp;#39;localhost&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#assessing execution time
system.time(
  
  #performing 1000 iterations in parallel
  importance.scores &amp;lt;- foreach(
    i = 1:1000, 
    .combine = &amp;#39;rbind&amp;#39;, 
    .packages = &amp;quot;ranger&amp;quot;
  ) %dopar% {
    
    #fit model
    m.i &amp;lt;- ranger::ranger(
      data = penguins,
      dependent.variable.name = &amp;quot;species&amp;quot;,
      importance = &amp;quot;permutation&amp;quot;,
      mtry = best.hyperparameters$mtry,
      num.trees = best.hyperparameters$num.trees,
      min.node.size = best.hyperparameters$min.node.size
    )
    
    #format importance
    m.importance.i &amp;lt;- importance_to_df(model = m.i)
    
    #returning output
    return(m.importance.i)
    
  }
  
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.308   0.028   6.382&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output of &lt;code&gt;system.time()&lt;/code&gt; goes as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;user&lt;/em&gt;: seconds the R session has been using the CPU.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;system&lt;/em&gt;: seconds the operating system has been using the CPU.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;elapsed&lt;/em&gt;: the total execution time experienced by the user.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This will make sense in a minute. In the meantime, let’s plot our results!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot2::ggplot(data = importance.scores) + 
  ggplot2::aes(
    y = reorder(variable, importance), 
    x = importance
  ) +
  ggplot2::geom_boxplot() + 
  ggplot2::ylab(&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;boxplot.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The figure shows that the variable &lt;em&gt;bill_length_mm&lt;/em&gt; is the most important in helping the model classifying penguin species, with no overlap with any other variable. In this particular case, since the distributions of the importance scores do not overlap, this analysis isn’t truly helpful, but now you know how to do it!&lt;/p&gt;
&lt;p&gt;I assessed the running time with &lt;code&gt;system.time()&lt;/code&gt; because &lt;code&gt;ranger()&lt;/code&gt; can run in parallel by itself just by setting the &lt;code&gt;num.threads&lt;/code&gt; argument to the number of cores available in the machine. This capability cannot be used when executing &lt;code&gt;ranger()&lt;/code&gt; inside a parallelized &lt;code&gt;foreach&lt;/code&gt; loop though, and it is only useful inside classic &lt;code&gt;for&lt;/code&gt; loops.&lt;/p&gt;
&lt;p&gt;What option is more efficient then? The code below executes a regular &lt;code&gt;for&lt;/code&gt; loop running the function sequentially to evaluate whether it is more efficient to run &lt;code&gt;ranger()&lt;/code&gt; in parallel using one core per model, as we did above, or sequentially while using several cores per model on each iteration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#list to save results
importance.scores.list &amp;lt;- list()

#performing 1000 iterations sequentially
system.time(
  
  for(i in 1:1000){
    
    #fit model
    m.i &amp;lt;- ranger::ranger(
      data = penguins,
      dependent.variable.name = &amp;quot;species&amp;quot;,
      importance = &amp;quot;permutation&amp;quot;,
      seed = i,
      num.threads = parallel::detectCores() - 1
    )
    
    #format importance
    importance.scores.list[[i]] &amp;lt;- importance_to_df(model = m.i)
    
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##  42.878   2.914  13.336&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, &lt;code&gt;ranger()&lt;/code&gt; takes longer to execute in a regular &lt;code&gt;for&lt;/code&gt; loop using several cores at once than in a parallel &lt;code&gt;foreach&lt;/code&gt; loop using one core at once. That’s a win for the parallelized loop!&lt;/p&gt;
&lt;p&gt;We can stop our cluster now, we are done with it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::stopCluster(cl = my.cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-few-things-to-take-in-mind&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A few things to take in mind&lt;/h2&gt;
&lt;p&gt;As I have shown in this post, using parallelized &lt;code&gt;foreach&lt;/code&gt; loops can accelerate long computing processes, even when some functions have the ability to run in parallel on their own. However, there are things to take in mind, that might vary depending on whether we are executing the parallelized task on a single computer or on a small cluster.&lt;/p&gt;
&lt;p&gt;In a single computer, the communication between workers and the director is usually pretty fast, so there are no obvious bottlenecks to take into account here. The only limitation that might arise comes from the availability of RAM memory. For example, if a computer has 8 cores and 8GB of RAM, less than 1GB of RAM will be available for each worker. So, if you need to repeat a process that consumes a significant amount of RAM, the ideal number of cores running in parallel might be lower than the total number of cores available in your system. Don’t be greedy, and try to understand the capabilities of your machine while designing a parallelized task.&lt;/p&gt;
&lt;p&gt;When running &lt;code&gt;foreach&lt;/code&gt; loops as in &lt;code&gt;x &amp;lt;- foreach(...){...}&lt;/code&gt;, the variable &lt;code&gt;x&lt;/code&gt; is receiving whatever results the workers are producing. For example, if you are only returning the prediction error of a model, or its importance scores, &lt;code&gt;x&lt;/code&gt; will have a very manageable size. But if you are returning heavy objects such as complete random forest models, the size of &lt;code&gt;x&lt;/code&gt; is going to grow VERY FAST, and at the end it will be competing for RAM resources with the workers, which might even crash your R session. Again, don’t be greedy, and size your outputs carefully.&lt;/p&gt;
&lt;p&gt;Clusters spanning several computers are a different beast, since the workers and the director communicate through a switch and network wires and interfaces. If the amount of data going to and coming from the workers is large, the network can get clogged easily, reducing the cluster’s efficiency drastically. In general, if the amount of data produced by a worker on each iteration takes longer to arrive to the director than the time it takes the worker to produce it, then a cluster is not going to be more efficient than a single machine. But this is not important if you don’t care about efficiency.&lt;/p&gt;
&lt;p&gt;Other issues you might come across while parallelizing tasks in R are thoroughly commented in &lt;a href=&#34;https://towardsdatascience.com/parallelization-caveats-in-r-1-the-basics-multiprocessing-and-multithreading-performance-eb584b7e850e&#34;&gt;this post&lt;/a&gt;, by Imre Gera.&lt;/p&gt;
&lt;p&gt;That’s all for now folks, happy parallelization!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Setting up a home cluster</title>
      <link>/post/01_home_cluster/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/01_home_cluster/</guid>
      <description>&lt;p&gt;In this post I explain how to setup a small 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Beowulf_cluster&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beowulf cluster&lt;/a&gt; with a personal PC running Ubuntu 20.04 and a couple of 
&lt;a href=&#34;https://www.intel.com/content/www/us/en/products/boards-kits/nuc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intel NUCs&lt;/a&gt; running Ubuntu Server 20.04, with the end-goal of parallelizing R tasks.&lt;/p&gt;
&lt;p&gt;The topics I cover here are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Required material&lt;/li&gt;
&lt;li&gt;Network setting&lt;/li&gt;
&lt;li&gt;Installing the secure shell protocol&lt;/li&gt;
&lt;li&gt;Installing Ubuntu server in the NUCs&lt;/li&gt;
&lt;li&gt;Installing R in the NUCs&lt;/li&gt;
&lt;li&gt;Managing the cluster&amp;rsquo;s network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;preamble&#34;&gt;Preamble&lt;/h2&gt;
&lt;p&gt;I have a little but nice HP ENVY model &lt;em&gt;TE01-0008ns&lt;/em&gt; with 32 GB RAM, 8 CPUs, and 3TB of hard disk running Ubuntu 20.04 that I use to do all my computational work (and most of my tweeting). A few months ago I connected it with my two laptops (one of them deceased now, RIP my dear &lt;em&gt;skynet&lt;/em&gt;) to create a little cluster to run parallel tasks in R.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Today I made a home cluster to spread parallel R tasks across all my computers. That was fun! &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://t.co/ic4plvO3Y6&#34;&gt;pic.twitter.com/ic4plvO3Y6&lt;/a&gt;&lt;/p&gt;&amp;mdash; Blas M. Benito (@BlasBenito) &lt;a href=&#34;https://twitter.com/BlasBenito/status/1254754362389417987?ref_src=twsrc%5Etfw&#34;&gt;April 27, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;It was just a draft cluster running on a wireless network, but it served me to think about getting a more permanent solution not requiring two additional laptops in my desk.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s were the nice INTEL NUCs (from 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Next_Unit_of_Computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Next Unit of Computing&lt;/em&gt;&lt;/a&gt;) come into play. NUCs are full-fledged computers fitted in small boxes usually sold without RAM memory sticks and no hard disk (hence the term &lt;em&gt;barebone&lt;/em&gt;). Since they have a low energy consumption footprint, I thought these would be ideal units for my soon-to-be home cluster.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;p&gt;I gifted myself with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 
&lt;a href=&#34;https://ark.intel.com/content/www/us/en/ark/products/95062/intel-nuc-kit-nuc6cayh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intel Barebone BOXNUC6CAYH&lt;/a&gt;, each with 4 cores, and a maximum RAM memory of 32GB (you might read they only accept 8GB, but that&amp;rsquo;s not the case anymore). Notice that these NUCs aren&amp;rsquo;t state-of-the-art now, they were released by the end of 2016.&lt;/li&gt;
&lt;li&gt;2 Hard disks SSD 2.5&amp;rdquo; 
&lt;a href=&#34;https://shop.westerndigital.com/es-es/products/internal-drives/wd-blue-sata-ssd#WDS250G2B0A&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Western Digital WDS250G2B0A WD Blue&lt;/a&gt; (250GB)&lt;/li&gt;
&lt;li&gt;4 Crucial CT102464BF186D DDR3 SODIMM (204 pins) RAM sticks with 8GB each.&lt;/li&gt;
&lt;li&gt;1 ethernet switch Netgear GS308-300PES with 8 ports.&lt;/li&gt;
&lt;li&gt;3 ethernet wires NanoCable 10.20.0400-BL of 
&lt;a href=&#34;https://www.electronics-notes.com/articles/connectivity/ethernet-ieee-802-3/how-to-buy-best-ethernet-cables-cat-5-6-7.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cat 6&lt;/a&gt; quality.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The whole set came to cost around 530€, but please notice that I had a clear goal in mind: &amp;ldquo;duplicating&amp;rdquo; my computing power with the minimum number of NUCs, while preserving a share of 4GB of RAM memory per CPU throughout the cluster (based on the features of my desk computer). A more basic setting with more modest NUCs and smaller RAM would cost half of that.&lt;/p&gt;
&lt;p&gt;This instructive video by 
&lt;a href=&#34;https://www.youtube.com/channel/UCYa3XeSHenvosy5wMRpeIww&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Harry&lt;/a&gt; shows how to install the SSD and the RAM sticks in an Intel NUC. It really takes 5 minutes tops, one only has to be a bit careful with the RAM sticks, the pins need to go all the way in into their slots before securing the sticks in place.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/6hzj7DogqXU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;network-settings&#34;&gt;Network settings&lt;/h2&gt;
&lt;p&gt;Before starting to install an operating system in the NUCS, the network setup goes as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;My desktop PC is connected to a router via WIFI and dynamic IP (DHCP).&lt;/li&gt;
&lt;li&gt;The PC and each NUC are connected to the switch with cat6 ethernet wires.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;network.png&#34; alt=&#34;Network diagram&#34;&gt;&lt;/p&gt;
&lt;p&gt;To share my PC&amp;rsquo;s WIFI connection with the NUCs I have to prepare a new &lt;em&gt;connection profile&lt;/em&gt; with the command line tool of Ubuntu&amp;rsquo;s 
&lt;a href=&#34;https://en.wikipedia.org/wiki/NetworkManager&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;NetworkManager&lt;/code&gt;&lt;/a&gt;, named &lt;code&gt;nmcli&lt;/code&gt;, as follows.&lt;/p&gt;
&lt;p&gt;First, I need to find the name of my ethernet interface by checking the status of my network devices with the command line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nmcli device status
DEVICE  TYPE      STATE        CONNECTION  
wlp3s0  wifi      connected    my_wifi 
enp2s0  ethernet  unavailable  --          
lo      loopback  unmanaged    --      
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There I can see that my ethernet interface is named &lt;code&gt;enp2s0&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Second, I have to configure the shared connection.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nmcli connection add type ethernet ifname enp2s0 ipv4.method shared con-name cluster
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Were &lt;code&gt;ifname enp2s0&lt;/code&gt; is the name of the interface I want to use for the new connection, &lt;code&gt;ipv4.method shared&lt;/code&gt; is the type of connection, and &lt;code&gt;con-name cluster&lt;/code&gt; is the name I want the connection to have. This operation adds firewall rules to manage traffic within the &lt;code&gt;cluster&lt;/code&gt; network, starts a DHCP server in the computer that serves IPs to the NUCS, and a DNS server that allows the NUCs to translate internet addresses.&lt;/p&gt;
&lt;p&gt;After turning on the switch, I can check the connection status again with&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nmcli device status
DEVICE  TYPE      STATE      CONNECTION  
enp2s0  ethernet  connected  cluster     
wlp3s0  wifi      connected  my_wifi 
lo      loopback  unmanaged  --    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When checking the IP of the device with &lt;code&gt;bash ifconfig&lt;/code&gt; it should yield &lt;code&gt;10.42.0.1&lt;/code&gt;. Any other computer in the &lt;code&gt;cluster&lt;/code&gt; network will have a dynamic IP in the range &lt;code&gt;10.42.0.1/24&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Further details about how to set a shared connection with &lt;code&gt;NetworkManager&lt;/code&gt; can be found in 
&lt;a href=&#34;https://fedoramagazine.org/internet-connection-sharing-networkmanager/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this nice post by Beniamino Galvani&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;ssh-setup&#34;&gt;SSH setup&lt;/h2&gt;
&lt;p&gt;My PC, as the director of the cluster, needs an &lt;code&gt;SSH client&lt;/code&gt; running, while the NUCs need an &lt;code&gt;SSH server&lt;/code&gt;. 
&lt;a href=&#34;https://www.ionos.com/digitalguide/server/tools/ssh-secure-shell/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;SSH&lt;/code&gt; (&lt;strong&gt;S&lt;/strong&gt;ecure &lt;strong&gt;Sh&lt;/strong&gt;ell)&lt;/a&gt; is a remote authentication protocol that allows secure connections to remote servers that I will be using all the time to manage the cluster. To install, run, and check its status I just have to run these lines in the console:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install ssh 
sudo systemctl enable --now ssh
sudo systemctl status ssh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, a secure certificate of the identity of a given computer, named &lt;code&gt;ssh-key&lt;/code&gt;, that grants access to remote ssh servers and services needs to be generated.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh-keygen &amp;quot;label&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, substitute &amp;ldquo;label&amp;rdquo; by the name of the computer to be used as cluster&amp;rsquo;s &amp;ldquo;director&amp;rdquo;. The system will ask for a file name and a 
&lt;a href=&#34;https://www.ssh.com/ssh/passphrase&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;passphrase&lt;/a&gt; that will be used to encrypt the ssh-key.&lt;/p&gt;
&lt;p&gt;The ssh-key needs to be added to the 
&lt;a href=&#34;https://www.ssh.com/ssh/agent&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ssh-agent&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh-add ~/.ssh/id_rsa
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To copy the ssh-key to my GitHub account, I have to copy the contents of the file &lt;code&gt;~/.ssh/id_rsa.pub&lt;/code&gt; (can be done just opening it with &lt;code&gt;gedit ~/.ssh/id_rsa.pub&lt;/code&gt; + &lt;code&gt;Ctrl + a&lt;/code&gt; + &lt;code&gt;Ctrl + c&lt;/code&gt;), and paste it on &lt;code&gt;GitHub account &amp;gt; Settings &amp;gt;  SSH and GPG keys &amp;gt; New SSH Key&lt;/code&gt; (green button in the upper right part of the window).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you don&amp;rsquo;t use GitHub, you&amp;rsquo;ll need to copy your ssh-key to the NUCs once they are up and running with &lt;code&gt;ssh-copy-id -i ~/.ssh/id_rsa.pub user_name@nuc_IP&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;installing-and-preparing-ubuntu-server-in-each-nuc&#34;&gt;Installing and preparing ubuntu server in each NUC&lt;/h2&gt;
&lt;p&gt;The NUCs don&amp;rsquo;t need to waste resources in a user graphical interface I won&amp;rsquo;t be using whatsoever. Since they will work in a 
&lt;a href=&#34;https://www.howtogeek.com/660841/what-is-a-headless-server/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;headless&lt;/em&gt; configuration&lt;/a&gt; once the cluster is ready, a Linux distro without graphical user interface such as Ubuntu server is the way to go.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;installing-ubuntu-server&#34;&gt;Installing Ubuntu server&lt;/h3&gt;
&lt;p&gt;First it is important to connect a display, a keyboard, and a mouse to the NUC in preparation, and turn it on while pushing F2 to start the visual BIOS. These BIOS parameters need to be modified:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Advanced (upper right) &amp;gt; Boot &amp;gt; Boot Configuration &amp;gt; UEFI Boot &amp;gt; OS Selection: Linux&lt;/li&gt;
&lt;li&gt;Advanced &amp;gt; Boot &amp;gt; Boot Configuration &amp;gt; UEFI Boot &amp;gt; OS Selection: mark &amp;ldquo;Boot USB Devices First&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;[optional] Advanced &amp;gt; Power &amp;gt; Secondary Power Settings &amp;gt; After Power Failure: &amp;ldquo;Power On&amp;rdquo;. I have the switch and nucs connected to an outlet plug extender with an interrupter. When I switch it on, the NUCs (and the switch) boot automatically after this option is enabled, so I only need to push one button to power up the cluster.&lt;/li&gt;
&lt;li&gt;F10 to save, and shutdown.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To prepare the USB boot device with Ubuntu server 20.04 I first download the .iso from 
&lt;a href=&#34;https://ubuntu.com/download/server&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, by choosing &amp;ldquo;Option 3&amp;rdquo;, which leads to the manual install. Once the .iso file is downloaded, I use 
&lt;a href=&#34;https://ubuntu.com/tutorials/create-a-usb-stick-on-ubuntu#1-overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ubuntu&amp;rsquo;s &lt;code&gt;Startup Disk Creator&lt;/code&gt;&lt;/a&gt; to prepare a bootable USB stick. Now I just have to plug the stick in the NUC and reboot it.&lt;/p&gt;
&lt;p&gt;The Ubuntu server install is pretty straightforward, and only a few things need to be decided along the way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As user name I choose the same I have in my personal computer.&lt;/li&gt;
&lt;li&gt;As name for the NUCs I choose &amp;ldquo;nuc1&amp;rdquo; and &amp;ldquo;nuc2&amp;rdquo;, but any other option will work well.&lt;/li&gt;
&lt;li&gt;As password, for comfort I use the same I have in my personal computer.&lt;/li&gt;
&lt;li&gt;During the network setup, choose DHCP. If the network is properly configured and the switch is powered on, after a few seconds the NUC will acquire an IP in the range &lt;code&gt;10.42.0.1/24&lt;/code&gt;, as any other machine within the &lt;code&gt;cluster&lt;/code&gt; network.&lt;/li&gt;
&lt;li&gt;When asked, mark the option &amp;ldquo;Install in the whole disk&amp;rdquo;, unless you have other plans for your NUC.&lt;/li&gt;
&lt;li&gt;Mark &amp;ldquo;Install OpenSSH&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Provide it with your GitHub user name if you have your ssh-key there, and it will download it right away, facilitating a lot the ssh setup.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reboot once the install is completed. Now I keep configuring the NUC&amp;rsquo;s operating system from my PC through ssh.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;configuring-a-nuc&#34;&gt;Configuring a NUC&lt;/h3&gt;
&lt;p&gt;First, to learn the IP of the NUC:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo arp-scan 10.42.0.1/24
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other alternatives to this command are &lt;code&gt;arp -a&lt;/code&gt; and &lt;code&gt;sudo arp-scan -I enp2s0 --localnet&lt;/code&gt;. Once I learn the IP of the NUC, I add it to the file &lt;code&gt;etc/hosts&lt;/code&gt; of my personal computer as follows.&lt;/p&gt;
&lt;p&gt;First I open the file as root.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo gedit /etc/hosts
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add a new line there: &lt;code&gt;10.42.0.XXX nuc1&lt;/code&gt; and save the file.&lt;/p&gt;
&lt;p&gt;Now I access the NUC trough ssh to keep preparing it without a keyboard and a display. I do it from &lt;code&gt;Tilix&lt;/code&gt;, that allows to open different command line tabs in the same window, which is quite handy to manage several NUCs at once.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;htop.png&#34; alt=&#34;Tilix showing htop on my PC and the two NUCS&#34;&gt;&lt;/p&gt;
&lt;p&gt;Another great option to manage the NUCs through ssh is &lt;code&gt;terminator&lt;/code&gt;, that allows to 
&lt;a href=&#34;https://opensource.com/article/20/2/terminator-ssh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;broadcast the same commands to several ssh sessions at once&lt;/a&gt;. I have been trying it, and it is much better for cluster management purposes than Tilix. Actually, using it would simplify this workflow a lot, because once Ubuntu server is installed on each NUC, the rest of the configuration commands can be broadcasted at once to both NUCs. It&amp;rsquo;s a bummer I discovered this possibility way too late!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh blas@10.42.0.XXX
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The NUC&amp;rsquo;s operating system probably has a bunch of pending software updates. To install these:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get upgrade
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I have to install a set of software packages that will facilitate managing the cluster&amp;rsquo;s network and the NUC itself.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install net-tools arp-scan lm-sensors dirmngr gnupg apt-transport-https ca-certificates software-properties-common samba libopenmpi3 libopenmpi-dev openmpi-bin openmpi-common htop
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;setting-the-system-time&#34;&gt;Setting the system time&lt;/h3&gt;
&lt;p&gt;To set the system time of the NUC to the same you have in your computer, just repeat these steps in every computer in the cluster network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#list time zones: 
timedatectl list-timezones
#set time zone
sudo timedatectl set-timezone Europe/Madrid
#enable timesyncd
sudo timedatectl set-ntp on
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;setting-the-locale&#34;&gt;Setting the locale&lt;/h3&gt;
&lt;p&gt;The operating systems of the NUCs and the PC need to have the same locale. It can be set by editing the file &lt;code&gt;/etc/default/locale&lt;/code&gt; with either &lt;code&gt;nano&lt;/code&gt; (in the NUCS) or &lt;code&gt;gedit&lt;/code&gt; (in the PC) and adding these lines, just replacing &lt;code&gt;en_US.UTF-8&lt;/code&gt; with your preferred locale.&lt;/p&gt;
&lt;p&gt;LANG=&amp;quot;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LANGUAGE=&amp;quot;en_US:en&amp;rdquo;&lt;br&gt;
LC_NUMERIC=&amp;quot;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_TIME=&amp;quot;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_MONETARY=&amp;quot;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_PAPER=&amp;quot;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_IDENTIFICATION=&amp;quot;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_NAME=&amp;quot;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_ADDRESS=&amp;quot;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_TELEPHONE=&amp;quot;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_MEASUREMENT=&amp;quot;en_US.UTF-8&amp;rdquo;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;temperature-monitoring&#34;&gt;Temperature monitoring&lt;/h3&gt;
&lt;p&gt;NUCs are 
&lt;a href=&#34;https://www.intel.com/content/www/us/en/support/articles/000033327/intel-nuc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;prone to overheating&lt;/a&gt; when under heavy loads for prolonged times. Therefore, monitoring the temperature of the NUCs CPUs is kinda important. In a step before I installed &lt;code&gt;lm-sensors&lt;/code&gt; in the NUC, which provides the tools to do so. To setup the sensors from an ssh session in the NUC:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo sensors-detect
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The program will request permission to find sensors in the NUC. I answered &amp;ldquo;yes&amp;rdquo; to every request. Once all sensors are identified, to check them&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sensors

iwlwifi_1-virtual-0
Adapter: Virtual device
temp1:            N/A  

acpitz-acpi-0
Adapter: ACPI interface
temp1:        +32.0°C  (crit = +100.0°C)

coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +30.0°C  (high = +105.0°C, crit = +105.0°C)
Core 0:        +30.0°C  (high = +105.0°C, crit = +105.0°C)
Core 1:        +30.0°C  (high = +105.0°C, crit = +105.0°C)
Core 2:        +29.0°C  (high = +105.0°C, crit = +105.0°C)
Core 3:        +30.0°C  (high = +105.0°C, crit = +105.0°C)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which gives the cpu temperatures at the moment the command was executed. The command &lt;code&gt;watch sensors&lt;/code&gt; gives continuous temperature readings instead.&lt;/p&gt;
&lt;p&gt;To control overheating in my NUCs I removed their top lids, and installed them into a custom LEGO &amp;ldquo;rack&amp;rdquo; with 
&lt;a href=&#34;http://www.eluteng.com/module/fan/12cm/details003.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;external USB fans&lt;/a&gt; with velocity control, as shown in the picture at the beginning of the post.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;installing-r&#34;&gt;Installing R&lt;/h3&gt;
&lt;p&gt;To install R in the NUCs I just proceed as I would when installing it in my personal computer. There is a thorough guide 
&lt;a href=&#34;https://linuxize.com/post/how-to-install-r-on-ubuntu-20-04/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In a step above I installed all the pre-required software packages. Now I only have to add the security key of the R repository, add the repository itself, update the information on the packages available in the new repository, and finally install R.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9
sudo add-apt-repository &#39;deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/&#39;
sudo apt update
sudo apt install r-base
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If R has issues to recognize the system locale&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nano ~/.profile
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;add the following lines, replacing &lt;code&gt;en_US.UTF-8&lt;/code&gt; with your preferred locale&lt;/p&gt;
&lt;p&gt;&lt;code&gt;export LANG=en_US.UTF-8&lt;/code&gt;
&lt;code&gt;export LC_ALL=en_US.UTF-8&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;save, and execute the file to export the locale so R can read it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;. ~/.profile
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;finalizing-the-network-configuration&#34;&gt;Finalizing the network configuration&lt;/h3&gt;
&lt;p&gt;Each NUC needs firewall rules to grant access from other computers withinn the cluster network. To activate the NUC&amp;rsquo;s firewall and check what ports are open:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ufw enable
sudo ufw status
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To grant access from the PC to the NUC through ssh, and later through R for parallel computing, the ports &lt;code&gt;22&lt;/code&gt; and &lt;code&gt;11000&lt;/code&gt; must be open for the IP of the PC (&lt;code&gt;10.42.0.1&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ufw allow ssh
sudo ufw allow from 10.42.0.1 to any port 11000
sudo ufw allow from 10.42.0.1 to any port 22
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the other members of the cluster network must be declared in the &lt;code&gt;/etc/hosts&lt;/code&gt; file of each computer.&lt;/p&gt;
&lt;p&gt;In each NUC edit the file through ssh with &lt;code&gt;bash sudo nano /etc/hosts&lt;/code&gt; and add the lines&lt;/p&gt;
&lt;p&gt;&lt;code&gt;10.42.0.1 pc_name&lt;/code&gt;&lt;br&gt;
&lt;code&gt;10.42.0.XXX name_of_the_other_nuc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In the PC, add the lines&lt;/p&gt;
&lt;p&gt;&lt;code&gt;10.42.0.XXX name_of_one_nuc&lt;/code&gt;&lt;br&gt;
&lt;code&gt;10.42.0.XXX name_of_the_other_nuc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;At this point, after rebooting every machine, the NUCs must be accessible through ssh by using their names (&lt;code&gt;ssh username@nuc_name&lt;/code&gt;) instead of their IPs (&lt;code&gt;ssh username@n10.42.0.XXX&lt;/code&gt;). Just take in mind that, since the &lt;code&gt;cluster&lt;/code&gt; network works with dynamic IPs (and such setting cannot be changed in a shared connection), the IPs of the NUCs might change if a new device is added to the network. That&amp;rsquo;s something you need to check from the PC with &lt;code&gt;sudo arp-scan 10.42.0.1/24&lt;/code&gt;, to update every &lt;code&gt;/etc/hosts&lt;/code&gt; file accordingly.&lt;/p&gt;
&lt;p&gt;I think that&amp;rsquo;s all folks. Good luck setting your home cluster! Next time I will describe how to use it for parallel computing in R.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
