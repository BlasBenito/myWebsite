<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>home-cluster | Blas M. Benito, PhD</title>
    <link>https://blasbenito.com/tag/home-cluster/</link>
      <atom:link href="https://blasbenito.com/tag/home-cluster/index.xml" rel="self" type="application/rss+xml" />
    <description>home-cluster</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2023 Blas M. Benito. All Rights Reserved.</copyright><lastBuildDate>Sun, 03 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://blasbenito.com/media/avatar.jpg</url>
      <title>home-cluster</title>
      <link>https://blasbenito.com/tag/home-cluster/</link>
    </image>
    
    <item>
      <title>Setup of a Shared Folder in a Home Cluster</title>
      <link>https://blasbenito.com/post/shared-folder-home-cluster/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/shared-folder-home-cluster/</guid>
      <description>&lt;p&gt;In the previous posts I have covered how to 
&lt;a href=&#34;https://www.blasbenito.com/post/01_home_cluster/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;setup a home cluster&lt;/a&gt;, and how to 
&lt;a href=&#34;https://www.blasbenito.com/post/02_parallelizing_loops_with_r/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;run parallel processes with &lt;code&gt;foreach&lt;/code&gt; in R&lt;/a&gt;. However, so far I haven&amp;rsquo;t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.&lt;/p&gt;
&lt;p&gt;This post (developed on Ubuntu and Ubuntu Server 20.04) assumes that the home cluster runs on Linux, and covers the following topics.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basics of the Network File System protocol (NFS).&lt;/li&gt;
&lt;li&gt;Setup of an NFS folder in a home cluster.&lt;/li&gt;
&lt;li&gt;Using an NFS folder in a parallelized loop.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;the-network-file-system-protocol-nfs&#34;&gt;The Network File System protocol (NFS)&lt;/h2&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Network_File_System&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Network File System&lt;/a&gt; protocol offers the means for a &lt;em&gt;host&lt;/em&gt; computer to allow other computers in the network (&lt;em&gt;clients&lt;/em&gt;) to read and write the contents of a given folder. The clients access such folder like if it was a local one, while in fact it is a &lt;em&gt;reference&lt;/em&gt; to the one in the host computer.&lt;/p&gt;
&lt;p&gt;The image at the beginning of the post illustrates the concept. There is a &lt;em&gt;host&lt;/em&gt; computer with a folder in the path &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; (were &lt;code&gt;user&lt;/code&gt; is your user name) that is broadcasted to the network, and there are one or several &lt;em&gt;clients&lt;/em&gt; that are mounting &lt;em&gt;mounting&lt;/em&gt; (making accessible) the same folder in their local paths &lt;code&gt;/home/user/cluster_shared&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If the host writes a file to the shared folder, it is available right away for the clients, and the other way around. At the end, the idea is to have a folder shared among all computers in the cluster, while having the same exact path on each one of them to write or read files from such shared folder.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;setup-of-an-nfs-folder-in-a-home-cluster&#34;&gt;Setup of an NFS folder in a home cluster&lt;/h2&gt;
&lt;p&gt;To setup the shared folder we&amp;rsquo;ll need to do some things in the &lt;em&gt;host&lt;/em&gt;, and some things in the &lt;em&gt;clients&lt;/em&gt;. Let&amp;rsquo;s start with the host.&lt;/p&gt;
&lt;h4 id=&#34;preparing-the-host-computer&#34;&gt;Preparing the host computer&lt;/h4&gt;
&lt;p&gt;First we need to install the &lt;code&gt;nfs-kernel-server&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt update
sudo apt install nfs-kernel-server
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create the shared folder. Remember to replace &lt;code&gt;user&lt;/code&gt; with your user name, and &lt;code&gt;cluster_shared&lt;/code&gt; with the actual folder name you want to use.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir /home/user/cluster_shared
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To broadcast it we need to open the file &lt;code&gt;/etc/exports&lt;/code&gt;&amp;hellip;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo gedit /etc/exports
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;hellip; and add the following line&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/home/user/cluster_shared IP_CLIENT1(rw,no_subtree_check) IP_CLIENT2(rw,no_subtree_check) IP_CLIENT3(rw,no_subtree_check)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/home/user/cluster_shared&lt;/code&gt; is the path of the shared folder.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IP_CLIENTx&lt;/code&gt; are the IPs of each one of the clients.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rw&lt;/code&gt; gives reading and writing permission on the shared folder to the given client.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;no_subtree_check&lt;/code&gt; prevents the host from checking the complete tree of shares before attending a request (read or write) by a client.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, the last line of my &lt;code&gt;/etc/exports&lt;/code&gt; file looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/home/blas/cluster_shared 10.42.0.34(rw,async,no_subtree_check) 10.42.0.104(rw,async,no_subtree_check)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Save the file, and to make the changes effective, execute:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo exportfs -ra
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To end preparing the host we have to update the firewall rules to allow nfs connections from the clients. Notice that one rule per client needs to be defined, using the clients IPs to identify them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ufw allow from IP_CLIENT1 to any port nfs
sudo ufw allow from IP_CLIENT2 to any port nfs
sudo ufw status
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;preparing-the-clients&#34;&gt;Preparing the clients&lt;/h4&gt;
&lt;p&gt;First we have to install the Linux package &lt;code&gt;nfs-common&lt;/code&gt; on each client.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt update
sudp apt install nfs-common
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create a folder in the clients and use it to mount the NFS folder of the host.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p /home/user/cluster_shared
sudo mount IP_HOST:/home/user/cluster_shared /home/user/cluster_shared
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second line of code is mounting the folder &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; of the host in the folder &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; of the client.&lt;/p&gt;
&lt;p&gt;To make the mount permanent, we have to open &lt;code&gt;/etc/fstab&lt;/code&gt; with super-user privilege in the clients&amp;hellip;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo gedit /etc/fstab
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;hellip; and add the line&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;IP_HOST:/home/user/cluster_shared /home/user/cluster_shared   nfs     defaults 0 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember to replace &lt;code&gt;IP_HOST&lt;/code&gt; and &lt;code&gt;user&lt;/code&gt; with the right values!&lt;/p&gt;
&lt;p&gt;Now we can test that the shared folder works as intended by writing one file into it from each computer in the network using the code below in the command line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd cluster_shared
touch filename.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;terminator.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once the files are created, we can check they are visible from each computer using the &lt;code&gt;ls&lt;/code&gt; command.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;terminator2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;using-an-nfs-folder-in-a-parallelized-loop&#34;&gt;Using an NFS folder in a parallelized loop&lt;/h2&gt;
&lt;p&gt;In a 
&lt;a href=&#34;https://www.blasbenito.com/post/02_parallelizing_loops_with_r/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt; I described how to run parallelized tasks with &lt;code&gt;foreach&lt;/code&gt; in R. This section intends to complete that post by showing how a shared NFS folder can be used to store input and output files during the execution of a parallelized loop&lt;/p&gt;
&lt;h3 id=&#34;the-task&#34;&gt;The task&lt;/h3&gt;
&lt;p&gt;In this hypothetical example we have a large number of data frames stored in &lt;code&gt;/home/user/cluster_shared/input&lt;/code&gt;. Each data frame has the same predictors &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;, and a different response variable, named &lt;code&gt;y1&lt;/code&gt; for the data frame &lt;code&gt;y1&lt;/code&gt;, &lt;code&gt;y2&lt;/code&gt; for the data frame &lt;code&gt;y2&lt;/code&gt;, and so on. In the example we will be using 100 data frames, but there could be thousands. This example would be valid as well for any other kind of object, such as raster files, time-series, etc etc.&lt;/p&gt;
&lt;p&gt;First we have to load the libraries we&amp;rsquo;ll be using.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#automatic install of packages if they are not installed already
list.of.packages &amp;lt;- c(
  &amp;quot;foreach&amp;quot;,
  &amp;quot;doParallel&amp;quot;,
  &amp;quot;ranger&amp;quot;
  )

new.packages &amp;lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&amp;quot;Package&amp;quot;])]

if(length(new.packages) &amp;gt; 0){
  install.packages(new.packages, dep=TRUE)
}

#loading packages
for(package.i in list.of.packages){
  suppressPackageStartupMessages(
    library(
      package.i, 
      character.only = TRUE
      )
    )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code chunk below generates the folder &lt;code&gt;/home/user/cluster_shared/input&lt;/code&gt; and populates it with the dummy files.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#creating the input folder
input.folder &amp;lt;- &amp;quot;/home/blas/cluster_shared/input&amp;quot;
dir.create(input.folder)

#data frame names
df.names &amp;lt;- paste0(&amp;quot;y&amp;quot;, 1:100)

#filling it with files
for(i in df.names){
  
  #creating the df
  df.i &amp;lt;- data.frame(
    y = rnorm(1000),
    a = rnorm(1000),
    b = rnorm(1000),
    c = rnorm(1000),
    d = rnorm(1000)
  )
  
  #changing name of the response variable
  colnames(df.i)[1] &amp;lt;- i
  
  #assign to a variable with name i
  assign(i, df.i)
  
  #saving the object
  save(
    list = i,
    file = paste0(input.folder, &amp;quot;/&amp;quot;, i, &amp;quot;.RData&amp;quot;)
  )
  
  #removing the generated data frame form the environment
  rm(list = i, df.i, i)
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our target now will be to fit one &lt;code&gt;ranger::ranger()&lt;/code&gt; model per data frame stored in &lt;code&gt;/home/blas/cluster_shared/input&lt;/code&gt;, save the model result to a folder with the path &lt;code&gt;/home/blas/cluster_shared/input&lt;/code&gt;, and write a small summary of the model to the output of &lt;code&gt;foreach&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Such target is based on this rationale: When executing a &lt;code&gt;foreach&lt;/code&gt; loop as in &lt;code&gt;x &amp;lt;- foreach(...) %dopar% {...}&lt;/code&gt;, the variable &lt;code&gt;x&lt;/code&gt; is going to grow in memory very fast, competing for RAM resources with the worker nodes. Furthermore, since &lt;code&gt;x&lt;/code&gt; is being written on the fly, the results would be lost if the computer crashes. When the size of the input and the output of our parallelized operation is larger than memory, we can use an NFS folder to store inputs and outputs, while keeping the RAM memory free for computational tasks only, with the positive side effect of having our outputs already stored should our computer decide to crash.&lt;/p&gt;
&lt;p&gt;Also, please notice that here I am focusing in a cluster setting, but using a folder to read and write data during a loop paralellized with &lt;code&gt;foreach&lt;/code&gt; can indeed be done in a single computer without an NFS folder. Any folder in your system will do the trick as well!&lt;/p&gt;
&lt;p&gt;So, from here, we are going to prepare the cluster, and execute a parallelized loop fitting one model per data frame that reads the inputs and writes the outputs to the shared folder.&lt;/p&gt;
&lt;h3 id=&#34;cluster-setup&#34;&gt;Cluster setup&lt;/h3&gt;
&lt;p&gt;We will also need the function I showed in the previous post to generate the cluster specification from a 
&lt;a href=&#34;https://gist.github.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Gist&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;source(&amp;quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below I use the function to create a cluster specification and initiate the cluster with &lt;code&gt;parallel::makeCluster()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#generate cluster specification
spec &amp;lt;- cluster_spec(
  ips = c(&#39;10.42.0.1&#39;, &#39;10.42.0.34&#39;, &#39;10.42.0.104&#39;),
  cores = c(7, 4, 4),
  user = &amp;quot;blas&amp;quot;
)

#define parallel port
Sys.setenv(R_PARALLEL_PORT = 11000)
Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;)

#setting up cluster
my.cluster &amp;lt;- parallel::makeCluster(
  master = &#39;10.42.0.1&#39;, 
  spec = spec,
  port = Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;),
  outfile = &amp;quot;&amp;quot;,
  homogeneous = TRUE
)

#check cluster definition (optional)
print(my.cluster)

#register cluster
doParallel::registerDoParallel(cl = my.cluster)

#check number of workers
foreach::getDoParWorkers()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;parallelized-loop&#34;&gt;Parallelized loop&lt;/h3&gt;
&lt;p&gt;For everything to work as intended, we first need to create the output folder.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;output.folder &amp;lt;- &amp;quot;/home/blas/cluster_shared/output&amp;quot;
dir.create(output.folder)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we are ready to execute the parallelized loop. Notice that I am using the output of &lt;code&gt;list.files()&lt;/code&gt; to have a vector of file names the loop will iterate over. The steps inside of the loop go as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;1.&lt;/em&gt; Remove the extension &lt;code&gt;.RData&lt;/code&gt; from the file name. We&amp;rsquo;ll later use the result to use &lt;code&gt;assign()&lt;/code&gt; on the fitted model to change its name to the same as the input file before saving it.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2.&lt;/em&gt; Read the input data frame and store in an object named &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;3.&lt;/em&gt; Fit the model with ranger, using the first column of &lt;code&gt;df&lt;/code&gt; as respose variable.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;4.&lt;/em&gt; Change the model name to the name of the input file without extension, resulting from the first step described above.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;5.&lt;/em&gt; Save the model into the output folder with the extension &lt;code&gt;.RData&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;6.&lt;/em&gt; Return a data frame with one line with the name of the response variable, the r-squared, and the importance of each predictor.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#list of input files as iterator
input.files &amp;lt;- list.files(
  path = input.folder,
  full.names = FALSE
)

modelling.summary &amp;lt;- foreach(
  input.file = input.files,
  .combine = &#39;rbind&#39;, 
  .packages = &amp;quot;ranger&amp;quot;
) %dopar% {
  
  # 1. input file name without extension
  input.file.name &amp;lt;- tools::file_path_sans_ext(input.file)
  
  # 2. read input file
  df &amp;lt;- get(load(paste0(input.folder, &amp;quot;/&amp;quot;, input.file)))
  
  # 3. fit model
  m.i &amp;lt;- ranger::ranger(
    data = df,
    dependent.variable.name = colnames(df)[1],
    importance = &amp;quot;permutation&amp;quot;
  )
  
  # 4. change name of the model to one of the response variable
  assign(input.file.name, m.i)
  
  # 5. save model
  save(
    list = input.file.name,
    file = paste0(output.folder, &amp;quot;/&amp;quot;, input.file)
  )
  
  # 6. returning summary
  return(
    data.frame(
      response.variable = input.file.name,
      r.squared = m.i$r.squared,
      importance.a = m.i$variable.importance[&amp;quot;a&amp;quot;],
      importance.b = m.i$variable.importance[&amp;quot;b&amp;quot;],
      importance.c = m.i$variable.importance[&amp;quot;c&amp;quot;],
      importance.d = m.i$variable.importance[&amp;quot;d&amp;quot;]
    )
  )
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once this parallelized loop is executed, the folder &lt;code&gt;/home/blas/cluster_shared/output&lt;/code&gt; should be filled with the results from the cluster workers, and the &lt;code&gt;modelling.summary&lt;/code&gt; data frame contains the summary of each fitted model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;output.png&#34; alt=&#34;Listing outputs in the shared folder&#34;&gt;
Now that the work is done, we can stop the cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parallel::stopCluster(cl = my.cluster)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you know how to work with data larger than memory in a parallelized loop!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Setting up a Beowulf Home Cluster</title>
      <link>https://blasbenito.com/post/home-cluster/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/home-cluster/</guid>
      <description>&lt;p&gt;In this post I explain how to setup a small 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Beowulf_cluster&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beowulf cluster&lt;/a&gt; with a personal PC running Ubuntu 20.04 and a couple of 
&lt;a href=&#34;https://www.intel.com/content/www/us/en/products/boards-kits/nuc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intel NUCs&lt;/a&gt; running Ubuntu Server 20.04, with the end-goal of parallelizing R tasks.&lt;/p&gt;
&lt;p&gt;The topics I cover here are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Required material&lt;/li&gt;
&lt;li&gt;Network setting&lt;/li&gt;
&lt;li&gt;Installing the secure shell protocol&lt;/li&gt;
&lt;li&gt;Installing Ubuntu server in the NUCs&lt;/li&gt;
&lt;li&gt;Installing R in the NUCs&lt;/li&gt;
&lt;li&gt;Managing the cluster&amp;rsquo;s network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;preamble&#34;&gt;Preamble&lt;/h2&gt;
&lt;p&gt;I have a little but nice HP ENVY model &lt;em&gt;TE01-0008ns&lt;/em&gt; with 32 GB RAM, 8 CPUs, and 3TB of hard disk running Ubuntu 20.04 that I use to do all my computational work (and most of my tweeting). A few months ago I connected it with my two laptops (one of them deceased now, RIP my dear &lt;em&gt;skynet&lt;/em&gt;) to create a little cluster to run parallel tasks in R.&lt;/p&gt;
&lt;p&gt;It was just a draft cluster running on a wireless network, but it served me to think about getting a more permanent solution not requiring two additional laptops in my desk.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s were the nice INTEL NUCs (from 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Next_Unit_of_Computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Next Unit of Computing&lt;/em&gt;&lt;/a&gt;) come into play. NUCs are full-fledged computers fitted in small boxes usually sold without RAM memory sticks and no hard disk (hence the term &lt;em&gt;barebone&lt;/em&gt;). Since they have a low energy consumption footprint, I thought these would be ideal units for my soon-to-be home cluster.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;p&gt;I gifted myself with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 
&lt;a href=&#34;https://ark.intel.com/content/www/us/en/ark/products/95062/intel-nuc-kit-nuc6cayh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intel Barebone BOXNUC6CAYH&lt;/a&gt;, each with 4 cores, and a maximum RAM memory of 32GB (you might read they only accept 8GB, but that&amp;rsquo;s not the case anymore). Notice that these NUCs aren&amp;rsquo;t state-of-the-art now, they were released by the end of 2016.&lt;/li&gt;
&lt;li&gt;2 Hard disks SSD 2.5&amp;quot; 
&lt;a href=&#34;https://shop.westerndigital.com/es-es/products/internal-drives/wd-blue-sata-ssd#WDS250G2B0A&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Western Digital WDS250G2B0A WD Blue&lt;/a&gt; (250GB)&lt;/li&gt;
&lt;li&gt;4 Crucial CT102464BF186D DDR3 SODIMM (204 pins) RAM sticks with 8GB each.&lt;/li&gt;
&lt;li&gt;1 ethernet switch Netgear GS308-300PES with 8 ports.&lt;/li&gt;
&lt;li&gt;3 ethernet wires NanoCable 10.20.0400-BL of 
&lt;a href=&#34;https://www.electronics-notes.com/articles/connectivity/ethernet-ieee-802-3/how-to-buy-best-ethernet-cables-cat-5-6-7.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cat 6&lt;/a&gt; quality.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The whole set came to cost around 530€, but please notice that I had a clear goal in mind: &amp;ldquo;duplicating&amp;rdquo; my computing power with the minimum number of NUCs, while preserving a share of 4GB of RAM memory per CPU throughout the cluster (based on the features of my desk computer). A more basic setting with more modest NUCs and smaller RAM would cost half of that.&lt;/p&gt;
&lt;p&gt;This instructive video by 
&lt;a href=&#34;https://www.youtube.com/channel/UCYa3XeSHenvosy5wMRpeIww&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Harry&lt;/a&gt; shows how to install the SSD and the RAM sticks in an Intel NUC. It really takes 5 minutes tops, one only has to be a bit careful with the RAM sticks, the pins need to go all the way in into their slots before securing the sticks in place.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/6hzj7DogqXU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;network-settings&#34;&gt;Network settings&lt;/h2&gt;
&lt;p&gt;Before starting to install an operating system in the NUCS, the network setup goes as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;My desktop PC is connected to a router via WIFI and dynamic IP (DHCP).&lt;/li&gt;
&lt;li&gt;The PC and each NUC are connected to the switch with cat6 ethernet wires.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;network.png&#34; alt=&#34;Network diagram&#34;&gt;&lt;/p&gt;
&lt;p&gt;To share my PC&amp;rsquo;s WIFI connection with the NUCs I have to prepare a new &lt;em&gt;connection profile&lt;/em&gt; with the command line tool of Ubuntu&amp;rsquo;s 
&lt;a href=&#34;https://en.wikipedia.org/wiki/NetworkManager&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;NetworkManager&lt;/code&gt;&lt;/a&gt;, named &lt;code&gt;nmcli&lt;/code&gt;, as follows.&lt;/p&gt;
&lt;p&gt;First, I need to find the name of my ethernet interface by checking the status of my network devices with the command line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nmcli device status
DEVICE  TYPE      STATE        CONNECTION  
wlp3s0  wifi      connected    my_wifi 
enp2s0  ethernet  unavailable  --          
lo      loopback  unmanaged    --      
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There I can see that my ethernet interface is named &lt;code&gt;enp2s0&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Second, I have to configure the shared connection.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nmcli connection add type ethernet ifname enp2s0 ipv4.method shared con-name cluster
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Were &lt;code&gt;ifname enp2s0&lt;/code&gt; is the name of the interface I want to use for the new connection, &lt;code&gt;ipv4.method shared&lt;/code&gt; is the type of connection, and &lt;code&gt;con-name cluster&lt;/code&gt; is the name I want the connection to have. This operation adds firewall rules to manage traffic within the &lt;code&gt;cluster&lt;/code&gt; network, starts a DHCP server in the computer that serves IPs to the NUCS, and a DNS server that allows the NUCs to translate internet addresses.&lt;/p&gt;
&lt;p&gt;After turning on the switch, I can check the connection status again with&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nmcli device status
DEVICE  TYPE      STATE      CONNECTION  
enp2s0  ethernet  connected  cluster     
wlp3s0  wifi      connected  my_wifi 
lo      loopback  unmanaged  --    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When checking the IP of the device with &lt;code&gt;bash ifconfig&lt;/code&gt; it should yield &lt;code&gt;10.42.0.1&lt;/code&gt;. Any other computer in the &lt;code&gt;cluster&lt;/code&gt; network will have a dynamic IP in the range &lt;code&gt;10.42.0.1/24&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Further details about how to set a shared connection with &lt;code&gt;NetworkManager&lt;/code&gt; can be found in 
&lt;a href=&#34;https://fedoramagazine.org/internet-connection-sharing-networkmanager/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this nice post by Beniamino Galvani&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;ssh-setup&#34;&gt;SSH setup&lt;/h2&gt;
&lt;p&gt;My PC, as the director of the cluster, needs an &lt;code&gt;SSH client&lt;/code&gt; running, while the NUCs need an &lt;code&gt;SSH server&lt;/code&gt;. 
&lt;a href=&#34;https://www.ionos.com/digitalguide/server/tools/ssh-secure-shell/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;SSH&lt;/code&gt; (&lt;strong&gt;S&lt;/strong&gt;ecure &lt;strong&gt;Sh&lt;/strong&gt;ell)&lt;/a&gt; is a remote authentication protocol that allows secure connections to remote servers that I will be using all the time to manage the cluster. To install, run, and check its status I just have to run these lines in the console:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install ssh 
sudo systemctl enable --now ssh
sudo systemctl status ssh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, a secure certificate of the identity of a given computer, named &lt;code&gt;ssh-key&lt;/code&gt;, that grants access to remote ssh servers and services needs to be generated.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh-keygen &amp;quot;label&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, substitute &amp;ldquo;label&amp;rdquo; by the name of the computer to be used as cluster&amp;rsquo;s &amp;ldquo;director&amp;rdquo;. The system will ask for a file name and a 
&lt;a href=&#34;https://www.ssh.com/ssh/passphrase&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;passphrase&lt;/a&gt; that will be used to encrypt the ssh-key.&lt;/p&gt;
&lt;p&gt;The ssh-key needs to be added to the 
&lt;a href=&#34;https://www.ssh.com/ssh/agent&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ssh-agent&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh-add ~/.ssh/id_rsa
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To copy the ssh-key to my GitHub account, I have to copy the contents of the file &lt;code&gt;~/.ssh/id_rsa.pub&lt;/code&gt; (can be done just opening it with &lt;code&gt;gedit ~/.ssh/id_rsa.pub&lt;/code&gt; + &lt;code&gt;Ctrl + a&lt;/code&gt; + &lt;code&gt;Ctrl + c&lt;/code&gt;), and paste it on &lt;code&gt;GitHub account &amp;gt; Settings &amp;gt;  SSH and GPG keys &amp;gt; New SSH Key&lt;/code&gt; (green button in the upper right part of the window).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you don&amp;rsquo;t use GitHub, you&amp;rsquo;ll need to copy your ssh-key to the NUCs once they are up and running with &lt;code&gt;ssh-copy-id -i ~/.ssh/id_rsa.pub user_name@nuc_IP&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;installing-and-preparing-ubuntu-server-in-each-nuc&#34;&gt;Installing and preparing ubuntu server in each NUC&lt;/h2&gt;
&lt;p&gt;The NUCs don&amp;rsquo;t need to waste resources in a user graphical interface I won&amp;rsquo;t be using whatsoever. Since they will work in a 
&lt;a href=&#34;https://www.howtogeek.com/660841/what-is-a-headless-server/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;headless&lt;/em&gt; configuration&lt;/a&gt; once the cluster is ready, a Linux distro without graphical user interface such as Ubuntu server is the way to go.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;installing-ubuntu-server&#34;&gt;Installing Ubuntu server&lt;/h3&gt;
&lt;p&gt;First it is important to connect a display, a keyboard, and a mouse to the NUC in preparation, and turn it on while pushing F2 to start the visual BIOS. These BIOS parameters need to be modified:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Advanced (upper right) &amp;gt; Boot &amp;gt; Boot Configuration &amp;gt; UEFI Boot &amp;gt; OS Selection: Linux&lt;/li&gt;
&lt;li&gt;Advanced &amp;gt; Boot &amp;gt; Boot Configuration &amp;gt; UEFI Boot &amp;gt; OS Selection: mark &amp;ldquo;Boot USB Devices First&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;[optional] Advanced &amp;gt; Power &amp;gt; Secondary Power Settings &amp;gt; After Power Failure: &amp;ldquo;Power On&amp;rdquo;. I have the switch and nucs connected to an outlet plug extender with an interrupter. When I switch it on, the NUCs (and the switch) boot automatically after this option is enabled, so I only need to push one button to power up the cluster.&lt;/li&gt;
&lt;li&gt;F10 to save, and shutdown.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To prepare the USB boot device with Ubuntu server 20.04 I first download the .iso from 
&lt;a href=&#34;https://ubuntu.com/download/server&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, by choosing &amp;ldquo;Option 3&amp;rdquo;, which leads to the manual install. Once the .iso file is downloaded, I use 
&lt;a href=&#34;https://ubuntu.com/tutorials/create-a-usb-stick-on-ubuntu#1-overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ubuntu&amp;rsquo;s &lt;code&gt;Startup Disk Creator&lt;/code&gt;&lt;/a&gt; to prepare a bootable USB stick. Now I just have to plug the stick in the NUC and reboot it.&lt;/p&gt;
&lt;p&gt;The Ubuntu server install is pretty straightforward, and only a few things need to be decided along the way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As user name I choose the same I have in my personal computer.&lt;/li&gt;
&lt;li&gt;As name for the NUCs I choose &amp;ldquo;nuc1&amp;rdquo; and &amp;ldquo;nuc2&amp;rdquo;, but any other option will work well.&lt;/li&gt;
&lt;li&gt;As password, for comfort I use the same I have in my personal computer.&lt;/li&gt;
&lt;li&gt;During the network setup, choose DHCP. If the network is properly configured and the switch is powered on, after a few seconds the NUC will acquire an IP in the range &lt;code&gt;10.42.0.1/24&lt;/code&gt;, as any other machine within the &lt;code&gt;cluster&lt;/code&gt; network.&lt;/li&gt;
&lt;li&gt;When asked, mark the option &amp;ldquo;Install in the whole disk&amp;rdquo;, unless you have other plans for your NUC.&lt;/li&gt;
&lt;li&gt;Mark &amp;ldquo;Install OpenSSH&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Provide it with your GitHub user name if you have your ssh-key there, and it will download it right away, facilitating a lot the ssh setup.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reboot once the install is completed. Now I keep configuring the NUC&amp;rsquo;s operating system from my PC through ssh.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;configuring-a-nuc&#34;&gt;Configuring a NUC&lt;/h3&gt;
&lt;p&gt;First, to learn the IP of the NUC:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo arp-scan 10.42.0.1/24
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other alternatives to this command are &lt;code&gt;arp -a&lt;/code&gt; and &lt;code&gt;sudo arp-scan -I enp2s0 --localnet&lt;/code&gt;. Once I learn the IP of the NUC, I add it to the file &lt;code&gt;etc/hosts&lt;/code&gt; of my personal computer as follows.&lt;/p&gt;
&lt;p&gt;First I open the file as root.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo gedit /etc/hosts
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add a new line there: &lt;code&gt;10.42.0.XXX nuc1&lt;/code&gt; and save the file.&lt;/p&gt;
&lt;p&gt;Now I access the NUC trough ssh to keep preparing it without a keyboard and a display. I do it from &lt;code&gt;Tilix&lt;/code&gt;, that allows to open different command line tabs in the same window, which is quite handy to manage several NUCs at once.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;htop.png&#34; alt=&#34;Tilix showing htop on my PC and the two NUCS&#34;&gt;&lt;/p&gt;
&lt;p&gt;Another great option to manage the NUCs through ssh is &lt;code&gt;terminator&lt;/code&gt;, that allows to 
&lt;a href=&#34;https://opensource.com/article/20/2/terminator-ssh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;broadcast the same commands to several ssh sessions at once&lt;/a&gt;. I have been trying it, and it is much better for cluster management purposes than Tilix. Actually, using it would simplify this workflow a lot, because once Ubuntu server is installed on each NUC, the rest of the configuration commands can be broadcasted at once to both NUCs. It&amp;rsquo;s a bummer I discovered this possibility way too late!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh blas@10.42.0.XXX
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The NUC&amp;rsquo;s operating system probably has a bunch of pending software updates. To install these:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get upgrade
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I have to install a set of software packages that will facilitate managing the cluster&amp;rsquo;s network and the NUC itself.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install net-tools arp-scan lm-sensors dirmngr gnupg apt-transport-https ca-certificates software-properties-common samba libopenmpi3 libopenmpi-dev openmpi-bin openmpi-common htop
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;setting-the-system-time&#34;&gt;Setting the system time&lt;/h3&gt;
&lt;p&gt;To set the system time of the NUC to the same you have in your computer, just repeat these steps in every computer in the cluster network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#list time zones: 
timedatectl list-timezones
#set time zone
sudo timedatectl set-timezone Europe/Madrid
#enable timesyncd
sudo timedatectl set-ntp on
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;setting-the-locale&#34;&gt;Setting the locale&lt;/h3&gt;
&lt;p&gt;The operating systems of the NUCs and the PC need to have the same locale. It can be set by editing the file &lt;code&gt;/etc/default/locale&lt;/code&gt; with either &lt;code&gt;nano&lt;/code&gt; (in the NUCS) or &lt;code&gt;gedit&lt;/code&gt; (in the PC) and adding these lines, just replacing &lt;code&gt;en_US.UTF-8&lt;/code&gt; with your preferred locale.&lt;/p&gt;
&lt;p&gt;LANG=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LANGUAGE=&amp;ldquo;en_US:en&amp;rdquo;&lt;br&gt;
LC_NUMERIC=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_TIME=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_MONETARY=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_PAPER=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_IDENTIFICATION=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_NAME=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_ADDRESS=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_TELEPHONE=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_MEASUREMENT=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;temperature-monitoring&#34;&gt;Temperature monitoring&lt;/h3&gt;
&lt;p&gt;NUCs are 
&lt;a href=&#34;https://www.intel.com/content/www/us/en/support/articles/000033327/intel-nuc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;prone to overheating&lt;/a&gt; when under heavy loads for prolonged times. Therefore, monitoring the temperature of the NUCs CPUs is kinda important. In a step before I installed &lt;code&gt;lm-sensors&lt;/code&gt; in the NUC, which provides the tools to do so. To setup the sensors from an ssh session in the NUC:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo sensors-detect
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The program will request permission to find sensors in the NUC. I answered &amp;ldquo;yes&amp;rdquo; to every request. Once all sensors are identified, to check them&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sensors

iwlwifi_1-virtual-0
Adapter: Virtual device
temp1:            N/A  

acpitz-acpi-0
Adapter: ACPI interface
temp1:        +32.0°C  (crit = +100.0°C)

coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +30.0°C  (high = +105.0°C, crit = +105.0°C)
Core 0:        +30.0°C  (high = +105.0°C, crit = +105.0°C)
Core 1:        +30.0°C  (high = +105.0°C, crit = +105.0°C)
Core 2:        +29.0°C  (high = +105.0°C, crit = +105.0°C)
Core 3:        +30.0°C  (high = +105.0°C, crit = +105.0°C)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which gives the cpu temperatures at the moment the command was executed. The command &lt;code&gt;watch sensors&lt;/code&gt; gives continuous temperature readings instead.&lt;/p&gt;
&lt;p&gt;To control overheating in my NUCs I removed their top lids, and installed them into a custom LEGO &amp;ldquo;rack&amp;rdquo; with 
&lt;a href=&#34;http://www.eluteng.com/module/fan/12cm/details003.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;external USB fans&lt;/a&gt; with velocity control, as shown in the picture at the beginning of the post.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;installing-r&#34;&gt;Installing R&lt;/h3&gt;
&lt;p&gt;To install R in the NUCs I just proceed as I would when installing it in my personal computer. There is a thorough guide 
&lt;a href=&#34;https://linuxize.com/post/how-to-install-r-on-ubuntu-20-04/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In a step above I installed all the pre-required software packages. Now I only have to add the security key of the R repository, add the repository itself, update the information on the packages available in the new repository, and finally install R.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9
sudo add-apt-repository &#39;deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/&#39;
sudo apt update
sudo apt install r-base
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If R has issues to recognize the system locale&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nano ~/.profile
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;add the following lines, replacing &lt;code&gt;en_US.UTF-8&lt;/code&gt; with your preferred locale&lt;/p&gt;
&lt;p&gt;&lt;code&gt;export LANG=en_US.UTF-8&lt;/code&gt;
&lt;code&gt;export LC_ALL=en_US.UTF-8&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;save, and execute the file to export the locale so R can read it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;. ~/.profile
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;finalizing-the-network-configuration&#34;&gt;Finalizing the network configuration&lt;/h3&gt;
&lt;p&gt;Each NUC needs firewall rules to grant access from other computers withinn the cluster network. To activate the NUC&amp;rsquo;s firewall and check what ports are open:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ufw enable
sudo ufw status
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To grant access from the PC to the NUC through ssh, and later through R for parallel computing, the ports &lt;code&gt;22&lt;/code&gt; and &lt;code&gt;11000&lt;/code&gt; must be open for the IP of the PC (&lt;code&gt;10.42.0.1&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ufw allow ssh
sudo ufw allow from 10.42.0.1 to any port 11000
sudo ufw allow from 10.42.0.1 to any port 22
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the other members of the cluster network must be declared in the &lt;code&gt;/etc/hosts&lt;/code&gt; file of each computer.&lt;/p&gt;
&lt;p&gt;In each NUC edit the file through ssh with &lt;code&gt;bash sudo nano /etc/hosts&lt;/code&gt; and add the lines&lt;/p&gt;
&lt;p&gt;&lt;code&gt;10.42.0.1 pc_name&lt;/code&gt;&lt;br&gt;
&lt;code&gt;10.42.0.XXX name_of_the_other_nuc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In the PC, add the lines&lt;/p&gt;
&lt;p&gt;&lt;code&gt;10.42.0.XXX name_of_one_nuc&lt;/code&gt;&lt;br&gt;
&lt;code&gt;10.42.0.XXX name_of_the_other_nuc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;At this point, after rebooting every machine, the NUCs must be accessible through ssh by using their names (&lt;code&gt;ssh username@nuc_name&lt;/code&gt;) instead of their IPs (&lt;code&gt;ssh username@n10.42.0.XXX&lt;/code&gt;). Just take in mind that, since the &lt;code&gt;cluster&lt;/code&gt; network works with dynamic IPs (and such setting cannot be changed in a shared connection), the IPs of the NUCs might change if a new device is added to the network. That&amp;rsquo;s something you need to check from the PC with &lt;code&gt;sudo arp-scan 10.42.0.1/24&lt;/code&gt;, to update every &lt;code&gt;/etc/hosts&lt;/code&gt; file accordingly.&lt;/p&gt;
&lt;p&gt;I think that&amp;rsquo;s all folks. Good luck setting your home cluster! Next time I will describe how to use it for parallel computing in R.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
