---
title: Managing Multicollinearity
author: ''
date: '2023-11-05'
slug: multicollinearity-management
links:
- icon: github
  icon_pack: fab
  name: GitHub
  url: https://github.com/BlasBenito/collinear
categories: []
tags: [R packages, Multicollinearity, Variable Selection]
subtitle: ''
summary: 'Identification of multicollinearity and variable selection for explanatory modeling with R'
authors: [admin]
lastmod: '2023-11-05T08:14:23+02:00'
featured: no
draft: true
image:
  caption: Graph by Blas M. Benito
  focal_point: Smart
  margin: auto
projects: []
---

{{% alert note %}}
This post, aimed at R users wishing to understand how to manage multicollinearity with R,  serves as a continuation of [Multicollinearity Hinders Model Interpretability](/post/multicollinearity-model-interpretability/).
{{% /alert %}}

# Summary

TODO

# R packages

This tutorial requires the development version (>= 1.0.2) of the newly released R package [`collinear`](https://blasbenito.github.io/collinear/), and a few more.

```{r, eval = FALSE}
#required
install.packages("remotes")
remotes::install_github(
  repo = "blasbenito/collinear", 
  ref = "development"
  )
install.packages("ranger")
install.packages("dplyr")
```


# Example data

To develop the examples in this post, I will use a toy dataset derived from the `vi`data frame shipped with the [`collinear`](https://blasbenito.github.io/collinear/) package. In `vi`, the variables "soil_clay" and "humidity_range" are not correlated at all (Pearson correlation = -0.06). Below I rename them to `a` and `b`, use a linear combination of them plus a bit of white noise to generate the response variable `y`, and generate the collinear variables `c` (highly correlated with `a`), and `d` (moderately correlated with `a` and `b`).

```{r, message = FALSE, warning = FALSE}
#load collinear and the example data
library(collinear)
data(vi)

#load dplyr for data manipulation
library(dplyr)

#generate the toy data set
set.seed(1)
df <- vi |>
  dplyr::slice_sample(n = 2000) |>
  dplyr::transmute(
    a = soil_clay,
    b = humidity_range
  ) |>
  scale() |>
  as.data.frame() |> 
  dplyr::mutate(
    y = a * 0.75 + b * 0.25 + runif(n = dplyr::n(), min = -0.5, max = 0.5),
    c = a + runif(n = dplyr::n(), min = -0.5, max = 0.5),
    d = (a + b)/2 + runif(n = dplyr::n(), min = -0.5, max = 0.5)
  )
```

Now that we have a toy dataset in which the relationship between all predictors and the response is well understood, let's start with identifying multicollinearity.

# Identifying Multicollinearity

The identification of multicollinearity is facilitated by two complementary methods: *pairwise correlation* and *variance inflation factors*. The former identifies highly correlated pairs of predictors, while the latter identifies predictors that are linear combinations of other predictors. 

## Pairwise correlation

This method involves finding the correlation between all pairs of predictors via `stats::cor()`, either using the Pearson method when the variables are continuous and their relationships are linear, or the Spearman method when relationships are non-linear or the data is ordinal. The function `collinear::cor_df()` returns a data frame of correlations ordered from higher to lower, no matter the sign.

```{r}
correlation_df <- collinear::cor_df(
  df = df,
  predictors = c("a", "b", "c", "d"),
  cor_method = "pearson" #default
)

#more concise, if df only has response and predictors
# correlation_df <- collinear::cor_df(
#   df = df,
#   response = "y"
# )

correlation_df
```

This data frame helps identifying pairs of highly correlated predictors right away. In general, anything above 0.5 or below -0.5 is worth inspecting, but the truth is that the only only consensus about relevant correlation values during a multicollinearity analysis is that there is no consensus at all. No matter what threshold we are looking for, in this case we can at least agree that the correlation between `a` and `c` is high enough to grant an eyebrow raise and decide that there is something wrong with our data. We'll see later what we can do with these pairs of predictors with intermediate correlations.

Pairwise correlations can be reshaped into a correlation matrix and plotted as a dendrogram to further our understanding of the relationships between predictors. 

First, the function `collinear::cor_matrix()` transforms the pairwise correlations data frame into a matrix, and it also can create the matrix directly from the data (see the commented code below).

```{r}
correlation_matrix <- collinear::cor_matrix(
  df = correlation_df
)

#it can also build the correlation matrix directly
# correlation_matrix <- collinear::cor_matrix(
#   df = df,
#   predictors = c("a", "b", "c", "d")
# )

correlation_matrix
```

Now, we convert the absolute values of the correlation matrix to a "dist" object, and subtract the result to 1 to convert pairwise correlations into pairwise distances.

```{r}
correlation_dist <- 1 - correlation_matrix |>
  abs() |>
  as.dist()

correlation_dist
```

Finally, we use `stats::hclust()` to compute a hierarchical clustering of the distance matrix, convert the result to a dendrogram, and plot it.

```{r}
correlation_dist |> 
  hclust(method = "complete") |> 
  as.dendrogram() |> 
  plot(horiz = TRUE)
```

The x axis in the dendrogram above represents the "linkage distance" between predictors, and maps approximately to `1 - correlation`. This one, for example, makes it clear that `a` and `c` have a correlation close to 1, as the node that separates them happens at a linkage distance very close to 0 . 
This kind of plot offers an intuitive view of the relationship between predictors, not only to perform a multicollinearity analysis, but also to obtain domain knowledge about the problem at hand and facilitate model design.

Now, the first step of multicollinearity identification is done. However, multicollinearity can present itself in ways that are slightly more complex to detect than pairwise correlations, and that's where Variance Inflation Factors come into play.

## Variance Inflation Factors

A second form of multicollinearity happens when a predictor is a linear combination of other predictors. A good example in our toy dataset is `d`, that was generated from a linear combination of the predictors `a` and `b`. 



# Managing Multicollinearity

At this point, we know that there is plenty of multicolinearity in our data, and that it can compromise the interpretation of our model. Now we need to somehow manage it to improve model interpretability.

## The old fashioned way

From here, we could use this data frame to identify pairs of highly correlated variables, and remove the one we dislike the most from each pair until the maximum correlation is below some magic number. Something like this:

```{r}
correlation_df |> 
  dplyr::filter(
    y != "a" | y != "a" #removing `a`
  )
```
And so on. I mean, multicollinearity looks much better now, but we made a bold wrong choice by letting our best predictor go, and we still have correlation values that could be too high.

But there is something important to remember here, the VIF of one predictor depends on what other predictors are in the data. If I remove a predictor with a high VIF, the VIF of other predictors will likely change. 

Since `a` has the highest VIF, let's make the logical mistake of removing it to reduce multicollinearity.

```{r}
collinear::vif_df(
  df = df,
  predictors = c("b", "c", "d")
)
```
So, yes, we made a bold choice, removed our most important predictor of `y` without even knowing, an in exchange, multicollinearity looks much better now. To end the job, we can remove `d` now.

```{r}
collinear::vif_df(
  df = df,
  predictors = c("b", "c")
)
```



A few of them are pretty high, and `a`, our most important predictor, has the higher VIF.

## Now

```{r}
preference <- collinear::preference_order(
  df = df,
  response = "y",
  predictors = c("a", "b", "c", "d")
)
preference
```

```{r}
variable_selection <- collinear::collinear(
  df = df,
  response = "y",
  predictors = c("a", "b", "c", "d"),
  preference_order = preference,
  max_cor = 0.5,
  max_vif = 2.5
)
variable_selection
```


Well, that's enough for today! I hope you found this post helpful.

