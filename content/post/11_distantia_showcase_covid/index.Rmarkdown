---
title: "Analyzing Epidemiological Time Series With The R Package {distantia}"
author: ""
date: '2025-01-25'
slug: distantia-showcase-covid
categories: []
tags:
- Rstats
- Dynamic Time Warping
- Data Science
- Time Series Analysis
- Tutorial
subtitle: ''
summary: "Tutorial on the applications of the R package {distantia} to the analysis of epidemiological time series."
authors: [admin]
lastmod: '2025-01-25T07:28:01+01:00'
featured: no
draft: false
image:
  caption: ''
  focal_point: Smart
  margin: auto
projects: []
toc: true
---

<style>
.alert-warning {
  background-color: #f4f4f4;
  color: #333333;
  border-color: #333333;
}
</style>

```{r, include = FALSE}
knitr::opts_chunk$set(
  fig.width = 8, 
  fig.height = 6
)
```


## Summary

In this post I show how the R package [`distantia`](https://blasbenito.github.io/distantia/) can help compare and analyze epidemiological time series. The the example data comprises weekly Covid-19 prevalence curves from 36 California counties between 2020-03-16 and 2023-12-18.

The post is structured as follows:

  - **Data Preparation**: Transform the raw data into a time series list, the format required by `distantia`.

  - **Exploration**: Overview of data features through visualization and descriptive statistics.

  - **Computing Dissimilarity Scores**: Calculate time series dissimilarity using lock-step and dynamic time warping.

  - **Analyzing Dissimilarity Scores**: Explore methods like hierarchical clustering, mapping, and dissimilarity modeling to analyze dissimilarity scores.

<div class="alert alert-warning">
  <strong>DISCLAIMER:</strong>  I am **not** an epidemiologist, so I will be focusing on the technical aspects of the applications of {distantia} to this kind of data, without any attempt to draw conclusions from the results.
</div>

## R packages

This tutorial requires the following packages:

  - [`distantia`](https://blasbenito.github.io/distantia/): time series dissimilarity analysis.
  - [`zoo`](https://cran.r-project.org/web/packages/zoo/index.html): management of regular and irregular time series.
  - [`dplyr`](https://dplyr.tidyverse.org/): data frame processing.
  - [`tidyr`](https://tidyr.tidyverse.org/): to pivot a few wide data frames.
  - [`ggplot2`](https://ggplot2.tidyverse.org/): to make a few charts.
  - [`mapview`](https://r-spatial.github.io/mapview/): interactive spatial visualization.
  - [`reactable`](https://glin.github.io/reactable/): interactive tables
  - [`factoextra`](https://rpkgs.datanovia.com/factoextra/index.html): plotting results of hierarchical clustering.
  - [`visreg`](https://CRAN.R-project.org/package=visreg): response curves of statistical models.

```{r, message = FALSE, warning = FALSE}
library(distantia)
library(zoo)
library(dplyr)
library(ggplot2)
library(mapview)
library(reactable)
library(factoextra)
library(visreg)
```

## Example Data

We will be working with two example data frames shipped with `distantia`: the simple features data frame `covid_counties`, and the data frame `covid_prevalence`.

### `covid_counties`

This [`simple features`](https://r-spatial.github.io/sf/) data frame comprises county polygons and several socioeconomic variables. It is connected to `covid_prevalence` by the column `name`.

```{r, echo = FALSE, cache=TRUE}
la_population <- mapview(
  covid_counties, 
  zcol = "population",
  label = "name"
  )

htmlwidgets::saveWidget(
  la_population@map, 
  file = "la_population.html", 
  selfcontained = TRUE
  )
```

<iframe src="la_population.html" name="LA_Population"
  width="600" height="600" scrolling="auto" frameborder="0">
   <p>California counties represented in the Covid-19 dataset.</p>
</iframe>

```{r, echo = FALSE}
rm(la_population)
```


The socioeconomic variables available in this dataset are:

  - `area_hectares`: county surface.
  - `population`: county population.
  - `poverty_percentage`: population percentage below the poverty line.
  - `median_income`: median county income in dollars.
  - `domestic_product`: yearly domestic product in **b**illions (not a typo) of dollars.
  - `daily_miles_traveled`: daily miles traveled by the average inhabitant.
  - `employed_percentage`: percentage of the county population under employment.
  

```{r, echo = FALSE}
covid_counties |> 
  sf::st_drop_geometry() |> 
  dplyr::transmute(
    name, 
    area_hectares = floor(area_hectares), 
    population, 
    poverty_percentage, 
    median_income, 
    domestic_product = floor(domestic_product), 
    daily_miles_traveled = round(daily_miles_traveled, 1), 
    employed_percentage = round(employed_percentage, 1)
  ) |> 
  reactable::reactable(
    pagination = TRUE,
    searchable = TRUE,
    sortable = TRUE,
    showSortable = TRUE,
    filterable = TRUE,
    resizable = TRUE,
    defaultPageSize = 10,
    showPageSizeOptions = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = TRUE
  )
```

<div class="alert alert-warning">
  <strong>Note:</strong> These variables were included in the dataset because they were easy to capture from on-line sources, not because of their importance for epidemiological analyses..
</div>

### `covid_prevalence`

Long format data frame with weekly Covid-19 prevalence in 36 California counties between 2020-03-16 and 2023-12-18. It has the columns `name`, `time`, and `prevalence`, with the county name, the date of the first day of the week each data point represents, and the maximum Covid-19 prevalence observed during the given week, expressed as proportion of positive tests. 
  
```{r, echo = FALSE}
covid_prevalence |> 
  reactable::reactable(
    pagination = TRUE,
    searchable = TRUE,
    sortable = TRUE,
    showSortable = TRUE,
    filterable = TRUE,
    resizable = TRUE,
    defaultPageSize = 10,
    showPageSizeOptions = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = FALSE
  )
```
<br>

## Data Preparation

Before starting with the analysis, we need to transform the data frame `covid_prevalence` into the format required by `distantia`.

A **time series list**, or `tsl` for short, is a list of [`zoo`](https://cran.r-project.org/web/packages/zoo/index.html) time series representing unique realizations of the same phenomena observed in different sites, times, or individuals. All [data manipulation](https://blasbenito.github.io/distantia/articles/time_series_lists.html) and analysis functions in `distantia` are designed to work with all time series stored in a `tsl` at once. 

The code below applies `tsl_initialize()` to `covid_prevalence` and returns the object `tsl`.

```{r tsl_init, cache=TRUE, cache.path="cache/"}
tsl <- distantia::tsl_initialize(
  x = covid_prevalence,
  name_column = "name",
  time_column = "time"
)
```

Each element in `tsl` is named after the county the data belongs to.

```{r}
names(tsl)
```

The `zoo` objects within `tsl` also have the attribute `name` to help track the data in case it is extracted from the time series list.

```{r}
attributes(tsl[[1]])$name
attributes(tsl[[2]])$name
```

Each individual `zoo` object comprises a time index, extracted with `zoo::index()`, and a data matrix accessed via `zoo::coredata()`.

```{r}
zoo::index(tsl[["Alameda"]]) |> 
  head()
```

```{r}
zoo::coredata(tsl[["Alameda"]]) |> 
  head()
```

## Exploration

In the package `distantia` there are several tools to help you visualize your time series and explore their basic properties.

### Visualization

The function `tsl_plot()` provides a static multipanel visualization of a large number of time series at once.

```{r, fig.height=12}
distantia::tsl_plot(
  tsl = tsl,
  columns = 3,
  guide = FALSE,
  text_cex = 1.2
)
```

If we combine `tsl_subset()` with `tsl_plot()`, we can focus on any particular sub-group of counties and zoom-in over specific time periods.

```{r, fig.height=3}
distantia::tsl_plot(
  tsl = tsl_subset(
    tsl = tsl,
    names = c("Los_Angeles", "Kings"),
    time = c("2021-09-01", "2022-01-31")
  ),
  guide = FALSE
)
```

### Descriptive Stats

Having a good understanding of the time in a time series is important, because details such as length, resolution, time units, or regularity usually define the design of an analysis. Here, the function `tsl_time()` will give us all relevant details about the time in a time series list. 

```{r}
df_time <- distantia::tsl_time(
  tsl = tsl
)
```


```{r, echo = FALSE}
df_time |> 
  dplyr::select(name, class, units, length, resolution, begin, end) |> 
  reactable::reactable(
    pagination = TRUE,
    searchable = TRUE,
    sortable = TRUE,
    showSortable = TRUE,
    filterable = TRUE,
    resizable = TRUE,
    defaultPageSize = 10,
    showPageSizeOptions = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = FALSE
  )

rm(df_time)
```
<br>

In our case all time series are regular and have the same time resolution and range, which makes the table above rightfully boring.

Once acquainted with time, having a summary of the values in each time series also helps make sense of the data, and that's where `tsl_stats()` comes into play.
  
```{r}
df_stats <- distantia::tsl_stats(
  tsl = tsl,
  lags = 1:6 #weeks
)
```

The first part of the output corresponds with common statistical descriptors, such as centrality and dispersion metrics, and higher moments.

```{r, echo=FALSE}
df_stats |> 
  dplyr::select(name, min, q1, median, q3, max, sd, range, iq_range, skewness, kurtosis) |> 
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~ round(.x, 2))) |> 
  reactable::reactable(
    pagination = TRUE,
    searchable = TRUE,
    sortable = TRUE,
    showSortable = TRUE,
    filterable = TRUE,
    resizable = TRUE,
    defaultPageSize = 10,
    showPageSizeOptions = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = FALSE
  )
```
<br>

The second part, produced by the `lags` argument, shows the temporal autocorrelation of the different time series. Given that the resolution of all time series is 7 days per data-point, each time lag represents a week. A value of 0.9 for the lag 1 indicates the Pearson correlation between prevalence values in consecutive weeks.

```{r, echo = FALSE}
df_lags <- df_stats |> 
  select(name, dplyr::contains("lag")) |> 
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~ round(.x, 2))) |> 
  dplyr::rename_with(~ gsub("ac_", "", .)) |> 
  dplyr::rename_with(~ gsub("_", "", .))

reactable::reactable(
  df_lags,
  pagination = TRUE,
  searchable = TRUE,
  sortable = TRUE,
  showSortable = TRUE,
  filterable = TRUE,
  resizable = TRUE,
  defaultPageSize = 10,
  showPageSizeOptions = TRUE,
  striped = TRUE,
  compact = TRUE,
  wrap = FALSE,
  fullWidth = TRUE
)
```
The autocorrelation values for each lag across all counties are summarized in the following boxplot.

```{r, message = FALSE, warning = FALSE, fig.width=5, fig.height=4, echo = FALSE}
df_lags |> 
  tidyr::pivot_longer(
    cols = starts_with("lag"),
    names_to = "week"
  ) |> 
  dplyr::mutate(
    week = as.numeric(gsub("lag", "", week))
  ) |> 
  ggplot() + 
  geom_smooth(
    aes(
      x = week,
      y = value,
      alpha = 0.5
    ),
    col = alpha("gray50", 0.5),
    se = FALSE
  ) +
  geom_boxplot(
    aes(
      x = week,
      y = value,
      group = week
    ),
    notch = TRUE, 
    fill = distantia::color_continuous(n = 6),
    alpha = 0.5
  ) + 
  guides(fill = FALSE, alpha = FALSE) +
  labs(
    x = "Lag (weeks)",
    y = "Autocorrelation (Pearson)"
  ) +
  scale_x_continuous(breaks = 1:6) +
  theme_bw()
```

According to the boxplot, our prevalence time series show a very strong short-term (1-2 weeks) memory, and a rapid diminishment of long-term memory (5-6 weeks), suggesting that the influence of past prevalence on future values dissipates quickly.

<br>

The stats produced by `tsl_stats()` can be joined to `covid_counties` using their common column `name`.

```{r}
sf_stats <- dplyr::inner_join(
  x = covid_counties[, "name"],
  y = df_stats,
  by = "name"
)
```

After the join, we can map any of these stats with `mapview()`. For example, the map below shows the maximum prevalence per county across the full period. 

```{r, eval = FALSE, warning = FALSE}
mapview::mapview(
  sf_stats, 
  zcol = "q3",
  layer.name = "Max prevalence",
  label = "name",
  col.regions = distantia::color_continuous()
  )
```

```{r, echo = FALSE, cache=TRUE, warning = FALSE}
max_prevalence <- mapview(
  sf_stats, 
  zcol = "max",
  label = "name",
  col.regions = distantia::color_continuous()
  )

htmlwidgets::saveWidget(
  max_prevalence@map, 
  file = "max_prevalence.html", 
  selfcontained = TRUE
  )

rm(max_prevalence)
```

<iframe src="max_prevalence.html" name="Max_prevalence"
  width="600" height="600" scrolling="auto" frameborder="0">
   <p>Maximum Covid-19 prevalence in California counties.</p>
</iframe>

If you wish to compute stats for a given period or group of counties, then you can combine `tsl_subset()` and `tsl_stats()`, as we did before.

```{r, eval = FALSE}
df_stats_2021 <- distantia::tsl_stats(
  tsl = tsl_subset(
    tsl = tsl,
    names = c("Napa", "San_Francisco"),
    time = c("2021-01-01", "2021-12-31")
  ),
  lags = 1:6
)
```

```{r, echo = FALSE}
rm(sf_stats, df_lags, df_stats)
```

## Computing Dissimilarity Scores

The package `distantia` was designed to streamline the comparison of time series using two methods: *Lock-Step (LS)* and and [*Dynamic time warping (DTW)*](https://www.blasbenito.com/post/dynamic-time-warping/). In this section I explain both methods in brief, demonstrate how they work, and show how to compute them to compare all our prevalence time series.

### Dynamic Time Warping and Lock-Step Comparison

**LS** directly compares values at the same time points in equal-length time series. It is efficient and interpretable but cannot adjust for differences in outbreak timing, potentially missing relevant time series similarities.

**DTW** aligns time series by warping the time axis to maximize shape similarity. It accounts for differences in outbreak timing, making it useful for uneven-length or misaligned time series. However, it is computationally expensive, harder to interpret, and sensitive to scaling.

Before moving forward, let me show you a small example focused on San Francisco, Napa, and Solano to illustrate how LS and DTW work.

```{r, fig.height=3.5, echo = FALSE}
tsl_smol <- distantia::tsl_subset(
  tsl = tsl,
  names = c("Napa", "Solano", "San_Francisco"),
  time = c("2021-01-01", "2023-01-01")
)

distantia::tsl_plot(
  tsl = tsl_smol, 
  guide = FALSE,
  text_cex = 1.3
  )
```

At first glance, Napa and Solano seem quite well synchronized, while San Francisco shows a timing difference of several months. The table below shows the **LS** comparison between these time series. The `psi` column indicates the dissimilarity between pairs of time series. The smaller the value, the more similar the time series are.

```{r}
tsl_smol |> 
  distantia::distantia_ls() |> 
  dplyr::mutate(
    psi = round(psi, 3)
  ) |> 
  reactable::reactable(
    resizable = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = FALSE
  )
```
<br>

As we could expect, Napa and Solano show the lower `psi` score, indicating that these are the most similar pair in this batch. LS seems to be capturing their synchronicity quite well! On the other hand, San Francisco shows a high dissimilarity with the other two time series, as expected due to its asynchronous pattern. So far so good!

Now, let's take a look at the **DTW** result:

```{r}
tsl_smol |> 
  distantia::distantia_dtw() |> 
  dplyr::mutate(
    psi = round(psi, 3)
  ) |> 
  reactable::reactable(
    resizable = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = FALSE
  )
```
<br>

Surprisingly, when adjusting for time shifts, San Francisco appears more similar to Solano than Napa!

This happens because DTW ignores absolute dates, and allows any sample in a time series to map to multiple samples from the other. That's where the *time warping* happens! The left side of the plot below shows San Francisco’s (dashed red line) first case aligning with a group of Napa samples with prevalence 0, effectively canceling their timing differences.

```{r, fig.height=3.5, echo = FALSE}
library(dtw)
xy_dtw <- dtw::dtw(
  x = tsl_smol$Solano$prevalence,
  y = tsl_smol$San_Francisco$prevalence,
  keep = TRUE
  )

dtw::dtwPlotTwoWay(
  xy_dtw, 
  offset = 0.5,
  xlab = "Sample index",
  ylab = "Prevalence",
  main = "DTW alignment: San Francisco vs. Napa"
  )
```

These results suggest that, in spite of their timing differences, San Francisco, Napa, and Solano have a similar dynamics. This is a conclusion LS alone would miss!

It is for this reason that a DTW analysis is preferable when outbreak timing varies significantly and shape similarity is more important than exact time alignment. On the other hand, LS is best applied when the question at hand is focused on time-series synchronization rather than shape similarity.

```{r, echo = FALSE}
rm(xy_dtw, tsl_smol)
```


### Computing DTW and LS with `distantia`

The most straightforward way of computing DTW and LS in `distantia` involves the functions `distantia_ls()` and `distantia_dtw()`. Both are simplified versions of the function `distantia()`, which has a whole set of bells and whistles that are out of the scope of this post.

The function `distantia_ls()` computes lock-step dissimilarity using euclidean distances by default. It returns a data frame with the columns `x` and `y` with the time series names, and the `psi` score.

```{r}
df_psi_ls <- distantia::distantia_ls(
  tsl = tsl
  )

str(df_psi_ls)
```

The function `distantia_dtw()` returns an output with the same structure. You will notice that it runs a bit slower than `distantia_ls()`.

```{r}
df_psi_dtw <- distantia::distantia_dtw(
  tsl = tsl
  )

str(df_psi_dtw)
```

The table below combines both results to facilitate a direct comparison between both dissimilarity scores.

```{r, echo = FALSE}
df_table <- df_psi_dtw |> 
  dplyr::select(x, y, psi) |> 
  dplyr::inner_join(
    y = dplyr::select(df_psi_ls, x, y, psi),
    by = c("x", "y")
  ) |> 
  dplyr::transmute(
    x, y,
    `psi_dtw` = round(psi.x, 3),
    psi_ls = round(psi.y, 3)
  )

reactable::reactable(
  df_table,
  pagination = TRUE,
  searchable = TRUE,
  sortable = TRUE,
  showSortable = TRUE,
  filterable = TRUE,
  resizable = TRUE,
  defaultPageSize = 10,
  showPageSizeOptions = TRUE,
  striped = TRUE,
  compact = TRUE,
  wrap = FALSE,
  fullWidth = FALSE
)
```

Here goes a scatterplot between the columns `psi_dtw` and `psi_ls`, in case you are wondering how these two metrics are related in our experiment.

```{r, fig.width=5, fig.height=4.5, echo = FALSE}
correlation <- stats::cor(df_table$psi_dtw, df_table$psi_ls)
df_table$difference <- abs(df_table$psi_ls -  df_table$psi_dtw)

ggplot(df_table) + 
  aes(
    x = psi_dtw,
    y = psi_ls,
    color = difference
  ) + 
  geom_point(
    alpha = 0.75
  ) + 
  geom_smooth(
    method = mgcv::gam, 
    col = "gray20",
    formula = y ~ s(x)
    ) + 
  geom_abline(
    intercept = 0, 
    slope = 1,
    col = "gray50",
    lty = 2
  ) + 
  scale_color_gradientn(
    colors = distantia::color_continuous()
  ) +
  labs(
    title = paste("Pearson Correlation between DTW and LS:", round(correlation, 2)),
    x = "Psi score - Dynamic Time Warping",
    y = "Psi score - Lock-Step",
    color = "Difference\nLS - DTW"
  ) +
  theme_bw()
```
The chart shows that there is good correlation between the `psi` scores for DTW and LS, so in principle, using one or the other might not be *that* important, as they may lead to similar conclusions. However, notice all these cases with a warmer color in the upper-left corner. These represent pairs of time series with a large dissimilarity mismatch between DTW and LS. Remember San Francisco *versus* Napa? If these cases are important to your research question, then DTW is the method of choice. Otherwise, LS is a reasonable alternative.

In the end, the research question must guide the methodological choice, but it is up to us to understand the consequences of such choice.

```{r, echo = FALSE}
rm(df_table, correlation)
```


## Analyzing Dissimilarity Scores

In this section we will have a look at the different kinds of analyses implemented in `distantia` that may help better understand how time series are related. For the sake of simplicity, I will focus on the DTW results alone. If you wish to use the LS results instead, just replace `df_psi_dtw` with `df_psi_ls` in the code below. 

```{r}
df_psi <- df_psi_dtw
```

```{r, echo = FALSE}
rm(df_psi_dtw, df_psi_ls)
```


### Summary Stats

The functions `distantia_stats()` and `distantia_boxplot()` provide alternative summarized representations of dissimilarity by aggregating `psi` scores across all time series.

```{r}
df_psi_stats <- distantia::distantia_stats(
  df = df_psi
)
```

```{r, echo = FALSE}
reactable::reactable(
  df_psi_stats |>  
    mutate_if(is.numeric, round, 3),
  pagination = FALSE,
  sortable = TRUE,
  showSortable = TRUE,
  filterable = TRUE,
  resizable = TRUE,
  defaultPageSize = nrow(df_psi_stats),
  showPageSizeOptions = TRUE,
  striped = TRUE,
  compact = TRUE,
  wrap = FALSE,
  fullWidth = TRUE
)
```
<br>

The boxplot generated by `distantia_boxplot()` serves as a quick visual summary of the table above.

```{r, fig.height=8, warning = FALSE, fig.width=5}
distantia::distantia_boxplot(
  df = df_psi,
  text_cex = 1.4
)
```
According to the boxplot, San Bernardino is the county with a higher overall dissimilarity with all others. On the other hand, at the bottom of the boxplot, Santa Barbara seems to be the most similar to all others.

```{r, echo = FALSE}
rm(df_psi_stats)
```


### Hierarchical Clustering

The functions `distantia_cluster_hclust()` and `distantia_cluster_kmeans()` are designed to apply hierarchical and K-means clustering to the data frames produced by `distantia_dtw()` and `distantia_ls()`. In this section we will be focusing on hierarchical clustering.

By default, the function `distantia_cluster_hclust()` selects the clustering method (see argument `method` in `help(hclust)`) and the number of groups in the clustering solution via silhouette-width maximization (see [`distantia::utils_cluster_silhouette()`](https://blasbenito.github.io/distantia/reference/utils_cluster_silhouette.html) and [`distantia::utils_cluster_hclust_optimizer()`](https://blasbenito.github.io/distantia/reference/utils_cluster_hclust_optimizer.html) for further details).

```{r}
psi_cluster <- distantia::distantia_cluster_hclust(
  df = df_psi
)

names(psi_cluster)
```

The output is a list with several components, but these are the key ones:

  - `df`: data frame with time series names, cluster assignation, and silhouette width. 
  - `cluster_object`: the output of `stats::hclust()`.
  - `clusters`: the number of distinct clusters selected by the algorithm.

The object `df` has the column `silhouette_width`, in the range [-1, 1], which informs about how well each case fits within its assigned cluster:

  - `<0`: misclassified.
  - `≈0`: near decision boundary.
  - `≈1`: well-clustered.
  
```{r, echo = FALSE}
reactable::reactable(
  psi_cluster$df |>  
    mutate(silhouette_width = round(silhouette_width, 3)),
  pagination = FALSE,
  sortable = TRUE,
  showSortable = TRUE,
  filterable = TRUE,
  resizable = TRUE,
  defaultPageSize = nrow(psi_cluster$df),
  showPageSizeOptions = TRUE,
  striped = TRUE,
  compact = TRUE,
  wrap = FALSE,
  fullWidth = FALSE
)
```
<br>

If you arrange the table above by `silhouette_width`, you will notice that "Santa Cruz", has a silhouette score of `r round(psi_cluster$df$silhouette_width[which(psi_cluster$df$name == "Santa_Cruz")], 3)`, which highlights it as the county with the most dubious cluster membership.

The cluster object can be plotted as a dendrogram with `factoextra::fviz_dend()`, using the number of clusters in the object `psi_cluster$clusters` as guide to colorize the different branches.

```{r, fig.height=10, warning = FALSE}
#number of clusters
k <- psi_cluster$clusters

factoextra::fviz_dend(
  x = psi_cluster$cluster_object, 
  k = k, 
  k_colors = distantia::color_discrete(n = k),
  cex = 1,
  label_cols = "gray20",
  rect = TRUE,  
  horiz = TRUE,
  main = "",
  ylab = "Dissimilarity"
  )
```

Mapping these groups may be helpful for the ones like me not fully acquainted with the distribution of counties in California. To get there we first need to join `psi_cluster$df` to the spatial data frame `covid_counties`.

```{r}
sf_cluster <- dplyr::inner_join(
  x = covid_counties[, "name"],
  y = psi_cluster$df,
  by = "name"
)
```

Now we can use `mapview::mapview()` again to plot cluster membership, but with a twist: we are going to code the silhouette score of each case as polygon transparency, with higher transparency indicating a lower silhouette width. This will help identify these cases that might not fully belong to their assigned groups. 

Notice that to do so, the code below applies `distantia::f_rescale_local()` to rescale silhouette width between 0.1 and 1 to adjust it to what the argument `alpha.regions` expects.

```{r, eval = FALSE, warning = FALSE}
mapview::mapview(
  sf_cluster, 
  zcol = "cluster",
  layer.name = "Cluster",
  label = "name",
  col.regions = distantia::color_discrete(n = k),
  alpha.regions = distantia::f_rescale_local(
    x = sf_cluster$silhouette_width, 
    new_min = 0.1
    )
  )
```



```{r, echo = FALSE, cache=TRUE, warning = FALSE}
clustering <- mapview::mapview(
  sf_cluster, 
  zcol = "cluster",
  layer.name = "Cluster",
  label = "name",
  col.regions = distantia::color_discrete(n = k),
  alpha.regions = distantia::f_rescale_local(
    x = sf_cluster$silhouette_width, 
    new_min = 0.1
    ),
  legend.position = "upperleft"
  )

htmlwidgets::saveWidget(
  clustering@map, 
  file = "clustering.html", 
  selfcontained = TRUE
  )

rm(max_prevalence, clustering, sf_cluster, k)
```

<iframe src="clustering.html" name="Covid_clustering"
  width="600" height="600" scrolling="auto" frameborder="0">
   <p>California counties grouped by similarity in their Covid-19 prevalence curves.</p>
</iframe>

The map shows something the dendrogram could not: the two groups of counties are geographically coherent! Additionally, the map shows that Santa Cruz, the county with a more dubious cluster membership, is surrounded by counties from the other cluster.


### Dissimilarity Modelling

Dissimilarity scores reveal the degree of interrelation between time series but do not explain the reasons behind their dissimilarity. 

To fill this gap, in this section I show how dissimilarity scores and geographic and socioeconomic variables can be combined to fit models  that may help unveil relevant drivers of dissimilarity between epidemiological time series. 

#### Building a Model Frame

The function [`distantia_model_frame()`](https://blasbenito.github.io/distantia/reference/distantia_model_frame.html) builds data frames for model fitting. In the code below we use it to combine the dissimilarity scores stored in `df_psi` with socioeconomic count variables stored in `covid_counties`.

```{r}
df_model <- distantia::distantia_model_frame(
  response_df = df_psi,
  predictors_df = covid_counties,
  scale = FALSE
)
```

Let's take a look at the output.

```{r, echo=FALSE}
df_model |> 
  dplyr::mutate(
    dplyr::across(
      dplyr::where(is.numeric), ~ round(.x, 2)
      )
    ) |> 
  reactable::reactable(
    pagination = TRUE,
    searchable = TRUE,
    sortable = TRUE,
    showSortable = TRUE,
    filterable = TRUE,
    resizable = TRUE,
    defaultPageSize = 10,
    showPageSizeOptions = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = TRUE
  )
```
<br>

The model frame `df_model` has the following components:

  - Time series identifiers: in this case, county names in the columns `x` and `y`. These columns are not used during model fitting.
  - Response variable: column `psi` from `df_psi`, representing time series dissimilarity.
  - Predictors: the columns with socioeconomic data in `covid_counties` are transformed into absolute differences between each pair of counties. For example, the value of the column `area_hectares` for the counties Sacramento and Sonoma is `r df_model$area_hectares[which(df_model$x == "Sacramento" & df_model$y == "Sonoma")]` hectares. This is the absolute difference between the area of Sacramento (`r covid_counties[["area_hectares"]][which(covid_counties$name == "Sacramento")]` hectares) and the area of Sonoma (`r covid_counties[["area_hectares"]][which(covid_counties$name == "Sonoma")]` hectares).
  - Geographic distance: the column `geographic_distance`, not in `covid_counties`, represents the distance between county limits. It is computed when the input of the argument `predictors_df` is spatial data frame.

The function `distantia_model_frame()` has anotehr neat feature: it can combine different predictors into a new one! 

In the example below, the predictors `poverty_percentage`, `median_income`, `employed_percentage`, `daily_miles_traveled` and `domestic_product` are scaled, and their multivariate distances between pairs of counties are computed and stored in the column `economy`. This new variable represents overall differences in economy between counties. 

Notice that the code below applies log transformations to the county areas and populations, and square root transformation to the geographic distances between counties. Finally, it also scales the predictors, because in `distantia` version <= 2.0.2 the argument `scale` of `distantia_model_frame()` does not work as it should (this issue is fixed in version 2.0.3).

```{r}
df_model <- distantia::distantia_model_frame(
  response_df = df_psi,
  predictors_df = covid_counties |> 
    dplyr::mutate(
      area = log(area_hectares),
      population = log(population)
    ),
  composite_predictors = list(
    economy = c(
      "poverty_percentage",
      "median_income",
      "domestic_product",
      "employed_percentage",
      "daily_miles_traveled"
    )
  ),
  #scale = TRUE is broken in version <= 2.0.2
  scale = FALSE 
  ) |> 
  #selecting and renaming columns
  dplyr::transmute(
    x,
    y,
    psi, 
    economy, 
    population,
    area,
    distance = sqrt(geographic_distance)
  ) |> 
  #scale
  dplyr::mutate(
    dplyr::across(
      dplyr::all_of(c("economy", "distance", "population", "area")), 
      ~ as.numeric(scale(.x))
    )
  )
```

```{r, echo=FALSE}
df_model |> 
  dplyr::mutate(
    dplyr::across(
      dplyr::where(is.numeric), ~ round(.x, 2)
      )
    ) |> 
  reactable::reactable(
    pagination = TRUE,
    searchable = TRUE,
    sortable = TRUE,
    showSortable = TRUE,
    filterable = TRUE,
    resizable = TRUE,
    defaultPageSize = 10,
    showPageSizeOptions = TRUE,
    striped = TRUE,
    compact = TRUE,
    wrap = FALSE,
    fullWidth = TRUE
  )
```
<br>

Now, with the model frame at hand, we are ready to better understand how the dissimilarity between prevalence time series changes as a function of differences between counties in distance, area, economy, and population.

#### Fitting a Linear Model

Since our model frame  shows no multicollinearity between predictors (you can check it with `cor(df_model[, -1])`), it can be used right away to fit a multiple regression model.

```{r}
m <- stats::lm(
  formula = psi ~ distance + area + economy + population,
  data = df_model
)
```

<br>

The model has an R-squared of `r round(summary(m)$r.squared, 2)`, which is *not great, not terrible*. 

```{r, echo = FALSE}
m |> 
  summary() |> 
  broom::tidy() |> 
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~ round(.x, 4))) |> 
  dplyr::arrange(dplyr::desc(estimate)) |> 
  dplyr::filter(term != "(Intercept)") |> 
  dplyr::select(term, estimate, std.error, p.value) |> 
  reactable(
    striped = TRUE,
    fullWidth = TRUE,
    resizable = TRUE,
    sortable = TRUE,
    showSortable = TRUE
    )
```
<br>

```{r, fig.height=6, fig.width=6, echo = FALSE}
par(mfrow = c(2, 2), cex.axis = 1.5, cex.lab = 2)
line_params <- list(col = "red4", lwd = 1)
visreg::visreg(
  fit = m,
  xvar = "distance",
  line = line_params
)
visreg::visreg(
  fit = m,
  xvar = "area",
  line = line_params
)
visreg::visreg(
  fit = m,
  xvar = "economy",
  line = line_params
)
visreg::visreg(
  fit = m,
  xvar = "population",
  line = line_params
)
```

The coefficients table and the effects plot support the idea that all predictors have a positive relationship with dissimilarity between prevalence time series. The predictor `distance` has the strongest effect, which is almost trhee times the effect `economy` and area, and more than six times the effect of `population`.

The scatterplot below confronts the observations versus the model predictions. This kind of plot is known as *calibration plot*, and helps assess prediction bias in a model.

```{r, fig.width=3.5, fig.height=3.5}
df_model$psi_predicted <- stats::predict(object = m)

ggplot(df_model) + 
  aes(
    x = psi,
    y = psi_predicted
  ) + 
  geom_point(alpha = 0.5, col = "gray50") + 
  coord_fixed(
    xlim = range(c(df_model$psi, df_model$psi_predicted)),
    ylim = range(c(df_model$psi, df_model$psi_predicted))
  ) + 
  geom_smooth(
    method = "lm", 
    col = "red4",
    formula = y ~ x
    ) + 
  geom_abline(
    intercept = 0, 
    slope = 1,
    col = "black",
    lty = 2
  ) +
  labs(
    x = "Observed psi",
    y = "Predicted psi",
    title = "Calibration plot - Linear Model"
  ) +
  theme_bw()
```


The calibration plot shows that our model underpredicts high and overpredicts low dissimilarity values. This kind of bias indicates that there are relevant predictors, interactions, or transformations missing from the model. At this point we could look for more meaningful variables, we could disaggregate the variable `economy`, or maybe decide that this linear model does not fit our purpose and try with a different modelling method.

In any case, we are going to stop the modelling exercise here, because it could go on for a while, and this post is freaking long already. Whatever comes next is up to you now!

## Closing Thoughts

In this rather long post I've gone through the main functionalities in `distantia` that are helpful to better understand how univariate epidemiological time series are related and to explore drivers of dissimilarity. However, there are other utilities in `distantia` that may be useful in this field, such as *network plots*, which are described in the post [Mapping Time Series Dissimilarity](https://blasbenito.github.io/distantia/articles/mapping_dissimilarity.html), and time-delay analysis, as implemented in the function [`distantia_time_delay()`](https://blasbenito.github.io/distantia/reference/distantia_time_delay.html). I hope I'll have time to combine these ideas in a new post soon.

And that's all folks, I hope you got something to take home from this tutorial!

BMB