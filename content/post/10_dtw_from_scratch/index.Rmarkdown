---
title: "Coding a Minimalistic Dynamic Time Warping Library with R"
author: ''
date: '2025-01-13'
slug: dynamic-time-warping-from-scratch
categories: []
tags: [Rstats]
subtitle: ''
summary: 'Tutorial on how to implement dynamic time warping in R'
authors: [admin]
lastmod: '2025-01-13T05:14:20+01:00'
featured: yes
draft: false
image:
  caption: Dynamic time warping represented as a landscape.
  focal_point: Smart
  margin: auto
projects: []
---

```{r, echo = FALSE, eval = FALSE}
install.packages("distantia")
install.packages("gridExtra")
options(max.print = 100)
knitr::opts_chunk$set(
  fig.width = 7, fig.height = 6
)
options(digits = 8)
```


# Summary

This post walks you through the implementation of a minimalistic yet fully functional [Dynamic Time Warping](https://www.blasbenito.com/post/dynamic-time-warping/) (DTW) library in R, built entirely from scratch without dependencies or complex abstractions. While there are many [open-source DTW implementations](https://blasbenito.github.io/distantia/articles/dtw_applications.html) readily available, understanding the inner workings of the algorithm can be invaluable. Whether youâ€™re simply curious or need a deeper grasp of DTW for your projects, this step-by-step guide offers a hands-on approach to demystify the method.

# Design

## Example Data

Having good example data at hand is a must when developing new code. For this tutorial we use a subset of two multivariate time series of temperature, rainfall, and normalized vegetation index. To facilitate our development, the time series, named `zoo_germany` and `zoo_sweden`, are stored as objects of the class [zoo](https://CRAN.R-project.org/package=zoo), which is a very robust time series management library.

```{r, echo = FALSE, message=FALSE}
library(distantia, quietly = TRUE)
library(zoo, quietly = TRUE)

tsl <- tsl_initialize(
  x = fagus_dynamics,
  name_column = "name",
  time_column = "time"
) |> 
  tsl_subset(
    time = c("2010-01-01", "2011-01-01")
  ) |> 
  tsl_colnames_set(
    names = c("evi", "rain", "temp")
  )

zoo_sweden <- tsl$Sweden
zoo_germany <- tsl$Germany
zoo_spain <- tsl$Spain

rm(tsl)
```

```{r, echo = FALSE, fig.height=3.5}
graphics::layout(matrix(1:2, ncol = 2))

plot(
  x = zoo_sweden, 
  col = "red4",
  mar = c(0.5, 5, 0, 5)
  )

plot(
  x = zoo_germany, 
  col = "red4",
  mar = c(0.5, 5, 0, 5)
  )

plot(
  x = zoo_spain, 
  col = "red4",
  mar = c(0.5, 5, 0, 5)
  )
```

Each zoo object has a *core data* of the class `matrix` with one observation per row and one variable per column, and an *index*, which is a vector of dates, one per row in the core data.

```{r}
zoo::coredata(zoo_sweden)
zoo::index(zoo_sweden)
```


## Required Library Functions

The section *DTW Step by Step* from the previous article [A Gentle Intro to Dynamic Time Warping](https://www.blasbenito.com/post/dynamic-time-warping/) describes the computational steps required by the algorithm. Below, these steps are broken down into sub-steps that will correspond to specific functions in our library:

**Time Series Pre-processing** 

These steps help DTW work seamlessly with the input time series.

  - **Linear detrending**: forces time series to be stationary by removing any upwards or downwards trends. 
  - **Z-score normalization**: equalizes the range of the time series to ensure that the different variables contribute evenly to the distance computation.

**Dynamic Time Warping*

These steps perform dynamic time warping and evaluate the similarity between the time series.

  - **Multivariate distance**: compute distances between pairs of samples from each time series.
  - **Distance matrix**: organize the multivariate distances in a matrix in which each axis represents a time series.
  - **Cost matrix**: this matrix accumulates the distances in the distance matrix across time and represents all possible alignments between two time series.
  - **Least-cost path**: path in the cost matrix that minimizes the overall distance between two time series.
  - **Dissimilarity metric**: value to summarize the similarity/dissimilarity between time series.
  
**Main function**

Once all the steps above are implemented in their respective functions, we will wrap all them in a single function to streamline the DTW analysis.
  
# Implementation

In this section we will be developing the library function by function. Remember that the content of each code chunk should be added to the file `mini_dtw.R`.

## Time Series Pre-processing

In this section we create the function `ts_preprocessing()`, which will prepare the time series data for dynamic time warping. This function contains two functionalities: linear detrending, and z-score normalization.
    
### Linear Detrending

Applying linear detrending to a multivariate time series involves computing a linear model of each variable against time, and subtracting the the model prediction to the original data. This operation only requires two steps: 

First, the function `stats::lm()` can be applied to all variables in one of our time series at once

```{r}
model_sweden <- stats::lm(
  formula = zoo_sweden ~ stats::time(zoo_sweden)
  )

model_sweden
```

Second, the residuals of the linear model, which represent the differences between the prediction and the observed data, correspond exactly with the detrended time series. As a plus, these residuals are returned as a zoo object when the `zoo` library is loaded.

```{r}
stats::residuals(model_sweden)
```

```{r, echo = FALSE, fig.height=3.5}
plot(
  x = stats::residuals(model_sweden), 
  col = "red4",
  mar = c(0.5, 5, 0, 5),
  main = "Detrended zoo_sweden"
  )
```
Then, the first function of our library could be something like this:

```{r}
#' Linear Detrending
#' @param x (required, zoo object) time series to detrend.
#' @return zoo object
ts_preprocessing <- function(x){
  m <- stats::lm(formula = x ~ stats::time(x))
  y <- stats::residuals(object = m)
  y
}
```

This function could be more concise, but it is written to facilitate line-by-line debugging instead. I have also added minimal roxygen documentation. Future me usually appreciates this kind of extra effort.

This function should check that `x` is really a zoo object, and any other condition that would make it fail. However, to keep code simple, in this tutorial we won't do any error catching.

Ok, we can now test the new function:

```{r}
ts_preprocessing(x = zoo_germany)
```

```{r, echo = FALSE, fig.height=3.5}
plot(
  x = ts_preprocessing(x = zoo_germany), 
  col = "red4",
  mar = c(0.5, 5, 0, 5),
  main = "Detrended zoo_germany"
  )
```
The effect of our new function is not very noticeable because none of our time series have long-term trends, but let's try something different to check that our function actually detrends time series. The code below creates a mock-up time series with an ascending trend.

```{r}
x <- zoo::zoo(0:10)
```

```{r, fig.height=2.5, echo = FALSE}
plot(
  x = x, 
  col = "red4",
  mar = c(0.5, 1, 0, 1)
  )
```

If we apply `ts_preprocessing()` to this time series, the result shows a horizontal line, which is a perfect linear detrending. Now we can be sure our implementation works!

```{r, fig.height=3}
x_detrended <- ts_preprocessing(x = x)
```

```{r, fig.height=2.5, echo = FALSE}
plot(
  x = x_detrended, 
  col = "red4",
  ylim = range(x),
  mar = c(0.5, 5, 0, 5)
  )
```

```{r, echo = FALSE}
#cleanup
rm(model_sweden, x, x_detrended)
```


### Z-score Normalization

Normalization consists of two operations: 

  - **Centering**: performed by subtracting the column mean to each case, and results in a column mean equal to zero. 
  - **Scaling**: divides each case by the standard deviation of the column, resulting in a column standard deviation equal to one.

The R base function `scale()` implements z-score normalization, so there's not much we have to do from scratch here. Also, when the library `zoo` is loaded, the method `zoo:::scale.zoo` (`:::` denotes methods and functions that are not exported) allows `scale()` to work seamlessly with zoo objects.

```{r}
scale(
  x = zoo_germany,
  center = TRUE,
  scale = TRUE
  )
```

Normalization can be easily added to `ts_preprocessing()`:

```{r}
#' Linear Detrending and Normalization
#' @param x (required, zoo object) time series to detrend.
#' @return zoo object
ts_preprocessing <- function(x){
  m <- stats::lm(formula = x ~ stats::time(x))
  y <- stats::residuals(object = m)
  z <- scale(y)
  z
}
```

We can now test `ts_preprocessing()` and move forward with our implementation.

```{r}
ts_preprocessing(x = zoo_germany)
```

```{r, echo = FALSE, fig.height=3.5}
plot(
  x = ts_preprocessing(x = zoo_germany), 
  col = "red4",
  mar = c(0.5, 5, 0, 5)
  )
```

## Dynamic Time Warping Functions

This section describes the implementation of the DTW algorithm, which requires functions to compute a distance matrix, convert it to a cost matrix, find a least-cost path maximizing the alignment between the time series, and compute their similarity.

### Distance Matrix

In DTW, a distance matrix represents all the distances between all pairs of samples in two time series. Hence, each time series is represented in one axis of the matrix. But before getting there, we need a function to obtain the distance between arbitrary pairs of rows from two separate zoo objects.

#### Distance Function

Let's say we have two vectors, `x` with one row of `zoo_germany`, and `y` with one row of `zoo_sweden`. Then, the expression to compute the Euclidean distances `x` and `y` is `sqrt(sum((x-y)^2))`. From there, implementing the distance function seems pretty trivial.

```{r}
#' Euclidean Distance
#' @param x (required, numeric) row of a zoo object.  
#' @param y (required, numeric) row of a zoo object.
#' @return numeric
distance_euclidean <- function(x, y){
  sqrt(sum((x - y)^2))
}
```

Notice that the function does not indicate the return explicitly. Since the function's body is a one-liner, one cannot be really worried about the function returning something unexpected. Also, implementing such a simple expression in a function might seem like too much, but it may facilitate the addition of new distance metrics to the library in the future. For example, we could create something like `d_manhattan()` with the Manhattan distance, and later switch between one or another depending on the user's needs.

The code below tests the function by computing the euclidean distance between the row 1 from `zoo_sweden` and the row 2 from `zoo_germany`.

```{r}
zoo_sweden[1, ]
zoo_germany[2, ]

distance_euclidean(
  x = zoo_sweden[1, ],
  y = zoo_germany[2, ]
)
```

What? That doesn't seem right! For whatever reason, `zoo_sweden[1, ]` and `zoo_germany[2, ]` are not being interpreted as numeric vectors by `distance_euclidean()`. Let's try something different:

```{r}
distance_euclidean(
  x = as.numeric(zoo_sweden[1, ]),
  y = as.numeric(zoo_germany[2, ])
)
```
Ok, that makes more sense! Then, we just have to move these `as.numeric()` inside `distance_euclidean()` to simplify the usage of the function:

```{r}
#' Euclidean Distance
#' @param x (required, numeric) row of a zoo object.  
#' @param y (required, numeric) row of a zoo object.
#' @return numeric
distance_euclidean <- function(x, y){
  x <- as.numeric(x)
  y <- as.numeric(y)
  z <- sqrt(sum((x - y)^2))
  z
}
```

The new function should have no issues returning the right distance between these rows now:

```{r}
distance_euclidean(
  x = zoo_sweden[1, ],
  y = zoo_germany[2, ]
)
```

That was kinda bumpy, but we can move on and go compute the distance matrix now.

#### Distance Matrix

To generate the distance matrix, the function `distance_euclidean()` must be applied to all pairs of rows in the two zoo objects. A simple yet inefficient way to do this involves creating an empty matrix, and traversing it cell by cell to compute the euclidean distances between the corresponding pair of rows.

```{r}
#empty distance matrix
m_dist <- matrix(
  data = NA, 
  nrow = nrow(zoo_germany), 
  ncol = nrow(zoo_sweden)
)

#iterate over rows
for(row in 1:nrow(zoo_germany)){
  
  #iterate over columns
  for(col in 1:nrow(zoo_sweden)){
    
    #distance between time series rows
    m_dist[row, col] <- distance_euclidean(
      x = zoo_germany[row, ],
      y = zoo_sweden[col, ]
    )
    
  }
}
```

This code generates a matrix with `zoo_germany` in the rows, from top to bottom, and `zoo_sweden` in the columns, from left to right. The first five rows and columns are shown below.

```{r}
m_dist[1:5, 1:5]
```

This matrix can be plotted with the function `graphics::image()`, but please be aware that it rotates the distance matrix 90 degrees counter clock-wise, which can be pretty confusing at first. Remember this: **in the matrix plot, the x axis represents the matrix rows**.

```{r}
graphics::image(
  x = seq_len(nrow(m_dist)),
  y = seq_len(ncol(m_dist)),
  z = m_dist,
  xlab = "zoo_germany",
  ylab = "zoo_sweden",
  main = "Euclidean Distance"
  )
```

Darker values in the plot above indicate larger distances between pairs of samples in each time series.

We can now wrap the code above (without the plot) in a new function named `distance_matrix()`.

```{r}
#' Distance Matrix Between Time Series
#' @param a (required, zoo object) time series.
#' @param b (required, zoo object) time series with same columns as `x`
#' @return matrix
distance_matrix <- function(a, b){
  
  m <- matrix(
    data = NA, 
    nrow = nrow(b), 
    ncol = nrow(a)
  )
  
  for (row in 1:nrow(b)) {
    for (col in 1:nrow(a)) {
      m[row, col] <- distance_euclidean(
        x = a[col, ],
        y = b[row, ] 
      )
    }
  }
  
  m
  
}
```

Let's run a little test before moving forward!

```{r}
m_dist <- distance_matrix(
  a = zoo_germany,
  b = zoo_sweden
)

m_dist[1:5, 1:5]
```

We are good to go! The next function will transform this distance matrix into a *cost matrix*.

```{r, echo = FALSE}
#cleanup
rm(col, row)
```


### Cost Matrix

Now we are getting into the important parts of the DTW algorithm! 

A cost matrix is like a valley's landscape, with hills in regions where the time series are different, and ravines where they are more similar. Such landscape is built by accumulating the values of the distance matrix cell by cell, from `[1, 1]` at the bottom of the valley (upper left corner of the matrix, but lower left in the plot), to `[m, n]` at the top (lower right corner of the matrix, upper right in the plot).

Let's see how that works.

First, we use the dimensions of the distance matrix to create an empty cost matrix.

```{r, warning = FALSE}
m_cost <- matrix(
  data = NA, 
  nrow = nrow(m_dist), 
  ncol = ncol(m_dist)
  )
```

Second, to initialize the cost matrix we accumulate the values of the first row and the first column of the distance matrix using `cumsum()`. This step is very important for the second part of the algorithm, as it provides the starting values.

```{r}
m_cost[1, ] <- cumsum(m_dist[1, ])
m_cost[, 1] <- cumsum(m_dist[, 1])
```


```{r, echo = FALSE}
graphics::image(
  x = seq_len(nrow(m_cost)),
  y = seq_len(ncol(m_cost)),
  z = m_cost,
  xlab = "zoo_germany",
  ylab = "zoo_sweden",
  main = "Cost Matrix (work in progress)"
  )
```

Now, before going into the third step, let's focus for a moment on the next cell of the cost matrix we need to fill, with coordinates `[2, 2]` and value `NA`.

```{r}
m_cost[1:2, 1:2]
```

The new value of this cell results from the addition of:

  - Its value in the distance matrix `m_dist` (`r round(m_dist[2, 2], 2)`).
  - The minimum accumulated distance of its neighbors, which are:
      - Upper neighbor with coordinates `[1, 2]`.
      - Left neighbor with coordinates `[2, 1]`. 
      - Upper-left neighbor with coordinates `[1, 1]`.

The general expression to find the value of the empty cell is shown below. It uses `min()` to get the value of the *smallest* neighbor, and then adds it to the vaue of the target cell in the distance matrix.

```{r}
m_cost[2, 2] <- min(
  m_cost[1, 2], 
  m_cost[2, 1]
  ) + m_dist[2, 2]

m_cost[1:2, 1:2]
```

But there are many cells to fill yet!

```{r, echo = FALSE}
graphics::image(
  x = seq_len(nrow(m_cost)),
  y = seq_len(ncol(m_cost)),
  z = m_cost,
  xlab = "zoo_germany",
  ylab = "zoo_sweden",
  main = "Cost Matrix (work in progress)"
  )
```

The expression we used to fill the cell `m_cost[2, 2]` can be generalized to fill all remaining empty cells. We just have to wrap it in a nested loop that for each new empty cell identifies the smallest neighbor in the x and y axies, and adds its cumulative cost to the distance of new cell.

```{r}
#iterate over rows of the cost matrix
for(row in 2:nrow(m_dist)){
  
  #iterate over columns of the cost matrix
  for(col in 2:ncol(m_dist)){
    
    #get cost of neighbor with minimum accumulated cost
    min_cost <- min(
      m_cost[row - 1, col], 
      m_cost[row, col - 1]
      )
    
    #add it to the distance of the target cell
    new_value <- min_cost + m_dist[row, col]
    
    #fill the empty cell with the new value
    m_cost[row, col] <- new_value
    
  }
}
```

Running the code above results in a nicely filled cost matrix!

```{r, echo = FALSE}
graphics::image(
  x = seq_len(nrow(m_cost)),
  y = seq_len(ncol(m_cost)),
  z = m_cost,
  xlab = "zoo_germany",
  ylab = "zoo_sweden",
  main = "Cost Matrix"
  )
```

Still, there is one more step left! For a reason that will remain unclear until the next section, the terminal cell `[13, 13]` of the cost matrix must have a higher value than any pf its immediate neighbors. If we look at the lower left corner (upper left in the plot) of our cost matrix, this is not the case yet.

```{r}
m <- nrow(m_cost)
n <- ncol(m_cost)
m_cost[(m-1):m, (n-1):n]
```

To fix this issue, it is customary to sum the cost of the first cell of the cost matrix, `m_cost[1, 1]`, to the last one `m_cost[m, n]`.

```{r}
m_cost[m, n] <- m_cost[m, n] + m_cost[1, 1]

m_cost[(m-1):m, (n-1):n]
```

Now that we have all the pieces figured out, we can define our new function to compute the cost matrix. Notice that the code within the nested loops is slightly more concise than shown before.

```{r}
#' Cost Matrix from Distance Matrix
#' @param distance_matrix (required, matrix) distance matrix.
#' @return matrix
cost_matrix <- function(distance_matrix){
  
  m <- matrix(
    data = NA, 
    nrow = nrow(distance_matrix), 
    ncol = ncol(distance_matrix)
  )
  
  m[1, ] <- cumsum(distance_matrix[1, ])
  m[, 1] <- cumsum(distance_matrix[, 1])
  
  for(row in 2:nrow(distance_matrix)){
    for(col in 2:ncol(distance_matrix)){
      
      m[row, col] <- min(
        m[row - 1, col], 
        m[row, col - 1]
      ) + distance_matrix[row, col]
      
    }
  }
  
  m[row, col] <- m[row, col] + m[1, 1]
  
  m
  
}
```

Let's test our new function using `m_dist` as input!

```{r}
m_cost <- cost_matrix(distance_matrix = m_dist)
```

```{r, echo = FALSE}
graphics::image(
  x = seq_len(nrow(m_cost)),
  y = seq_len(ncol(m_cost)),
  z = m_cost,
  xlab = "zoo_germany",
  ylab = "zoo_sweden",
  main = "Cost Matrix"
  )
```

So far so good! We can now dive into the generation of the least-cost path.

```{r, echo = FALSE}
#cleanup
rm(col, row, m, min_cost, n, new_value, row)
```

### Least-Cost Path

If we describe the cost matrix as a valley with its hills and ravines, then the least-cost path is the river flowing all its way to the bottom following the line of maximum slope. Following the analogy, the least-cost path starts in the terminal cell of the cost matrix (`[13, 13]`), and ends in the first cell.

To find the least-cost path we first define a data frame with the coordinates of the terminal cell in the cost matrix.

```{r}
path <- data.frame(
  row = ncol(m_cost),
  col = nrow(m_cost)
)

path
```

This is the first step of the least cost path. Now, there are two alternative new steps to consider:

  - *one column left*: `[row, col - 1]`.
  - *one row up*: `[row - 1, col]`.
  
But before moving forward, notice that if we apply these steps indefinitely in our cost matrix, at some point a move up `row - 1` or left `col - 1` will go out of bounds and produce an error. That's why it's safer to define the next move as...

  - *one column left*: `[row, max(col - 1, 1)]`.
  - *one row up*: `[max(row - 1, 1), col]`


...which confines all steps within the first row and column of the cost matrix. 

With that out of the way, now we have to select the move towards a cell with a lower cost. There are many ways to accomplish this task! Let's look one of them.

First, we define the candidate moves using the first row of the least-cost path as reference.

```{r}
steps <- list(
  left = c(path$row, max(path$col - 1, 1)),
  up = c(max(path$row - 1, 1), path$col)
)

steps
```
Notice that the function returns a list with the coordinates of the candidate moves. The move `left` is one column to the left (second number is 12 instead of 13), and the move `up` is one row above (first number is 12 instead of 13).

Second, we need to extract the values of the cost matrix for the coordinates of these two steps.

```{r}
costs <- list(
  left = m_cost[steps$left[1], steps$left[2]],
  up = m_cost[steps$up[1], steps$up[2]]
)

costs
```

Finally, we choose the candidate step with the lower cost using `which.min()`, a function that returns the index of the smallest value in a vector or list. Notice that we use `[1]` in `which.min(costs)[1]` to resolve potential ties that may be returned by `which.min()` if the two costs are the same (unlikely, but possible).

```{r}
steps[[which.min(costs)[1]]]
```

Combining these pieces we can now build a function named `least_cost_step()` that takes the cost matrix and the last row of a least cost path, and returns a new row with the coordinates of the next step.
  
```{r}
#' Identify Next Step of Least-Cost Path
#' @param cost_matrix (required, matrix) cost matrix.
#' @param last_step (required, data frame) one row data frame with columns "row" and "col" representing the last step of a least-cost path.
#' @return one row data frame, new step in least-cost path
least_cost_step <- function(cost_matrix, last_step){
  
  steps <- list(
    left = c(last_step$row, max(last_step$col - 1, 1)),
    up = c(max(last_step$row - 1, 1), last_step$col)
  )
  
  costs <- list(
    left = cost_matrix[steps$left[1], steps$left[2]],
    up = cost_matrix[steps$up[1], steps$up[2]]
  )
  
  coords <- steps[[which.min(costs)[1]]]
  
  #rewrite input with new values
  new_step <- last_step
  new_step[,] <- c(coords[1], coords[2])
  
  new_step
  
}
```

Notice that the function overwrites the input data frame `step` with the new values. Let's check how it works:

```{r}
least_cost_step(
  cost_matrix = m_cost, 
  last_step = path
  )
```

Good, it returned the move to the left. Now, if you think about the function for a bit, you'll see that it takes a step in the least-cost path, and returns a new one. From there, it seems we can feed it its own result again and again until it runs out of steps to find.

We can do that in a concise way using a `repeat{}` loop. Notice that it will keep running until both coordinates in the last row of the path are equal to 1.

```{r}
repeat{
  
  #find next step
  new.step <- least_cost_step(
    cost_matrix = m_cost, 
    last_step = tail(path, n = 1)
    )
  
  #join the new step with path
  path <- rbind(
    path, new.step,
    make.row.names = FALSE
    )
  
  #stop when coordinates are 1, 1
  if(all(tail(path, n = 1) == 1)){break}
  
}

path
```
  
The resulting least-cost path can be plotted on top of the cost matrix. Please, remember that the data is not pre-processed, and the plot below does not represent the real alignment (yet) between our target time series.

```{r}
graphics::image(
    x = seq_len(nrow(m_cost)),
    y = seq_len(ncol(m_cost)),
    z = m_cost,
    xlab = "zoo_germany",
    ylab = "zoo_sweden",
    main = "Cost Matrix and Least-Cost Path"
    )

graphics::lines(
  x = path$row, 
  y = path$col,
  lwd = 2
  )
```

At this point we have all the pieces required to write the function `least_cost_path()`. Notice that the `repeat{}` statement is slightly more concise than before, as `least_cost_step()` is directly wrapped within `rbind()`. But take a word of advice: using `rbind()` in a loop to add rows to a data frame is not a computationally efficient operation, but it was used here anyway because it makes the code more concise.

```{r}
#' Least-Cost Path from Cost Matrix
#' @param cost_matrix (required, matrix) cost matrix.
#' @return data frame with least-cost path coordinates
least_cost_path <- function(cost_matrix){
  
  #first step of the least cost path
  path <- data.frame(
    row = nrow(cost_matrix),
    col = ncol(cost_matrix)
  )
  
  #iterate until path is completed
  repeat{
    
    #merge path with result of least_cost_step()
    path <- rbind(
      path, 
      #find next step
      least_cost_step(
        cost_matrix = cost_matrix, 
        last_step = tail(path, n = 1)
      ),
      make.row.names = FALSE
    )
    
    #stop when coordinates are 1, 1
    if(all(tail(path, n = 1) == 1)){break}
    
  }
  
  path
  
}
```

We can give it a go now to see that it works as expected.

```{r}
least_cost_path(cost_matrix = m_cost)
```

Nice, it worked! 

But before continuing, there is just a little detail to notice about the least-cost path: every time the same row index (such as `9`) is linked to different column indices (`8` to `11`), it means that the samples of the time series identified by these column indices are having their time *compressed* (or *warped*) to the time of the row index. Hence, according to the least-cost path, the samples `8` to `11` of `zoo_sweden` would be aligned in time with the sample `9` of `zoo_germany`. 

Now it's time to use the least-cost path to quantify the similarity between the time series.

```{r, echo = FALSE}
#cleanup
rm(new.step, steps, costs)

```


### Dissimilarity Metric

The objective of dynamic time warping is to compute a metric of *dissimilarity* (or *similarity*, it just depends on the side from where you are looking at the issue) between time series. This operation requires two steps:

  - Obtain the accumulated distance of the least cost path.
  - Normalize the sum of distances by some number to help make results comparable across pairs of time series of different lengths.

First, the method designed to build the cost matrix accumulates the distance of the least-cost path in the terminal cell. Since we added the distance of the cell `[1, 1]` to this terminal cell, the total distance of the least cost path can be found by undoing this operation:

```{r}
distance <- m_cost[nrow(m_cost), ncol(m_cost)] - m_cost[1, 1]
distance
```

Second, we need to find a number to normalize this distance value to make it comparable across different time series. There are several options, such as dividing `distance` by the sum of lengths of the two time series, or by the length of the least-cost path (`nrow(path)`). 

```{r}
distance/sum(dim(m_cost))
distance/nrow(path)
```
Another elegant normalization option requires computing the sum of distances between consecutive samples on each time series. This operation, named *auto-sum*, requires applying `distance_euclidean()` between the samples 1 and 2 of the given time series, then between the samples 2 and the 3, and so on until all consecutive sample pairs are processed. 

To apply this operation to a time series we iterate between pairs of consecutive samples and save the distance between each pair of samples in a vector (named `autodistance` in the code below). Once the loop is done, then the auto-sum of the time series is the sum tof the values in this vector. For example, for `zoo_germany` we would have:

```{r}
autodistance <- vector(
  mode = "numeric", 
  length = nrow(zoo_germany) - 1
  )

for (row in 2:nrow(zoo_germany)) {
  autodistance[row - 1] <- distance_euclidean(
    x = zoo_germany[row, ],
    y = zoo_germany[row - 1, ]
  )
}

sum(autodistance)
```

Since we'll need to apply this operation to many time series, it's better to formalize this logic as a function:

```{r}
#' Time Series Autosum
#' @param x (required, zoo object) time series
#' @return numeric
auto_sum <- function(x){
  
  autodistance <- vector(
    mode = "numeric", 
    length = nrow(x) - 1
  )
  
  for (row in 2:nrow(x)) {
    autodistance[row - 1] <- distance_euclidean(
      x = x[row, ],
      y = x[row - 1, ]
    )
  }
  
  sum(autodistance)
  
}
```

We can now apply it to our two time series:

```{r}
zoo_germany_autosum <- auto_sum(
  x = zoo_germany
)

zoo_sweden_autosum <- auto_sum(
  x = zoo_sweden
)
```

Once we have the auto-sum of both time series, we just have to add them together to obtain our normalization value.

```{r}
normalizer <- zoo_germany_autosum + zoo_sweden_autosum
normalizer
```

Now that we obtained `distance` from the cost matrix, and the `normalizer` from the auto-sum of the time series, we can compute our dissimilarity score, which follows the expression below:

```{r}
((2.0 * distance) / normalizer) - 1.0
```
Why this particular formula? Because it returns zero when comparing a time series with itself! In such case, `2 * distance` equals `normalizer` because DTW and auto-sum are equivalent, and dividing them returns one. We then subtract one to it, and get zero as result, which represents a perfect similarity score. 

The same affect cannot be achieved when using other normalization values, such as the sum of lengths of the time series, or the length of the least cost path.

We can integrate these pieces into the function `dissimilarity_score()`.

```{r}
#' Similarity Metric from Least Cost Path and Cost Matrix
#' @param a (required, zoo object) time series.
#' @param b (required, zoo object) time series with same columns as `x`
#' @param cost_path (required, data frame) least cost path with the columns "row" and "col".
#' @return numeric, similarity metric
dissimilarity_score <- function(a, b, cost_matrix){
  
  #distance of the least cost path
  distance <- cost_matrix[nrow(cost_matrix), ncol(cost_matrix)] - cost_matrix[1, 1]
  
  #compute normalization factor from autosum
  autosum_a <- auto_sum(x = a)
    
  autosum_b <- auto_sum(x = b)
  
  normalizer <- autosum_a + autosum_b
  
  #compute dissimilarity
  psi <- ((2 * distance) / normalizer) - 1
  
  psi
  
}
```

We can give it a test run now:

```{r}
dissimilarity_score(
  a = zoo_germany,
  b = zoo_sweden,
  cost_matrix = m_cost
)
```

With the `dissimilarity_score()` function ready, it is time to go write our main function to facilitate the computation of the similarity metric for two arbitrary time series with a single R command.

```{r, echo = FALSE, warning = FALSE}
#cleanup
rm(m_cost, m_dist, path, autodistance, distance, normalizer, row, zoo_germany_autosum, zoo_sweden_autosum)

```


## Main Function

To recapitulate before moving forward, we have the following functions:

  - `ts_preprocessing()` applies linear detrending and z-score normalization to a time series.
  - `distance_matrix()` and `distance_euclidean()` work together to compute a distance matrix.
  - `cost_matrix()` transforms the distance matrix into a cost matrix.
  - `least_cost_path()` applies `least_cost_step()` recursively to build a least-cost path.
  - `dissimilarity_score()`, which calls `auto_sum()`, quantifies time series dissimilarity.
  
We can wrap together all these functions into a new one with the unimaginative name `dynamic_time_warping()`.

```{r}
#' Similarity Between Time Series
#' @param a (required, zoo object) time series.
#' @param b (required, zoo object) time series with same columns as `x`
#' @param plot (optional, logical) if `TRUE`, a dynamic time warping plot is produced.
#' @return psi score
dynamic_time_warping <- function(a, b, plot = FALSE){
  
  #linear detrending and z-score normalization
  a_ <- ts_preprocessing(x = a)
  b_ <- ts_preprocessing(x = b)
  
  #distance matrix
  m_dist <- distance_matrix(
    a = a_,
    b = b_
  )
  
  #cost matrix
  m_cost <- cost_matrix(
    distance_matrix = m_dist
  )
  
  #least-cost path
  cost_path <- least_cost_path(
    cost_matrix = m_cost
  )
  
  #similarity metric
  score <- dissimilarity_score(
    a = a_,
    b = b_,
    cost_matrix = m_cost
  )
  
  #plot
  if(plot == TRUE){
    
    graphics::image(
      x = seq_len(nrow(m_cost)),
      y = seq_len(ncol(m_cost)),
      z = m_cost,
      xlab = "a",
      ylab = "b",
      main = paste0("Similarity score = ", round(score, 3))
    )
    
    graphics::lines(
      x = cost_path$row, 
      y = cost_path$col,
      lwd = 2
    )
    
  }
  
  score
  
}
```

Before I said that one advantage of the presented dissimilarity formula is that it returns 0 when comparing a time series with itself. Let's challenge that idea comparing `zoo_germany` with itself:

```{r}
dynamic_time_warping(
  a = zoo_germany,
  b = zoo_germany
)
```

That's good, right? Perfect similarity happens at 0, which is a lovely number to start with. If we now compare `zoo_germany` and `zoo_sweden`, we should expect a larger number:

```{r}
dynamic_time_warping(
  a = zoo_germany,
  b = zoo_sweden,
  plot = TRUE
)
```

And now, when comapring `zoo_sweden` with `zoo_spain` we should expect an even higher dissimilarity score:

```{r}
dynamic_time_warping(
  a = zoo_sweden,
  b = zoo_spain,
  plot = TRUE
)
```

## Library Source

Once we have all our DTW functions written and tested, the easiest way to make them usable without any extra hassle is to write them to a source file. Having them all in a single file allows loading them at once via the `source()` command. 

For example, if our library file is [`dtw.R`](https://www.dropbox.com/scl/fi/z2n9hnenxwuxwol5i0zcn/dtw.R?rlkey=bhsyew12r1glnqihtnocxwri6&dl=1), running `source("dtw.R")` will load all functions into your R environment and they will be readily available for your DTW analysis.

## Closing Thoughts

I hope you found this tutorial useful in one way or another. Writing a methodological library from scratch is hard work. There are many moving parts to consider, many concepts that need to be mapped and then translated into code, that making mistakes becomes exceedingly easy. Never worry about that, take your time, and persevere, because hard things take work. But above everything, enjoy the learning journey!

## Coming Next

In my TODO list there is a post planned about how to identify computational bottlenecks in the DTW library we just wrote, and optimize these parts that are worth optimized. But there's no timeline for such post yet.

Best,

Blas

