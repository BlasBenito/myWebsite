toy$c <- toy$a
#with multicollinearity
gb_abcd <- xgboost::xgboost(
x = as.matrix(toy[, c("a", "b", "c", "d")]),
y = toy$y,
objective = "reg:squarederror",
nrounds = 100
)
xgboost::xgb.importance(model = gb_abcd)[, c(1:2)] |>
dplyr::arrange(Feature)
library(blogdown)
install.packages("blogdown")
blogdown::build_site()
blogdown::serve_site()
blogdown::build_site()
blogdown::serve_site()
library(dplyr)
library(ggplot2)
library(collinear)
toy |>
round(3) |>
head()
collinear::cor_df(
df = toy,
predictors = c("a", "b", "c", "d")
)
#model of a against all other predictors
abcd_model <- lm(
formula = a ~ b + c + d,
data = toy
)
#r-squared of the a_model
abcd_R2 <- summary(abcd_model)$r.squared
abcd_R2
abcd_vif <- 1/(1-abcd_R2)
abcd_vif
ab_model <- lm(
formula = a ~ b,
data = toy
)
ab_vif <- 1/(1 - summary(ab_model)$r.squared)
ab_vif
collinear::vif_df(
df = toy[, c("a", "b")]
)
data.frame(R2 = seq(from = 0, to = 0.99, by = 0.05)) |>
dplyr::mutate(VIF = 1/(1 - R2)) |>
ggplot() +
aes(
x = VIF,
y = R2
) +
geom_line() +
geom_vline(xintercept = 2.5, col = "gray70", lwd = 0.5, lty = 2) +
geom_vline(xintercept = 5, col = "gray70", lwd = 0.5, lty = 2) +
geom_vline(xintercept = 10, col = "gray70", lwd = 0.5, lty = 2) +
geom_vline(xintercept = abcd_vif, col = "red4") +
geom_label(
aes(x = abcd_vif, y = 0.75, label = "VIF of a vs b, c, d"),
fill = "white",
color = "red4",
label.size = NA,
size = 3.5
) +
geom_vline(xintercept = ab_vif, col = "forestgreen") +
geom_label(
aes(x = 2.5, y = 0.85, label = "VIF of a vs b"),
fill = "white",
color = "forestgreen",
label.size = NA,
size = 3.5
) +
labs(title = "R-squared vs Variance Inflation Factor") +
scale_x_continuous(breaks = c(0, 1, 2.5, 5, 10, 20)) +
theme_bw()
library(dplyr)
library(ggplot2)
library(collinear)
toy |>
round(3) |>
head()
collinear::cor_df(
df = toy,
predictors = c("a", "b", "c", "d")
)
#model of a against all other predictors
abcd_model <- lm(
formula = a ~ b + c + d,
data = toy
)
#r-squared of the a_model
abcd_R2 <- summary(abcd_model)$r.squared
abcd_R2
abcd_vif <- 1/(1-abcd_R2)
abcd_vif
ab_model <- lm(
formula = a ~ b,
data = toy
)
ab_vif <- 1/(1 - summary(ab_model)$r.squared)
ab_vif
collinear::vif_df(
df = toy[, c("a", "b")]
)
data.frame(R2 = seq(from = 0, to = 0.99, by = 0.05)) |>
dplyr::mutate(VIF = 1/(1 - R2)) |>
ggplot() +
aes(
x = VIF,
y = R2
) +
geom_line() +
geom_vline(xintercept = 2.5, col = "gray70", lwd = 0.5, lty = 2) +
geom_vline(xintercept = 5, col = "gray70", lwd = 0.5, lty = 2) +
geom_vline(xintercept = 10, col = "gray70", lwd = 0.5, lty = 2) +
geom_vline(xintercept = abcd_vif, col = "red4") +
geom_label(
aes(x = abcd_vif, y = 0.75, label = "VIF of a vs b, c, d"),
fill = "white",
color = "red4",
label.size = NA,
size = 3.5
) +
geom_vline(xintercept = ab_vif, col = "forestgreen") +
geom_label(
aes(x = 2.5, y = 0.85, label = "VIF of a vs b"),
fill = "white",
color = "forestgreen",
label.size = NA,
size = 3.5
) +
labs(title = "R-squared vs Variance Inflation Factor") +
scale_x_continuous(breaks = c(0, 1, 2.5, 5, 10, 20)) +
theme_bw()
ci <- function(b, se){
x <- se * 1.96
as.numeric(c(b-x, b+x))
}
#note: stats::confint() which uses t-critical values to compute more precise confidence intervals.
yab_model <- lm(
formula = y ~ a + b,
data = toy
) |>
summary()
#coefficient estimate and standard error of a
a_coef <- yab_model$coefficients[2, 1:2]
a_coef
a_ci <- ci(
b = a_coef[1],
se = a_coef[2]
)
a_ci
old_width <- diff(a_ci)
old_width
#model y against all predictors and get summary
yabcd_model <- lm(
formula = y ~ a + b + c + d,
data = toy
) |>
summary()
#compute confidence interval of a
a_ci <- ci(
b = yabcd_model$coefficients["a", "Estimate"],
se = yabcd_model$coefficients["a", "Std. Error"]
)
#compute width of confidence interval of a
new_width <- diff(a_ci)
new_width
new_width/old_width
#vif of b vs a
ba_vif <- collinear::vif_df(
df = toy[, c("a", "b")]
) |>
dplyr::filter(predictor == "b")
#vif of b vs a c d
bacd_vif <- collinear::vif_df(
df = toy[, c("a", "b", "c", "d")]
) |>
dplyr::filter(predictor == "b")
#expeced inflation of the confidence interval
sqrt(bacd_vif$vif)
#compute confidence interval of b in y ~ a + b
b_ci_old <- ci(
b = yab_model$coefficients["b", "Estimate"],
se = yab_model$coefficients["b", "Std. Error"]
)
#compute confidence interval of b in y ~ a + b + c + d
b_ci_new <- ci(
b = yabcd_model$coefficients["b", "Estimate"],
se = yabcd_model$coefficients["b", "Std. Error"]
)
#compute inflation
diff(b_ci_new)/diff(b_ci_old)
yabcd_model$coefficients[-1, ] |>
round(4)
ci(
b = yabcd_model$coefficients["c", "Estimate"],
se = yabcd_model$coefficients["c", "Std. Error"]
)
preference <- collinear::preference_order(
df = toy,
response = "y",
predictors = c("a", "b", "c", "d"),
f = f_auto,
quiet = TRUE
)
preference
selected_predictors <- collinear::vif_select(
df = toy,
predictors = c("a", "b", "c", "d"),
preference_order = preference,
max_vif = 2.5,
quiet = TRUE
)
selected_predictors
selected_predictors <- collinear::collinear_select(
df = toy,
predictors = c("a", "b", "c", "d"),
preference_order = preference,
max_vif = 2.5,
quiet = TRUE
)
selected_predictors
selected_predictors <- collinear::vif_select(
df = toy,
predictors = c("a", "b", "c", "d"),
preference_order = preference,
max_vif = 2.5,
quiet = TRUE
)
selected_predictors
warnings()
#required
install.packages("collinear")
install.packages("fastDummies")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("rpart")
install.packages("collinear")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("dplyr")
library(rpart)
library(rpart.plot)
library(collinear)
library(fastDummies)
library(dplyr)
library(ggplot2)
data(
vi_smol,
vi_predictors
)
dplyr::glimpse(vi_smol)
vi_categorical <- collinear::identify_predictors_categorical(
df = vi_smol,
predictors = vi_predictors
)
vi_categorical <- collinear::identify_categorical_variables(
df = vi_smol,
predictors = vi_predictors
)
vi_categorical
data.frame(
name = vi_categorical,
categories = lapply(
X = vi_categorical,
FUN = function(x) length(unique(vi_smol[[x]]))
) |>
unlist()
) |>
dplyr::arrange(
dplyr::desc(categories)
)
vi_categorical
data.frame(
name = vi_categorical,
categories = lapply(
X = vi_categorical,
FUN = function(x) length(unique(vi_smol[[x]]))
) |>
unlist()
) |>
dplyr::arrange(
dplyr::desc(categories)
)
data.frame(
name = vi_categorical,
categories = lapply(
X = vi_categorical,
FUN = function(x) length(unique(vi_smol[[x]]))
)
data.frame(
name = vi_categorical,
categories = lapply(
X = vi_categorical,
FUN = function(x) length(unique(vi_smol[[x]]))
) |>
unlist()
) |>
dplyr::arrange(
dplyr::desc(categories)
)
lapply(
X = vi_categorical,
FUN = function(x) length(unique(vi_smol[[x]]))
) |>
unlist()
data.frame(
name = vi_categorical,
categories = lapply(
X = vi_categorical,
FUN = function(x) length(unique(vi_smol[[x]]))
) |>
unlist()
) |>
dplyr::arrange(
dplyr::desc(categories)
)
lapply(
X = vi_categorical,
FUN = function(x) length(unique(vi_smol[[x]]))
)
vi_categorical
data.frame(
name = vi_categorical$valid,
categories = lapply(
X = vi_categorical$valid,
FUN = function(x) length(unique(vi_smol[[x]]))
) |>
unlist()
) |>
dplyr::arrange(
dplyr::desc(categories)
)
sort(unique(vi_smol$koppen_zone))
help(lm)
stats::lm(
formula = vi_numeric ~ koppen_zone,
data = vi_smol
) |>
summary()
dummy_variables <- stats::model.matrix(
~ koppen_zone,
data = vi_smol
)
ncol(dummy_variables)
dummy_variables[1:10, 1:10]
df <- fastDummies::dummy_cols(
.data = vi_smol[, "koppen_zone", drop = FALSE],
select_columns = "koppen_zone",
remove_selected_columns = TRUE
)
dplyr::glimpse(df)
collinear::vif_df(
df = df,
quiet = TRUE
)
#add response variable to df
df$vi_numeric <- vi_smol$vi_numeric
#fit model using all one-hot encoded variables
koppen_zone_one_hot <- rpart::rpart(
formula = vi_numeric ~ .,
data = df
)
koppen_zone_categorical <- rpart::rpart(
formula = vi_numeric ~ koppen_zone,
data = vi_smol
)
#plot tree skeleton
par(mfrow = c(1, 2))
plot(koppen_zone_one_hot, main = "One-hot encoding")
plot(koppen_zone_categorical, main = "Categorical")
yx <- data.frame(
y = 1:7,
x = c("a", "a", "a", "b", "b", "b", "c")
)
yx |>
dplyr::group_by(x) |>
dplyr::mutate(
x_encoded = mean(y)
)
y_mean <- mean(yx$y)
m <- 3
yx |>
dplyr::group_by(x) |>
dplyr::mutate(
x_encoded =
(dplyr::n() * mean(y) + m * y_mean) / (dplyr::n() + m)
)
yx |>
dplyr::group_by(x) |>
dplyr::mutate(
x_encoded = (sum(y) - y) / (dplyr::n() - 1)
)
#maximum noise to add
max_noise <- max(yx$y)/100
#set seed for reproducibility
set.seed(1)
yx |>
dplyr::group_by(x) |>
dplyr::mutate(
x_encoded = mean(y) + runif(n = dplyr::n(), max = max_noise)
)
yx |>
dplyr::group_by(x) |>
dplyr::mutate(
x_encoded = (sum(y) - y) / (dplyr::n() - 1)
)
#maximum noise to add
max_noise <- max(yx$y)/100
#set seed for reproducibility
set.seed(1)
yx |>
dplyr::group_by(x) |>
dplyr::mutate(
x_encoded = mean(y) + runif(n = dplyr::n(), max = max_noise)
)
#maximum noise as function of the number of categories
max_noise <- length(unique(yx$x))/100
yx |>
dplyr::arrange(y) |>
dplyr::group_by(x) |>
dplyr::mutate(
x_encoded = dplyr::cur_group_id() + runif(n = dplyr::n(), max = max_noise)
)
yx |>
dplyr::arrange(y) |>
dplyr::group_by(x) |>
dplyr::mutate(
x_encoded = dplyr::cur_group_id()
)
help(target_encoding_lab)
library(collinear)
help(target_encoding_lab)
yx_encoded <- target_encoding_lab(
df = yx,
response = "y",
predictors = "x",
smoothing = c(0, 2),
quiet = FALSE,
seed = 1, #for reproducibility
overwrite = FALSE #to overwrite or not the predictors with their encodings
)
dplyr::glimpse(yx_encoded)
yx_encoded <- target_encoding_lab(
df = yx,
response = "y",
predictors = "x",
smoothing = c(0, 2),
quiet = FALSE,
seed = 1, #for reproducibility
overwrite = FALSE, #to keep original predictors intact
quiet = TRUE
)
yx_encoded <- target_encoding_lab(
df = yx,
response = "y",
predictors = "x",
smoothing = c(0, 2),
seed = 1, #for reproducibility
overwrite = FALSE, #to keep original predictors intact
quiet = TRUE
)
dplyr::glimpse(yx_encoded)
yx_encoded <- collinear::target_encoding_lab(
df = yx,
response = "y",
predictors = "x",
encoding_method = c("mean", "loo", "rank")
overwrite = FALSE, #to keep original predictors intact
yx_encoded <- collinear::target_encoding_lab(
df = yx,
response = "y",
predictors = "x",
encoding_method = c("mean", "loo", "rank"),
overwrite = FALSE, #to keep original predictors intact
quiet = TRUE
)
dplyr::glimpse(yx_encoded)
yx_encoded |>
tidyr::pivot_longer(
cols = dplyr::contains("__encoded"),
values_to = "x_encoded"
) |>
ggplot() +
facet_wrap("name") +
aes(
x = x_encoded,
y = y,
color = x
) +
geom_point(size = 3) +
theme_bw()
yx_encoded <- collinear::target_encoding_lab(
df = yx,
response = "y",
predictors = "x",
methods = "mean", #selected encoding method
quiet = FALSE,
overwrite = TRUE
)
dplyr::glimpse(yx_encoded)
yx_encoded <- collinear::target_encoding_lab(
df = yx,
response = "y",
predictors = "x",
methods = "mean",
overwrite = TRUE,
quiet = TRUE
)
dplyr::glimpse(yx_encoded)
blogdown::build_site()
