<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Blas M. Benito, PhD</title>
    <link>https://blasbenito.com/post/</link>
      <atom:link href="https://blasbenito.com/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2023 Blas M. Benito. All Rights Reserved.</copyright><lastBuildDate>Tue, 05 Mar 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://blasbenito.com/media/avatar.jpg</url>
      <title>Posts</title>
      <link>https://blasbenito.com/post/</link>
    </image>
    
    <item>
      <title>My Reading List: Data Science</title>
      <link>https://blasbenito.com/post/my-reading-list-data-science/</link>
      <pubDate>Tue, 05 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/my-reading-list-data-science/</guid>
      <description>&lt;p&gt;This is a live post listing links to Data Science related posts and videos I consider to be interesting, high-quality, or even essential to better understand particular topics within such a wide field.&lt;/p&gt;
&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://towardsdatascience.com/extending-target-encoding-443aa9414cae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Extending Target Encoding&lt;/strong&gt;&lt;/a&gt;: post by 
&lt;a href=&#34;https://www.aitimejournal.com/interview-with-daniele-micci-barreca-product-analytics-lead-data-science-google/30110/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniele Micci-Barreca&lt;/a&gt; explaining how he came up with the idea of target encoding, and its possible extensions.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://maxhalford.github.io/blog/target-encoding/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Target encoding done the right way&lt;/strong&gt;&lt;/a&gt;: post by 
&lt;a href=&#34;https://maxhalford.github.io/bio/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Max Halford&lt;/a&gt;, Head of Data at 
&lt;a href=&#34;https://www.carbonfact.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Carbonfact&lt;/a&gt;, explaining in detail how to combine additive smoothing and target encoding.&lt;/p&gt;
&lt;h2 id=&#34;handling-and-management&#34;&gt;Handling and Management&lt;/h2&gt;
&lt;h3 id=&#34;apache-parquet&#34;&gt;Apache Parquet&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://airbyte.com/data-engineering-resources/parquet-data-format&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A Deep Dive into Parquet: The Data Format Engineers Need to Know&lt;/strong&gt;&lt;/a&gt;: This by Aditi Prakash, published in the 
&lt;a href=&#34;https://airbyte.com/blog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Airbyte Blog&lt;/a&gt; offers a complete guide about the 
&lt;a href=&#34;https://parquet.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apache Parquet&lt;/a&gt; file format.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.influxdata.com/blog/querying-parquet-millisecond-latency/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Querying Parquet with Millisecond Latency&lt;/strong&gt;&lt;/a&gt; this post from by Raphael Taylor-Davies and Andrew Lamb explains in deep the optimization methods used in 
&lt;a href=&#34;https://parquet.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apache Parquet&lt;/a&gt; files. Warning, this is a very technical read!&lt;/p&gt;
&lt;h3 id=&#34;duckdb&#34;&gt;DuckDB&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://duckdb.org/2024/01/26/multi-database-support-in-duckdb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Multi-Database Support in DuckDB&lt;/strong&gt;&lt;/a&gt; This post by Mark Raasveldt published in the DuckDB blog explains how to query together data from different databases at once.&lt;/p&gt;
&lt;h1 id=&#34;analysis-and-modeling&#34;&gt;Analysis and Modeling&lt;/h1&gt;
&lt;h2 id=&#34;modeling-methods&#34;&gt;Modeling Methods&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://m-clark.github.io/posts/2019-10-20-big-mixed-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Mixed Models for Big Data&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://m-clark.github.io/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Clark&lt;/a&gt; (see entry below by the same author) reviews several mixed modelling approach for large data in R.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://m-clark.github.io/generalized-additive-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Generalized Additive Models&lt;/strong&gt;&lt;/a&gt;: A good online book on Generalized Additive Models by 
&lt;a href=&#34;https://m-clark.github.io/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Clark&lt;/a&gt;, Senior Machine Learning Scientist at 
&lt;a href=&#34;https://www.strong.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Strong Analytics&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;model-explainability&#34;&gt;Model Explainability&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://towardsdatascience.com/a-simple-model-independent-score-explanation-method-c17002d66da7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Model-Independent Score Explanation&lt;/strong&gt;&lt;/a&gt;: Post by 
&lt;a href=&#34;https://www.aitimejournal.com/interview-with-daniele-micci-barreca-product-analytics-lead-data-science-google/30110/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniele Micci-Barreca&lt;/a&gt; on model explainability. It also explains a very clever method to better understand any model just from it&amp;rsquo;s predictions.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AI Explanations whitepaper&lt;/strong&gt;&lt;/a&gt;: White paper of Google&amp;rsquo;s &amp;ldquo;AI Explanations&amp;rdquo; product with a pretty good overall view of the state of the art of model explainability.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1702.08608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Towards A Rigorous Science of Interpretable Machine Learning&lt;/strong&gt;&lt;/a&gt;: Pre-print by Finale Doshi-Velez and Been Kim offering a rigorous definition and evaluation of model interpretability.&lt;/p&gt;
&lt;h2 id=&#34;spatial-analysis&#34;&gt;Spatial Analysis&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://duckdb.org/2023/04/28/spatial.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;PostGEESE? Introducing The DuckDB Spatial Extension&lt;/strong&gt;&lt;/a&gt;: In this post, the authors of 
&lt;a href=&#34;https://duckdb.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DuckDB&lt;/a&gt; present the new PostGIS-like &lt;em&gt;spatial&lt;/em&gt; extension for this popular in-process data base engine.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://py.geocompx.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Geocomputation with Python&lt;/strong&gt;&lt;/a&gt;: A very nice book on geographic data analysis with Python.&lt;/p&gt;
&lt;h1 id=&#34;coding&#34;&gt;Coding&lt;/h1&gt;
&lt;h2 id=&#34;general-concepts&#34;&gt;General Concepts&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://archive.org/details/a-philosophy-of-software-design/mode/2up&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A Philosophy Of Software Design&lt;/strong&gt;&lt;/a&gt;: This book by 
&lt;a href=&#34;https://web.stanford.edu/~ouster/cgi-bin/home.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Ousterhout&lt;/a&gt; is full of high-level concepts and tips to help tackle software complexity. It&amp;rsquo;s so good I had to buy a hard copy that now lives in my desk. 
&lt;a href=&#34;https://blog.pragmaticengineer.com/a-philosophy-of-software-design-review/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This post&lt;/a&gt; by 
&lt;a href=&#34;https://blog.pragmaticengineer.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gergely Orosz&lt;/a&gt; offers a balanced review of the book.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://youtu.be/CFRhGnuXG-4?si=7Xr3E9L7GFvoRJqA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Why You Shouldn&amp;rsquo;t Nest Your Code&lt;/strong&gt;&lt;/a&gt;: In this wonderful video, 
&lt;a href=&#34;https://www.youtube.com/@CodeAesthetic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CodeAesthetic&lt;/a&gt; explains in detail (and beautiful graphics!) a couple of methods to reduce the level of nesting in our code to improve readability and maintainability. This video has truly changed how I code in R!&lt;/p&gt;
&lt;h2 id=&#34;r&#34;&gt;R&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://ropensci.org/blog/2024/02/22/beautiful-code/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Beautiful Code, Because We’re Worth It!&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://mastodon.social/@maelle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maëlle Salmon&lt;/a&gt; (research software engineer), and 
&lt;a href=&#34;https://fosstodon.org/@yabellini&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yanina Bellini Saibene&lt;/a&gt; (rOpenSci Community Manager) provides simple tips to help write more visually pleasant R code.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://journal.r-project.org/articles/RJ-2023-071/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Coloring in R’s Blind Spot&lt;/strong&gt;&lt;/a&gt;: This article published in 
&lt;a href=&#34;https://journal.r-project.org/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The R Journal&lt;/a&gt; by 
&lt;a href=&#34;https://www.zeileis.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Achim Zeileis&lt;/a&gt; (he has a 
&lt;a href=&#34;https://www.zeileis.org/blog/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;great analytics blog&lt;/a&gt; too!) and 
&lt;a href=&#34;https://www.stat.auckland.ac.nz/~paul/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paul Murrel&lt;/a&gt; offers a great overview of the base R color functions, and offers specific advice on what color palettes work better in different scenarios.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://peerj.com/preprints/26605v1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Taking R to its limits: 70+ tips&lt;/strong&gt;&lt;/a&gt;: This pre-print (not peer-reviewed AFAIK) by Tsagris and Papadakis offers a long list of tips to speed-up computation with the R language. I think a few of these tips lack enough context or are poorly explained, but it&amp;rsquo;s still a good resource to help optimize our R code.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.njtierney.com/post/2023/12/06/long-errors-smell/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Code Smell: Error Handling Eclipse&lt;/strong&gt; &lt;/a&gt;: This post by 
&lt;a href=&#34;https://fosstodon.org/@njtierney@aus.social&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nick Tierney&lt;/a&gt; explains how to address these situations when &lt;em&gt;error checking code totally eclipses the intent of the code&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.emilyriederer.com/post/team-of-packages/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Building a team of internal R packages&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://www.emilyriederer.com/about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emily Riederer&lt;/a&gt; delves into the particularities of building a team of R packages to do jobs helping a organization answer impactful questions.&lt;/p&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://archive.org/details/francois-chollet-deep-learning-with-python-manning-2018&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Deep Learning With Python&lt;/strong&gt;&lt;/a&gt;: This book by 
&lt;a href=&#34;https://fchollet.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;François Chollet&lt;/a&gt;, Software Engineer at Google and creator of the Keras library, seems to me like the best resource out there for those wanting to understand and build deep learning models from scratch. I have a hard copy on my desk, and I am finding it pretty easy to follow. Also, the code examples are clearly explained, and they ramp up in a very consistent manner.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.emilyriederer.com/post/py-rgo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Python Rgonomics&lt;/strong&gt;&lt;/a&gt;: In this post, 
&lt;a href=&#34;https://www.emilyriederer.com/about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emily Riederer&lt;/a&gt; offers a list of Python libraries with an &amp;ldquo;R feeling&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;coding-workflow&#34;&gt;Coding Workflow&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://graphite.dev/blog/stacked-prs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;How to use stacked PRs to unblock your entire team&lt;/strong&gt;&lt;/a&gt;: This post in 
&lt;a href=&#34;https://graphite.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graphite&lt;/a&gt;&amp;rsquo;s blog explains how to split large coding changes into small managed PRs (aka &amp;ldquo;stacked PRs&amp;rdquo;) to avoid blocks when PR reviews are hard to come by.&lt;/p&gt;
&lt;h1 id=&#34;other-fancy-things&#34;&gt;Other Fancy Things&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://vickiboykis.com/2024/01/15/whats-new-with-ml-in-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;What&amp;rsquo;s new with ML in production&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://vickiboykis.com/about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vicki Boykis&lt;/a&gt;, machine learning engineer at Mozilla.ai, goes deep into the differences and similarities between classical Machine Learning approaches and Large Language Models. I learned a lot from this read!&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://youtu.be/T-D1OfcDW1M?si=sAZO-5NGD8yF2WYe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;What is Retrieval-Augmented Generation (RAG)?&lt;/strong&gt;&lt;/a&gt;: In this video, Marina Danilevsky, Senior Data Scientist at IBM, offers a pretty good explanation on how the 
&lt;a href=&#34;https://research.ibm.com/blog/retrieval-augmented-generation-RAG?utm_id=YT-101-What-is-RAG&amp;amp;_gl=1*p6ef17*_ga*MTQwMzQ5NjMwMi4xNjkxNDE2MDc0*_ga_FYECCCS21D*MTY5MjcyMjgyNy40My4xLjE2OTI3MjMyMTcuMC4wLjA.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Retrieval-Augmented Generation&lt;/a&gt; method can improve the credibility of large language models.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.nature.com/articles/s41598-020-79148-7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A novel framework for spatio-temporal prediction of environmental data using deep learning&lt;/strong&gt;&lt;/a&gt;: This paper by 
&lt;a href=&#34;https://www.linkedin.com/in/federico-amato-66208637&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Federico Amato&lt;/a&gt; and collaborators describes an intriguing regression method combining a feedforward neural network with empirical orthogonal functions for spatio-temporal interpolation. Regrettably, the paper offers no code or data at all, but it&amp;rsquo;s still an interesting read.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2310.10196.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Large Models for Time Series and
Spatio-Temporal Data A Survey and Outlook&lt;/strong&gt;&lt;/a&gt;: This pre-print by Weng and collaborators reviews the current state of the art in spatio-temporal modelling with Large Language Models and Pre-Trained Foundation Models.&lt;/p&gt;
&lt;h1 id=&#34;management-and-leadership&#34;&gt;Management and Leadership&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://zaidesanton.substack.com/p/when-engineering-managers-become&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;You are hurting your team without even noticing&lt;/strong&gt;&lt;/a&gt; This post by 
&lt;a href=&#34;https://substack.com/@zaidesanton&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anton Zaides&lt;/a&gt; (Development Team Leader), and 
&lt;a href=&#34;https://substack.com/@crushingtecheducation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eugene Shulga&lt;/a&gt;, (Software Engineer) offers insight on the harmful effects of a manager&amp;rsquo;s ego in their team dynamics.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://the.managers.guide/p/teamwork-habits-for-leaders&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Teamwork Habits for Leaders&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://substack.com/@ochronus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Csaba Okrona&lt;/a&gt; focuses on how shifting from talker to listener in team meetings offers a good insight to better address the team&amp;rsquo;s needs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mapping Categorical Predictors to Numeric With Target Encoding</title>
      <link>https://blasbenito.com/post/target-encoding/</link>
      <pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/target-encoding/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;Categorical predictors are annoying stringy monsters that can turn any data analysis and modeling effort into a real annoyance. The post delves into the complexities of dealing with these types of predictors using methods such as one-hot encoding (please don&amp;rsquo;t) or target encoding, and provides insights into its mechanisms and quirks&lt;/p&gt;
&lt;h2 id=&#34;key-highlights&#34;&gt;Key Highlights:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Categorical Predictors are Kinda Annoying:&lt;/strong&gt; This section discusses the common issues encountered with categorical predictors during data analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;One-Hot Encoding Pitfalls:&lt;/strong&gt; While discussing one-hot encoding, the post focuses on its limitations, including dimensionality explosion, increased multicollinearity, and sparsity in tree-based models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intro to Target Encoding:&lt;/strong&gt; Introducing target encoding as an alternative, the post explains its concept, illustrating the basic form with mean encoding and subsequent enhancements with additive smoothing, leave-one-out encoding, and more.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Handling Sparsity and Repetition:&lt;/strong&gt; It emphasizes the potential pitfalls of target encoding, such as repeated values within categories and their impact on model performance, prompting the exploration of strategies like white noise addition and random encoding to mitigate these issues.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Target Encoding Lab:&lt;/strong&gt; The post concludes with a detailed demonstration using the &lt;code&gt;collinear::target_encoding_lab()&lt;/code&gt; function, offering a hands-on exploration of various target encoding methods, parameter combinations, and their visual representations.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The post intends to serve as a useful resource for data scientists exploring alternative encoding techniques for categorical predictors.&lt;/p&gt;
&lt;h1 id=&#34;resources&#34;&gt;Resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/BlasBenito/notebooks/blob/main/target_encoding.Rmd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactive notebook of this post&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://doi.org/10.1145/507533.507538&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://towardsdatascience.com/extending-target-encoding-443aa9414cae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Extending Target Encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://maxhalford.github.io/blog/target-encoding/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Target encoding done the right way&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;r-packages&#34;&gt;R packages&lt;/h1&gt;
&lt;p&gt;This tutorial requires the development version (&amp;gt;= 1.0.3) of the newly released R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;, and a few more.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#required
install.packages(&amp;quot;remotes&amp;quot;)
remotes::install_github(
  repo = &amp;quot;blasbenito/collinear&amp;quot;, 
  ref = &amp;quot;development&amp;quot;,
  force = TRUE
  )
install.packages(&amp;quot;fastDummies&amp;quot;)
install.packages(&amp;quot;rpart&amp;quot;)
install.packages(&amp;quot;rpart.plot&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)
install.packages(&amp;quot;ggplot2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rpart)
library(rpart.plot)
library(collinear)
library(fastDummies)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Thank you for using fastDummies!
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## To acknowledge our work, please cite the package:
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Kaplan, J. &amp;amp; Schlegel, B. (2023). fastDummies: Fast Creation of Dummy (Binary) Columns and Rows from Categorical Variables. Version 1.7.1. URL: https://github.com/jacobkap/fastDummies, https://jacobkap.github.io/fastDummies/.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;dplyr&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;categorical-predictors-are-kinda-annoying&#34;&gt;Categorical Predictors are Kinda Annoying&lt;/h1&gt;
&lt;p&gt;I mean, the title of this section says it already, and I bet you have experienced it during an Exploratory Data Analysis (EDA) or a feature selection for model training and the likes. You likely had a nice bunch of numerical variables to use as predictors, no issues there. But then, you discovered among your columns thingies like &amp;ldquo;sampling_location&amp;rdquo;, &amp;ldquo;region_name&amp;rdquo;, &amp;ldquo;favorite_color&amp;rdquo;, or any other type of predictor made of strings, lots of strings. And some of these made sense, and some of them didn&amp;rsquo;t, because who knows where they came from. And you had to branch your code to deal with numeric and categorical variables separately. Or maybe chose to ignore them, as I have done plenty of times.&lt;/p&gt;
&lt;p&gt;Yeah, nobody likes them much at all, But sometimes, these stringy monsters are all you have to move on with your work. And you are not the only one. That&amp;rsquo;s why many efforts have been made to convert them to numeric and kill the problem at once, so now we all have two problems instead.&lt;/p&gt;
&lt;p&gt;Let me go ahead and illustrate the issue. There is a nice data frame in the &lt;code&gt;collinear&lt;/code&gt; R package named &lt;code&gt;vi&lt;/code&gt;, with one response variable named &lt;code&gt;vi_mean&lt;/code&gt;, and several numeric and categorical predictors named in the vector &lt;code&gt;vi_predictors&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data(
  vi,
  vi_predictors
)

dplyr::glimpse(vi)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 30,000
## Columns: 68
## $ longitude                  &amp;lt;dbl&amp;gt; -114.254306, 114.845693, -122.145972, 108.3…
## $ latitude                   &amp;lt;dbl&amp;gt; 45.0540272, 26.2706940, 56.3790272, 29.9456…
## $ vi_mean                    &amp;lt;dbl&amp;gt; 0.38, 0.53, 0.45, 0.69, 0.42, 0.68, 0.70, 0…
## $ vi_max                     &amp;lt;dbl&amp;gt; 0.57, 0.67, 0.65, 0.85, 0.64, 0.78, 0.77, 0…
## $ vi_min                     &amp;lt;dbl&amp;gt; 0.12, 0.41, 0.25, 0.50, 0.25, 0.48, 0.60, 0…
## $ vi_range                   &amp;lt;dbl&amp;gt; 0.45, 0.26, 0.40, 0.34, 0.39, 0.31, 0.17, 0…
## $ vi_binary                  &amp;lt;dbl&amp;gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0…
## $ koppen_zone                &amp;lt;chr&amp;gt; &amp;quot;BSk&amp;quot;, &amp;quot;Cfa&amp;quot;, &amp;quot;Dfc&amp;quot;, &amp;quot;Cfb&amp;quot;, &amp;quot;Aw&amp;quot;, &amp;quot;Cfa&amp;quot;, &amp;quot;A…
## $ koppen_group               &amp;lt;chr&amp;gt; &amp;quot;Arid&amp;quot;, &amp;quot;Temperate&amp;quot;, &amp;quot;Cold&amp;quot;, &amp;quot;Temperate&amp;quot;, &amp;quot;…
## $ koppen_description         &amp;lt;chr&amp;gt; &amp;quot;steppe, cold&amp;quot;, &amp;quot;no dry season, hot summer&amp;quot;…
## $ soil_type                  &amp;lt;chr&amp;gt; &amp;quot;Cambisols&amp;quot;, &amp;quot;Acrisols&amp;quot;, &amp;quot;Luvisols&amp;quot;, &amp;quot;Aliso…
## $ topo_slope                 &amp;lt;int&amp;gt; 6, 2, 0, 10, 0, 10, 6, 0, 2, 0, 0, 1, 0, 1,…
## $ topo_diversity             &amp;lt;int&amp;gt; 29, 24, 21, 25, 19, 30, 26, 20, 26, 22, 25,…
## $ topo_elevation             &amp;lt;int&amp;gt; 1821, 143, 765, 1474, 378, 485, 604, 1159, …
## $ swi_mean                   &amp;lt;dbl&amp;gt; 27.5, 56.1, 41.4, 59.3, 37.4, 56.3, 52.3, 2…
## $ swi_max                    &amp;lt;dbl&amp;gt; 62.9, 74.4, 81.9, 81.1, 83.2, 73.8, 55.8, 3…
## $ swi_min                    &amp;lt;dbl&amp;gt; 24.5, 33.3, 42.2, 31.3, 8.3, 28.8, 25.3, 11…
## $ swi_range                  &amp;lt;dbl&amp;gt; 38.4, 41.2, 39.7, 49.8, 74.9, 45.0, 30.5, 2…
## $ soil_temperature_mean      &amp;lt;dbl&amp;gt; 4.8, 19.9, 1.2, 13.0, 28.2, 18.1, 21.5, 23.…
## $ soil_temperature_max       &amp;lt;dbl&amp;gt; 29.9, 32.6, 20.4, 24.6, 41.6, 29.1, 26.4, 4…
## $ soil_temperature_min       &amp;lt;dbl&amp;gt; -12.4, 3.9, -16.0, -0.4, 16.8, 4.1, 17.3, 5…
## $ soil_temperature_range     &amp;lt;dbl&amp;gt; 42.3, 28.8, 36.4, 25.0, 24.8, 24.9, 9.1, 38…
## $ soil_sand                  &amp;lt;int&amp;gt; 41, 39, 27, 29, 48, 33, 30, 78, 23, 64, 54,…
## $ soil_clay                  &amp;lt;int&amp;gt; 20, 24, 28, 31, 27, 29, 40, 15, 26, 22, 23,…
## $ soil_silt                  &amp;lt;int&amp;gt; 38, 35, 43, 38, 23, 36, 29, 6, 49, 13, 22, …
## $ soil_ph                    &amp;lt;dbl&amp;gt; 6.5, 5.9, 5.6, 5.5, 6.5, 5.8, 5.2, 7.1, 7.3…
## $ soil_soc                   &amp;lt;dbl&amp;gt; 43.1, 14.6, 36.4, 34.9, 8.1, 20.8, 44.5, 4.…
## $ soil_nitrogen              &amp;lt;dbl&amp;gt; 2.8, 1.3, 2.9, 3.6, 1.2, 1.9, 2.8, 0.6, 3.1…
## $ solar_rad_mean             &amp;lt;dbl&amp;gt; 17.634, 19.198, 13.257, 14.163, 24.512, 17.…
## $ solar_rad_max              &amp;lt;dbl&amp;gt; 31.317, 24.498, 25.283, 17.237, 28.038, 22.…
## $ solar_rad_min              &amp;lt;dbl&amp;gt; 5.209, 13.311, 1.587, 9.642, 19.102, 12.196…
## $ solar_rad_range            &amp;lt;dbl&amp;gt; 26.108, 11.187, 23.696, 7.595, 8.936, 10.20…
## $ growing_season_length      &amp;lt;dbl&amp;gt; 139, 365, 164, 333, 228, 365, 365, 60, 365,…
## $ growing_season_temperature &amp;lt;dbl&amp;gt; 12.65, 19.35, 11.55, 12.45, 26.45, 17.75, 2…
## $ growing_season_rainfall    &amp;lt;dbl&amp;gt; 224.5, 1493.4, 345.4, 1765.5, 984.4, 1860.5…
## $ growing_degree_days        &amp;lt;dbl&amp;gt; 2140.5, 7080.9, 2053.2, 4162.9, 10036.7, 64…
## $ temperature_mean           &amp;lt;dbl&amp;gt; 3.65, 19.35, 1.45, 11.35, 27.55, 17.65, 22.…
## $ temperature_max            &amp;lt;dbl&amp;gt; 24.65, 33.35, 21.15, 23.75, 38.35, 30.55, 2…
## $ temperature_min            &amp;lt;dbl&amp;gt; -14.05, 3.05, -18.25, -3.55, 19.15, 2.45, 1…
## $ temperature_range          &amp;lt;dbl&amp;gt; 38.7, 30.3, 39.4, 27.3, 19.2, 28.1, 7.0, 29…
## $ temperature_seasonality    &amp;lt;dbl&amp;gt; 882.6, 786.6, 1070.9, 724.7, 219.3, 747.2, …
## $ rainfall_mean              &amp;lt;int&amp;gt; 446, 1493, 560, 1794, 990, 1860, 3150, 356,…
## $ rainfall_min               &amp;lt;int&amp;gt; 25, 37, 24, 29, 0, 60, 122, 1, 10, 12, 0, 0…
## $ rainfall_max               &amp;lt;int&amp;gt; 62, 209, 87, 293, 226, 275, 425, 62, 256, 3…
## $ rainfall_range             &amp;lt;int&amp;gt; 37, 172, 63, 264, 226, 215, 303, 61, 245, 2…
## $ evapotranspiration_mean    &amp;lt;dbl&amp;gt; 78.32, 105.88, 50.03, 64.65, 156.60, 108.50…
## $ evapotranspiration_max     &amp;lt;dbl&amp;gt; 164.70, 190.86, 117.53, 115.79, 187.71, 191…
## $ evapotranspiration_min     &amp;lt;dbl&amp;gt; 13.67, 50.44, 3.53, 28.01, 128.59, 51.39, 8…
## $ evapotranspiration_range   &amp;lt;dbl&amp;gt; 151.03, 140.42, 113.99, 87.79, 59.13, 139.9…
## $ cloud_cover_mean           &amp;lt;int&amp;gt; 31, 48, 42, 64, 38, 52, 60, 13, 53, 20, 11,…
## $ cloud_cover_max            &amp;lt;int&amp;gt; 39, 61, 49, 71, 58, 67, 77, 18, 60, 27, 23,…
## $ cloud_cover_min            &amp;lt;int&amp;gt; 16, 34, 33, 54, 19, 39, 45, 6, 45, 14, 2, 1…
## $ cloud_cover_range          &amp;lt;int&amp;gt; 23, 27, 15, 17, 38, 27, 32, 11, 15, 12, 21,…
## $ aridity_index              &amp;lt;dbl&amp;gt; 0.54, 1.27, 0.90, 2.08, 0.55, 1.67, 2.88, 0…
## $ humidity_mean              &amp;lt;dbl&amp;gt; 55.56, 62.14, 59.87, 69.32, 51.60, 62.76, 7…
## $ humidity_max               &amp;lt;dbl&amp;gt; 63.98, 65.00, 68.19, 71.90, 67.07, 65.68, 7…
## $ humidity_min               &amp;lt;dbl&amp;gt; 48.41, 58.97, 53.75, 67.21, 33.89, 59.92, 7…
## $ humidity_range             &amp;lt;dbl&amp;gt; 15.57, 6.03, 14.44, 4.69, 33.18, 5.76, 3.99…
## $ biogeo_ecoregion           &amp;lt;chr&amp;gt; &amp;quot;South Central Rockies forests&amp;quot;, &amp;quot;Jian Nan …
## $ biogeo_biome               &amp;lt;chr&amp;gt; &amp;quot;Temperate Conifer Forests&amp;quot;, &amp;quot;Tropical &amp;amp; Su…
## $ biogeo_realm               &amp;lt;chr&amp;gt; &amp;quot;Nearctic&amp;quot;, &amp;quot;Indomalayan&amp;quot;, &amp;quot;Nearctic&amp;quot;, &amp;quot;Pal…
## $ country_name               &amp;lt;chr&amp;gt; &amp;quot;United States of America&amp;quot;, &amp;quot;China&amp;quot;, &amp;quot;Canad…
## $ country_population         &amp;lt;dbl&amp;gt; 313973000, 1338612970, 33487208, 1338612970…
## $ country_gdp                &amp;lt;dbl&amp;gt; 15094000, 7973000, 1300000, 7973000, 15860,…
## $ country_income             &amp;lt;chr&amp;gt; &amp;quot;1. High income: OECD&amp;quot;, &amp;quot;3. Upper middle in…
## $ continent                  &amp;lt;chr&amp;gt; &amp;quot;North America&amp;quot;, &amp;quot;Asia&amp;quot;, &amp;quot;North America&amp;quot;, &amp;quot;…
## $ region                     &amp;lt;chr&amp;gt; &amp;quot;Americas&amp;quot;, &amp;quot;Asia&amp;quot;, &amp;quot;Americas&amp;quot;, &amp;quot;Asia&amp;quot;, &amp;quot;Af…
## $ subregion                  &amp;lt;chr&amp;gt; &amp;quot;Northern America&amp;quot;, &amp;quot;Eastern Asia&amp;quot;, &amp;quot;Northe…
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The categorical variables in this data frame are identified below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vi_categorical &amp;lt;- collinear::identify_non_numeric_predictors(
  df = vi,
  predictors = vi_predictors
)
vi_categorical
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;koppen_zone&amp;quot;        &amp;quot;koppen_group&amp;quot;       &amp;quot;koppen_description&amp;quot;
##  [4] &amp;quot;soil_type&amp;quot;          &amp;quot;biogeo_ecoregion&amp;quot;   &amp;quot;biogeo_biome&amp;quot;      
##  [7] &amp;quot;biogeo_realm&amp;quot;       &amp;quot;country_name&amp;quot;       &amp;quot;country_income&amp;quot;    
## [10] &amp;quot;continent&amp;quot;          &amp;quot;region&amp;quot;             &amp;quot;subregion&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And finally, their number of categories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data.frame(
  name = vi_categorical,
  categories = lapply(
  X = vi_categorical,
  FUN = function(x) length(unique(vi[[x]]))
) |&amp;gt; 
  unlist()
) |&amp;gt; 
  dplyr::arrange(
    dplyr::desc(categories)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  name categories
## 1    biogeo_ecoregion        604
## 2        country_name        176
## 3           soil_type         29
## 4         koppen_zone         25
## 5           subregion         21
## 6  koppen_description         19
## 7        biogeo_biome         13
## 8        biogeo_realm          7
## 9           continent          7
## 10     country_income          6
## 11             region          6
## 12       koppen_group          5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A few, like &lt;code&gt;country_name&lt;/code&gt; and &lt;code&gt;biogeo_ecoregion&lt;/code&gt;, show a cardinality high enough to ruin our day, don&amp;rsquo;t they? But ok, let&amp;rsquo;s start with one with a moderate number of categories, like &lt;code&gt;koppen_zone&lt;/code&gt;. This variable has 25 categories representing climate zones.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sort(unique(vi$koppen_zone))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Af&amp;quot;  &amp;quot;Am&amp;quot;  &amp;quot;Aw&amp;quot;  &amp;quot;BSh&amp;quot; &amp;quot;BSk&amp;quot; &amp;quot;BWh&amp;quot; &amp;quot;BWk&amp;quot; &amp;quot;Cfa&amp;quot; &amp;quot;Cfb&amp;quot; &amp;quot;Cfc&amp;quot; &amp;quot;Csa&amp;quot; &amp;quot;Csb&amp;quot;
## [13] &amp;quot;Cwa&amp;quot; &amp;quot;Cwb&amp;quot; &amp;quot;Dfa&amp;quot; &amp;quot;Dfb&amp;quot; &amp;quot;Dfc&amp;quot; &amp;quot;Dfd&amp;quot; &amp;quot;Dsa&amp;quot; &amp;quot;Dsb&amp;quot; &amp;quot;Dsc&amp;quot; &amp;quot;Dwa&amp;quot; &amp;quot;Dwb&amp;quot; &amp;quot;Dwc&amp;quot;
## [25] &amp;quot;ET&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;one-hot-encoding-is-here&#34;&gt;One-hot Encoding is here&amp;hellip;&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s use it as predictor of &lt;code&gt;vi_mean&lt;/code&gt; in a linear model and take a look at the summary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;lm(
  formula = vi_mean ~ koppen_zone, 
  data = vi
  ) |&amp;gt; 
  summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = vi_mean ~ koppen_zone, data = vi)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.57090 -0.05592 -0.00305  0.05695  0.49212 
## 
## Coefficients:
##                 Estimate Std. Error  t value Pr(&amp;gt;|t|)    
## (Intercept)     0.670899   0.002054  326.651  &amp;lt; 2e-16 ***
## koppen_zoneAm  -0.022807   0.003151   -7.239 4.64e-13 ***
## koppen_zoneAw  -0.143375   0.002434  -58.903  &amp;lt; 2e-16 ***
## koppen_zoneBSh -0.347894   0.002787 -124.839  &amp;lt; 2e-16 ***
## koppen_zoneBSk -0.422162   0.002823 -149.523  &amp;lt; 2e-16 ***
## koppen_zoneBWh -0.537854   0.002392 -224.859  &amp;lt; 2e-16 ***
## koppen_zoneBWk -0.543022   0.002906 -186.883  &amp;lt; 2e-16 ***
## koppen_zoneCfa -0.104730   0.003087  -33.928  &amp;lt; 2e-16 ***
## koppen_zoneCfb -0.081909   0.003949  -20.744  &amp;lt; 2e-16 ***
## koppen_zoneCfc -0.120899   0.017419   -6.941 3.99e-12 ***
## koppen_zoneCsa -0.274720   0.005145  -53.399  &amp;lt; 2e-16 ***
## koppen_zoneCsb -0.136575   0.006142  -22.237  &amp;lt; 2e-16 ***
## koppen_zoneCwa -0.149006   0.003318  -44.910  &amp;lt; 2e-16 ***
## koppen_zoneCwb -0.177753   0.004579  -38.817  &amp;lt; 2e-16 ***
## koppen_zoneDfa -0.214981   0.004437  -48.453  &amp;lt; 2e-16 ***
## koppen_zoneDfb -0.179080   0.003347  -53.499  &amp;lt; 2e-16 ***
## koppen_zoneDfc -0.237050   0.003937  -60.207  &amp;lt; 2e-16 ***
## koppen_zoneDfd -0.395899   0.065900   -6.008 1.91e-09 ***
## koppen_zoneDsa -0.462494   0.011401  -40.567  &amp;lt; 2e-16 ***
## koppen_zoneDsb -0.330969   0.008056  -41.084  &amp;lt; 2e-16 ***
## koppen_zoneDsc -0.327097   0.011244  -29.090  &amp;lt; 2e-16 ***
## koppen_zoneDwa -0.282620   0.005248  -53.850  &amp;lt; 2e-16 ***
## koppen_zoneDwb -0.254027   0.005981  -42.473  &amp;lt; 2e-16 ***
## koppen_zoneDwc -0.306156   0.005660  -54.096  &amp;lt; 2e-16 ***
## koppen_zoneET  -0.297869   0.011649  -25.571  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.09315 on 29975 degrees of freedom
## Multiple R-squared:  0.805,	Adjusted R-squared:  0.8049 
## F-statistic:  5157 on 24 and 29975 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look at this monster! What the hell happened here? Linear models cannot deal with categorical predictors, so they create numeric &lt;strong&gt;dummy variables&lt;/strong&gt; instead. The function &lt;code&gt;stats::model.matrix()&lt;/code&gt; does exactly that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dummy_variables &amp;lt;- stats::model.matrix( 
  ~ koppen_zone,
  data = vi
  )
ncol(dummy_variables)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 25
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dummy_variables[1:10, 1:10]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    (Intercept) koppen_zoneAm koppen_zoneAw koppen_zoneBSh koppen_zoneBSk
## 1            1             0             0              0              1
## 2            1             0             0              0              0
## 3            1             0             0              0              0
## 4            1             0             0              0              0
## 5            1             0             1              0              0
## 6            1             0             0              0              0
## 7            1             0             0              0              0
## 8            1             0             0              1              0
## 9            1             0             0              0              0
## 10           1             0             0              0              0
##    koppen_zoneBWh koppen_zoneBWk koppen_zoneCfa koppen_zoneCfb koppen_zoneCfc
## 1               0              0              0              0              0
## 2               0              0              1              0              0
## 3               0              0              0              0              0
## 4               0              0              0              1              0
## 5               0              0              0              0              0
## 6               0              0              1              0              0
## 7               0              0              0              0              0
## 8               0              0              0              0              0
## 9               0              0              0              0              0
## 10              1              0              0              0              0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function first creates an Intercept column with all ones. Then, for each original category except the first one (&amp;ldquo;Af&amp;rdquo;), a new column with value 1 in the cases where the given category was present and 0 otherwise is created. The category with no column (&amp;ldquo;Af&amp;rdquo;) is represented in these cases in the intercept where all other dummy columns are zero. This is, essentially, &lt;strong&gt;one-hot encoding&lt;/strong&gt; with a little twist. You will find most people use the terms &lt;em&gt;dummy variables&lt;/em&gt; and &lt;em&gt;one-hot encoding&lt;/em&gt; interchangeably, and that&amp;rsquo;s ok. But in the end, the little twist of omitting the first category is what differentiates them. Most functions performing one-hot encoding, no matter their name, are creating as many columns as categories. That is for example the case of &lt;code&gt;fastDummies::dummy_cols()&lt;/code&gt;, from the R package 
&lt;a href=&#34;https://jacobkap.github.io/fastDummies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;fastDummies&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- fastDummies::dummy_cols(
  .data = vi[, &amp;quot;koppen_zone&amp;quot;, drop = FALSE],
  select_columns = &amp;quot;koppen_zone&amp;quot;,
  remove_selected_columns = TRUE
)
dplyr::glimpse(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 30,000
## Columns: 25
## $ koppen_zone_Af  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Am  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Aw  &amp;lt;int&amp;gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_BSh &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …
## $ koppen_zone_BSk &amp;lt;int&amp;gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_BWh &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, …
## $ koppen_zone_BWk &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …
## $ koppen_zone_Cfa &amp;lt;int&amp;gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Cfb &amp;lt;int&amp;gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …
## $ koppen_zone_Cfc &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Csa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Csb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Cwa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Cwb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dfa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …
## $ koppen_zone_Dfb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dfc &amp;lt;int&amp;gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dfd &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dsa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dsb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dsc &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dwa &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dwb &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_Dwc &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ koppen_zone_ET  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;to-mess-up-your-models&#34;&gt;&amp;hellip;to mess-up your models&lt;/h1&gt;
&lt;p&gt;As good as one-hot encoding is to fit linear models when predictors are categorical, it creates a couple of glaring issues that are hard to address when the number of encoded categories is high.&lt;/p&gt;
&lt;p&gt;The first issue can easily be named the &lt;strong&gt;dimensionality explosion&lt;/strong&gt;. If we created dummy variables for all categorical predictors in &lt;code&gt;vi&lt;/code&gt;, then we&amp;rsquo;d go from the original 61 predictors to a total of 967 new columns to handle. This alone can degrade the computational performance of a model due to increased data size.&lt;/p&gt;
&lt;p&gt;The second issue is &lt;strong&gt;increased multicollinearity&lt;/strong&gt;. One-hot encoded features are highly collinear, which makes obtaining accurate estimates for the coefficients of the encoded categories very hard. Look at the Variance Inflation Factors of the encoded Koppen zones, they have incredibly high values!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::vif_df(
  df = df
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           variable          vif
## 1  koppen_zone_Dfd 1.031226e+12
## 2  koppen_zone_Cfc 1.493932e+13
## 3   koppen_zone_ET 3.395786e+13
## 4  koppen_zone_Dsa 3.549784e+13
## 5  koppen_zone_Dsc 3.652432e+13
## 6  koppen_zone_Dsb 7.338609e+13
## 7  koppen_zone_Csb 1.323997e+14
## 8  koppen_zone_Dwb 1.405032e+14
## 9  koppen_zone_Dwc 1.592088e+14
## 10 koppen_zone_Dwa 1.894423e+14
## 11 koppen_zone_Csa 1.984881e+14
## 12 koppen_zone_Cwb 2.624933e+14
## 13 koppen_zone_Dfa 2.838687e+14
## 14 koppen_zone_Cfb 3.834325e+14
## 15 koppen_zone_Dfc 3.863684e+14
## 16 koppen_zone_Dfb 6.139201e+14
## 17 koppen_zone_Cwa 6.309241e+14
## 18  koppen_zone_Am 7.440723e+14
## 19 koppen_zone_Cfa 7.966760e+14
## 20 koppen_zone_BWk 9.866240e+14
## 21  koppen_zone_Af 9.879589e+14
## 22 koppen_zone_BSk 1.100300e+15
## 23 koppen_zone_BSh 1.158438e+15
## 24  koppen_zone_Aw 2.177627e+15
## 25 koppen_zone_BWh 2.403991e+15
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On top of those issues, one-hot encoding also causes &lt;strong&gt;sparsity&lt;/strong&gt; in tree-based models. Let me show you an example. Below I train a recursive partition tree using &lt;code&gt;vi_mean&lt;/code&gt; as response, and the one-hot encoded version of &lt;code&gt;koppen_zone&lt;/code&gt; we have in &lt;code&gt;df&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#add response variable to df
df$vi_mean &amp;lt;- vi$vi_mean

#fit model using all one-hot encoded variables
koppen_zone_one_hot &amp;lt;- rpart::rpart(
  formula = vi_mean ~ .,
  data = df
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I do the same using the categorical version of &lt;code&gt;koppen_zone&lt;/code&gt; in &lt;code&gt;vi&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;koppen_zone_categorical &amp;lt;- rpart::rpart(
  formula = vi_mean ~ koppen_zone,
  data = vi
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I am plotting the skeletons of these trees side by side (we don&amp;rsquo;t care about numbers here).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#plot tree skeleton
par(mfrow = c(1, 2))
plot(koppen_zone_one_hot, main = &amp;quot;One-hot encoding&amp;quot;)
plot(koppen_zone_categorical, main = &amp;quot;Categorical&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/target-encoding/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;576&#34; /&gt;
&lt;p&gt;Notice the stark differences in tree structure between both options. On the left, the tree trained on the one-hot encoded data only shows growth on one side! This is the &lt;em&gt;sparsity&lt;/em&gt; I was talking about before. On the right side, however, the tree based on the categorical variable shows a balanced and healthy structure. One-hot encoded data can easily mess up a single univariate regression tree, so imagine what it can do to your fancy random forest model with hundreds of these trees.&lt;/p&gt;
&lt;p&gt;In the end, the magic of one-hot encoding is in its inherent ability to create two or three problems for each one it promised to solve. We all know someone like that. Not so hot, if you ask me.&lt;/p&gt;
&lt;h1 id=&#34;target-encoding-mean-encoding-and-dummy-variables-all-the-same&#34;&gt;Target Encoding, Mean Encoding, and Dummy Variables (All The Same)&lt;/h1&gt;
&lt;p&gt;On a bright summer day of 2001, 
&lt;a href=&#34;https://www.aitimejournal.com/interview-with-daniele-micci-barreca-product-analytics-lead-data-science-google/30110/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniele Micci-Barreca&lt;/a&gt; finally got sick of the one-hot encoding wonders and decided to publish 
&lt;a href=&#34;https://doi.org/10.1145/507533.507538&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;his ideas on a suitable alternative&lt;/a&gt; others later named &lt;em&gt;mean encoding&lt;/em&gt; or &lt;em&gt;target encoding&lt;/em&gt;. He told the story himself 20 years later, in a nice blog post titled 
&lt;a href=&#34;https://towardsdatascience.com/extending-target-encoding-443aa9414cae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Extending Target Encoding&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But what is target encoding? Let&amp;rsquo;s start with a continuous response variable &lt;code&gt;y&lt;/code&gt; (a.k.a &lt;em&gt;the target&lt;/em&gt;) and a categorical predictor &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;mean-encoding&#34;&gt;Mean Encoding&lt;/h2&gt;
&lt;p&gt;In &lt;em&gt;it&amp;rsquo;s simplest form&lt;/em&gt;, target encoding replaces each category in &lt;code&gt;x&lt;/code&gt; with the mean of &lt;code&gt;y&lt;/code&gt; across the category cases. This results in a new numeric version of &lt;code&gt;x&lt;/code&gt; named &lt;code&gt;x_encoded&lt;/code&gt; in the example below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx |&amp;gt; 
  dplyr::group_by(x) |&amp;gt; 
  dplyr::mutate(
    x_encoded = mean(y)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a             2
## 2     2 a             2
## 3     3 a             2
## 4     4 b             5
## 5     5 b             5
## 6     6 b             5
## 7     7 c             7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simple is good, right? But sometimes it&amp;rsquo;s not. In our toy case, the category &amp;ldquo;c&amp;rdquo; has only one case that maps directly to an actual value of &lt;code&gt;y&lt;/code&gt;.Imagine the worst case scenario of &lt;code&gt;x&lt;/code&gt; having one different category per row, then &lt;code&gt;x_encoded&lt;/code&gt; would be identical to &lt;code&gt;y&lt;/code&gt;!&lt;/p&gt;
&lt;h2 id=&#34;mean-encoding-with-additive-smoothing&#34;&gt;Mean Encoding With Additive Smoothing&lt;/h2&gt;
&lt;p&gt;The issue can be solved by pushing the mean of &lt;code&gt;y&lt;/code&gt; for each category in &lt;code&gt;x&lt;/code&gt; towards the global mean of &lt;code&gt;y&lt;/code&gt; by the weighted sample size of the category, as suggested by the expression&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$x\_encoded_i = \frac{n_i \times \overline{y}_i + m \times \overline{y}}{n_i + m}$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;\(n_i\)&lt;/code&gt; is the size of the category &lt;code&gt;\(i\)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\(\overline{y}_i\)&lt;/code&gt; is the mean of the target over the category &lt;code&gt;\(i\)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\(m\)&lt;/code&gt; is the smoothing parameter.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\(\overline{y}\)&lt;/code&gt; is the global mean of the target.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y_mean &amp;lt;- mean(yx$y)

m &amp;lt;- 3

yx |&amp;gt; 
  dplyr::group_by(x) |&amp;gt; 
  dplyr::mutate(
    x_encoded = 
      (dplyr::n() * mean(y) + m * y_mean) / (dplyr::n() + m)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a          3   
## 2     2 a          3   
## 3     3 a          3   
## 4     4 b          4.5 
## 5     5 b          4.5 
## 6     6 b          4.5 
## 7     7 c          4.75
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far so good! But still, the simplest implementations of target encoding generate repeated values for all cases within a category. This can still mess-up tree-based models a bit, because splits may happen again and again in the same values of the predictor. However, there are several strategies to limit this issue as well.&lt;/p&gt;
&lt;h2 id=&#34;leave-one-out-target-encoding&#34;&gt;Leave-one-out Target Encoding&lt;/h2&gt;
&lt;p&gt;In this version of target encoding, the encoded value of one case within a category is the mean of all other cases within the same category. This results in a robust encoding that avoids direct reference to the target value of the sample being encoded, and does not generate repeated values.&lt;/p&gt;
&lt;p&gt;The code below implements the idea in a way so simple that it cannot even deal with one-case categories.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx |&amp;gt;
  dplyr::group_by(x) |&amp;gt;
  dplyr::mutate(
    x_encoded = (sum(y) - y) / (dplyr::n() - 1)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a           2.5
## 2     2 a           2  
## 3     3 a           1.5
## 4     4 b           5.5
## 5     5 b           5  
## 6     6 b           4.5
## 7     7 c         NaN
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;mean-encoding-with-white-noise&#34;&gt;Mean Encoding with White Noise&lt;/h2&gt;
&lt;p&gt;Another way to avoid repeated values while keeping the encoding as simple as possible consists of just adding a white noise to the encoded values. The code below adds noise generated by &lt;code&gt;stats::runif()&lt;/code&gt; to the mean-encoded values, but other options such as &lt;code&gt;stats::rnorm()&lt;/code&gt; (noise from a normal distribution) can be useful here. Since white noise is random, we need to set the seed of the pseudo-random number generator (with &lt;code&gt;set.seed()&lt;/code&gt;) to obtain constant results every time we run the code below.&lt;/p&gt;
&lt;p&gt;When using this method we have to be careful with the amount of noise we add. It should be a harmless fraction of target, small enough to not throw a model off the signal provided by the encoded variable. In our toy case &lt;code&gt;y&lt;/code&gt; is between 1 and 7, so something like &amp;ldquo;one percent of the maximum&amp;rdquo; could work well here.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#maximum noise to add
max_noise &amp;lt;- max(yx$y)/100

#set seed for reproducibility
set.seed(1)

yx |&amp;gt; 
  dplyr::group_by(x) |&amp;gt; 
  dplyr::mutate(
    x_encoded = mean(y) + runif(n = dplyr::n(), max = max_noise)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a          2.02
## 2     2 a          2.03
## 3     3 a          2.04
## 4     4 b          5.06
## 5     5 b          5.01
## 6     6 b          5.06
## 7     7 c          7.07
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This method can deal with one-case categories without issues, and does not generate repeated values, but in exchange, we have to be mindful of the amount of noise we add, and we have to set a random seed to ensure reproducibility.&lt;/p&gt;
&lt;h2 id=&#34;random-encoding&#34;&gt;Random Encoding&lt;/h2&gt;
&lt;p&gt;A more exotic non-deterministic method of encoding consists of computing the mean and the standard deviation of the target over the category, and then using these values to parameterize a normal distribution to extract randomized values from. This kind of encoding also requires to set the random seed to ensure reproducibility.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(1)

yx |&amp;gt;
  dplyr::group_by(x) |&amp;gt;
  dplyr::mutate(
    x_encoded = stats::rnorm(
      n = dplyr::n(),
      mean = mean(y),
      sd = ifelse(
        dplyr::n() == 1,
        stats::sd(yx$y), #use global sd for one-case groups
        stats::sd(y)     #use local sd for n-cases groups
      )
    )
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a          1.37
## 2     2 a          2.18
## 3     3 a          1.16
## 4     4 b          6.60
## 5     5 b          5.33
## 6     6 b          4.18
## 7     7 c          8.05
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;rank-encoding-plus-white-noise&#34;&gt;Rank Encoding plus White Noise&lt;/h2&gt;
&lt;p&gt;This is a little different from all the other methods, because it does not map the categories to values from the target, but to the rank/order of the target means per category. It basically converts the categorical variable into an ordinal one arranged along with the target, and then adds white noise on top to avoid value repetition.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#maximum noise as function of the number of categories
max_noise &amp;lt;- length(unique(yx$x))/100

yx |&amp;gt; 
  dplyr::arrange(y) |&amp;gt; 
  dplyr::group_by(x) |&amp;gt; 
  dplyr::mutate(
    x_encoded = dplyr::cur_group_id() + runif(n = dplyr::n(), max = max_noise)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 × 3
## # Groups:   x [3]
##       y x     x_encoded
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1 a          1.02
## 2     2 a          1.01
## 3     3 a          1.02
## 4     4 b          2.03
## 5     5 b          2.01
## 6     6 b          2.02
## 7     7 c          3.03
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-target-encoding-lab&#34;&gt;The Target Encoding Lab&lt;/h2&gt;
&lt;p&gt;The function &lt;code&gt;collinear::target_encoding_lab()&lt;/code&gt; implements all these encoding methods, and allows defining different combinations of parameters. It was designed to help understand how they work, and maybe help make choices about what&amp;rsquo;s the right encoding for a given categorical predictor.&lt;/p&gt;
&lt;p&gt;In the example below, the methods rank, mean, and leave-one-out are computed with white noise of 0 and 0.1 (that&amp;rsquo;s the width of the uniform distribution the noise is extracted from), the mean is also with and without smoothing, and the rnorm is computed using two different multipliers of the standard deviation of the normal distribution computed for each group in the predictor, just to help control the data spread.&lt;/p&gt;
&lt;p&gt;The function also uses a random seed to generate the same noise across the encoded versions of the predictor to make them as comparable as possible. Every time you change the seed, results using white noise and the rnorm method should change as well.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx_encoded &amp;lt;- target_encoding_lab(
  df = yx,
  response = &amp;quot;y&amp;quot;,
  predictors = &amp;quot;x&amp;quot;,
  white_noise = c(0, 0.1),
  smoothing = c(0, 2),
  rnorm_sd_multiplier = c(0.25, 0.5),
  verbose = TRUE,
  seed = 1, #for reproducibility
  replace = FALSE #to replace or not the predictors with their encodings
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Encoding the predictor: x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New encoded predictor: &#39;x__encoded_rank&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New encoded predictor: &#39;x__encoded_mean&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New encoded predictor: &#39;x__encoded_mean__smoothing_2&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New encoded predictor: &#39;x__encoded_loo&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New encoded predictor: &#39;x__encoded_rank__noise_0.1&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New encoded predictor: &#39;x__encoded_mean__noise_0.1&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New encoded predictor: &#39;x__encoded_mean__smoothing_2__noise_0.1&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New encoded predictor: &#39;x__encoded_loo__noise_0.1&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New encoded predictor: &#39;x__encoded_rnorm__sd_multiplier_0.25&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New encoded predictor: &#39;x__encoded_rnorm__sd_multiplier_0.5&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dplyr::glimpse(yx_encoded)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 7
## Columns: 12
## $ y                                       &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7
## $ x                                       &amp;lt;chr&amp;gt; &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;b&amp;quot;, …
## $ x__encoded_rank                         &amp;lt;int&amp;gt; 1, 1, 1, 2, 2, 2, 3
## $ x__encoded_mean                         &amp;lt;dbl&amp;gt; 2, 2, 2, 5, 5, 5, 7
## $ x__encoded_mean__smoothing_2            &amp;lt;dbl&amp;gt; 2.8, 2.8, 2.8, 4.6, 4.6, 4.6, …
## $ x__encoded_loo                          &amp;lt;dbl&amp;gt; 2.5, 2.0, 1.5, 5.5, 5.0, 4.5, …
## $ x__encoded_rank__noise_0.1              &amp;lt;dbl&amp;gt; 0.5030858, 0.6752789, 0.999474…
## $ x__encoded_mean__noise_0.1              &amp;lt;dbl&amp;gt; 1.503086, 1.675279, 1.999475, …
## $ x__encoded_mean__smoothing_2__noise_0.1 &amp;lt;dbl&amp;gt; 2.303086, 2.475279, 2.799475, …
## $ x__encoded_loo__noise_0.1               &amp;lt;dbl&amp;gt; 2.003086, 1.675279, 1.499475, …
## $ x__encoded_rnorm__sd_multiplier_0.25    &amp;lt;dbl&amp;gt; 1.843387, 2.045911, 1.791093, …
## $ x__encoded_rnorm__sd_multiplier_0.5     &amp;lt;dbl&amp;gt; 1.686773, 2.091822, 1.582186, …
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx_encoded |&amp;gt; 
  tidyr::pivot_longer(
    cols = dplyr::contains(&amp;quot;__encoded&amp;quot;),
    values_to = &amp;quot;x_encoded&amp;quot;
  ) |&amp;gt; 
  ggplot() + 
  facet_wrap(&amp;quot;name&amp;quot;) +
  aes(
    x = x_encoded,
    y = y,
    color = x
  ) +
  geom_point(size = 3) + 
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/target-encoding/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;1152&#34; /&gt;
The function also allows to replace a given predictor with their selected encoding.
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yx_encoded &amp;lt;- collinear::target_encoding_lab(
  df = yx,
  response = &amp;quot;y&amp;quot;,
  predictors = &amp;quot;x&amp;quot;,
  encoding_methods = &amp;quot;mean&amp;quot;, #selected encoding method
  smoothing = 2,
  verbose = TRUE,
  replace = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in validate_df(df = df, min_rows = 30): the number of rows in &#39;df&#39; is
## lower than 30. A multicollinearity analysis may fail or yield meaningless
## results.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dplyr::glimpse(yx_encoded)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 7
## Columns: 2
## $ y &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7
## $ x &amp;lt;dbl&amp;gt; 2.8, 2.8, 2.8, 4.6, 4.6, 4.6, 5.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s all about target encoding so far!&lt;/p&gt;
&lt;p&gt;I have a post in my TODO list with a little real experiment comparing target encoding with one-hot encoding in tree-based models. If you are interested, stay tuned!&lt;/p&gt;
&lt;p&gt;Cheers,&lt;/p&gt;
&lt;p&gt;Blas&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Everything You Don&#39;t Need to Know About Variance Inflation Factors</title>
      <link>https://blasbenito.com/post/variance-inflation-factor/</link>
      <pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/variance-inflation-factor/</guid>
      <description>&lt;h1 id=&#34;resources&#34;&gt;Resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/BlasBenito/notebooks/blob/main/variance_inflation_factors.Rmd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rmarkdown notebook used in this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://blasbenito.com/post/multicollinearity-model-interpretability/&#34;&gt;Multicollinearity Hinders Model Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R package &lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post, I dive deep into Variance Inflation Factors (VIF) and their crucial role in identifying multicollinearity within linear models.&lt;/p&gt;
&lt;p&gt;The post covers the following main points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VIF meaning and interpretation&lt;/strong&gt;: Through practical examples, I demonstrate how to compute VIF values and their significance in model design. Particularly, I try to shed light on their influence on coefficient estimates and their confidence intervals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Impact of High VIF&lt;/strong&gt;: I use a small simulation to show how having a model design with a high VIF hinders the identification of predictors with moderate effects, particularly in situations with limited data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Effective VIF Management&lt;/strong&gt;: I introduce how to use the &lt;code&gt;collinear&lt;/code&gt; package and its &lt;code&gt;vif_select()&lt;/code&gt; function. to aid in the selection of predictors with low VIF, thereby enhancing model stability and interpretability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ultimately, this post serves as a comprehensive resource for understanding, interpreting, and managing VIF in the context of linear modeling. It caters to those with a strong command of R and a keen interest in statistical modeling.&lt;/p&gt;
&lt;h1 id=&#34;r-packages&#34;&gt;R packages&lt;/h1&gt;
&lt;p&gt;This tutorial requires the development version (&amp;gt;= 1.0.3) of the newly released R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;, and a few more.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#required
install.packages(&amp;quot;remotes&amp;quot;)
remotes::install_github(
  repo = &amp;quot;blasbenito/collinear&amp;quot;, 
  ref = &amp;quot;development&amp;quot;
  )
install.packages(&amp;quot;ranger&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)
install.packages(&amp;quot;ggplot2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;example-data&#34;&gt;Example data&lt;/h1&gt;
&lt;p&gt;This post uses the &lt;code&gt;toy&lt;/code&gt; data set shipped with the version &amp;gt;= 1.0.3 of the R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;. It is a data frame of centered and scaled variables representing a model design of the form &lt;code&gt;y ~ a + b + c + d&lt;/code&gt;, where the predictors show varying degrees of relatedness. Let&amp;rsquo;s load and check it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
library(ggplot2)
library(collinear)

toy |&amp;gt; 
  round(3) |&amp;gt; 
  head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        y      a      b      c      d
## 1  0.655  0.342 -0.158  0.254  0.502
## 2  0.610  0.219  1.814  0.450  1.373
## 3  0.316  1.078 -0.643  0.580  0.673
## 4  0.202  0.956 -0.815  1.168 -0.147
## 5 -0.509 -0.149 -0.356 -0.456  0.187
## 6  0.675  0.465  1.292 -0.020  0.983
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The columns in &lt;code&gt;toy&lt;/code&gt; are related as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: response generated from &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; using the expression &lt;code&gt;y = a * 0.75 + b * 0.25 + noise&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt;: predictor of &lt;code&gt;y&lt;/code&gt; uncorrelated with &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt;: predictor of &lt;code&gt;y&lt;/code&gt; uncorrelated with &lt;code&gt;a&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: predictor generated as &lt;code&gt;c = a + noise&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d&lt;/code&gt;: predictor generated as &lt;code&gt;d = (a + b)/2 + noise&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The pairwise correlations between all predictors in &lt;code&gt;toy&lt;/code&gt; are shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::cor_df(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x y correlation
## 1 c a  0.96154984
## 2 d b  0.63903887
## 3 d a  0.63575882
## 4 d c  0.61480312
## 5 b a -0.04740881
## 6 c b -0.04218308
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep these pairwise correlations in mind for what comes next!&lt;/p&gt;
&lt;h1 id=&#34;the-meaning-of-variance-inflation-factors&#34;&gt;The Meaning of Variance Inflation Factors&lt;/h1&gt;
&lt;p&gt;There are two general cases of multicollinearity in model designs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When there are pairs of predictors highly correlated.&lt;/li&gt;
&lt;li&gt;When there are &lt;strong&gt;predictors that are linear combinations of other predictors&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The focus of this post is on the second one.&lt;/p&gt;
&lt;p&gt;We can say a predictor is a linear combination of other predictors when it can be reasonably predicted from a multiple regression model against all other predictors.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say we focus on &lt;code&gt;a&lt;/code&gt; and fit the multiple regression model &lt;code&gt;a ~ b + c + d&lt;/code&gt;. The higher the R-squared of this model, the more confident we are to say that &lt;code&gt;a&lt;/code&gt; is a linear combination of &lt;code&gt;b + c + d&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#model of a against all other predictors
abcd_model &amp;lt;- lm(
  formula = a ~ b + c + d,
  data = toy
)

#r-squared of the a_model
abcd_R2 &amp;lt;- summary(abcd_model)$r.squared
abcd_R2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9381214
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the R-squared of &lt;code&gt;a&lt;/code&gt; against all other predictors is pretty high, it definitely seems that &lt;code&gt;a&lt;/code&gt; is a linear combination of the other predictors, and we can conclude that there is multicollinearity in the model design.&lt;/p&gt;
&lt;p&gt;However, as informative as this R-squared is, it tells us nothing about the consequences of having multicollinearity in our model design. And this is where &lt;strong&gt;Variance Inflation Factors&lt;/strong&gt;, or &lt;strong&gt;VIF&lt;/strong&gt; for short, come into play.&lt;/p&gt;
&lt;h2 id=&#34;what-are-variance-inflation-factors&#34;&gt;What are Variance Inflation Factors?&lt;/h2&gt;
&lt;p&gt;The Variance Inflation Factor (VIF) of a predictor is computed as &lt;code&gt;$1/(1 - R^2)$&lt;/code&gt;, where &lt;code&gt;\(R^²\)&lt;/code&gt; is the R-squared of the multiple linear regression of the predictor against all other predictors.&lt;/p&gt;
&lt;p&gt;In the case of &lt;code&gt;a&lt;/code&gt;, we just have to apply the VIF expression to the R-squared of the regression model against all other predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;abcd_vif &amp;lt;- 1/(1-abcd_R2)
abcd_vif
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16.16067
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This VIF score is relative to the other predictors in the model design. If we change the model design, so does the VIF of all predictors! For example, if we remove &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt; from the model design, we are left with this VIF for &lt;code&gt;a&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ab_model &amp;lt;- lm(
  formula = a ~ b,
  data = toy
)

ab_vif &amp;lt;- 1/(1 - summary(ab_model)$r.squared)
ab_vif
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.002253
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An almost perfect VIF score!&lt;/p&gt;
&lt;p&gt;We can simplify the VIF computation using &lt;code&gt;collinear::vif_df()&lt;/code&gt;, which returns the VIF of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; at once.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::vif_df(
  df = toy[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   predictor    vif
## 1         a 1.0023
## 2         b 1.0023
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In plot below, the worst and best VIF scores of &lt;code&gt;a&lt;/code&gt; are shown in the context of the relationship between R-squared and VIF, and three VIF thresholds commonly mentioned in the literature. These thresholds are represented as vertical dashed lines at VIF 2.5, 5, and 10, and are used as criteria to control multicollinearity in model designs. I will revisit this topic later in the post.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/variance-inflation-factor/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;When the R-squared of the linear regression model is 0, then the VIF expression becomes &lt;code&gt;\(1/(1 - 0) = 1\)&lt;/code&gt; and returns the minimum possible VIF. On the other end, when R-squared is 1, then we get &lt;code&gt;\(1/(1 - 1) = Inf\)&lt;/code&gt;, the maximum VIF.&lt;/p&gt;
&lt;p&gt;So far, we have learned that to assess whether the predictor &lt;code&gt;a&lt;/code&gt; induces multicollinearity in the model design &lt;code&gt;y ~ a + b + c + d&lt;/code&gt; we can compute it&amp;rsquo;s Variance Inflation Factor from the R-squared of the model &lt;code&gt;a ~ b + c + d&lt;/code&gt;. We have also learned that if the model design changes, so does the VIF of &lt;code&gt;a&lt;/code&gt;. We also know that there are some magic numbers (the VIF thresholds) we can use as reference.&lt;/p&gt;
&lt;p&gt;But still, we have no indication of what these VIF values actually mean! I will try to fix that in the next section.&lt;/p&gt;
&lt;h2 id=&#34;but-really-what-are-variance-inflation-factors&#34;&gt;But really, what are Variance Inflation Factors?&lt;/h2&gt;
&lt;p&gt;Variance Inflation Factors are inherently linked to these fundamental linear modeling concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Coefficient Estimate&lt;/strong&gt; (&lt;code&gt;\(\hat{\beta}\)&lt;/code&gt;): The estimated slope of the relationship between a predictor and the response in a linear model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Standard Error&lt;/strong&gt; (&lt;code&gt;\(\text{SE}\)&lt;/code&gt;): Represents the uncertainty around the estimation of the coefficient due to data variability.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Significance level&lt;/strong&gt; (&lt;code&gt;\(1.96\)&lt;/code&gt;): The acceptable level of error when determining the significance of the &lt;em&gt;coefficient estimate&lt;/em&gt;. Here it is simplified to 1.96, the 97.5th percentile of a normal distribution, to approximate a significance level of 0.05.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Confidence Interval&lt;/strong&gt; (&lt;code&gt;\(CI\)&lt;/code&gt;): The range of values containing the true value of the &lt;em&gt;coefficient estimate&lt;/em&gt; withing a certain &lt;em&gt;significance level&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These terms are related by the expression to compute the confidence interval of the coefficient estimate:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$\text{CI} = \beta \pm 1.96 \cdot \text{SE}$$&lt;/code&gt;
Let me convert this equation into a small function to compute confidence intervals of coefficient estimates named &lt;code&gt;ci()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ci &amp;lt;- function(b, se){
  x &amp;lt;- se * 1.96
  as.numeric(c(b-x, b+x))
}
#note: stats::confint() which uses t-critical values to compute more precise confidence intervals. 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are going to look at the coefficient estimate and standard error of &lt;code&gt;a&lt;/code&gt; in the model &lt;code&gt;y ~ a + b&lt;/code&gt;. We know that &lt;code&gt;a&lt;/code&gt; in this model has a vif of 1.0022527.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yab_model &amp;lt;- lm(
  formula = y ~ a + b,
  data = toy
) |&amp;gt; 
  summary()

#coefficient estimate and standard error of a
a_coef &amp;lt;- yab_model$coefficients[2, 1:2]
a_coef
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Estimate  Std. Error 
## 0.747689326 0.006636511
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we plug them into our little function to compute the confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;a_ci &amp;lt;- ci(
  b = a_coef[1], 
  se = a_coef[2]
  )
a_ci
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7346818 0.7606969
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And, finally, we compute the width of the confidence interval for &lt;code&gt;a&lt;/code&gt; as the difference between the extremes of the confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;old_width &amp;lt;- diff(a_ci)
old_width
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02601512
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep this number in mind, it&amp;rsquo;s important.&lt;/p&gt;
&lt;p&gt;Now, let me tell you something weird: &lt;strong&gt;The confidence interval of a predictor is widened by a factor equal to the square root of its Variance Inflation Factor&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So, if the VIF of a predictor is, let&amp;rsquo;s say, 16, then this means that, in a linear model, multicollinearity is inflating the width of its confidence interval by a factor of 4.&lt;/p&gt;
&lt;p&gt;In case you don&amp;rsquo;t want to take my word for it, here goes a demonstration. Now we fit the model &lt;code&gt;y ~ a + b + c + d&lt;/code&gt;, where &lt;code&gt;a&lt;/code&gt; has a vif of 16.1606674. If we follow the definition above, we could now expect an inflation of the confidence interval for &lt;code&gt;a&lt;/code&gt; of about 4.0200333. Let&amp;rsquo;s find out if that&amp;rsquo;s the case!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#model y against all predictors and get summary
yabcd_model &amp;lt;- lm(
  formula = y ~ a + b + c + d,
  data = toy
) |&amp;gt; 
  summary()

#compute confidence interval of a
a_ci &amp;lt;- ci(
  b = yabcd_model$coefficients[&amp;quot;a&amp;quot;, &amp;quot;Estimate&amp;quot;], 
  se = yabcd_model$coefficients[&amp;quot;a&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )

#compute width of confidence interval of a
new_width &amp;lt;- diff(a_ci)
new_width
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1044793
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to find out the inflation factor of this new confidence interval, we divide it by the width of the old one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;new_width/old_width
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.016101
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the result is VERY CLOSE to the square root of the VIF of &lt;code&gt;a&lt;/code&gt; (4.0200333) in this model. &lt;strong&gt;Notice that this works because in the model &lt;code&gt;y ~ a + b&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt; has a perfect VIF of 1.0022527. This demonstration needs a model with a quasi-perfect VIF as reference.&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now we can confirm our experiment about the meaning of VIF by repeating the exercise with &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;First we compute the VIF of &lt;code&gt;b&lt;/code&gt; against &lt;code&gt;a&lt;/code&gt; alone, and against &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;, and the expected level of inflation of the confidence interval as the square root of the second VIF.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#vif of b vs a
ba_vif &amp;lt;- collinear::vif_df(
  df = toy[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]
) |&amp;gt; 
  dplyr::filter(predictor == &amp;quot;b&amp;quot;)

#vif of b vs a c d
bacd_vif &amp;lt;- collinear::vif_df(
  df = toy[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]
) |&amp;gt; 
  dplyr::filter(predictor == &amp;quot;b&amp;quot;)

#expeced inflation of the confidence interval
sqrt(bacd_vif$vif)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.015515
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, since &lt;code&gt;b&lt;/code&gt; is already in the models &lt;code&gt;y ~ a + b&lt;/code&gt; and &lt;code&gt;y ~ a + b + c + d&lt;/code&gt;, we just need to extract its coefficients, compute their confidence intervals, and divide one by the other to obtain the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#compute confidence interval of b in y ~ a + b
b_ci_old &amp;lt;- ci(
  b = yab_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;], 
  se = yab_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )

#compute confidence interval of b in y ~ a + b + c + d
b_ci_new &amp;lt;- ci(
  b = yabcd_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;], 
  se = yabcd_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )

#compute inflation
diff(b_ci_new)/diff(b_ci_old)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.013543
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, the square root of the VIF of &lt;code&gt;b&lt;/code&gt; in &lt;code&gt;y ~ a + b + c + d&lt;/code&gt; is a great indicator of how much the confidence interval of &lt;code&gt;b&lt;/code&gt; is inflated by multicollinearity in the model.&lt;/p&gt;
&lt;p&gt;And that, folks, is the meaning of VIF.&lt;/p&gt;
&lt;h1 id=&#34;when-the-vif-hurts&#34;&gt;When the VIF Hurts&lt;/h1&gt;
&lt;p&gt;In the previous sections we acquired an intuition of how Variance Inflation Factors measure the effect of multicollinearity in the precision of the coefficient estimates in a linear model. But there is more to that!&lt;/p&gt;
&lt;p&gt;A coefficient estimate divided by its standard error results in the &lt;strong&gt;T statistic&lt;/strong&gt;. This number is named &amp;ldquo;t value&amp;rdquo; in the table of coefficients shown below, and represents the distance (in number of standard errors) between the estimate and zero.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yabcd_model$coefficients[-1, ] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Estimate Std. Error t value Pr(&amp;gt;|t|)
## a   0.7184     0.0267 26.9552   0.0000
## b   0.2596     0.0134 19.4253   0.0000
## c   0.0273     0.0232  1.1757   0.2398
## d   0.0039     0.0230  0.1693   0.8656
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;p-value&lt;/strong&gt;, named &amp;ldquo;Pr(&amp;gt;|t|)&amp;rdquo; above, is the probability of getting the T statistic when there is &lt;em&gt;no effect of the predictor over the response&lt;/em&gt;. The part in italics is named the &lt;em&gt;null hypothesis&lt;/em&gt; (H0), and happens when the confidence interval of the estimate intersects with zero, as in &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ci(
  b = yabcd_model$coefficients[&amp;quot;c&amp;quot;, &amp;quot;Estimate&amp;quot;],
  se = yabcd_model$coefficients[&amp;quot;c&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.01819994  0.07276692
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ci(
  b = yabcd_model$coefficients[&amp;quot;d&amp;quot;, &amp;quot;Estimate&amp;quot;],
  se = yabcd_model$coefficients[&amp;quot;d&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.04111146  0.04888457
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value of any predictor in the coefficients table above is computed as:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#predictor
predictor &amp;lt;- &amp;quot;d&amp;quot;

#number of cases
n &amp;lt;- nrow(toy)

#number of model terms
p &amp;lt;- nrow(yabcd_model$coefficients)

#one-tailed p-value
#q = absolute t-value
#df = degrees of freedom
p_value_one_tailed &amp;lt;- stats::pt(
  q = abs(yabcd_model$coefficients[predictor, &amp;quot;t value&amp;quot;]), 
  df = n - p #degrees of freedom
  )

#two-tailed p-value
2 * (1 - p_value_one_tailed)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8655869
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This p-value is then compared to a &lt;strong&gt;significance level&lt;/strong&gt; (for example, 0.05 for a 95% confidence), which is just the lowest p-value acceptable as strong evidence to make a claim:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;p-value &amp;gt; significance&lt;/strong&gt;: Evidence to claim that the predictor has no effect on the response. If the claim is wrong (we&amp;rsquo;ll see whey we could be wrong), we fall into a &lt;em&gt;false negative&lt;/em&gt; (also &lt;em&gt;Type II Error&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;p-value &amp;lt;= significance&lt;/strong&gt;: Evidence to claim that the predictor has an effect on the response. If the claim is wrong, we fall into a &lt;em&gt;false positive&lt;/em&gt; (also &lt;em&gt;Type I Error&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, how does all this matter when talking about the Variance Inflation Factor? Because a high VIF triggers a cascade of effects that increases p-values that can mess up your claims about the importance of the predictors!&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    ↑ VIF ► ↑ Std. Error ► ↓ T statistic  ► ↑ p-value  ► ↑  false negatives (Type II Error)
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;This cascade becomes a problem when the predictor has a small effect on the response, and the number of cases is small.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see how this works with &lt;code&gt;b&lt;/code&gt;. This predictor has a solid effect on the response &lt;code&gt;y&lt;/code&gt; (nonetheless, &lt;code&gt;y&lt;/code&gt; was created as &lt;code&gt;a * 0.75 + b * 0.25 + noise&lt;/code&gt;). It has a coefficient around 0.25, and a p-value of 0, so there is little to no risk of falling into a false negative when claiming that it is important to explain &lt;code&gt;y&lt;/code&gt;, even when its confidence interval is inflated by a factor of two in the full model.&lt;/p&gt;
&lt;p&gt;But let&amp;rsquo;s try a little experiment. We are going to create many small versions of &lt;code&gt;toy&lt;/code&gt;, using only 30 cases selected by chance over a number of iterations, we are going to fit models in which &lt;code&gt;b&lt;/code&gt; has a lower and a higher VIF, to monitor its p-values and estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#number of repetitions
repetitions &amp;lt;- 1000

#number of cases to subset in toy
sample_size &amp;lt;- 30

#vectors to store results
lowvif_p_value &amp;lt;- 
  highvif_p_value &amp;lt;- 
  lowvif_estimate &amp;lt;-
  highvif_estimate &amp;lt;- 
  vector(length = repetitions)

#repetitions
for(i in 1:repetitions){
  
  #seed to make randomization reproducible
  set.seed(i)
  
  #toy subset
  toy.i &amp;lt;- toy[sample(x = 1:nrow(toy), size = sample_size), ]
  
  #high vif model
  highvif_model &amp;lt;- lm(
    formula =  y ~ a + b + c + d,
    data = toy.i
  ) |&amp;gt; 
    summary()
  
  #gather results of high vif model
  highvif_p_value[i] &amp;lt;- highvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
  highvif_estimate[i] &amp;lt;- highvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;]
  
  #low_vif_model
  lowvif_model &amp;lt;- lm(
    formula =  y ~ a + b,
    data = toy.i
  ) |&amp;gt; 
    summary()
  
  #gather results of lowvif
  lowvif_p_value[i] &amp;lt;- lowvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
  lowvif_estimate[i] &amp;lt;- lowvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;]
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below shows all p-values of the predictor &lt;code&gt;b&lt;/code&gt; for the high and low VIF models across the experiment repetitions.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/variance-inflation-factor/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;At a significance level of 0.05, the &lt;strong&gt;high VIF&lt;/strong&gt; model rejects &lt;code&gt;b&lt;/code&gt; as an important predictor of &lt;code&gt;y&lt;/code&gt; on 53.5% of the model repetitions, while the &lt;strong&gt;low VIf model&lt;/strong&gt; does the same on 2.2% of repetitions. This is a clear case of increase in Type II Error (false negatives) under multicollinearity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Under multicollinearity, the probability of overlooking predictors with moderate effects increases dramatically!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The plot below shifts the focus towards the coefficient estimates for &lt;code&gt;b&lt;/code&gt; across repetitions.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/variance-inflation-factor/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;The gray vertical line represents the real value of the slope of &lt;code&gt;b&lt;/code&gt;, and each dot represents a model repetition. The coefficients of the &lt;strong&gt;high VIF&lt;/strong&gt; model are all over the place when compared to the &lt;strong&gt;low VIF&lt;/strong&gt; one. Probably you have read somewhere that &amp;ldquo;multicollinearity induces model instability&amp;rdquo;, or something similar, and that is exactly what we are seeing here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finding the true effect of a predictor with a moderate effect becomes harder under multicollinearity.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;managing-vif-in-a-model-design&#34;&gt;Managing VIF in a Model Design&lt;/h1&gt;
&lt;p&gt;The second most common form of modeling self-sabotage is &lt;em&gt;having high VIF predictors in a model design&lt;/em&gt;, just right after &lt;em&gt;throwing deep learning at tabular problems to see what sticks&lt;/em&gt;. I don&amp;rsquo;t have solutions for the deep learning issue, but I have some pointers for the VIFs one: &lt;strong&gt;letting things go!&lt;/strong&gt;. And with &lt;em&gt;things&lt;/em&gt; I mean &lt;em&gt;predictors&lt;/em&gt;, not the pictures of your old love. There is no rule &lt;em&gt;the more predictors the better&lt;/em&gt; rule written anywhere relevant, and letting your model shed some fat is the best way to go here.&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt; package has something to help here. The function 
&lt;a href=&#34;https://blasbenito.github.io/collinear/reference/vif_select.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear::vif_select()&lt;/code&gt;&lt;/a&gt; is specifically designed to help reduce VIF in a model design. And it can do it in two ways: either using domain knowledge to guide the process, or applying quantitative criteria instead.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s follow the domain knowledge route first. Imagine you know a lot about &lt;code&gt;y&lt;/code&gt;, you have read that &lt;code&gt;a&lt;/code&gt; is very important to explain it, and you need to discuss this predictor in your results. But you are on the fence about the other predictors, so you don&amp;rsquo;t really care about what others are in the design. You can express such an idea using the argument &lt;code&gt;preference_order&lt;/code&gt;, as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = &amp;quot;a&amp;quot;,
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you have it, your new model design with a VIF below 2.5 is now &lt;code&gt;y ~ a + b&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;But what if you get new information and it turns out that &lt;code&gt;d&lt;/code&gt; is also a variable of interest? Then you should just modify &lt;code&gt;preference_order&lt;/code&gt; to include this new information.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = c(&amp;quot;a&amp;quot;, &amp;quot;d&amp;quot;),
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;d&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that if your favorite variables are highly correlated, some of them are going to be removed anyway. For example, if &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; are your faves, since they are highly correlated, &lt;code&gt;c&lt;/code&gt; is removed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = c(&amp;quot;a&amp;quot;, &amp;quot;c&amp;quot;),
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In either case, you can now build your model while being sure that the coefficients of these predictors are going to be stable and precise.&lt;/p&gt;
&lt;p&gt;Now, what if &lt;code&gt;y&lt;/code&gt; is totally new for you, and you have no idea about what to use? In this case, the function 
&lt;a href=&#34;https://blasbenito.github.io/collinear/reference/preference_order.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear::preference_order()&lt;/code&gt;&lt;/a&gt; helps you rank the predictors following a quantiative criteria, and after that, &lt;code&gt;collinear::vif_select()&lt;/code&gt; can use it to reduce your VIFs.&lt;/p&gt;
&lt;p&gt;By default, &lt;code&gt;collinear::preference_order()&lt;/code&gt; calls 
&lt;a href=&#34;https://blasbenito.github.io/collinear/reference/f_rsquared.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear::f_rsquared()&lt;/code&gt;&lt;/a&gt; to compute the R-squared between each predictor and the response variable (that&amp;rsquo;s why the argument &lt;code&gt;response&lt;/code&gt; is required here), to return a data frame with the variables ranked from &amp;ldquo;better&amp;rdquo; to &amp;ldquo;worse&amp;rdquo;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;preference &amp;lt;- collinear::preference_order(
  df = toy,
  response = &amp;quot;y&amp;quot;,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  f = collinear::f_r2_pearson,
  quiet = TRUE
)

preference
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   response predictor                       f preference
## 1        y         a collinear::f_r2_pearson 0.77600503
## 2        y         c collinear::f_r2_pearson 0.72364944
## 3        y         d collinear::f_r2_pearson 0.59345954
## 4        y         b collinear::f_r2_pearson 0.07343563
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can use this data frame as input for the argument &lt;code&gt;preference_order&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = preference,
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;d&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now at least you can be sure that the predictors in your model design have low VIF, and were selected taking their correlation with the response as criteria.&lt;/p&gt;
&lt;p&gt;Well, I think that&amp;rsquo;s enough for today. I hope you found this post helpful. Have a great time!&lt;/p&gt;
&lt;p&gt;Blas&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multicollinearity Hinders Model Interpretability</title>
      <link>https://blasbenito.com/post/multicollinearity-model-interpretability/</link>
      <pubDate>Sun, 29 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/multicollinearity-model-interpretability/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This post is written for beginner to intermediate R users wishing to learn what multicollinearity is and how it can turn model interpretation into a challenge.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post, I delve into the intricacies of model interpretation under the influence of multicollinearity, and use R and a toy data set to demonstrate how this phenomenon impacts both linear and machine learning models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The section &lt;em&gt;Multicollinearity Explained&lt;/em&gt; explains the origin of the word and the nature of the problem.&lt;/li&gt;
&lt;li&gt;The section &lt;em&gt;Model Interpretation Challenges&lt;/em&gt; describes how to create the toy data set, and applies it to &lt;em&gt;Linear Models&lt;/em&gt; and &lt;em&gt;Random Forest&lt;/em&gt; to explain how multicollinearity can make model interpretation a challenge.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;Appendix&lt;/em&gt; shows extra examples of linear and machine learning models affected by multicollinearity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I hope you&amp;rsquo;ll enjoy it!&lt;/p&gt;
&lt;h1 id=&#34;r-packages&#34;&gt;R packages&lt;/h1&gt;
&lt;p&gt;This tutorial requires the newly released R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;, and a few more listed below. The optional ones are used only in the &lt;em&gt;Appendix&lt;/em&gt; at the end of the post.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#required
install.packages(&amp;quot;collinear&amp;quot;)
install.packages(&amp;quot;ranger&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)

#optional
install.packages(&amp;quot;nlme&amp;quot;)
install.packages(&amp;quot;glmnet&amp;quot;)
install.packages(&amp;quot;xgboost&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;multicollinearity-explained&#34;&gt;Multicollinearity Explained&lt;/h1&gt;
&lt;p&gt;This cute word comes from the amalgamation of these three Latin terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;multus&lt;/em&gt;: adjective meaning &lt;em&gt;many&lt;/em&gt; or &lt;em&gt;multiple&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;con&lt;/em&gt;: preposition often converted to &lt;em&gt;co-&lt;/em&gt; (as in &lt;em&gt;co-worker&lt;/em&gt;) meaning &lt;em&gt;together&lt;/em&gt; or &lt;em&gt;mutually&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;linealis&lt;/em&gt; (later converted to &lt;em&gt;linearis&lt;/em&gt;): from &lt;em&gt;linea&lt;/em&gt; (line), adjective meaning &amp;ldquo;resembling a line&amp;rdquo; or &amp;ldquo;belonging to a line&amp;rdquo;, among others.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After looking at these serious words, we can come up with a (VERY) liberal translation: &amp;ldquo;several things together in the same line&amp;rdquo;. From here, we just have to replace the word &amp;ldquo;things&amp;rdquo; with &amp;ldquo;predictors&amp;rdquo; (or &amp;ldquo;features&amp;rdquo;, or &amp;ldquo;independent variables&amp;rdquo;, whatever rocks your boat) to build an intuition of the whole meaning of the word in the context of statistical and machine learning modeling.&lt;/p&gt;
&lt;p&gt;If I lost you there, we can move forward with this idea instead: &lt;strong&gt;multicollinearity happens when there are redundant predictors in a modeling dataset&lt;/strong&gt;. A predictor can be redundant because it shows a high pairwise correlation with other predictors, or because it is a linear combination of other predictors. For example, in a data frame with the columns &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt;, if the correlation between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; is high, we can say that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are mutually redundant and there is multicollinearity. But also, if &lt;code&gt;c&lt;/code&gt; is the result of a linear operation between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, like &lt;code&gt;c &amp;lt;- a + b&lt;/code&gt;, or &lt;code&gt;c &amp;lt;- a * 1 + b * 0.5&lt;/code&gt;, then we can also say that there is multicollinearity between &lt;code&gt;c&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt;, and &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Multicollinearity is a fact of life that lurks in most data sets. For example, in climate data, variables like temperature, humidity and air pressure are closely intertwined, leading to multicollinearity. That&amp;rsquo;s the case as well in medical research, where parameters like blood pressure, heart rate, and body mass index frequently display common patterns. Economic analysis is another good example, as variables such as Gross Domestic Product (GDP), unemployment rate, and consumer spending often exhibit multicollinearity.&lt;/p&gt;
&lt;h1 id=&#34;model-interpretation-challenges&#34;&gt;Model Interpretation Challenges&lt;/h1&gt;
&lt;p&gt;Multicollinearity isn&amp;rsquo;t inherently problematic, but it can be a real buzz kill when the goal is interpreting predictor importance in explanatory models. In the presence of highly correlated predictors, most modelling methods, from the veteran linear models to the fancy gradient boosting, attribute a large part of the importance to only one of the predictors and not the others. In such cases, neglecting multicollinearity will certainly lead to underestimate the relevance of certain predictors.&lt;/p&gt;
&lt;p&gt;Let me go ahead and develop a toy data set to showcase this issue. But let&amp;rsquo;s load the required libraries first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#load the collinear package and its example data
library(collinear)
data(vi)

#other required libraries
library(ranger)
library(dplyr)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;vi&lt;/code&gt; data frame shipped with the 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt; package, the variables &amp;ldquo;soil_clay&amp;rdquo; and &amp;ldquo;humidity_range&amp;rdquo; are not correlated at all (Pearson correlation = -0.06).&lt;/p&gt;
&lt;p&gt;In the code block below, the &lt;code&gt;dplyr::transmute()&lt;/code&gt; command selects and renames them as &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. After that, the two variables are scaled and centered, and &lt;code&gt;dplyr::mutate()&lt;/code&gt; generates a few new columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: response variable resulting from a linear model where &lt;code&gt;a&lt;/code&gt; has a slope of 0.75, &lt;code&gt;b&lt;/code&gt; has a slope of 0.25, plus a bit of white noise generated with &lt;code&gt;runif()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: a new predictor highly correlated with &lt;code&gt;a&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d&lt;/code&gt;: a new predictor resulting from a linear combination of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(1)
df &amp;lt;- vi |&amp;gt;
  dplyr::slice_sample(n = 2000) |&amp;gt;
  dplyr::transmute(
    a = soil_clay,
    b = humidity_range
  ) |&amp;gt;
  scale() |&amp;gt;
  as.data.frame() |&amp;gt; 
  dplyr::mutate(
    y = a * 0.75 + b * 0.25 + runif(n = dplyr::n(), min = -0.5, max = 0.5),
    c = a + runif(n = dplyr::n(), min = -0.5, max = 0.5),
    d = (a + b)/2 + runif(n = dplyr::n(), min = -0.5, max = 0.5)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Pearson correlation between all pairs of these predictors is shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::cor_df(
  df = df,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 3
##   x     y     correlation
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 c     a           0.962
## 2 d     b           0.639
## 3 d     a           0.636
## 4 d     c           0.615
## 5 b     a          -0.047
## 6 c     b          -0.042
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, we have are two groups of predictors useful to understand how multicollinearity muddles model interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictors with &lt;strong&gt;no&lt;/strong&gt; multicollinearity: &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Predictors with multicollinearity: &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the next two sections and the &lt;em&gt;Appendix&lt;/em&gt;, I show how and why model interpretation becomes challenging when multicollinearity is high. Let&amp;rsquo;s start with linear models.&lt;/p&gt;
&lt;h3 id=&#34;linear-models&#34;&gt;Linear Models&lt;/h3&gt;
&lt;p&gt;The code below fits &lt;em&gt;multiple linear regression models&lt;/em&gt; for both groups of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#non-collinear predictors
lm_ab &amp;lt;- lm(
  formula = y ~ a + b,
  data = df
  )

#collinear predictors
lm_abcd &amp;lt;- lm(
  formula = y ~ a + b + c + d,
  data = df
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I would like you to pay attention to the estimates of the predictors &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; for both models. The estimates are the slopes in the linear model, a direct indication of the effect of a predictor over the response.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coefficients(lm_ab)[2:3] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coefficients(lm_abcd)[2:5] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On one hand, the model with no multicollinearity (&lt;code&gt;lm_ab&lt;/code&gt;) achieved a pretty good solution for the coefficients of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. Remember that we created &lt;code&gt;y&lt;/code&gt; as &lt;code&gt;a * 0.75 + b * 0.25&lt;/code&gt; plus some noise, and that&amp;rsquo;s exactly what the model is telling us here, so the interpretation is pretty straightforward.&lt;/p&gt;
&lt;p&gt;On the other hand, the model with multicollinearity (&lt;code&gt;lm_abcd&lt;/code&gt;) did well with &lt;code&gt;b&lt;/code&gt;, but there are a few things in there that make the interpretation harder.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The coefficient of &lt;code&gt;a&lt;/code&gt; (0.7165) is slightly smaller than the true one (0.75), which could lead us to downplay its relationship with &lt;code&gt;y&lt;/code&gt; by a tiny bit. This is kinda OK though, as long as one is not using the model&amp;rsquo;s results to build nukes in the basement.&lt;/li&gt;
&lt;li&gt;The coefficient of &lt;code&gt;c&lt;/code&gt; is so small that it could led us to believe that this predictor not important at all to explain &lt;code&gt;y&lt;/code&gt;. But we know that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; are almost identical copies, so model interpretation here is being definitely muddled by multicollinearity.&lt;/li&gt;
&lt;li&gt;The coefficient of &lt;code&gt;d&lt;/code&gt; is tiny. Since &lt;code&gt;d&lt;/code&gt; results from the sum of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, we could expect this predictor to be important in explaining &lt;code&gt;y&lt;/code&gt;, but it got the shorter end of the stick in this case.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is not that the model it&amp;rsquo;s wrong though. This behavior of the linear model results from the &lt;em&gt;QR decomposition&lt;/em&gt; (also &lt;em&gt;QR factorization&lt;/em&gt;) applied by functions like &lt;code&gt;lm()&lt;/code&gt;, &lt;code&gt;glm()&lt;/code&gt;, &lt;code&gt;glmnet::glmnet()&lt;/code&gt;, and &lt;code&gt;nlme::gls()&lt;/code&gt; to improve numerical stability and computational efficiency, and to&amp;hellip; address multicollinearity in the model predictors.&lt;/p&gt;
&lt;p&gt;The QR decomposition transforms the original predictors into a set of orthogonal predictors with no multicollinearity. This is the &lt;em&gt;Q matrix&lt;/em&gt;, created in a fashion that resembles the way in which a Principal Components Analysis generates uncorrelated components from a set of correlated variables.&lt;/p&gt;
&lt;p&gt;The code below applies QR decomposition to our multicollinear predictors, extracts the Q matrix, and shows the correlation between the new versions of &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#predictors names
predictors &amp;lt;- c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)

#QR decomposition of predictors
df.qr &amp;lt;- qr(df[, predictors])

#extract Q matrix
df.q &amp;lt;- qr.Q(df.qr)
colnames(df.q) &amp;lt;- predictors

#correlation between transformed predictors
collinear::cor_df(df = df.q)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 3
##   x     y     correlation
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 d     c               0
## 2 c     b               0
## 3 d     b               0
## 4 d     a               0
## 5 c     a               0
## 6 b     a               0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new set of predictors we are left with after the QR decomposition have exactly zero correlation! And now they are not our original predictors anymore, and have a different interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt; is now &amp;ldquo;the part of &lt;code&gt;a&lt;/code&gt; not in &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt; is now &amp;ldquo;the part of &lt;code&gt;b&lt;/code&gt; not in &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&amp;hellip;and so on&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The result of the QR decomposition can be plugged into the &lt;code&gt;solve()&lt;/code&gt; function along with the response vector to estimate the coefficients of the linear model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solve(a = df.qr, b = df$y) |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7189 0.2595 0.0268 0.0040
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are almost exactly the ones we got for our model with multicollinearity. In the end, the coefficients resulting from a linear model are not those of the original predictors, but the ones of their uncorrelated versions generated by the QR decomposition.&lt;/p&gt;
&lt;p&gt;But this is not the only issue of model interpretability under multicollinearity. Let&amp;rsquo;s take a look at the standard errors of the estimates. These are a measure of the coefficient estimation uncertainty, and are used to compute the p-values of the estimates. As such, they are directly linked with the &amp;ldquo;statistical significance&amp;rdquo; (whatever that means) of the predictors within the model.&lt;/p&gt;
&lt;p&gt;The code below shows the standard errors of the model without and with multicollinearity.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(lm_ab)$coefficients[, &amp;quot;Std. Error&amp;quot;][2:3] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.0066 0.0066
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(lm_abcd)$coefficients[, &amp;quot;Std. Error&amp;quot;][2:5] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.0267 0.0134 0.0232 0.0230
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These standard errors of the model with multicollinearity are an order of magnitude higher than the ones of the model without multicollinearity.&lt;/p&gt;
&lt;p&gt;Since our toy dataset is relatively large (2000 cases) and the relationship between the response and a few of the predictors pretty robust, there are no real issues arising, as these differences in estimation precision are not enough to change the p-values of the estimates. However, in a small data set with high multicollinearity and a weaker relationship between the response and the predictors, standard errors of the estimate become wide, which increases p-values and reduces &amp;ldquo;significance&amp;rdquo;. Such a situation might lead us to believe that a predictor does not explain the response, when in fact it does. And this, again, is a model interpretability issue caused by multicollinearity.&lt;/p&gt;
&lt;p&gt;At the end of this post there is an appendix with code examples of other types of linear models that use QR decomposition and become challenging to interpret in the presence of multicollinearity. Play with them as you please!&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s take a look at how multicollinearity can also mess up the interpretation of a commonly used machine learning algorithm.&lt;/p&gt;
&lt;h3 id=&#34;random-forest&#34;&gt;Random Forest&lt;/h3&gt;
&lt;p&gt;It is not uncommon to hear something like &amp;ldquo;random forest is insensitive to multicollinearity&amp;rdquo;. Actually, I cannot confirm nor deny that I have said that before. Anyway, it is kind of true if one is focused on prediction problmes. However, when the aim is interpreting predictor importance scores, then one has to be mindful about multicollinearity as well.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see an example. The code below fits two random forest models with our two sets of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#non-collinear predictors
rf_ab &amp;lt;- ranger::ranger(
  formula = y ~ a + b,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1 #for reproducibility
)

#collinear predictors
rf_abcd &amp;lt;- ranger::ranger(
  formula = y ~ a + b + c + d,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at the prediction error the two models on the out-of-bag data. While building each regression tree, Random Forest leaves a random subset of the data out. Then, each case gets a prediction from all trees that had it in the out-of-bag data, and the prediction error is averaged across all cases to get the numbers below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_ab$prediction.error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1026779
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abcd$prediction.error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1035678
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to these numbers, these two models are basically equivalent in their ability to predict our response &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But now, you noticed that I set the argument &lt;code&gt;importance&lt;/code&gt; to &amp;ldquo;permutation&amp;rdquo;. Permutation importance quantifies how the out-of-bag error increases when a predictor is permuted across all trees where the predictor is used. It is pretty robust importance metric that bears no resemblance whatsoever with the coefficients of a linear model. Think of it as a very different way to answer the question &amp;ldquo;what variables are important in this model?&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The permutation importance scores of the two random forest models are show below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_ab$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 1.0702 0.1322
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abcd$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.5019 0.0561 0.1662 0.0815
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is one interesting detail here. The predictor &lt;code&gt;a&lt;/code&gt; has a permutation error three times higher than &lt;code&gt;c&lt;/code&gt; in the second model, even though we could expect them to be similar due to their very high correlation. There are two reasons for this mismatch:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Random Forest is much more sensitive to the white noise in &lt;code&gt;c&lt;/code&gt; than linear models, especially in the deep parts of the regression trees, due to local (within-split data) decoupling with the response &lt;code&gt;y&lt;/code&gt;. In consequence, it does not get selected as often as &lt;code&gt;a&lt;/code&gt; in these deeper areas of the trees, and has less overall importance.&lt;/li&gt;
&lt;li&gt;The predictor &lt;code&gt;c&lt;/code&gt; competes with &lt;code&gt;d&lt;/code&gt;, that has around 50% of the information in &lt;code&gt;c&lt;/code&gt; (and &lt;code&gt;a&lt;/code&gt;). If we remove &lt;code&gt;d&lt;/code&gt; from the model, then the permutation importance of &lt;code&gt;c&lt;/code&gt; doubles up. Then, with &lt;code&gt;d&lt;/code&gt; in the model, we underestimate the real importance of &lt;code&gt;c&lt;/code&gt; due to multicollinearity alone.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abc &amp;lt;- ranger::ranger(
  formula = y ~ a + b + c,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1
)
rf_abc$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c 
## 0.5037 0.1234 0.3133
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With all that in mind, we can conclude that interpreting importance scores in Random Forest models is challenging when multicollinearity is high. But Random Forest is not the only machine learning affected by this issue. In the Appendix below I have left an example with Extreme Gradient Boosting so you can play with it.&lt;/p&gt;
&lt;p&gt;And that&amp;rsquo;s all for now, folks, I hope you found this post useful!&lt;/p&gt;
&lt;h1 id=&#34;appendix&#34;&gt;Appendix&lt;/h1&gt;
&lt;p&gt;This section shows several extra examples of linear and machine learning models you can play with.&lt;/p&gt;
&lt;h2 id=&#34;other-linear-models-using-qr-decomposition&#34;&gt;Other linear models using QR decomposition&lt;/h2&gt;
&lt;p&gt;As I commented above, many linear modeling functions use QR decomposition, and you will have to be careful interpreting model coefficients in the presence of strong multicollinearity in the predictors.&lt;/p&gt;
&lt;p&gt;Here I show several examples with &lt;code&gt;glm()&lt;/code&gt; (Generalized Linear Models), &lt;code&gt;nlme::gls()&lt;/code&gt; (Generalized Least Squares), and &lt;code&gt;glmnet::cv.glmnet()&lt;/code&gt; (Elastic Net Regularization). In all them, no matter how fancy, the interpretation of coefficients becomes tricky when multicollinearity is high.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generalized Linear Models with glm()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Generalized Linear Models
#non-collinear predictors
glm_ab &amp;lt;- glm(
  formula = y ~ a + b,
  data = df
  )

round(coefficients(glm_ab), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
glm_abcd &amp;lt;- glm(
  formula = y ~ a + b + c + d,
  data = df
  )

round(coefficients(glm_abcd), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Generalized Least Squares with nlme::gls()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(nlme)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;nlme&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:dplyr&#39;:
## 
##     collapse
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Generalized Least Squares
#non-collinear predictors
gls_ab &amp;lt;- nlme::gls(
  model = y ~ a + b,
  data = df
  )

round(coefficients(gls_ab), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
gls_abcd &amp;lt;- nlme::gls(
  model = y ~ a + b + c + d,
  data = df
  )

round(coefficients(gls_abcd), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Elastic Net Regularization and Lasso penalty with glmnet::glmnet()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(glmnet)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loaded glmnet 4.1-8
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Elastic net regularization with Lasso penalty
#non-collinear predictors
glmnet_ab &amp;lt;- glmnet::cv.glmnet(
  x = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]),
  y = df$y,
  alpha = 1 #lasso penalty
)

round(coef(glmnet_ab$glmnet.fit, s = glmnet_ab$lambda.min), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7438 0.2578
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
glmnet_abcd &amp;lt;- glmnet::cv.glmnet(
  x = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  y = df$y,
  alpha = 1 
)

#notice that the lasso regularization nuked the coefficients of predictors b and c
round(coef(glmnet_abcd$glmnet.fit, s = glmnet_abcd$lambda.min), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7101 0.2507 0.0267 0.0149
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;extreme-gradient-boosting-under-multicollinearity&#34;&gt;Extreme Gradient Boosting under multicollinearity&lt;/h2&gt;
&lt;p&gt;Gradient Boosting models trained with multicollinear predictors behave in a way similar to linear models with QR decomposition. When two variables are highly correlated, one of them is going to have an importance much higher than the other.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(xgboost)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;xgboost&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:dplyr&#39;:
## 
##     slice
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#without multicollinearity
gb_ab &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
  )

#with multicollinearity
gb_abcd &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xgb.importance(model = gb_ab)[, c(1:2)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature      Gain
## 1:       a 0.8463005
## 2:       b 0.1536995
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xgb.importance(model = gb_abcd)[, c(1:2)] |&amp;gt; 
  dplyr::arrange(Feature)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature       Gain
## 1:       a 0.78129661
## 2:       b 0.07386393
## 3:       c 0.03595619
## 4:       d 0.10888327
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But there is a twist too. When two variables are perfectly correlated, one of them is removed right away from the model!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#replace c with perfect copy of a
df$c &amp;lt;- df$a

#with multicollinearity
gb_abcd &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
)

xgb.importance(model = gb_abcd)[, c(1:2)] |&amp;gt; 
  dplyr::arrange(Feature)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature       Gain
## 1:       a 0.79469959
## 2:       b 0.07857141
## 3:       d 0.12672900
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Designing R functions to compute betadiversity indices from species lists</title>
      <link>https://blasbenito.com/post/04_betadiversity/</link>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/04_betadiversity/</guid>
      <description>
&lt;script src=&#34;https://blasbenito.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://blasbenito.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://blasbenito.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This is a tutorial written for R users needing to compute betadiversity indices from species lists rather than from presence-absence matrices, and for R beginners or intermediate users that want to start using their own functions. If you are an advanced R user, this post will likely waste your time.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;We ecologists like to measure all things in nature, and compositional changes in biological communities over time or space, a.k.a &lt;em&gt;betadiversity&lt;/em&gt;, is one of these things. I am not going to explain what betadiversity is because others that know better than me have done it already. Good examples are &lt;a href=&#34;https://methodsblog.com/2015/05/27/beta_diversity/&#34;&gt;this post published the blog of Methods in Ecology and Evolution&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/andres_baselga&#34;&gt;Andres Baselga&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=WQGN30YSc_U&#34;&gt;and this lecture by Tim Seipel&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;What I am actually going to do in this post is to explain how to write functions to compute betadiversity indices in R from species lists rather than from presence-absence matrices. For the latter there are a few packages such as &lt;a href=&#34;https://cran.r-project.org/package=vegan&#34;&gt;vegan&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/package=BAT&#34;&gt;BAT&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/package=MBI&#34;&gt;MBI&lt;/a&gt;, or &lt;a href=&#34;https://cran.r-project.org/package=betapart&#34;&gt;betapart&lt;/a&gt;, but for the former I was unable to find anything suitable. To make this post useful for R beginners, I will go step by step on the rationale behind the design of the functions to compute betadiversity indices, and by the end of the post I will explain how to organize them to achieve a clean R workflow.&lt;/p&gt;
&lt;p&gt;Let’s go!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div id=&#34;betadiversity-indices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Betadiversity indices&lt;/h2&gt;
&lt;p&gt;There are a few betadiversity indices out there, and I totally recommend you to start with &lt;a href=&#34;https://besjournals.onlinelibrary.wiley.com/doi/10.1046/j.1365-2656.2003.00710.x&#34;&gt;Koleff &lt;em&gt;et al.&lt;/em&gt; (2003)&lt;/a&gt; as a primer. They review the literature and analyze the properties of 24 different indices to provide guidance on how to use them.&lt;/p&gt;
&lt;div id=&#34;betadiversity-components-a-b-and-c&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Betadiversity components &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;, and &lt;em&gt;c&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;Betadiversity indices are designed to compare the taxa pools of two sites at a time, and require the computation of three components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;a&lt;/strong&gt;: number of common taxa of both sites.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;b&lt;/strong&gt;: number of exclusive taxa of one site.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;c&lt;/strong&gt;: number of exclusive taxa of the other site.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s see how can we use these diversity components to compute betadiversity indices.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sørensens-beta&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sørensen’s Beta&lt;/h3&gt;
&lt;p&gt;Let’s start with the &lt;strong&gt;Sørensen’s Beta&lt;/strong&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\beta_{sor}\)&lt;/span&gt; hereafter), as presented in Koleff &lt;em&gt;et al.&lt;/em&gt; (2003).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta_{sor} = \frac{2a}{2a + b + c}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_{sor}\)&lt;/span&gt; is a similarity index in the range [0, 1] (the closer to one, the more similar the taxa pools of both sites are) that puts a lot of weight in the &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; component, and is therefore a measure of &lt;em&gt;continuity&lt;/em&gt;, as it focuses the most in the common taxa among sites.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simpsons-beta&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simpson’s Beta&lt;/h3&gt;
&lt;p&gt;Another popular betadiversity index is the &lt;strong&gt;Simpson’s Beta&lt;/strong&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\beta_{sim}\)&lt;/span&gt; hereafter).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta_{sim} = \frac{min(b, c)}{min(b, c) + a}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(min()\)&lt;/span&gt; is a function that takes the minimum value among the diversity components within the parenthesis. &lt;span class=&#34;math inline&#34;&gt;\(\beta_{sim}\)&lt;/span&gt; is a dissimilarity measure that focuses on compositional turnover among sites because it focuses the most on the values of &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. It has its lower bound in zero, and an open upper value.&lt;/p&gt;
&lt;p&gt;To bring these ideas into R, first we have to load a few R packages, and generate some fake data to help us develop the functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)
library(foreach)
library(doParallel)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: iterators&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: parallel&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code chunk below generates 15 fake taxa names, from &lt;code&gt;taxon_1&lt;/code&gt; to &lt;code&gt;taxon_15&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;taxa &amp;lt;- paste0(&amp;quot;taxon_&amp;quot;, 1:15)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these fake taxa we are going to generate taxa lists for four hypothetical sites named &lt;em&gt;site1&lt;/em&gt;, &lt;em&gt;site2&lt;/em&gt;, &lt;em&gt;site3&lt;/em&gt;, and &lt;em&gt;site4&lt;/em&gt;. Two of the sites will have identical taxa lists, two will have non-overlapping taxa lists, and two of them will have some overlap.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;site1 &amp;lt;- site2 &amp;lt;- taxa[1:7]
site3 &amp;lt;- taxa[8:12]
site4 &amp;lt;- taxa[10:15]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now we have these taxa lists:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;site1 #and site2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;taxon_1&amp;quot; &amp;quot;taxon_2&amp;quot; &amp;quot;taxon_3&amp;quot; &amp;quot;taxon_4&amp;quot; &amp;quot;taxon_5&amp;quot; &amp;quot;taxon_6&amp;quot; &amp;quot;taxon_7&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;site3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;taxon_8&amp;quot;  &amp;quot;taxon_9&amp;quot;  &amp;quot;taxon_10&amp;quot; &amp;quot;taxon_11&amp;quot; &amp;quot;taxon_12&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;site4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;taxon_10&amp;quot; &amp;quot;taxon_11&amp;quot; &amp;quot;taxon_12&amp;quot; &amp;quot;taxon_13&amp;quot; &amp;quot;taxon_14&amp;quot; &amp;quot;taxon_15&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-by-step-computation-of-betadiversity-indices-with-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step-by-step computation of betadiversity indices with R&lt;/h2&gt;
&lt;p&gt;For a given pair of sites, how can we compute the diversity components &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;, and &lt;em&gt;c&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;Looking at it from an R perspective, each site is a character vector, so &lt;em&gt;a&lt;/em&gt; can be found by counting the number of common elements between two vectors. These common elements can be found with the function &lt;code&gt;intersect()&lt;/code&gt;, and the number of elements can be computed by applying &lt;code&gt;length()&lt;/code&gt; on the result of &lt;code&gt;intersect()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- length(intersect(site3, site4))
a&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To compute &lt;em&gt;b&lt;/em&gt; and &lt;em&gt;c&lt;/em&gt; we can use the function &lt;code&gt;setdiff()&lt;/code&gt;, that finds the exclusive elements of one character vector when comparing it with another. In this case, &lt;em&gt;b&lt;/em&gt; is computed for the first vector introduced in the function, &lt;em&gt;site3&lt;/em&gt; in this case…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b &amp;lt;- length(setdiff(site3, site4))
b&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… so to compute the &lt;em&gt;c&lt;/em&gt; component we only need to switch the sites.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c &amp;lt;- length(setdiff(site4, site3))
c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we know &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;, and &lt;em&gt;c&lt;/em&gt;, we can compute &lt;span class=&#34;math inline&#34;&gt;\(\beta_{sor}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{sim}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Bsor &amp;lt;- 2 * a / (2 * a + b + c)
Bsor&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5454545&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Bsim&amp;lt;- min(b, c) / (min(b, c) + a)
Bsim&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, if we have a long list of sites, computing betadiversity indices like this can get quite boring quite fast. Let’s put everything in a set of functions to make it easier to work with.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;writing-functions-to-compute-betadiversity-indices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Writing functions to compute betadiversity indices&lt;/h2&gt;
&lt;p&gt;The basic structure of a function definition in R looks as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;function_name &amp;lt;- function(x, y, ...){
  output &amp;lt;- [body]
  output #also return(output)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;function_name&lt;/code&gt; is the name of your function. Ideally, a verb, or otherwise, something indicating somehow what the function will do with the input data and arguments.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;function()&lt;/code&gt; is a function to define functions, there isn’t much more to it…&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is the first argument of the function, and ideally, represents the input data. If that is the case, you can later use &lt;a href=&#34;https://r4ds.had.co.nz/pipes.html&#34;&gt;pipes&lt;/a&gt; (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) to chain functions together.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; (it could have any other name) is another function argument, an can be either another input dataset, or an argument defining how the function has to behave.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;...&lt;/code&gt; refers to other arguments the function may require.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;body&lt;/code&gt; is the code that operates with the data and function arguments. This can be one line of code, or a thousand, it all comes down to the function’s objective. In any case, the &lt;code&gt;body&lt;/code&gt; must return an object (or an error if something went wrong) that will be the function’s &lt;code&gt;output&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;output&lt;/code&gt; is the object ultimately produced by the function. It can have any name, and can be any kind of structure, such a number, a vector, a data frame, a list, etc. R functions return one output object only. Since R functions return the last evaluated value, it is good practice to put the &lt;code&gt;output&lt;/code&gt; object at the end of the function as an explicit way to state what the actual output of the function is.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s start writing a function to compute &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;, and &lt;em&gt;c&lt;/em&gt; from a pair of sites.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#x: taxa list of one site
#y: taxa list of another site
abc &amp;lt;- function(x, y){
  
  #list to store output
  out &amp;lt;- list()
  
  #filling the list
  out$a &amp;lt;- length(intersect(x, y))
  out$b &amp;lt;- length(setdiff(x, y))
  out$c &amp;lt;- length(setdiff(y, x))
  
  #returning the output
  out
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that to to return the three values I am wrapping them in a list. Let’s run a little test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- abc(
  x = site3,
  y = site4
)
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $a
## [1] 3
## 
## $b
## [1] 2
## 
## $c
## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far so good! From here we build the functions &lt;code&gt;sorensen_beta()&lt;/code&gt; and &lt;code&gt;simpson_beta()&lt;/code&gt; making sure they can accept the output of &lt;code&gt;abc()&lt;/code&gt;, and return it with an added slot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sorensen_beta &amp;lt;- function(x){
  
  x$bsor &amp;lt;- round(2 * x$a / (2 * x$a + x$b + x$c), 3)
  
  x
  
}


simpson_beta &amp;lt;- function(x){
  
  x$bsim &amp;lt;- round(min(x$b, x$c) / (min(x$b, x$c) + x$a), 3)
  
  x
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that both functions are returning the input &lt;code&gt;x&lt;/code&gt; with an added slot named after the given betadiversity index. Let’s test them first, to later see why returning the input object gives these functions a lot of flexibility.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sorensen_beta(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $a
## [1] 3
## 
## $b
## [1] 2
## 
## $c
## [1] 3
## 
## $bsor
## [1] 0.545&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpson_beta(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $a
## [1] 3
## 
## $b
## [1] 2
## 
## $c
## [1] 3
## 
## $bsim
## [1] 0.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When I said that returning the input object with an added slot gave these functions a lot of flexibility I was talking about this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- abc(
  x = site3, 
  y = site4
  ) %&amp;gt;% 
  sorensen_beta() %&amp;gt;% 
  simpson_beta()
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $a
## [1] 3
## 
## $b
## [1] 2
## 
## $c
## [1] 3
## 
## $bsor
## [1] 0.545
## 
## $bsim
## [1] 0.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Chaining the functions through the &lt;code&gt;%&amp;gt;%&lt;/code&gt; pipe of the &lt;code&gt;magrittr&lt;/code&gt; package now allows us to combine their results in a single output no matter whether we use &lt;code&gt;sorensen_beta()&lt;/code&gt; or &lt;code&gt;sorensen_beta()&lt;/code&gt; first, or whether we omit one of them. The only thing the pipe is doing here is moving the output of the first function into the next. There are a couple of very nice tutorials about the &lt;code&gt;magrittr&lt;/code&gt; package and the &lt;code&gt;%&amp;gt;%&lt;/code&gt; &lt;a href=&#34;https://uc-r.github.io/pipe&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://towardsdatascience.com/an-introduction-to-the-pipe-in-r-823090760d64&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can put that idea right away into a function to compute both betadiversity indices at once from the taxa list of a pair of sites.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;betadiversity &amp;lt;- function(x, y){
  
  require(magrittr)
  
  abc(x, y) %&amp;gt;%
    sorensen_beta() %&amp;gt;%
    simpson_beta()
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function now works as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- betadiversity(
  x = site3, 
  y = site4
  )
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $a
## [1] 3
## 
## $b
## [1] 2
## 
## $c
## [1] 3
## 
## $bsor
## [1] 0.545
## 
## $bsim
## [1] 0.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far we have four functions…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;abc()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;simpson_beta()&lt;/code&gt;, that requires &lt;code&gt;abc()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sorensen_beta()&lt;/code&gt;, that requires &lt;code&gt;abc()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;betadiversity()&lt;/code&gt;, that requires &lt;code&gt;abc()&lt;/code&gt;, &lt;code&gt;simpson_beta()&lt;/code&gt;, and &lt;code&gt;sorensen_beta()&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;… and one limitation: so far we can only return betadiversity indices for two sites at a time. So at the moment, to compute betadiversity indices for all combinations of sites we have to do a pretty ridiculous thing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x1 &amp;lt;- betadiversity(x = site1, y = site2)
x2 &amp;lt;- betadiversity(x = site1, y = site3)
x3 &amp;lt;- betadiversity(x = site1, y = site4)
#... and so on&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I see you doing this I’ll come to haunt you in your nightmares! Since a real analysis may involve hundreds of sites, the next step is to use the functions above to build a new one able to intake an arbitrary number of sites.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;writing-a-function-to-compute-betadiversity-indices-for-an-arbitrary-number-of-sites.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Writing a function to compute betadiversity indices for an arbitrary number of sites.&lt;/h2&gt;
&lt;p&gt;First we have to organize our sites in a data frame with a &lt;em&gt;long format&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sites &amp;lt;- data.frame(
  site = c(
    rep(&amp;quot;site1&amp;quot;, length(site1)),
    rep(&amp;quot;site2&amp;quot;, length(site2)),
    rep(&amp;quot;site3&amp;quot;, length(site3)),
    rep(&amp;quot;site4&amp;quot;, length(site4))
    ),
  taxon = c(
    site1,
    site2,
    site3,
    site4
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
site
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
taxon
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_11
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_11
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_13
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_14
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
taxon_15
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Our new function will need to do several things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generate combinations of the unique values of the column &lt;code&gt;site&lt;/code&gt; two by two without repetition.&lt;/li&gt;
&lt;li&gt;Iterate through these combinations of two sites to compute betadiversity components and indices.&lt;/li&gt;
&lt;li&gt;Return a dataframe with the results to facilitate further analyses.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The combinations of site pairs are done with &lt;code&gt;utils::combn()&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;site.combinations &amp;lt;- utils::combn(
  x = unique(sites$site),
  m = 2
  )
site.combinations&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]    [,2]    [,3]    [,4]    [,5]    [,6]   
## [1,] &amp;quot;site1&amp;quot; &amp;quot;site1&amp;quot; &amp;quot;site1&amp;quot; &amp;quot;site2&amp;quot; &amp;quot;site2&amp;quot; &amp;quot;site3&amp;quot;
## [2,] &amp;quot;site2&amp;quot; &amp;quot;site3&amp;quot; &amp;quot;site4&amp;quot; &amp;quot;site3&amp;quot; &amp;quot;site4&amp;quot; &amp;quot;site4&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is a matrix, and each pair of rows in a column contain a pair of sites. The idea now is to iterate over the matrix columns, obtain the set of taxa from each site from the &lt;code&gt;taxon&lt;/code&gt; column of the &lt;code&gt;sites&lt;/code&gt; data frame, and use these taxa lists to compute the betadiversity components and indices.&lt;/p&gt;
&lt;p&gt;To easily generate the output data frame, I use the &lt;code&gt;foreach::foreach()&lt;/code&gt; function to iterate through pairs instead of a more traditional &lt;code&gt;for&lt;/code&gt; loop. You can read more about &lt;code&gt;foreach()&lt;/code&gt; in a &lt;a href=&#34;https://www.blasbenito.com/post/02_parallelizing_loops_with_r/&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;betadiversity.df &amp;lt;- foreach::foreach(
  i = 1:ncol(site.combinations), #iterates through columns of site.combinations
  .combine = &amp;#39;rbind&amp;#39; #to produce a data frame
  ) %do% {
  
  #site names
  site.one &amp;lt;- site.combinations[1, i] #from column i, row 1
  site.two &amp;lt;- site.combinations[2, i] #from column i, row 2
  
  #getting taxa lists
  taxa.list.one &amp;lt;- sites[sites$site %in% site.one, &amp;quot;taxon&amp;quot;]
  taxa.list.two &amp;lt;- sites[sites$site %in% site.two, &amp;quot;taxon&amp;quot;]
  
  #betadiversity
  beta &amp;lt;- betadiversity(
    x = taxa.list.one,
    y = taxa.list.two
  )
  
  #adding site names
  beta$site.one &amp;lt;- site.one
  beta$site.two &amp;lt;- site.two
  
  #returning output
  beta
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
a
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
b
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
c
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
bsor
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
bsim
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
site.one
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
site.two
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.545
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site4
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now that we know it works, we can put everything together in a function. Notice that to make the function more general, I have added arguments requesting the names of the columns with the site and the taxa names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;betadiversity_multisite &amp;lt;- function(
  x, 
  site.column, #column with site names
  taxa.column #column with taxa names
){
  
  #get site combinations
  site.combinations &amp;lt;- utils::combn(
    x = unique(x[, site.column]),
    m = 2
  )
  
  #iterating through site pairs
  betadiversity.df &amp;lt;- foreach::foreach(
    i = 1:ncol(site.combinations),
    .combine = &amp;#39;rbind&amp;#39;
  ) %do% {
    
    #site names
    site.one &amp;lt;- site.combinations[1, i]
    site.two &amp;lt;- site.combinations[2, i]
    
    #getting taxa lists
    taxa.list.one &amp;lt;- x[x[, site.column] %in% site.one, taxa.column]
    taxa.list.two &amp;lt;- x[x[, site.column] %in% site.two, taxa.column]
    
    #betadiversity
    beta &amp;lt;- betadiversity(
      x = taxa.list.one,
      y = taxa.list.two
    )
    
    #adding site names
    beta$site.one &amp;lt;- site.one
    beta$site.two &amp;lt;- site.two
    
    #returning output
    beta
    
  }
  
  #remove bad rownames
  rownames(betadiversity.df) &amp;lt;- NULL
  
  #reordering columns
  betadiversity.df &amp;lt;- betadiversity.df[, c(
    &amp;quot;site.one&amp;quot;,
    &amp;quot;site.two&amp;quot;,
    &amp;quot;a&amp;quot;,
    &amp;quot;b&amp;quot;,
    &amp;quot;c&amp;quot;,
    &amp;quot;bsor&amp;quot;,
    &amp;quot;bsim&amp;quot;
    )]
  
  #returning output
  return(betadiversity.df)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the test!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sites.betadiversity &amp;lt;- betadiversity_multisite(
  x = sites, 
  site.column = &amp;quot;site&amp;quot;,
  taxa.column = &amp;quot;taxon&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
site.one
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
site.two
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
a
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
b
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
c
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
bsor
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
bsim
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
site4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.545
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;That went well!&lt;/p&gt;
&lt;p&gt;Finally, to have these functions available in my R session I always put them all in a single file in the same folder where my Rstudio project lives, name it something like &lt;code&gt;functions_betadiversity.R&lt;/code&gt;, and source it at the beginning of my script or .Rmd file by running a line like the one below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;functions_betadiversity.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have placed the file &lt;code&gt;functions_betadiversity.R&lt;/code&gt; in &lt;a href=&#34;https://gist.github.com/BlasBenito/4c3740b056a0c9bb3602f33dfd35990c&#34;&gt;this GitHub Gist&lt;/a&gt; in case you want to give it a look. You can also source it right away to your R environment by executing the following line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;https://gist.githubusercontent.com/BlasBenito/4c3740b056a0c9bb3602f33dfd35990c/raw/bbb40d868787fc5d10e391a2121045eb5d75f165/functions_betadiversity.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I hope this post helped you to better understand how to write and organize R functions!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Setup of a shared folder in a home cluster</title>
      <link>https://blasbenito.com/post/03_shared_folder_in_cluster/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/03_shared_folder_in_cluster/</guid>
      <description>
&lt;script src=&#34;https://blasbenito.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://blasbenito.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://blasbenito.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In the previous posts I have covered how to &lt;a href=&#34;https://www.blasbenito.com/post/01_home_cluster/&#34;&gt;setup a home cluster&lt;/a&gt;, and how to &lt;a href=&#34;https://www.blasbenito.com/post/02_parallelizing_loops_with_r/&#34;&gt;run parallel processes with &lt;code&gt;foreach&lt;/code&gt; in R&lt;/a&gt;. However, so far I haven’t covered how to setup a folder shared among the cluster nodes to store the results of parallel computations.&lt;/p&gt;
&lt;p&gt;This post (developed on Ubuntu and Ubuntu Server 20.04) assumes that the home cluster runs on Linux, and covers the following topics.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basics of the Network File System protocol (NFS).&lt;/li&gt;
&lt;li&gt;Setup of an NFS folder in a home cluster.&lt;/li&gt;
&lt;li&gt;Using an NFS folder in a parallelized loop.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div id=&#34;the-network-file-system-protocol-nfs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Network File System protocol (NFS)&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Network_File_System&#34;&gt;Network File System&lt;/a&gt; protocol offers the means for a &lt;em&gt;host&lt;/em&gt; computer to allow other computers in the network (&lt;em&gt;clients&lt;/em&gt;) to read and write the contents of a given folder. The clients access such folder like if it was a local one, while in fact it is a &lt;em&gt;reference&lt;/em&gt; to the one in the host computer.&lt;/p&gt;
&lt;p&gt;The image at the beginning of the post illustrates the concept. There is a &lt;em&gt;host&lt;/em&gt; computer with a folder in the path &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; (were &lt;code&gt;user&lt;/code&gt; is your user name) that is broadcasted to the network, and there are one or several &lt;em&gt;clients&lt;/em&gt; that are mounting &lt;em&gt;mounting&lt;/em&gt; (making accessible) the same folder in their local paths &lt;code&gt;/home/user/cluster_shared&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If the host writes a file to the shared folder, it is available right away for the clients, and the other way around. At the end, the idea is to have a folder shared among all computers in the cluster, while having the same exact path on each one of them to write or read files from such shared folder.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setup-of-an-nfs-folder-in-a-home-cluster&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup of an NFS folder in a home cluster&lt;/h2&gt;
&lt;p&gt;To setup the shared folder we’ll need to do some things in the &lt;em&gt;host&lt;/em&gt;, and some things in the &lt;em&gt;clients&lt;/em&gt;. Let’s start with the host.&lt;/p&gt;
&lt;div id=&#34;preparing-the-host-computer&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Preparing the host computer&lt;/h4&gt;
&lt;p&gt;First we need to install the &lt;code&gt;nfs-kernel-server&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo apt update
sudo apt install nfs-kernel-server&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create the shared folder. Remember to replace &lt;code&gt;user&lt;/code&gt; with your user name, and &lt;code&gt;cluster_shared&lt;/code&gt; with the actual folder name you want to use.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;mkdir /home/user/cluster_shared&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To broadcast it we need to open the file &lt;code&gt;/etc/exports&lt;/code&gt;…&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo gedit /etc/exports&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and add the following line&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;/home/user/cluster_shared IP_CLIENT1(rw,no_subtree_check) IP_CLIENT2(rw,no_subtree_check) IP_CLIENT3(rw,no_subtree_check)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/home/user/cluster_shared&lt;/code&gt; is the path of the shared folder.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IP_CLIENTx&lt;/code&gt; are the IPs of each one of the clients.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rw&lt;/code&gt; gives reading and writing permission on the shared folder to the given client.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;no_subtree_check&lt;/code&gt; prevents the host from checking the complete tree of shares before attending a request (read or write) by a client.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, the last line of my &lt;code&gt;/etc/exports&lt;/code&gt; file looks like this:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;/home/blas/cluster_shared 10.42.0.34(rw,async,no_subtree_check) 10.42.0.104(rw,async,no_subtree_check)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Save the file, and to make the changes effective, execute:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo exportfs -ra&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To end preparing the host we have to update the firewall rules to allow nfs connections from the clients. Notice that one rule per client needs to be defined, using the clients IPs to identify them.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo ufw allow from IP_CLIENT1 to any port nfs
sudo ufw allow from IP_CLIENT2 to any port nfs
sudo ufw status&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;preparing-the-clients&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Preparing the clients&lt;/h4&gt;
&lt;p&gt;First we have to install the Linux package &lt;code&gt;nfs-common&lt;/code&gt; on each client.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo apt update
sudp apt install nfs-common&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create a folder in the clients and use it to mount the NFS folder of the host.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;mkdir -p /home/user/cluster_shared
sudo mount IP_HOST:/home/user/cluster_shared /home/user/cluster_shared&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second line of code is mounting the folder &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; of the host in the folder &lt;code&gt;/home/user/cluster_shared&lt;/code&gt; of the client.&lt;/p&gt;
&lt;p&gt;To make the mount permanent, we have to open &lt;code&gt;/etc/fstab&lt;/code&gt; with super-user privilege in the clients…&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo gedit /etc/fstab&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and add the line&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;IP_HOST:/home/user/cluster_shared /home/user/cluster_shared   nfs     defaults 0 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember to replace &lt;code&gt;IP_HOST&lt;/code&gt; and &lt;code&gt;user&lt;/code&gt; with the right values!&lt;/p&gt;
&lt;p&gt;Now we can test that the shared folder works as intended by writing one file into it from each computer in the network using the code below in the command line.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd cluster_shared
touch filename.txt&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;terminator.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once the files are created, we can check they are visible from each computer using the &lt;code&gt;ls&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;ls&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;terminator2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-an-nfs-folder-in-a-parallelized-loop&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using an NFS folder in a parallelized loop&lt;/h2&gt;
&lt;p&gt;In a &lt;a href=&#34;https://www.blasbenito.com/post/02_parallelizing_loops_with_r/&#34;&gt;previous post&lt;/a&gt; I described how to run parallelized tasks with &lt;code&gt;foreach&lt;/code&gt; in R. This section intends to complete that post by showing how a shared NFS folder can be used to store input and output files during the execution of a parallelized loop&lt;/p&gt;
&lt;div id=&#34;the-task&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The task&lt;/h3&gt;
&lt;p&gt;In this hypothetical example we have a large number of data frames stored in &lt;code&gt;/home/user/cluster_shared/input&lt;/code&gt;. Each data frame has the same predictors &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;, and a different response variable, named &lt;code&gt;y1&lt;/code&gt; for the data frame &lt;code&gt;y1&lt;/code&gt;, &lt;code&gt;y2&lt;/code&gt; for the data frame &lt;code&gt;y2&lt;/code&gt;, and so on. In the example we will be using 100 data frames, but there could be thousands. This example would be valid as well for any other kind of object, such as raster files, time-series, etc etc.&lt;/p&gt;
&lt;p&gt;First we have to load the libraries we’ll be using.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#automatic install of packages if they are not installed already
list.of.packages &amp;lt;- c(
  &amp;quot;foreach&amp;quot;,
  &amp;quot;doParallel&amp;quot;,
  &amp;quot;ranger&amp;quot;
  )

new.packages &amp;lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&amp;quot;Package&amp;quot;])]

if(length(new.packages) &amp;gt; 0){
  install.packages(new.packages, dep=TRUE)
}

#loading packages
for(package.i in list.of.packages){
  suppressPackageStartupMessages(
    library(
      package.i, 
      character.only = TRUE
      )
    )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code chunk below generates the folder &lt;code&gt;/home/user/cluster_shared/input&lt;/code&gt; and populates it with the dummy files.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#creating the input folder
input.folder &amp;lt;- &amp;quot;/home/blas/cluster_shared/input&amp;quot;
dir.create(input.folder)

#data frame names
df.names &amp;lt;- paste0(&amp;quot;y&amp;quot;, 1:100)

#filling it with files
for(i in df.names){
  
  #creating the df
  df.i &amp;lt;- data.frame(
    y = rnorm(1000),
    a = rnorm(1000),
    b = rnorm(1000),
    c = rnorm(1000),
    d = rnorm(1000)
  )
  
  #changing name of the response variable
  colnames(df.i)[1] &amp;lt;- i
  
  #assign to a variable with name i
  assign(i, df.i)
  
  #saving the object
  save(
    list = i,
    file = paste0(input.folder, &amp;quot;/&amp;quot;, i, &amp;quot;.RData&amp;quot;)
  )
  
  #removing the generated data frame form the environment
  rm(list = i, df.i, i)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our target now will be to fit one &lt;code&gt;ranger::ranger()&lt;/code&gt; model per data frame stored in &lt;code&gt;/home/blas/cluster_shared/input&lt;/code&gt;, save the model result to a folder with the path &lt;code&gt;/home/blas/cluster_shared/input&lt;/code&gt;, and write a small summary of the model to the output of &lt;code&gt;foreach&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Such target is based on this rationale: When executing a &lt;code&gt;foreach&lt;/code&gt; loop as in &lt;code&gt;x &amp;lt;- foreach(...) %dopar% {...}&lt;/code&gt;, the variable &lt;code&gt;x&lt;/code&gt; is going to grow in memory very fast, competing for RAM resources with the worker nodes. Furthermore, since &lt;code&gt;x&lt;/code&gt; is being written on the fly, the results would be lost if the computer crashes. When the size of the input and the output of our parallelized operation is larger than memory, we can use an NFS folder to store inputs and outputs, while keeping the RAM memory free for computational tasks only, with the positive side effect of having our outputs already stored should our computer decide to crash.&lt;/p&gt;
&lt;p&gt;Also, please notice that here I am focusing in a cluster setting, but using a folder to read and write data during a loop paralellized with &lt;code&gt;foreach&lt;/code&gt; can indeed be done in a single computer without an NFS folder. Any folder in your system will do the trick as well!&lt;/p&gt;
&lt;p&gt;So, from here, we are going to prepare the cluster, and execute a parallelized loop fitting one model per data frame that reads the inputs and writes the outputs to the shared folder.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cluster-setup&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cluster setup&lt;/h3&gt;
&lt;p&gt;We will also need the function I showed in the previous post to generate the cluster specification from a &lt;a href=&#34;https://gist.github.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca&#34;&gt;GitHub Gist&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below I use the function to create a cluster specification and initiate the cluster with &lt;code&gt;parallel::makeCluster()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#generate cluster specification
spec &amp;lt;- cluster_spec(
  ips = c(&amp;#39;10.42.0.1&amp;#39;, &amp;#39;10.42.0.34&amp;#39;, &amp;#39;10.42.0.104&amp;#39;),
  cores = c(7, 4, 4),
  user = &amp;quot;blas&amp;quot;
)

#define parallel port
Sys.setenv(R_PARALLEL_PORT = 11000)
Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;)

#setting up cluster
my.cluster &amp;lt;- parallel::makeCluster(
  master = &amp;#39;10.42.0.1&amp;#39;, 
  spec = spec,
  port = Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;),
  outfile = &amp;quot;&amp;quot;,
  homogeneous = TRUE
)

#check cluster definition (optional)
print(my.cluster)

#register cluster
doParallel::registerDoParallel(cl = my.cluster)

#check number of workers
foreach::getDoParWorkers()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;parallelized-loop&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parallelized loop&lt;/h3&gt;
&lt;p&gt;For everything to work as intended, we first need to create the output folder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;output.folder &amp;lt;- &amp;quot;/home/blas/cluster_shared/output&amp;quot;
dir.create(output.folder)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we are ready to execute the parallelized loop. Notice that I am using the output of &lt;code&gt;list.files()&lt;/code&gt; to have a vector of file names the loop will iterate over. The steps inside of the loop go as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;1.&lt;/em&gt; Remove the extension &lt;code&gt;.RData&lt;/code&gt; from the file name. We’ll later use the result to use &lt;code&gt;assign()&lt;/code&gt; on the fitted model to change its name to the same as the input file before saving it.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2.&lt;/em&gt; Read the input data frame and store in an object named &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;3.&lt;/em&gt; Fit the model with ranger, using the first column of &lt;code&gt;df&lt;/code&gt; as respose variable.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;4.&lt;/em&gt; Change the model name to the name of the input file without extension, resulting from the first step described above.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;5.&lt;/em&gt; Save the model into the output folder with the extension &lt;code&gt;.RData&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;6.&lt;/em&gt; Return a data frame with one line with the name of the response variable, the r-squared, and the importance of each predictor.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#list of input files as iterator
input.files &amp;lt;- list.files(
  path = input.folder,
  full.names = FALSE
)

modelling.summary &amp;lt;- foreach(
  input.file = input.files,
  .combine = &amp;#39;rbind&amp;#39;, 
  .packages = &amp;quot;ranger&amp;quot;
) %dopar% {
  
  # 1. input file name without extension
  input.file.name &amp;lt;- tools::file_path_sans_ext(input.file)
  
  # 2. read input file
  df &amp;lt;- get(load(paste0(input.folder, &amp;quot;/&amp;quot;, input.file)))
  
  # 3. fit model
  m.i &amp;lt;- ranger::ranger(
    data = df,
    dependent.variable.name = colnames(df)[1],
    importance = &amp;quot;permutation&amp;quot;
  )
  
  # 4. change name of the model to one of the response variable
  assign(input.file.name, m.i)
  
  # 5. save model
  save(
    list = input.file.name,
    file = paste0(output.folder, &amp;quot;/&amp;quot;, input.file)
  )
  
  # 6. returning summary
  return(
    data.frame(
      response.variable = input.file.name,
      r.squared = m.i$r.squared,
      importance.a = m.i$variable.importance[&amp;quot;a&amp;quot;],
      importance.b = m.i$variable.importance[&amp;quot;b&amp;quot;],
      importance.c = m.i$variable.importance[&amp;quot;c&amp;quot;],
      importance.d = m.i$variable.importance[&amp;quot;d&amp;quot;]
    )
  )
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once this parallelized loop is executed, the folder &lt;code&gt;/home/blas/cluster_shared/output&lt;/code&gt; should be filled with the results from the cluster workers, and the &lt;code&gt;modelling.summary&lt;/code&gt; data frame contains the summary of each fitted model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;output.png&#34; alt=&#34;Listing outputs in the shared folder&#34; /&gt;
Now that the work is done, we can stop the cluster.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::stopCluster(cl = my.cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you know how to work with data larger than memory in a parallelized loop!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Parallelized loops with R</title>
      <link>https://blasbenito.com/post/02_parallelizing_loops_with_r/</link>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/02_parallelizing_loops_with_r/</guid>
      <description>
&lt;script src=&#34;https://blasbenito.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://blasbenito.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://blasbenito.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; to better follow this tutorial you can download the .Rmd file &lt;a href=&#34;https://www.dropbox.com/s/wsl2hcex3w0lr6u/parallelized_loops.Rmd?dl=1&#34;&gt;from here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://www.blasbenito.com/post/01_home_cluster/&#34;&gt;a previous post&lt;/a&gt; I explained how to set up a small home cluster. Many things can be done with a cluster, and parallelizing loops is one of them. But there is no need of a cluster to parallelize loops and improve the efficiency of your coding!&lt;/p&gt;
&lt;p&gt;I believe that coding parallelized loops is an important asset for anyone working with R. That’s why this post covers the following topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Beyond &lt;code&gt;for&lt;/code&gt;: building loops with &lt;code&gt;foreach&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;What is a parallel backend?&lt;/li&gt;
&lt;li&gt;Setup of a parallel backend for a single computer.&lt;/li&gt;
&lt;li&gt;Setup for a Beowulf cluster.&lt;/li&gt;
&lt;li&gt;Practical examples.
&lt;ul&gt;
&lt;li&gt;Tuning of random forest hyperparameters.&lt;/li&gt;
&lt;li&gt;Confidence intervals of the importance scores of the predictors in random forest models.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div id=&#34;for-loops-are-fine-but&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;for&lt;/code&gt; loops are fine, but…&lt;/h2&gt;
&lt;p&gt;Many experienced R users frequently say that nobody should write loops with R because they are tacky or whatever. However, I find loops easy to write, read, and debug, and are therefore my workhorse whenever I need to repeat a task and I don’t feel like using &lt;code&gt;apply()&lt;/code&gt; and the likes. However, regular &lt;code&gt;for&lt;/code&gt; loops in R are highly inefficient, because they only use one of your computer cores to perform the iterations.&lt;/p&gt;
&lt;p&gt;For example, the &lt;code&gt;for&lt;/code&gt; loop below sorts vectors of random numbers a given number of times, and will only work on one of your computer cores for a few seconds, while the others are there, procrastinating with no shame.&lt;/p&gt;
&lt;div class=&#34;tenor-gif-embed&#34; data-postid=&#34;16563810&#34; data-share-method=&#34;host&#34; data-width=&#34;60%&#34; data-aspect-ratio=&#34;1.7785714285714287&#34;&gt;
&lt;a href=&#34;https://tenor.com/view/wot-cpu-danceing-break-dancing-cool-gif-16563810&#34;&gt;Wot Cpu GIF&lt;/a&gt; from &lt;a href=&#34;https://tenor.com/search/wot-gifs&#34;&gt;Wot GIFs&lt;/a&gt;
&lt;/div&gt;
&lt;script type=&#34;text/javascript&#34; async src=&#34;https://tenor.com/embed.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;(gif kindly suggested by &lt;a href=&#34;https://twitter.com/AndrosSpica&#34;&gt;Andreas Angourakis&lt;/a&gt;)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:10000){
  sort(runif(10000))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If every &lt;code&gt;i&lt;/code&gt; could run in a different core, the operation would indeed run a bit faster, and we would get rid of lazy cores. This is were packages like &lt;a href=&#34;https://cran.r-project.org/web/packages/foreach&#34;&gt;&lt;code&gt;foreach&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/doParallel&#34;&gt;&lt;code&gt;doParallel&lt;/code&gt;&lt;/a&gt; come into play. Let’s start installing these packages and a few others that will be useful throughout this tutorial.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#automatic install of packages if they are not installed already
list.of.packages &amp;lt;- c(
  &amp;quot;foreach&amp;quot;,
  &amp;quot;doParallel&amp;quot;,
  &amp;quot;ranger&amp;quot;,
  &amp;quot;palmerpenguins&amp;quot;,
  &amp;quot;tidyverse&amp;quot;,
  &amp;quot;kableExtra&amp;quot;
  )

new.packages &amp;lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&amp;quot;Package&amp;quot;])]

if(length(new.packages) &amp;gt; 0){
  install.packages(new.packages, dep=TRUE)
}

#loading packages
for(package.i in list.of.packages){
  suppressPackageStartupMessages(
    library(
      package.i, 
      character.only = TRUE
      )
    )
}

#loading example data
data(&amp;quot;penguins&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;beyond-for-building-loops-with-foreach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beyond &lt;code&gt;for&lt;/code&gt;: building loops with &lt;code&gt;foreach&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;foreach&lt;/code&gt; package (the vignette is &lt;a href=&#34;https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html&#34;&gt;here&lt;/a&gt;) provides a way to build loops that support parallel execution, and easily gather the results provided by each iteration in the loop.&lt;/p&gt;
&lt;p&gt;For example, this classic &lt;code&gt;for&lt;/code&gt; loop computes the square root of the numbers 1 to 5 with &lt;code&gt;sqrt()&lt;/code&gt; (the function is vectorized, but let’s conveniently forget that for a moment). Notice that I have to create a vector &lt;code&gt;x&lt;/code&gt; to gather the results before executing the loop.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- vector()
for(i in 1:10){
  x[i] &amp;lt;- sqrt(i)
  }
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;foreach&lt;/code&gt; version returns a list with the results automatically. Notice that &lt;code&gt;%do%&lt;/code&gt; operator after the loop definition, I’ll talk more about it later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- foreach(i = 1:10) %do% {
  sqrt(i)
  }
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1.414214
## 
## [[3]]
## [1] 1.732051
## 
## [[4]]
## [1] 2
## 
## [[5]]
## [1] 2.236068
## 
## [[6]]
## [1] 2.44949
## 
## [[7]]
## [1] 2.645751
## 
## [[8]]
## [1] 2.828427
## 
## [[9]]
## [1] 3
## 
## [[10]]
## [1] 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the &lt;code&gt;.combine&lt;/code&gt; argument of &lt;code&gt;foreach&lt;/code&gt; to arrange the list as a vector. Other options such as &lt;code&gt;cbind&lt;/code&gt;, &lt;code&gt;rbind&lt;/code&gt;, or even custom functions can be used as well, only depending on the structure of the output of each iteration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- foreach(
  i = 1:10, 
  .combine = &amp;#39;c&amp;#39;
) %do% {
    sqrt(i)
  }
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another interesting capability of &lt;code&gt;foreach&lt;/code&gt; is that it supports several iterators of the same length at once. Notice that the values of the iterators are not combined. When the first value of one iterator is being used, the first value of the other iterators will be used as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- foreach(
  i = 1:3, 
  j = 1:3, 
  k = 1:3, 
  .combine = &amp;#39;c&amp;#39;
  ) %do% {
  i + j + k
  }
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 6 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-foreach-loops-in-parallel&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running &lt;code&gt;foreach&lt;/code&gt; loops in parallel&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;foreach&lt;/code&gt; loops shown above use the operator &lt;code&gt;%do%&lt;/code&gt;, that processes the tasks sequentially. To run tasks in parallel, &lt;code&gt;foreach&lt;/code&gt; uses the operator &lt;code&gt;%dopar%&lt;/code&gt;, that has to be supported by a parallel &lt;em&gt;backend&lt;/em&gt;. If there is no parallel backend, &lt;code&gt;%dopar%&lt;/code&gt; warns the user that it is being run sequentially, as shown below. But what the heck is a parallel backend?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- foreach(
  i = 1:10, 
  .combine = &amp;#39;c&amp;#39;
) %dopar% {
    sqrt(i)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: executing %dopar% sequentially: no parallel backend registered&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div id=&#34;what-is-a-parallel-backend&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is a parallel backend?&lt;/h3&gt;
&lt;p&gt;When running tasks in parallel, there should be a &lt;em&gt;director&lt;/em&gt; node that tells a group of &lt;em&gt;workers&lt;/em&gt; what to do with a given set of data and functions. The &lt;em&gt;workers&lt;/em&gt; execute the iterations, and the &lt;em&gt;director&lt;/em&gt; manages execution and gathers the results provided by the &lt;em&gt;workers&lt;/em&gt;. A parallel backend provides the means for the director and workers to communicate, while allocating and managing the required computing resources (processors, RAM memory, and network bandwidth among others).&lt;/p&gt;
&lt;p&gt;There are two types of parallel backends that can be used with &lt;code&gt;foreach&lt;/code&gt;, &lt;strong&gt;FORK&lt;/strong&gt; and &lt;strong&gt;PSOCK&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div id=&#34;fork&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;FORK&lt;/h4&gt;
&lt;p&gt;FORK backends are only available on UNIX machines (Linux, Mac, and the likes), and do not work in clusters [sad face], so only single-machine environments are appropriate for this backend. In a FORK backend, the workers share the same environment (data, loaded packages, and functions) as the director. This setup is highly efficient because the main environment doesn’t have to be copied, and only worker outputs need to be sent back to the director.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;FORK.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;psock&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;PSOCK&lt;/h4&gt;
&lt;p&gt;PSOCK backends (Parallel Socket Cluster) are available for both UNIX and WINDOWS systems, and are the default option provided with &lt;code&gt;foreach&lt;/code&gt;. As their main disadvantage, the environment of the director needs to be copied to the environment of each worker, which increases network overhead while decreasing the overall efficiency of the cluster. By default, all the functions available in base R are copied to each worker, and if a particular set of R packages are needed in the workers, they need to be copied to the respective environments of the workers as well.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.r-bloggers.com/2019/06/parallel-r-socket-or-fork/&#34;&gt;This post&lt;/a&gt; compares both backends and concludes that FORK is about a 40% faster than PSOCK.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;PSOCK.png&#34; /&gt;
 &lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;setup-of-a-parallel-backend&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup of a parallel backend&lt;/h2&gt;
&lt;p&gt;Here I explain how to setup the parallel backend for a simple computer and for a Beowulf cluster as &lt;a href=&#34;(https://www.blasbenito.com/post/01_home_cluster/)&#34;&gt;the one I described in a previous post&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;setup-for-a-single-computer&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setup for a single computer&lt;/h3&gt;
&lt;p&gt;Setting up a cluster in a single computer requires first to find out how many cores we want to use from the ones we have available. It is recommended to leave one free core for other tasks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::detectCores()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n.cores &amp;lt;- parallel::detectCores() - 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to define the cluster with &lt;code&gt;parallel::makeCluster()&lt;/code&gt; and register it so it can be used by &lt;code&gt;%dopar%&lt;/code&gt; with &lt;code&gt;doParallel::registerDoParallel(my.cluster)&lt;/code&gt;. The &lt;code&gt;type&lt;/code&gt; argument of &lt;code&gt;parallel::makeCluster()&lt;/code&gt; accepts the strings “PSOCK” and “FORK” to define the type of parallel backend to be used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create the cluster
my.cluster &amp;lt;- parallel::makeCluster(
  n.cores, 
  type = &amp;quot;PSOCK&amp;quot;
  )

#check cluster definition (optional)
print(my.cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## socket cluster with 7 nodes on host &amp;#39;localhost&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#register it to be used by %dopar%
doParallel::registerDoParallel(cl = my.cluster)

#check if it is registered (optional)
foreach::getDoParRegistered()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#how many workers are available? (optional)
foreach::getDoParWorkers()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can run a set of tasks in parallel!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- foreach(
  i = 1:10, 
  .combine = &amp;#39;c&amp;#39;
) %dopar% {
    sqrt(i)
  }
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If everything went well, now &lt;code&gt;%dopar%&lt;/code&gt; should not be throwing the warning &lt;code&gt;executing %dopar% sequentially: no parallel backend registered&lt;/code&gt;, meaning that the parallel execution is working as it should. In this little example there is no gain in execution speed, because the operation being executed is extremely fast, but this will change when the operations running inside of the loop take longer times to run.&lt;/p&gt;
&lt;p&gt;Finally, it is always recommendable to stop the cluster when we are done working with it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::stopCluster(cl = my.cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;setup-for-a-beowulf-cluster&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setup for a Beowulf cluster&lt;/h3&gt;
&lt;p&gt;This setup is a bit more complex, because it requires to open a &lt;em&gt;port&lt;/em&gt; in every computer of the cluster. Ports are virtual communication channels, and are identified by a number.&lt;/p&gt;
&lt;p&gt;First, lets tell R what port we want to use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#define port
Sys.setenv(R_PARALLEL_PORT = 11000)

#check that it
Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we need to open the selected port in every computer of the network. In Linux we need to setup the firewall to allow connections from the network &lt;code&gt;10.42.1.0/24&lt;/code&gt; (replace this with your network range if different!) to the port &lt;code&gt;11000&lt;/code&gt; by splitting the window of the &lt;a href=&#34;https://gnometerminator.blogspot.com/p/introduction.html&#34;&gt;Terminator console&lt;/a&gt; in as many computers available in your network (the figure below shows three, one for my PC and two for my Intel NUCs), opening an ssh session on each remote machine, and setting Terminator with &lt;em&gt;Grouping&lt;/em&gt; equal to &lt;em&gt;Broadcast all&lt;/em&gt; so we only need to type the commands once.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;terminator.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Opening port 11000 in three computers at once with Terminator&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now we have to create an object defining the IPs of the computers in the network, the number of cores to use from each computer, the user name, and the identity of the &lt;em&gt;director&lt;/em&gt;. This will be the &lt;code&gt;spec&lt;/code&gt; argument required by &lt;code&gt;parallel::makeCluster()&lt;/code&gt; to create the cluster throughtout the machines in the network. It is a list of lists, with as many lists as nodes are defined. Each &lt;em&gt;sub-list&lt;/em&gt; has a slot named &lt;em&gt;host&lt;/em&gt; with the IP of the computer where the given node is, and &lt;em&gt;user&lt;/em&gt;, with the name of the user in each computer.&lt;/p&gt;
&lt;p&gt;The code below shows how this would be done, step by step. Yes, this is CUMBERSOME.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#main parameters
director &amp;lt;- &amp;#39;10.42.0.1&amp;#39;
nuc2 &amp;lt;- &amp;#39;10.42.0.34&amp;#39;
nuc1 &amp;lt;- &amp;#39;10.42.0.104&amp;#39;
user &amp;lt;- &amp;quot;blas&amp;quot;

#list of machines, user names, and cores
spec &amp;lt;- list(
  list(
    host = director, 
    user = user,
    ncore = 7
  ), 
  list(
    host = nuc1, 
    user = user,
    ncore = 4
  ),
  list(
    host = nuc2, 
    user = user,
    ncore = 4
  )
)

#generating nodes from the list of machines
spec &amp;lt;- lapply(
  spec, 
  function(spec.i) rep(
    list(
      list(
        host = spec.i$host, 
        user = spec.i$user)
      ), 
    spec.i$ncore
    )
)

#formating into a list of lists
spec &amp;lt;- unlist(
  spec, 
  recursive = FALSE
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Generating the &lt;code&gt;spec&lt;/code&gt; definition is a bit easier with the function below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#function to generate cluster specifications from a vector of IPs, a vector with the number of cores to use on each IP, and a user name
cluster_spec &amp;lt;- function(
  ips,
  cores,
  user
){
  
  #creating initial list
  spec &amp;lt;- list()
  
  for(i in 1:length(ips)){
    spec[[i]] &amp;lt;- list()
    spec[[i]]$host &amp;lt;- ips[i]
    spec[[i]]$user &amp;lt;- user
    spec[[i]]$ncore &amp;lt;- cores[i]
  }

  #generating nodes from the list of machines
  spec &amp;lt;- lapply(
    spec, 
    function(spec.i) rep(
      list(
        list(
          host = spec.i$host, 
          user = spec.i$user)
        ), 
      spec.i$ncore
      )
  )

  #formating into a list of lists
  spec &amp;lt;- unlist(
    spec, 
    recursive = FALSE
  )
  
  return(spec)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function is also available in &lt;a href=&#34;https://gist.github.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca&#34;&gt;this GitHub Gist&lt;/a&gt;, so you can load it into your R environment by executing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;https://gist.githubusercontent.com/BlasBenito/93ee54d3a98d101754aaff0d658dccca/raw/de57b23740ca90bc02fbd0d5cd3551106ff2fb6d/cluster_spec.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below I use it to generate the input to the &lt;code&gt;spec&lt;/code&gt; argument to start the cluster with &lt;code&gt;parallel::makeCluster()&lt;/code&gt;. Notice that I have added several arguments.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The argument &lt;code&gt;outfile&lt;/code&gt; determines where the workers write a log. In this case it is set to &lt;em&gt;nowhere&lt;/em&gt; with the double quotes, but the path to a text file in the director could be provided here.&lt;/li&gt;
&lt;li&gt;The argument &lt;code&gt;homogeneous = TRUE&lt;/code&gt; indicates that all machines have the &lt;code&gt;Rscript&lt;/code&gt; in the same location. In this case all three machines have it at “/usr/lib/R/bin/Rscript”. Otherwise, set it up to &lt;code&gt;FALSE&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#generate cluster specification
spec &amp;lt;- cluster_spec(
  ips = c(&amp;#39;10.42.0.1&amp;#39;, &amp;#39;10.42.0.34&amp;#39;, &amp;#39;10.42.0.104&amp;#39;),
  cores = c(7, 4, 4),
  user = &amp;quot;blas&amp;quot;
)

#setting up cluster
my.cluster &amp;lt;- parallel::makeCluster(
  master = &amp;#39;10.42.0.1&amp;#39;, 
  spec = spec,
  port = Sys.getenv(&amp;quot;R_PARALLEL_PORT&amp;quot;),
  outfile = &amp;quot;&amp;quot;,
  homogeneous = TRUE
)

#check cluster definition (optional)
print(my.cluster)

#register cluster
doParallel::registerDoParallel(cl = my.cluster)

#how many workers are available? (optional)
foreach::getDoParWorkers()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can use the cluster to execute a dummy operation in parallel using all machines in the network.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- foreach(
  i = 1:20, 
  .combine = &amp;#39;c&amp;#39;
) %dopar% {
    sqrt(i)
  }
x&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once everything is done, remember to close the cluster.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::stopCluster(cl = my.cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;practical-examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Practical examples&lt;/h2&gt;
&lt;p&gt;In this section I cover two examples on how to use parallelized loops to explore model outputs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tuning random forest &lt;a href=&#34;https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)&#34;&gt;&lt;em&gt;hyperparameters&lt;/em&gt;&lt;/a&gt; to maximize classification accuracy.&lt;/li&gt;
&lt;li&gt;Obtain a confidence interval for the importance score of each predictor from a set random forest models fitted with &lt;a href=&#34;https://github.com/imbs-hl/ranger&#34;&gt;&lt;code&gt;ranger()&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the examples I use the &lt;code&gt;penguins&lt;/code&gt; data from the &lt;a href=&#34;https://github.com/allisonhorst/palmerpenguins&#34;&gt;&lt;code&gt;palmerpenguins&lt;/code&gt;&lt;/a&gt; package to fit classification models with random forest using &lt;em&gt;species&lt;/em&gt; as a response, and &lt;em&gt;bill_length_mm&lt;/em&gt;, &lt;em&gt;bill_depth_mm&lt;/em&gt;, &lt;em&gt;flipper_length_mm&lt;/em&gt;, and &lt;em&gt;body_mass_g&lt;/em&gt; as predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#removing NA and subsetting columns
penguins &amp;lt;- as.data.frame(
  na.omit(
    penguins[, c(
      &amp;quot;species&amp;quot;,
      &amp;quot;bill_length_mm&amp;quot;,
      &amp;quot;bill_depth_mm&amp;quot;,
      &amp;quot;flipper_length_mm&amp;quot;,
      &amp;quot;body_mass_g&amp;quot;
    )]
    )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
species
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
bill_length_mm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
bill_depth_mm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
flipper_length_mm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
body_mass_g
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
181
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3750
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
186
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3800
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
195
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3250
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
36.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
193
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3450
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
190
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3650
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
181
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3625
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
195
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4675
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
193
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3475
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
190
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4250
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
186
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3300
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
180
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3700
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
41.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
182
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3200
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
38.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
191
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3800
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
198
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4400
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
36.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
185
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3700
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
38.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
195
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3450
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
197
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4500
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
184
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3325
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
194
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4200
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adelie
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
174
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3400
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We’ll fit random forest models with the &lt;a href=&#34;https://cran.r-project.org/package=ranger&#34;&gt;&lt;code&gt;ranger&lt;/code&gt;&lt;/a&gt; package, which works as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#fitting classification model
m &amp;lt;- ranger::ranger(
  data = penguins,
  dependent.variable.name = &amp;quot;species&amp;quot;,
  importance = &amp;quot;permutation&amp;quot;
)

#summary
m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Ranger result
## 
## Call:
##  ranger::ranger(data = penguins, dependent.variable.name = &amp;quot;species&amp;quot;,      importance = &amp;quot;permutation&amp;quot;) 
## 
## Type:                             Classification 
## Number of trees:                  500 
## Sample size:                      342 
## Number of independent variables:  4 
## Mtry:                             2 
## Target node size:                 1 
## Variable importance mode:         permutation 
## Splitrule:                        gini 
## OOB prediction error:             2.34 %&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#variable importance
m$variable.importance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    bill_length_mm     bill_depth_mm flipper_length_mm       body_mass_g 
##        0.30464149        0.16554689        0.22329574        0.07775624&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output shows that the percentage of misclassified cases is 2.34, and that &lt;em&gt;bill_length_mm&lt;/em&gt; is the variable that contributes the most to the accuracy of the classification.&lt;/p&gt;
&lt;p&gt;If you are not familiar with random forest, &lt;a href=&#34;https://victorzhou.com/blog/intro-to-random-forests/&#34;&gt;this post&lt;/a&gt; and the video below do a pretty good job in explaining the basics:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/D_2LkhMJcfY&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;div id=&#34;tuning-random-forest-hyperparameters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tuning random forest hyperparameters&lt;/h3&gt;
&lt;p&gt;Random forest has several hyperparameters that influence model fit:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;num.trees&lt;/code&gt; is the total number of trees to fit. The default value is 500.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mtry&lt;/code&gt; is the number of variables selected by chance (from the total pool of variables) as candidates for a tree split. The minimum is 2, and the maximum is the total number of predictors.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min.node.size&lt;/code&gt; is the minimum number of cases that shall go together in the terminal nodes of each tree. For classification models as the ones we are going to fit, 1 is the minimum.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we are going to explore how combinations of these values increase or decrease the prediction error of the model (percentage of misclassified cases) on the out-of-bag data (not used to train each decision tree). This operation is usually named &lt;strong&gt;grid search for hyperparameter optimization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To create these combinations of hyperparameters we use &lt;code&gt;expand.grid()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sensitivity.df &amp;lt;- expand.grid(
  num.trees = c(500, 1000, 1500),
  mtry = 2:4,
  min.node.size = c(1, 10, 20)
)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
num.trees
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
mtry
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
min.node.size
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Each row in &lt;code&gt;sensitivity.df&lt;/code&gt; corresponds to a combination of parameters to test, so there are 27 models to fit. The code below prepares the cluster, and uses the ability of &lt;code&gt;foreach&lt;/code&gt; to work with several iterators at once to easily introduce the right set of hyperparameters to each fitted model.&lt;/p&gt;
&lt;p&gt;Notice how in the &lt;code&gt;foreach&lt;/code&gt; definition I use the &lt;code&gt;.packages&lt;/code&gt; argument to export the &lt;code&gt;ranger&lt;/code&gt; package to the environments of the workers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create and register cluster
my.cluster &amp;lt;- parallel::makeCluster(n.cores)
doParallel::registerDoParallel(cl = my.cluster)
  
#fitting each rf model with different hyperparameters
prediction.error &amp;lt;- foreach(
  num.trees = sensitivity.df$num.trees,
  mtry = sensitivity.df$mtry,
  min.node.size = sensitivity.df$min.node.size,
  .combine = &amp;#39;c&amp;#39;, 
  .packages = &amp;quot;ranger&amp;quot;
) %dopar% {
  
  #fit model
  m.i &amp;lt;- ranger::ranger(
    data = penguins,
    dependent.variable.name = &amp;quot;species&amp;quot;,
    num.trees = num.trees,
    mtry = mtry,
    min.node.size = min.node.size
  )
  
  #returning prediction error as percentage
  return(m.i$prediction.error * 100)
  
}

#adding the prediction error column
sensitivity.df$prediction.error &amp;lt;- prediction.error&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To plot the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot2::ggplot(data = sensitivity.df) + 
  ggplot2::aes(
    x = mtry,
    y = as.factor(min.node.size),
    fill = prediction.error
  ) + 
  ggplot2::facet_wrap(as.factor(num.trees)) +
  ggplot2::geom_tile() + 
  ggplot2::scale_y_discrete(breaks = c(1, 10, 20)) +
  ggplot2::scale_fill_viridis_c() + 
  ggplot2::ylab(&amp;quot;min.node.size&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;sensitivity.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The figure shows that combinations of lower values of &lt;code&gt;min.node.size&lt;/code&gt; and &lt;code&gt;mtry&lt;/code&gt; generally lead to models with a lower prediction error across different numbers of trees. Retrieving the first line of &lt;code&gt;sensitivity.df&lt;/code&gt; ordered by ascending &lt;code&gt;prediction.error&lt;/code&gt; will give us the values of the hyperparameters we need to use to reduce the prediction error as much as possible.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best.hyperparameters &amp;lt;- sensitivity.df %&amp;gt;% 
  dplyr::arrange(prediction.error) %&amp;gt;% 
  dplyr::slice(1)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
num.trees
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
mtry
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
min.node.size
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
prediction.error
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.339181
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-intervals-of-variable-importance-scores&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confidence intervals of variable importance scores&lt;/h3&gt;
&lt;p&gt;Random forest has an important stochastic component during model fitting, and as consequence, the same model will return slightly different results in different runs (unless &lt;code&gt;set.seed()&lt;/code&gt; or the &lt;code&gt;seed&lt;/code&gt; argument of &lt;code&gt;ranger&lt;/code&gt; are used). This variability also affects the importance scores of the predictors, and can be use to our advantage to assess whether the importance scores of different variables do really overlap or not.&lt;/p&gt;
&lt;p&gt;I have written a little function to transform the vector of importance scores returned by &lt;code&gt;ranger&lt;/code&gt; into a data frame (of one row). It helps arranging the importance scores of different runs into a long format, which helps a lot to plot a boxplot with &lt;code&gt;ggplot2&lt;/code&gt; right away. This function could have been just some code thrown inside the &lt;code&gt;foreach&lt;/code&gt; loop, but I want to illustrate how &lt;code&gt;foreach&lt;/code&gt; automatically transfers functions available in the R environment into the environments of the workers when required, without the intervention of the user. The same will happen with the &lt;code&gt;best.hyperparameters&lt;/code&gt; tiny data frame we created in the previous section.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;importance_to_df &amp;lt;- function(model){
  x &amp;lt;- as.data.frame(model$variable.importance)
  x$variable &amp;lt;- rownames(x)
  colnames(x)[1] &amp;lt;- &amp;quot;importance&amp;quot;
  rownames(x) &amp;lt;- NULL
  return(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code chunk below setups the cluster and runs 1000 random forest models in parallel (using the best hyperparameters computed in the previous section) while using &lt;code&gt;system.time()&lt;/code&gt; to assess running time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#we don&amp;#39;t need to create the cluster, it is still up
print(my.cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## socket cluster with 7 nodes on host &amp;#39;localhost&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#assessing execution time
system.time(
  
  #performing 1000 iterations in parallel
  importance.scores &amp;lt;- foreach(
    i = 1:1000, 
    .combine = &amp;#39;rbind&amp;#39;, 
    .packages = &amp;quot;ranger&amp;quot;
  ) %dopar% {
    
    #fit model
    m.i &amp;lt;- ranger::ranger(
      data = penguins,
      dependent.variable.name = &amp;quot;species&amp;quot;,
      importance = &amp;quot;permutation&amp;quot;,
      mtry = best.hyperparameters$mtry,
      num.trees = best.hyperparameters$num.trees,
      min.node.size = best.hyperparameters$min.node.size
    )
    
    #format importance
    m.importance.i &amp;lt;- importance_to_df(model = m.i)
    
    #returning output
    return(m.importance.i)
    
  }
  
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.267   0.027   6.556&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output of &lt;code&gt;system.time()&lt;/code&gt; goes as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;user&lt;/em&gt;: seconds the R session has been using the CPU.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;system&lt;/em&gt;: seconds the operating system has been using the CPU.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;elapsed&lt;/em&gt;: the total execution time experienced by the user.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This will make sense in a minute. In the meantime, let’s plot our results!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot2::ggplot(data = importance.scores) + 
  ggplot2::aes(
    y = reorder(variable, importance), 
    x = importance
  ) +
  ggplot2::geom_boxplot() + 
  ggplot2::ylab(&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;boxplot.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The figure shows that the variable &lt;em&gt;bill_length_mm&lt;/em&gt; is the most important in helping the model classifying penguin species, with no overlap with any other variable. In this particular case, since the distributions of the importance scores do not overlap, this analysis isn’t truly helpful, but now you know how to do it!&lt;/p&gt;
&lt;p&gt;I assessed the running time with &lt;code&gt;system.time()&lt;/code&gt; because &lt;code&gt;ranger()&lt;/code&gt; can run in parallel by itself just by setting the &lt;code&gt;num.threads&lt;/code&gt; argument to the number of cores available in the machine. This capability cannot be used when executing &lt;code&gt;ranger()&lt;/code&gt; inside a parallelized &lt;code&gt;foreach&lt;/code&gt; loop though, and it is only useful inside classic &lt;code&gt;for&lt;/code&gt; loops.&lt;/p&gt;
&lt;p&gt;What option is more efficient then? The code below executes a regular &lt;code&gt;for&lt;/code&gt; loop running the function sequentially to evaluate whether it is more efficient to run &lt;code&gt;ranger()&lt;/code&gt; in parallel using one core per model, as we did above, or sequentially while using several cores per model on each iteration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#list to save results
importance.scores.list &amp;lt;- list()

#performing 1000 iterations sequentially
system.time(
  
  for(i in 1:1000){
    
    #fit model
    m.i &amp;lt;- ranger::ranger(
      data = penguins,
      dependent.variable.name = &amp;quot;species&amp;quot;,
      importance = &amp;quot;permutation&amp;quot;,
      seed = i,
      num.threads = parallel::detectCores() - 1
    )
    
    #format importance
    importance.scores.list[[i]] &amp;lt;- importance_to_df(model = m.i)
    
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##  43.663   2.815  12.948&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, &lt;code&gt;ranger()&lt;/code&gt; takes longer to execute in a regular &lt;code&gt;for&lt;/code&gt; loop using several cores at once than in a parallel &lt;code&gt;foreach&lt;/code&gt; loop using one core at once. That’s a win for the parallelized loop!&lt;/p&gt;
&lt;p&gt;We can stop our cluster now, we are done with it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::stopCluster(cl = my.cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-few-things-to-take-in-mind&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A few things to take in mind&lt;/h2&gt;
&lt;p&gt;As I have shown in this post, using parallelized &lt;code&gt;foreach&lt;/code&gt; loops can accelerate long computing processes, even when some functions have the ability to run in parallel on their own. However, there are things to take in mind, that might vary depending on whether we are executing the parallelized task on a single computer or on a small cluster.&lt;/p&gt;
&lt;p&gt;In a single computer, the communication between workers and the director is usually pretty fast, so there are no obvious bottlenecks to take into account here. The only limitation that might arise comes from the availability of RAM memory. For example, if a computer has 8 cores and 8GB of RAM, less than 1GB of RAM will be available for each worker. So, if you need to repeat a process that consumes a significant amount of RAM, the ideal number of cores running in parallel might be lower than the total number of cores available in your system. Don’t be greedy, and try to understand the capabilities of your machine while designing a parallelized task.&lt;/p&gt;
&lt;p&gt;When running &lt;code&gt;foreach&lt;/code&gt; loops as in &lt;code&gt;x &amp;lt;- foreach(...){...}&lt;/code&gt;, the variable &lt;code&gt;x&lt;/code&gt; is receiving whatever results the workers are producing. For example, if you are only returning the prediction error of a model, or its importance scores, &lt;code&gt;x&lt;/code&gt; will have a very manageable size. But if you are returning heavy objects such as complete random forest models, the size of &lt;code&gt;x&lt;/code&gt; is going to grow VERY FAST, and at the end it will be competing for RAM resources with the workers, which might even crash your R session. Again, don’t be greedy, and size your outputs carefully.&lt;/p&gt;
&lt;p&gt;Clusters spanning several computers are a different beast, since the workers and the director communicate through a switch and network wires and interfaces. If the amount of data going to and coming from the workers is large, the network can get clogged easily, reducing the cluster’s efficiency drastically. In general, if the amount of data produced by a worker on each iteration takes longer to arrive to the director than the time it takes the worker to produce it, then a cluster is not going to be more efficient than a single machine. But this is not important if you don’t care about efficiency.&lt;/p&gt;
&lt;p&gt;Other issues you might come across while parallelizing tasks in R are thoroughly commented in &lt;a href=&#34;https://towardsdatascience.com/parallelization-caveats-in-r-1-the-basics-multiprocessing-and-multithreading-performance-eb584b7e850e&#34;&gt;this post&lt;/a&gt;, by Imre Gera.&lt;/p&gt;
&lt;p&gt;That’s all for now folks, happy parallelization!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Setting up a home cluster</title>
      <link>https://blasbenito.com/post/01_home_cluster/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/01_home_cluster/</guid>
      <description>&lt;p&gt;In this post I explain how to setup a small 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Beowulf_cluster&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beowulf cluster&lt;/a&gt; with a personal PC running Ubuntu 20.04 and a couple of 
&lt;a href=&#34;https://www.intel.com/content/www/us/en/products/boards-kits/nuc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intel NUCs&lt;/a&gt; running Ubuntu Server 20.04, with the end-goal of parallelizing R tasks.&lt;/p&gt;
&lt;p&gt;The topics I cover here are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Required material&lt;/li&gt;
&lt;li&gt;Network setting&lt;/li&gt;
&lt;li&gt;Installing the secure shell protocol&lt;/li&gt;
&lt;li&gt;Installing Ubuntu server in the NUCs&lt;/li&gt;
&lt;li&gt;Installing R in the NUCs&lt;/li&gt;
&lt;li&gt;Managing the cluster&amp;rsquo;s network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;preamble&#34;&gt;Preamble&lt;/h2&gt;
&lt;p&gt;I have a little but nice HP ENVY model &lt;em&gt;TE01-0008ns&lt;/em&gt; with 32 GB RAM, 8 CPUs, and 3TB of hard disk running Ubuntu 20.04 that I use to do all my computational work (and most of my tweeting). A few months ago I connected it with my two laptops (one of them deceased now, RIP my dear &lt;em&gt;skynet&lt;/em&gt;) to create a little cluster to run parallel tasks in R.&lt;/p&gt;
&lt;p&gt;It was just a draft cluster running on a wireless network, but it served me to think about getting a more permanent solution not requiring two additional laptops in my desk.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s were the nice INTEL NUCs (from 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Next_Unit_of_Computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Next Unit of Computing&lt;/em&gt;&lt;/a&gt;) come into play. NUCs are full-fledged computers fitted in small boxes usually sold without RAM memory sticks and no hard disk (hence the term &lt;em&gt;barebone&lt;/em&gt;). Since they have a low energy consumption footprint, I thought these would be ideal units for my soon-to-be home cluster.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;
&lt;p&gt;I gifted myself with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 
&lt;a href=&#34;https://ark.intel.com/content/www/us/en/ark/products/95062/intel-nuc-kit-nuc6cayh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intel Barebone BOXNUC6CAYH&lt;/a&gt;, each with 4 cores, and a maximum RAM memory of 32GB (you might read they only accept 8GB, but that&amp;rsquo;s not the case anymore). Notice that these NUCs aren&amp;rsquo;t state-of-the-art now, they were released by the end of 2016.&lt;/li&gt;
&lt;li&gt;2 Hard disks SSD 2.5&amp;quot; 
&lt;a href=&#34;https://shop.westerndigital.com/es-es/products/internal-drives/wd-blue-sata-ssd#WDS250G2B0A&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Western Digital WDS250G2B0A WD Blue&lt;/a&gt; (250GB)&lt;/li&gt;
&lt;li&gt;4 Crucial CT102464BF186D DDR3 SODIMM (204 pins) RAM sticks with 8GB each.&lt;/li&gt;
&lt;li&gt;1 ethernet switch Netgear GS308-300PES with 8 ports.&lt;/li&gt;
&lt;li&gt;3 ethernet wires NanoCable 10.20.0400-BL of 
&lt;a href=&#34;https://www.electronics-notes.com/articles/connectivity/ethernet-ieee-802-3/how-to-buy-best-ethernet-cables-cat-5-6-7.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cat 6&lt;/a&gt; quality.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The whole set came to cost around 530€, but please notice that I had a clear goal in mind: &amp;ldquo;duplicating&amp;rdquo; my computing power with the minimum number of NUCs, while preserving a share of 4GB of RAM memory per CPU throughout the cluster (based on the features of my desk computer). A more basic setting with more modest NUCs and smaller RAM would cost half of that.&lt;/p&gt;
&lt;p&gt;This instructive video by 
&lt;a href=&#34;https://www.youtube.com/channel/UCYa3XeSHenvosy5wMRpeIww&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Harry&lt;/a&gt; shows how to install the SSD and the RAM sticks in an Intel NUC. It really takes 5 minutes tops, one only has to be a bit careful with the RAM sticks, the pins need to go all the way in into their slots before securing the sticks in place.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/6hzj7DogqXU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;network-settings&#34;&gt;Network settings&lt;/h2&gt;
&lt;p&gt;Before starting to install an operating system in the NUCS, the network setup goes as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;My desktop PC is connected to a router via WIFI and dynamic IP (DHCP).&lt;/li&gt;
&lt;li&gt;The PC and each NUC are connected to the switch with cat6 ethernet wires.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;network.png&#34; alt=&#34;Network diagram&#34;&gt;&lt;/p&gt;
&lt;p&gt;To share my PC&amp;rsquo;s WIFI connection with the NUCs I have to prepare a new &lt;em&gt;connection profile&lt;/em&gt; with the command line tool of Ubuntu&amp;rsquo;s 
&lt;a href=&#34;https://en.wikipedia.org/wiki/NetworkManager&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;NetworkManager&lt;/code&gt;&lt;/a&gt;, named &lt;code&gt;nmcli&lt;/code&gt;, as follows.&lt;/p&gt;
&lt;p&gt;First, I need to find the name of my ethernet interface by checking the status of my network devices with the command line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nmcli device status
DEVICE  TYPE      STATE        CONNECTION  
wlp3s0  wifi      connected    my_wifi 
enp2s0  ethernet  unavailable  --          
lo      loopback  unmanaged    --      
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There I can see that my ethernet interface is named &lt;code&gt;enp2s0&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Second, I have to configure the shared connection.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nmcli connection add type ethernet ifname enp2s0 ipv4.method shared con-name cluster
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Were &lt;code&gt;ifname enp2s0&lt;/code&gt; is the name of the interface I want to use for the new connection, &lt;code&gt;ipv4.method shared&lt;/code&gt; is the type of connection, and &lt;code&gt;con-name cluster&lt;/code&gt; is the name I want the connection to have. This operation adds firewall rules to manage traffic within the &lt;code&gt;cluster&lt;/code&gt; network, starts a DHCP server in the computer that serves IPs to the NUCS, and a DNS server that allows the NUCs to translate internet addresses.&lt;/p&gt;
&lt;p&gt;After turning on the switch, I can check the connection status again with&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nmcli device status
DEVICE  TYPE      STATE      CONNECTION  
enp2s0  ethernet  connected  cluster     
wlp3s0  wifi      connected  my_wifi 
lo      loopback  unmanaged  --    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When checking the IP of the device with &lt;code&gt;bash ifconfig&lt;/code&gt; it should yield &lt;code&gt;10.42.0.1&lt;/code&gt;. Any other computer in the &lt;code&gt;cluster&lt;/code&gt; network will have a dynamic IP in the range &lt;code&gt;10.42.0.1/24&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Further details about how to set a shared connection with &lt;code&gt;NetworkManager&lt;/code&gt; can be found in 
&lt;a href=&#34;https://fedoramagazine.org/internet-connection-sharing-networkmanager/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this nice post by Beniamino Galvani&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;ssh-setup&#34;&gt;SSH setup&lt;/h2&gt;
&lt;p&gt;My PC, as the director of the cluster, needs an &lt;code&gt;SSH client&lt;/code&gt; running, while the NUCs need an &lt;code&gt;SSH server&lt;/code&gt;. 
&lt;a href=&#34;https://www.ionos.com/digitalguide/server/tools/ssh-secure-shell/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;SSH&lt;/code&gt; (&lt;strong&gt;S&lt;/strong&gt;ecure &lt;strong&gt;Sh&lt;/strong&gt;ell)&lt;/a&gt; is a remote authentication protocol that allows secure connections to remote servers that I will be using all the time to manage the cluster. To install, run, and check its status I just have to run these lines in the console:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install ssh 
sudo systemctl enable --now ssh
sudo systemctl status ssh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, a secure certificate of the identity of a given computer, named &lt;code&gt;ssh-key&lt;/code&gt;, that grants access to remote ssh servers and services needs to be generated.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh-keygen &amp;quot;label&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, substitute &amp;ldquo;label&amp;rdquo; by the name of the computer to be used as cluster&amp;rsquo;s &amp;ldquo;director&amp;rdquo;. The system will ask for a file name and a 
&lt;a href=&#34;https://www.ssh.com/ssh/passphrase&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;passphrase&lt;/a&gt; that will be used to encrypt the ssh-key.&lt;/p&gt;
&lt;p&gt;The ssh-key needs to be added to the 
&lt;a href=&#34;https://www.ssh.com/ssh/agent&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;ssh-agent&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh-add ~/.ssh/id_rsa
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To copy the ssh-key to my GitHub account, I have to copy the contents of the file &lt;code&gt;~/.ssh/id_rsa.pub&lt;/code&gt; (can be done just opening it with &lt;code&gt;gedit ~/.ssh/id_rsa.pub&lt;/code&gt; + &lt;code&gt;Ctrl + a&lt;/code&gt; + &lt;code&gt;Ctrl + c&lt;/code&gt;), and paste it on &lt;code&gt;GitHub account &amp;gt; Settings &amp;gt;  SSH and GPG keys &amp;gt; New SSH Key&lt;/code&gt; (green button in the upper right part of the window).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you don&amp;rsquo;t use GitHub, you&amp;rsquo;ll need to copy your ssh-key to the NUCs once they are up and running with &lt;code&gt;ssh-copy-id -i ~/.ssh/id_rsa.pub user_name@nuc_IP&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;installing-and-preparing-ubuntu-server-in-each-nuc&#34;&gt;Installing and preparing ubuntu server in each NUC&lt;/h2&gt;
&lt;p&gt;The NUCs don&amp;rsquo;t need to waste resources in a user graphical interface I won&amp;rsquo;t be using whatsoever. Since they will work in a 
&lt;a href=&#34;https://www.howtogeek.com/660841/what-is-a-headless-server/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;headless&lt;/em&gt; configuration&lt;/a&gt; once the cluster is ready, a Linux distro without graphical user interface such as Ubuntu server is the way to go.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;installing-ubuntu-server&#34;&gt;Installing Ubuntu server&lt;/h3&gt;
&lt;p&gt;First it is important to connect a display, a keyboard, and a mouse to the NUC in preparation, and turn it on while pushing F2 to start the visual BIOS. These BIOS parameters need to be modified:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Advanced (upper right) &amp;gt; Boot &amp;gt; Boot Configuration &amp;gt; UEFI Boot &amp;gt; OS Selection: Linux&lt;/li&gt;
&lt;li&gt;Advanced &amp;gt; Boot &amp;gt; Boot Configuration &amp;gt; UEFI Boot &amp;gt; OS Selection: mark &amp;ldquo;Boot USB Devices First&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;[optional] Advanced &amp;gt; Power &amp;gt; Secondary Power Settings &amp;gt; After Power Failure: &amp;ldquo;Power On&amp;rdquo;. I have the switch and nucs connected to an outlet plug extender with an interrupter. When I switch it on, the NUCs (and the switch) boot automatically after this option is enabled, so I only need to push one button to power up the cluster.&lt;/li&gt;
&lt;li&gt;F10 to save, and shutdown.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To prepare the USB boot device with Ubuntu server 20.04 I first download the .iso from 
&lt;a href=&#34;https://ubuntu.com/download/server&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, by choosing &amp;ldquo;Option 3&amp;rdquo;, which leads to the manual install. Once the .iso file is downloaded, I use 
&lt;a href=&#34;https://ubuntu.com/tutorials/create-a-usb-stick-on-ubuntu#1-overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ubuntu&amp;rsquo;s &lt;code&gt;Startup Disk Creator&lt;/code&gt;&lt;/a&gt; to prepare a bootable USB stick. Now I just have to plug the stick in the NUC and reboot it.&lt;/p&gt;
&lt;p&gt;The Ubuntu server install is pretty straightforward, and only a few things need to be decided along the way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As user name I choose the same I have in my personal computer.&lt;/li&gt;
&lt;li&gt;As name for the NUCs I choose &amp;ldquo;nuc1&amp;rdquo; and &amp;ldquo;nuc2&amp;rdquo;, but any other option will work well.&lt;/li&gt;
&lt;li&gt;As password, for comfort I use the same I have in my personal computer.&lt;/li&gt;
&lt;li&gt;During the network setup, choose DHCP. If the network is properly configured and the switch is powered on, after a few seconds the NUC will acquire an IP in the range &lt;code&gt;10.42.0.1/24&lt;/code&gt;, as any other machine within the &lt;code&gt;cluster&lt;/code&gt; network.&lt;/li&gt;
&lt;li&gt;When asked, mark the option &amp;ldquo;Install in the whole disk&amp;rdquo;, unless you have other plans for your NUC.&lt;/li&gt;
&lt;li&gt;Mark &amp;ldquo;Install OpenSSH&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Provide it with your GitHub user name if you have your ssh-key there, and it will download it right away, facilitating a lot the ssh setup.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reboot once the install is completed. Now I keep configuring the NUC&amp;rsquo;s operating system from my PC through ssh.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;configuring-a-nuc&#34;&gt;Configuring a NUC&lt;/h3&gt;
&lt;p&gt;First, to learn the IP of the NUC:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo arp-scan 10.42.0.1/24
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other alternatives to this command are &lt;code&gt;arp -a&lt;/code&gt; and &lt;code&gt;sudo arp-scan -I enp2s0 --localnet&lt;/code&gt;. Once I learn the IP of the NUC, I add it to the file &lt;code&gt;etc/hosts&lt;/code&gt; of my personal computer as follows.&lt;/p&gt;
&lt;p&gt;First I open the file as root.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo gedit /etc/hosts
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add a new line there: &lt;code&gt;10.42.0.XXX nuc1&lt;/code&gt; and save the file.&lt;/p&gt;
&lt;p&gt;Now I access the NUC trough ssh to keep preparing it without a keyboard and a display. I do it from &lt;code&gt;Tilix&lt;/code&gt;, that allows to open different command line tabs in the same window, which is quite handy to manage several NUCs at once.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;htop.png&#34; alt=&#34;Tilix showing htop on my PC and the two NUCS&#34;&gt;&lt;/p&gt;
&lt;p&gt;Another great option to manage the NUCs through ssh is &lt;code&gt;terminator&lt;/code&gt;, that allows to 
&lt;a href=&#34;https://opensource.com/article/20/2/terminator-ssh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;broadcast the same commands to several ssh sessions at once&lt;/a&gt;. I have been trying it, and it is much better for cluster management purposes than Tilix. Actually, using it would simplify this workflow a lot, because once Ubuntu server is installed on each NUC, the rest of the configuration commands can be broadcasted at once to both NUCs. It&amp;rsquo;s a bummer I discovered this possibility way too late!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh blas@10.42.0.XXX
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The NUC&amp;rsquo;s operating system probably has a bunch of pending software updates. To install these:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get upgrade
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I have to install a set of software packages that will facilitate managing the cluster&amp;rsquo;s network and the NUC itself.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install net-tools arp-scan lm-sensors dirmngr gnupg apt-transport-https ca-certificates software-properties-common samba libopenmpi3 libopenmpi-dev openmpi-bin openmpi-common htop
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;setting-the-system-time&#34;&gt;Setting the system time&lt;/h3&gt;
&lt;p&gt;To set the system time of the NUC to the same you have in your computer, just repeat these steps in every computer in the cluster network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#list time zones: 
timedatectl list-timezones
#set time zone
sudo timedatectl set-timezone Europe/Madrid
#enable timesyncd
sudo timedatectl set-ntp on
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;setting-the-locale&#34;&gt;Setting the locale&lt;/h3&gt;
&lt;p&gt;The operating systems of the NUCs and the PC need to have the same locale. It can be set by editing the file &lt;code&gt;/etc/default/locale&lt;/code&gt; with either &lt;code&gt;nano&lt;/code&gt; (in the NUCS) or &lt;code&gt;gedit&lt;/code&gt; (in the PC) and adding these lines, just replacing &lt;code&gt;en_US.UTF-8&lt;/code&gt; with your preferred locale.&lt;/p&gt;
&lt;p&gt;LANG=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LANGUAGE=&amp;ldquo;en_US:en&amp;rdquo;&lt;br&gt;
LC_NUMERIC=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_TIME=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_MONETARY=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_PAPER=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_IDENTIFICATION=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_NAME=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_ADDRESS=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_TELEPHONE=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;br&gt;
LC_MEASUREMENT=&amp;ldquo;en_US.UTF-8&amp;rdquo;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;temperature-monitoring&#34;&gt;Temperature monitoring&lt;/h3&gt;
&lt;p&gt;NUCs are 
&lt;a href=&#34;https://www.intel.com/content/www/us/en/support/articles/000033327/intel-nuc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;prone to overheating&lt;/a&gt; when under heavy loads for prolonged times. Therefore, monitoring the temperature of the NUCs CPUs is kinda important. In a step before I installed &lt;code&gt;lm-sensors&lt;/code&gt; in the NUC, which provides the tools to do so. To setup the sensors from an ssh session in the NUC:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo sensors-detect
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The program will request permission to find sensors in the NUC. I answered &amp;ldquo;yes&amp;rdquo; to every request. Once all sensors are identified, to check them&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sensors

iwlwifi_1-virtual-0
Adapter: Virtual device
temp1:            N/A  

acpitz-acpi-0
Adapter: ACPI interface
temp1:        +32.0°C  (crit = +100.0°C)

coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +30.0°C  (high = +105.0°C, crit = +105.0°C)
Core 0:        +30.0°C  (high = +105.0°C, crit = +105.0°C)
Core 1:        +30.0°C  (high = +105.0°C, crit = +105.0°C)
Core 2:        +29.0°C  (high = +105.0°C, crit = +105.0°C)
Core 3:        +30.0°C  (high = +105.0°C, crit = +105.0°C)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which gives the cpu temperatures at the moment the command was executed. The command &lt;code&gt;watch sensors&lt;/code&gt; gives continuous temperature readings instead.&lt;/p&gt;
&lt;p&gt;To control overheating in my NUCs I removed their top lids, and installed them into a custom LEGO &amp;ldquo;rack&amp;rdquo; with 
&lt;a href=&#34;http://www.eluteng.com/module/fan/12cm/details003.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;external USB fans&lt;/a&gt; with velocity control, as shown in the picture at the beginning of the post.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;installing-r&#34;&gt;Installing R&lt;/h3&gt;
&lt;p&gt;To install R in the NUCs I just proceed as I would when installing it in my personal computer. There is a thorough guide 
&lt;a href=&#34;https://linuxize.com/post/how-to-install-r-on-ubuntu-20-04/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In a step above I installed all the pre-required software packages. Now I only have to add the security key of the R repository, add the repository itself, update the information on the packages available in the new repository, and finally install R.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9
sudo add-apt-repository &#39;deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/&#39;
sudo apt update
sudo apt install r-base
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If R has issues to recognize the system locale&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nano ~/.profile
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;add the following lines, replacing &lt;code&gt;en_US.UTF-8&lt;/code&gt; with your preferred locale&lt;/p&gt;
&lt;p&gt;&lt;code&gt;export LANG=en_US.UTF-8&lt;/code&gt;
&lt;code&gt;export LC_ALL=en_US.UTF-8&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;save, and execute the file to export the locale so R can read it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;. ~/.profile
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;finalizing-the-network-configuration&#34;&gt;Finalizing the network configuration&lt;/h3&gt;
&lt;p&gt;Each NUC needs firewall rules to grant access from other computers withinn the cluster network. To activate the NUC&amp;rsquo;s firewall and check what ports are open:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ufw enable
sudo ufw status
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To grant access from the PC to the NUC through ssh, and later through R for parallel computing, the ports &lt;code&gt;22&lt;/code&gt; and &lt;code&gt;11000&lt;/code&gt; must be open for the IP of the PC (&lt;code&gt;10.42.0.1&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ufw allow ssh
sudo ufw allow from 10.42.0.1 to any port 11000
sudo ufw allow from 10.42.0.1 to any port 22
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the other members of the cluster network must be declared in the &lt;code&gt;/etc/hosts&lt;/code&gt; file of each computer.&lt;/p&gt;
&lt;p&gt;In each NUC edit the file through ssh with &lt;code&gt;bash sudo nano /etc/hosts&lt;/code&gt; and add the lines&lt;/p&gt;
&lt;p&gt;&lt;code&gt;10.42.0.1 pc_name&lt;/code&gt;&lt;br&gt;
&lt;code&gt;10.42.0.XXX name_of_the_other_nuc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In the PC, add the lines&lt;/p&gt;
&lt;p&gt;&lt;code&gt;10.42.0.XXX name_of_one_nuc&lt;/code&gt;&lt;br&gt;
&lt;code&gt;10.42.0.XXX name_of_the_other_nuc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;At this point, after rebooting every machine, the NUCs must be accessible through ssh by using their names (&lt;code&gt;ssh username@nuc_name&lt;/code&gt;) instead of their IPs (&lt;code&gt;ssh username@n10.42.0.XXX&lt;/code&gt;). Just take in mind that, since the &lt;code&gt;cluster&lt;/code&gt; network works with dynamic IPs (and such setting cannot be changed in a shared connection), the IPs of the NUCs might change if a new device is added to the network. That&amp;rsquo;s something you need to check from the PC with &lt;code&gt;sudo arp-scan 10.42.0.1/24&lt;/code&gt;, to update every &lt;code&gt;/etc/hosts&lt;/code&gt; file accordingly.&lt;/p&gt;
&lt;p&gt;I think that&amp;rsquo;s all folks. Good luck setting your home cluster! Next time I will describe how to use it for parallel computing in R.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
