<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science | Blas M. Benito, PhD</title>
    <link>https://blasbenito.com/tag/data-science/</link>
      <atom:link href="https://blasbenito.com/tag/data-science/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Science</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2023 Blas M. Benito. All Rights Reserved.</copyright><lastBuildDate>Sun, 12 Jan 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://blasbenito.com/media/avatar.jpg</url>
      <title>Data Science</title>
      <link>https://blasbenito.com/tag/data-science/</link>
    </image>
    
    <item>
      <title>R package collinear</title>
      <link>https://blasbenito.com/project/collinear/</link>
      <pubDate>Sun, 12 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/project/collinear/</guid>
      <description>&lt;!-- badges: start --&gt;
&lt;p&gt;
&lt;a href=&#34;https://doi.org/10.5281/zenodo.10039489&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.10039489.svg&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://cran.r-project.org/package=collinear&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version/collinear&#34; alt=&#34;CRAN status&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://CRAN.R-project.org/package=collinear&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://cranlogs.r-pkg.org/badges/grand-total/collinear&#34; alt=&#34;CRAN\_Download\_Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;!-- badges: end --&gt;
&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.blasbenito.com/post/multicollinearity-model-interpretability/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multicollinearity hinders the interpretability&lt;/a&gt; of linear and machine learning models.&lt;/p&gt;
&lt;p&gt;The R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;, 
&lt;a href=&#34;https://CRAN.R-project.org/package=collinear&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available on CRAN&lt;/a&gt;, combines four methods for easy management of multicollinearity in modelling data frames with numeric and categorical variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Target Encoding&lt;/strong&gt;: Transforms categorical predictors to numeric using a numeric response as reference.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Preference Order&lt;/strong&gt;: Ranks predictors by their association with a response variable to preserve important ones in multicollinearity filtering.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pairwise Correlation Filtering&lt;/strong&gt;: Automated multicollinearity filtering of numeric and categorical predictors based&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;main-improvements-in-version-200&#34;&gt;Main Improvements in Version 2.0.0&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Expanded Functionality&lt;/strong&gt;: Functions &lt;code&gt;collinear()&lt;/code&gt; and &lt;code&gt;preference_order()&lt;/code&gt; support both categorical and numeric responses and predictors, and can handle several responses at once.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Robust Selection Algorithms&lt;/strong&gt;: Enhanced selection in &lt;code&gt;vif_select()&lt;/code&gt; and &lt;code&gt;cor_select()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enhanced Functionality to Rank Predictors&lt;/strong&gt;: New functions to compute association between response and predictors covering most use-cases, and automated function selection depending on data features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simplified Target Encoding&lt;/strong&gt;: Streamlined and parallelized for better efficiency, and new default is &amp;ldquo;loo&amp;rdquo; (leave-one-out).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallelization and Progress Bars&lt;/strong&gt;: Utilizes &lt;code&gt;future&lt;/code&gt; and &lt;code&gt;progressr&lt;/code&gt; for enhanced performance and user experience.on pairwise correlations.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Variance Inflation Factor Filtering&lt;/strong&gt;: Automated multicollinearity filtering of numeric predictors based on Variance Inflation Factors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The article 
&lt;a href=&#34;https://blasbenito.github.io/collinear/articles/how_it_works.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How It Works&lt;/a&gt; explains how the package works in detail.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you find this package useful, please cite it as:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Blas M. Benito (2024). collinear: R Package for Seamless Multicollinearity Management. Version 2.0.0. doi: 10.5281/zenodo.10039489&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R package distantia</title>
      <link>https://blasbenito.com/project/distantia/</link>
      <pubDate>Sun, 12 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/project/distantia/</guid>
      <description>&lt;!-- badges: start --&gt;
&lt;p&gt;
&lt;a href=&#34;https://zenodo.org/badge/latestdoi/187805264&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/187805264.svg&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://CRAN.R-project.org/package=distantia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version-ago/distantia&#34; alt=&#34;CRAN\_Release\_Badge&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://CRAN.R-project.org/package=distantia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://cranlogs.r-pkg.org/badges/distantia&#34; alt=&#34;CRAN\_Download\_Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;!-- badges: end --&gt;
&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;The R package 
&lt;a href=&#34;https://blasbenito.github.io/distantia/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;&lt;code&gt;distantia&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;, 
&lt;a href=&#34;https://CRAN.R-project.org/package=distantia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available on CRAN&lt;/a&gt;, offers an efficient, feature-rich toolkit for managing, comparing, and analyzing time series data. It is designed to handle a wide range of scenarios, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multivariate and univariate time series.&lt;/li&gt;
&lt;li&gt;Regular and irregular sampling.&lt;/li&gt;
&lt;li&gt;Time series of different lengths.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;h3 id=&#34;comprehensive-analytical-tools&#34;&gt;Comprehensive Analytical Tools&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;10 distance metrics: see &lt;code&gt;distantia::distances&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The normalized dissimilarity metric &lt;code&gt;psi&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Free and Restricted Dynamic Time Warping (DTW) for shape-based comparison.&lt;/li&gt;
&lt;li&gt;A Lock-Step method for sample-to-sample comparison&lt;/li&gt;
&lt;li&gt;Restricted permutation tests for robust inferential support.&lt;/li&gt;
&lt;li&gt;Analysis of contribution to dissimilarity of individual variables in multivariate time series.&lt;/li&gt;
&lt;li&gt;Hierarchical and K-means clustering of time series based on dissimilarity matrices.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;computational-efficiency&#34;&gt;Computational Efficiency&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;C++ back-end&lt;/strong&gt; powered by 
&lt;a href=&#34;https://www.rcpp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rcpp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel processing&lt;/strong&gt; managed through the 
&lt;a href=&#34;https://future.futureverse.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;future&lt;/a&gt; package.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficient data handling&lt;/strong&gt; via 
&lt;a href=&#34;https://CRAN.R-project.org/package=zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;zoo&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;time-series-management-tools&#34;&gt;Time Series Management Tools&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Introduces &lt;strong&gt;time series lists&lt;/strong&gt; (TSL), a versatile format for handling collections of time series stored as lists of &lt;code&gt;zoo&lt;/code&gt; objects.&lt;/li&gt;
&lt;li&gt;Includes a suite of &lt;code&gt;tsl_...()&lt;/code&gt; functions for generating, resampling, transforming, analyzing, and visualizing univariate and multivariate time series.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you find this package useful, please cite it as:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Blas M. Benito, H. John B. Birks (2020). distantia: an open-source toolset to quantify dissimilarity between multivariate ecological time-series. Ecography, 43(5), 660-667. doi: 
&lt;a href=&#34;https://nsojournals.onlinelibrary.wiley.com/doi/10.1111/ecog.04895&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.1111/ecog.04895&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Blas M. Benito (2024). distantia: A Toolset for Time Series Dissimilarity Analysis. R package version 2.0.0. url:  
&lt;a href=&#34;https://blasbenito.github.io/distantia/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://blasbenito.github.io/distantia/&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Reading List: Data Science</title>
      <link>https://blasbenito.com/post/my-reading-list-data-science/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/my-reading-list-data-science/</guid>
      <description>&lt;p&gt;This is a live post listing links to Data Science related posts and videos I consider to be interesting, high-quality, or even essential to better understand particular topics within such a wide field.&lt;/p&gt;
&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://towardsdatascience.com/extending-target-encoding-443aa9414cae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Extending Target Encoding&lt;/strong&gt;&lt;/a&gt;: post by 
&lt;a href=&#34;https://www.aitimejournal.com/interview-with-daniele-micci-barreca-product-analytics-lead-data-science-google/30110/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniele Micci-Barreca&lt;/a&gt; explaining how he came up with the idea of target encoding, and its possible extensions.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://maxhalford.github.io/blog/target-encoding/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Target encoding done the right way&lt;/strong&gt;&lt;/a&gt;: post by 
&lt;a href=&#34;https://maxhalford.github.io/bio/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Max Halford&lt;/a&gt;, Head of Data at 
&lt;a href=&#34;https://www.carbonfact.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Carbonfact&lt;/a&gt;, explaining in detail how to combine additive smoothing and target encoding.&lt;/p&gt;
&lt;h2 id=&#34;handling-and-management&#34;&gt;Handling and Management&lt;/h2&gt;
&lt;h3 id=&#34;apache-parquet&#34;&gt;Apache Parquet&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://airbyte.com/data-engineering-resources/parquet-data-format&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A Deep Dive into Parquet: The Data Format Engineers Need to Know&lt;/strong&gt;&lt;/a&gt;: This by Aditi Prakash, published in the 
&lt;a href=&#34;https://airbyte.com/blog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Airbyte Blog&lt;/a&gt; offers a complete guide about the 
&lt;a href=&#34;https://parquet.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apache Parquet&lt;/a&gt; file format.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.influxdata.com/blog/querying-parquet-millisecond-latency/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Querying Parquet with Millisecond Latency&lt;/strong&gt;&lt;/a&gt; this post from by Raphael Taylor-Davies and Andrew Lamb explains in deep the optimization methods used in 
&lt;a href=&#34;https://parquet.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apache Parquet&lt;/a&gt; files. Warning, this is a very technical read!&lt;/p&gt;
&lt;h3 id=&#34;duckdb&#34;&gt;DuckDB&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://duckdb.org/2024/01/26/multi-database-support-in-duckdb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Multi-Database Support in DuckDB&lt;/strong&gt;&lt;/a&gt; This post by Mark Raasveldt published in the DuckDB blog explains how to query together data from different databases at once.&lt;/p&gt;
&lt;h1 id=&#34;analysis-and-modeling&#34;&gt;Analysis and Modeling&lt;/h1&gt;
&lt;h2 id=&#34;modeling-methods&#34;&gt;Modeling Methods&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://loreley.one/2024-09-pca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Unraveling Principal Component Analysis&lt;/strong&gt;&lt;/a&gt;: This book, reviewed 
&lt;a href=&#34;https://loreley.one/2024-09-pca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, is a &lt;em&gt;tour of linear algebra&lt;/em&gt; focused on intuitive explanations rather than mathematical demonstrations.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://m-clark.github.io/posts/2019-10-20-big-mixed-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Mixed Models for Big Data&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://m-clark.github.io/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Clark&lt;/a&gt; (see entry below by the same author) reviews several mixed modelling approach for large data in R.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://m-clark.github.io/generalized-additive-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Generalized Additive Models&lt;/strong&gt;&lt;/a&gt;: A good online book on Generalized Additive Models by 
&lt;a href=&#34;https://m-clark.github.io/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Clark&lt;/a&gt;, Senior Machine Learning Scientist at 
&lt;a href=&#34;https://www.strong.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Strong Analytics&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;model-explainability&#34;&gt;Model Explainability&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://towardsdatascience.com/a-simple-model-independent-score-explanation-method-c17002d66da7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Model-Independent Score Explanation&lt;/strong&gt;&lt;/a&gt;: Post by 
&lt;a href=&#34;https://www.aitimejournal.com/interview-with-daniele-micci-barreca-product-analytics-lead-data-science-google/30110/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniele Micci-Barreca&lt;/a&gt; on model explainability. It also explains a very clever method to better understand any model just from it&amp;rsquo;s predictions.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AI Explanations whitepaper&lt;/strong&gt;&lt;/a&gt;: White paper of Google&amp;rsquo;s &amp;ldquo;AI Explanations&amp;rdquo; product with a pretty good overall view of the state of the art of model explainability.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1702.08608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Towards A Rigorous Science of Interpretable Machine Learning&lt;/strong&gt;&lt;/a&gt;: Pre-print by Finale Doshi-Velez and Been Kim offering a rigorous definition and evaluation of model interpretability.&lt;/p&gt;
&lt;h2 id=&#34;spatial-analysis&#34;&gt;Spatial Analysis&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://duckdb.org/2023/04/28/spatial.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;PostGEESE? Introducing The DuckDB Spatial Extension&lt;/strong&gt;&lt;/a&gt;: In this post, the authors of 
&lt;a href=&#34;https://duckdb.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DuckDB&lt;/a&gt; present the new PostGIS-like &lt;em&gt;spatial&lt;/em&gt; extension for this popular in-process data base engine.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://py.geocompx.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Geocomputation with Python&lt;/strong&gt;&lt;/a&gt;: A very nice book on geographic data analysis with Python.&lt;/p&gt;
&lt;h1 id=&#34;coding&#34;&gt;Coding&lt;/h1&gt;
&lt;h2 id=&#34;general-concepts&#34;&gt;General Concepts&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://archive.org/details/a-philosophy-of-software-design/mode/2up&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A Philosophy Of Software Design&lt;/strong&gt;&lt;/a&gt;: This book by 
&lt;a href=&#34;https://web.stanford.edu/~ouster/cgi-bin/home.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Ousterhout&lt;/a&gt; is full of high-level concepts and tips to help tackle software complexity. It&amp;rsquo;s so good I had to buy a hard copy that now lives in my desk. 
&lt;a href=&#34;https://blog.pragmaticengineer.com/a-philosophy-of-software-design-review/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This post&lt;/a&gt; by 
&lt;a href=&#34;https://blog.pragmaticengineer.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gergely Orosz&lt;/a&gt; offers a balanced review of the book.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://youtu.be/CFRhGnuXG-4?si=7Xr3E9L7GFvoRJqA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Why You Shouldn&amp;rsquo;t Nest Your Code&lt;/strong&gt;&lt;/a&gt;: In this wonderful video, 
&lt;a href=&#34;https://www.youtube.com/@CodeAesthetic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CodeAesthetic&lt;/a&gt; explains in detail (and beautiful graphics!) a couple of methods to reduce the level of nesting in our code to improve readability and maintainability. This video has truly changed how I code in R!&lt;/p&gt;
&lt;h2 id=&#34;r&#34;&gt;R&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://ropensci.org/blog/2024/02/22/beautiful-code/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Beautiful Code, Because Weâre Worth It!&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://mastodon.social/@maelle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MaÃ«lle Salmon&lt;/a&gt; (research software engineer), and 
&lt;a href=&#34;https://fosstodon.org/@yabellini&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yanina Bellini Saibene&lt;/a&gt; (rOpenSci Community Manager) provides simple tips to help write more visually pleasant R code.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://journal.r-project.org/articles/RJ-2023-071/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Coloring in Râs Blind Spot&lt;/strong&gt;&lt;/a&gt;: This article published in 
&lt;a href=&#34;https://journal.r-project.org/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The R Journal&lt;/a&gt; by 
&lt;a href=&#34;https://www.zeileis.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Achim Zeileis&lt;/a&gt; (he has a 
&lt;a href=&#34;https://www.zeileis.org/blog/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;great analytics blog&lt;/a&gt; too!) and 
&lt;a href=&#34;https://www.stat.auckland.ac.nz/~paul/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paul Murrel&lt;/a&gt; offers a great overview of the base R color functions, and offers specific advice on what color palettes work better in different scenarios.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://peerj.com/preprints/26605v1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Taking R to its limits: 70+ tips&lt;/strong&gt;&lt;/a&gt;: This pre-print (not peer-reviewed AFAIK) by Tsagris and Papadakis offers a long list of tips to speed-up computation with the R language. I think a few of these tips lack enough context or are poorly explained, but it&amp;rsquo;s still a good resource to help optimize our R code.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.njtierney.com/post/2023/12/06/long-errors-smell/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Code Smell: Error Handling Eclipse&lt;/strong&gt; &lt;/a&gt;: This post by 
&lt;a href=&#34;https://fosstodon.org/@njtierney@aus.social&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nick Tierney&lt;/a&gt; explains how to address these situations when &lt;em&gt;error checking code totally eclipses the intent of the code&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.emilyriederer.com/post/team-of-packages/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Building a team of internal R packages&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://www.emilyriederer.com/about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emily Riederer&lt;/a&gt; delves into the particularities of building a team of R packages to do jobs helping a organization answer impactful questions.&lt;/p&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://archive.org/details/francois-chollet-deep-learning-with-python-manning-2018&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Deep Learning With Python&lt;/strong&gt;&lt;/a&gt;: This book by 
&lt;a href=&#34;https://fchollet.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FranÃ§ois Chollet&lt;/a&gt;, Software Engineer at Google and creator of the Keras library, seems to me like the best resource out there for those wanting to understand and build deep learning models from scratch. I have a hard copy on my desk, and I am finding it pretty easy to follow. Also, the code examples are clearly explained, and they ramp up in a very consistent manner.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.emilyriederer.com/post/py-rgo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Python Rgonomics&lt;/strong&gt;&lt;/a&gt;: In this post, 
&lt;a href=&#34;https://www.emilyriederer.com/about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emily Riederer&lt;/a&gt; offers a list of Python libraries with an &amp;ldquo;R feeling&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;coding-workflow&#34;&gt;Coding Workflow&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://graphite.dev/blog/stacked-prs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;How to use stacked PRs to unblock your entire team&lt;/strong&gt;&lt;/a&gt;: This post in 
&lt;a href=&#34;https://graphite.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graphite&lt;/a&gt;&amp;rsquo;s blog explains how to split large coding changes into small managed PRs (aka &amp;ldquo;stacked PRs&amp;rdquo;) to avoid blocks when PR reviews are hard to come by.&lt;/p&gt;
&lt;h1 id=&#34;other-fancy-things&#34;&gt;Other Fancy Things&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://vickiboykis.com/2024/01/15/whats-new-with-ml-in-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;What&amp;rsquo;s new with ML in production&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://vickiboykis.com/about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vicki Boykis&lt;/a&gt;, machine learning engineer at Mozilla.ai, goes deep into the differences and similarities between classical Machine Learning approaches and Large Language Models. I learned a lot from this read!&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://youtu.be/T-D1OfcDW1M?si=sAZO-5NGD8yF2WYe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;What is Retrieval-Augmented Generation (RAG)?&lt;/strong&gt;&lt;/a&gt;: In this video, Marina Danilevsky, Senior Data Scientist at IBM, offers a pretty good explanation on how the 
&lt;a href=&#34;https://research.ibm.com/blog/retrieval-augmented-generation-RAG?utm_id=YT-101-What-is-RAG&amp;amp;_gl=1*p6ef17*_ga*MTQwMzQ5NjMwMi4xNjkxNDE2MDc0*_ga_FYECCCS21D*MTY5MjcyMjgyNy40My4xLjE2OTI3MjMyMTcuMC4wLjA.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Retrieval-Augmented Generation&lt;/a&gt; method can improve the credibility of large language models.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.nature.com/articles/s41598-020-79148-7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A novel framework for spatio-temporal prediction of environmental data using deep learning&lt;/strong&gt;&lt;/a&gt;: This paper by 
&lt;a href=&#34;https://www.linkedin.com/in/federico-amato-66208637&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Federico Amato&lt;/a&gt; and collaborators describes an intriguing regression method combining a feedforward neural network with empirical orthogonal functions for spatio-temporal interpolation. Regrettably, the paper offers no code or data at all, but it&amp;rsquo;s still an interesting read.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2310.10196.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Large Models for Time Series and
Spatio-Temporal Data A Survey and Outlook&lt;/strong&gt;&lt;/a&gt;: This pre-print by Weng and collaborators reviews the current state of the art in spatio-temporal modelling with Large Language Models and Pre-Trained Foundation Models.&lt;/p&gt;
&lt;h1 id=&#34;management-and-leadership&#34;&gt;Management and Leadership&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://zaidesanton.substack.com/p/using-fake-deadlines-without-driving?publication_id=1804629&amp;amp;post_id=152010688&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Using fake deadlines without driving your engineers crazy&lt;/strong&gt;&lt;/a&gt;: In this post, 
&lt;a href=&#34;https://substack.com/@jstanier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;James Stanier&lt;/a&gt; explains how fake deadlines can help push projects forward in healthy work environments.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://zaidesanton.substack.com/p/when-engineering-managers-become&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;You are hurting your team without even noticing&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://substack.com/@zaidesanton&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anton Zaides&lt;/a&gt; (Development Team Leader), and 
&lt;a href=&#34;https://substack.com/@crushingtecheducation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eugene Shulga&lt;/a&gt;, (Software Engineer) offers insight on the harmful effects of a manager&amp;rsquo;s ego in their team dynamics.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://the.managers.guide/p/teamwork-habits-for-leaders&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Teamwork Habits for Leaders&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://substack.com/@ochronus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Csaba Okrona&lt;/a&gt; focuses on how shifting from talker to listener in team meetings offers a good insight to better address the team&amp;rsquo;s needs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multicollinearity Hinders Model Interpretability</title>
      <link>https://blasbenito.com/post/multicollinearity-model-interpretability/</link>
      <pubDate>Sun, 29 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/multicollinearity-model-interpretability/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This post is written for beginner to intermediate R users wishing to learn what multicollinearity is and how it can turn model interpretation into a challenge.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post, I delve into the intricacies of model interpretation under the influence of multicollinearity, and use R and a toy data set to demonstrate how this phenomenon impacts both linear and machine learning models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The section &lt;em&gt;Multicollinearity Explained&lt;/em&gt; explains the origin of the word and the nature of the problem.&lt;/li&gt;
&lt;li&gt;The section &lt;em&gt;Model Interpretation Challenges&lt;/em&gt; describes how to create the toy data set, and applies it to &lt;em&gt;Linear Models&lt;/em&gt; and &lt;em&gt;Random Forest&lt;/em&gt; to explain how multicollinearity can make model interpretation a challenge.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;Appendix&lt;/em&gt; shows extra examples of linear and machine learning models affected by multicollinearity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I hope you&amp;rsquo;ll enjoy it!&lt;/p&gt;
&lt;h1 id=&#34;r-packages&#34;&gt;R packages&lt;/h1&gt;
&lt;p&gt;This tutorial requires the newly released R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;, and a few more listed below. The optional ones are used only in the &lt;em&gt;Appendix&lt;/em&gt; at the end of the post.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#required
install.packages(&amp;quot;collinear&amp;quot;)
install.packages(&amp;quot;ranger&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)

#optional
install.packages(&amp;quot;nlme&amp;quot;)
install.packages(&amp;quot;glmnet&amp;quot;)
install.packages(&amp;quot;xgboost&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;multicollinearity-explained&#34;&gt;Multicollinearity Explained&lt;/h1&gt;
&lt;p&gt;This cute word comes from the amalgamation of these three Latin terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;multus&lt;/em&gt;: adjective meaning &lt;em&gt;many&lt;/em&gt; or &lt;em&gt;multiple&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;con&lt;/em&gt;: preposition often converted to &lt;em&gt;co-&lt;/em&gt; (as in &lt;em&gt;co-worker&lt;/em&gt;) meaning &lt;em&gt;together&lt;/em&gt; or &lt;em&gt;mutually&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;linealis&lt;/em&gt; (later converted to &lt;em&gt;linearis&lt;/em&gt;): from &lt;em&gt;linea&lt;/em&gt; (line), adjective meaning &amp;ldquo;resembling a line&amp;rdquo; or &amp;ldquo;belonging to a line&amp;rdquo;, among others.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After looking at these serious words, we can come up with a (VERY) liberal translation: &amp;ldquo;several things together in the same line&amp;rdquo;. From here, we just have to replace the word &amp;ldquo;things&amp;rdquo; with &amp;ldquo;predictors&amp;rdquo; (or &amp;ldquo;features&amp;rdquo;, or &amp;ldquo;independent variables&amp;rdquo;, whatever rocks your boat) to build an intuition of the whole meaning of the word in the context of statistical and machine learning modeling.&lt;/p&gt;
&lt;p&gt;If I lost you there, we can move forward with this idea instead: &lt;strong&gt;multicollinearity happens when there are redundant predictors in a modeling dataset&lt;/strong&gt;. A predictor can be redundant because it shows a high pairwise correlation with other predictors, or because it is a linear combination of other predictors. For example, in a data frame with the columns &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt;, if the correlation between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; is high, we can say that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are mutually redundant and there is multicollinearity. But also, if &lt;code&gt;c&lt;/code&gt; is the result of a linear operation between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, like &lt;code&gt;c &amp;lt;- a + b&lt;/code&gt;, or &lt;code&gt;c &amp;lt;- a * 1 + b * 0.5&lt;/code&gt;, then we can also say that there is multicollinearity between &lt;code&gt;c&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt;, and &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Multicollinearity is a fact of life that lurks in most data sets. For example, in climate data, variables like temperature, humidity and air pressure are closely intertwined, leading to multicollinearity. That&amp;rsquo;s the case as well in medical research, where parameters like blood pressure, heart rate, and body mass index frequently display common patterns. Economic analysis is another good example, as variables such as Gross Domestic Product (GDP), unemployment rate, and consumer spending often exhibit multicollinearity.&lt;/p&gt;
&lt;h1 id=&#34;model-interpretation-challenges&#34;&gt;Model Interpretation Challenges&lt;/h1&gt;
&lt;p&gt;Multicollinearity isn&amp;rsquo;t inherently problematic, but it can be a real buzz kill when the goal is interpreting predictor importance in explanatory models. In the presence of highly correlated predictors, most modelling methods, from the veteran linear models to the fancy gradient boosting, attribute a large part of the importance to only one of the predictors and not the others. In such cases, neglecting multicollinearity will certainly lead to underestimate the relevance of certain predictors.&lt;/p&gt;
&lt;p&gt;Let me go ahead and develop a toy data set to showcase this issue. But let&amp;rsquo;s load the required libraries first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#load the collinear package and its example data
library(collinear)
data(vi)

#other required libraries
library(ranger)
library(dplyr)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;vi&lt;/code&gt; data frame shipped with the 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt; package, the variables &amp;ldquo;soil_clay&amp;rdquo; and &amp;ldquo;humidity_range&amp;rdquo; are not correlated at all (Pearson correlation = -0.06).&lt;/p&gt;
&lt;p&gt;In the code block below, the &lt;code&gt;dplyr::transmute()&lt;/code&gt; command selects and renames them as &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. After that, the two variables are scaled and centered, and &lt;code&gt;dplyr::mutate()&lt;/code&gt; generates a few new columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: response variable resulting from a linear model where &lt;code&gt;a&lt;/code&gt; has a slope of 0.75, &lt;code&gt;b&lt;/code&gt; has a slope of 0.25, plus a bit of white noise generated with &lt;code&gt;runif()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: a new predictor highly correlated with &lt;code&gt;a&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d&lt;/code&gt;: a new predictor resulting from a linear combination of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(1)
df &amp;lt;- vi |&amp;gt;
  dplyr::slice_sample(n = 2000) |&amp;gt;
  dplyr::transmute(
    a = soil_clay,
    b = humidity_range
  ) |&amp;gt;
  scale() |&amp;gt;
  as.data.frame() |&amp;gt; 
  dplyr::mutate(
    y = a * 0.75 + b * 0.25 + runif(n = dplyr::n(), min = -0.5, max = 0.5),
    c = a + runif(n = dplyr::n(), min = -0.5, max = 0.5),
    d = (a + b)/2 + runif(n = dplyr::n(), min = -0.5, max = 0.5)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Pearson correlation between all pairs of these predictors is shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::cor_df(
  df = df,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 Ã 3
##   x     y     correlation
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 c     a           0.962
## 2 d     b           0.639
## 3 d     a           0.636
## 4 d     c           0.615
## 5 b     a          -0.047
## 6 c     b          -0.042
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, we have are two groups of predictors useful to understand how multicollinearity muddles model interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictors with &lt;strong&gt;no&lt;/strong&gt; multicollinearity: &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Predictors with multicollinearity: &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the next two sections and the &lt;em&gt;Appendix&lt;/em&gt;, I show how and why model interpretation becomes challenging when multicollinearity is high. Let&amp;rsquo;s start with linear models.&lt;/p&gt;
&lt;h3 id=&#34;linear-models&#34;&gt;Linear Models&lt;/h3&gt;
&lt;p&gt;The code below fits &lt;em&gt;multiple linear regression models&lt;/em&gt; for both groups of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#non-collinear predictors
lm_ab &amp;lt;- lm(
  formula = y ~ a + b,
  data = df
  )

#collinear predictors
lm_abcd &amp;lt;- lm(
  formula = y ~ a + b + c + d,
  data = df
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I would like you to pay attention to the estimates of the predictors &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; for both models. The estimates are the slopes in the linear model, a direct indication of the effect of a predictor over the response.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coefficients(lm_ab)[2:3] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coefficients(lm_abcd)[2:5] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On one hand, the model with no multicollinearity (&lt;code&gt;lm_ab&lt;/code&gt;) achieved a pretty good solution for the coefficients of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. Remember that we created &lt;code&gt;y&lt;/code&gt; as &lt;code&gt;a * 0.75 + b * 0.25&lt;/code&gt; plus some noise, and that&amp;rsquo;s exactly what the model is telling us here, so the interpretation is pretty straightforward.&lt;/p&gt;
&lt;p&gt;On the other hand, the model with multicollinearity (&lt;code&gt;lm_abcd&lt;/code&gt;) did well with &lt;code&gt;b&lt;/code&gt;, but there are a few things in there that make the interpretation harder.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The coefficient of &lt;code&gt;a&lt;/code&gt; (0.7165) is slightly smaller than the true one (0.75), which could lead us to downplay its relationship with &lt;code&gt;y&lt;/code&gt; by a tiny bit. This is kinda OK though, as long as one is not using the model&amp;rsquo;s results to build nukes in the basement.&lt;/li&gt;
&lt;li&gt;The coefficient of &lt;code&gt;c&lt;/code&gt; is so small that it could led us to believe that this predictor not important at all to explain &lt;code&gt;y&lt;/code&gt;. But we know that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; are almost identical copies, so model interpretation here is being definitely muddled by multicollinearity.&lt;/li&gt;
&lt;li&gt;The coefficient of &lt;code&gt;d&lt;/code&gt; is tiny. Since &lt;code&gt;d&lt;/code&gt; results from the sum of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, we could expect this predictor to be important in explaining &lt;code&gt;y&lt;/code&gt;, but it got the shorter end of the stick in this case.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is not that the model it&amp;rsquo;s wrong though. This behavior of the linear model results from the &lt;em&gt;QR decomposition&lt;/em&gt; (also &lt;em&gt;QR factorization&lt;/em&gt;) applied by functions like &lt;code&gt;lm()&lt;/code&gt;, &lt;code&gt;glm()&lt;/code&gt;, &lt;code&gt;glmnet::glmnet()&lt;/code&gt;, and &lt;code&gt;nlme::gls()&lt;/code&gt; to improve numerical stability and computational efficiency, and to&amp;hellip; address multicollinearity in the model predictors.&lt;/p&gt;
&lt;p&gt;The QR decomposition transforms the original predictors into a set of orthogonal predictors with no multicollinearity. This is the &lt;em&gt;Q matrix&lt;/em&gt;, created in a fashion that resembles the way in which a Principal Components Analysis generates uncorrelated components from a set of correlated variables.&lt;/p&gt;
&lt;p&gt;The code below applies QR decomposition to our multicollinear predictors, extracts the Q matrix, and shows the correlation between the new versions of &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#predictors names
predictors &amp;lt;- c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)

#QR decomposition of predictors
df.qr &amp;lt;- qr(df[, predictors])

#extract Q matrix
df.q &amp;lt;- qr.Q(df.qr)
colnames(df.q) &amp;lt;- predictors

#correlation between transformed predictors
collinear::cor_df(df = df.q)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 Ã 3
##   x     y     correlation
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 d     c               0
## 2 c     b               0
## 3 d     b               0
## 4 d     a               0
## 5 c     a               0
## 6 b     a               0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new set of predictors we are left with after the QR decomposition have exactly zero correlation! And now they are not our original predictors anymore, and have a different interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt; is now &amp;ldquo;the part of &lt;code&gt;a&lt;/code&gt; not in &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt; is now &amp;ldquo;the part of &lt;code&gt;b&lt;/code&gt; not in &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&amp;hellip;and so on&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The result of the QR decomposition can be plugged into the &lt;code&gt;solve()&lt;/code&gt; function along with the response vector to estimate the coefficients of the linear model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solve(a = df.qr, b = df$y) |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7189 0.2595 0.0268 0.0040
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are almost exactly the ones we got for our model with multicollinearity. In the end, the coefficients resulting from a linear model are not those of the original predictors, but the ones of their uncorrelated versions generated by the QR decomposition.&lt;/p&gt;
&lt;p&gt;But this is not the only issue of model interpretability under multicollinearity. Let&amp;rsquo;s take a look at the standard errors of the estimates. These are a measure of the coefficient estimation uncertainty, and are used to compute the p-values of the estimates. As such, they are directly linked with the &amp;ldquo;statistical significance&amp;rdquo; (whatever that means) of the predictors within the model.&lt;/p&gt;
&lt;p&gt;The code below shows the standard errors of the model without and with multicollinearity.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(lm_ab)$coefficients[, &amp;quot;Std. Error&amp;quot;][2:3] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.0066 0.0066
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(lm_abcd)$coefficients[, &amp;quot;Std. Error&amp;quot;][2:5] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.0267 0.0134 0.0232 0.0230
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These standard errors of the model with multicollinearity are an order of magnitude higher than the ones of the model without multicollinearity.&lt;/p&gt;
&lt;p&gt;Since our toy dataset is relatively large (2000 cases) and the relationship between the response and a few of the predictors pretty robust, there are no real issues arising, as these differences in estimation precision are not enough to change the p-values of the estimates. However, in a small data set with high multicollinearity and a weaker relationship between the response and the predictors, standard errors of the estimate become wide, which increases p-values and reduces &amp;ldquo;significance&amp;rdquo;. Such a situation might lead us to believe that a predictor does not explain the response, when in fact it does. And this, again, is a model interpretability issue caused by multicollinearity.&lt;/p&gt;
&lt;p&gt;At the end of this post there is an appendix with code examples of other types of linear models that use QR decomposition and become challenging to interpret in the presence of multicollinearity. Play with them as you please!&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s take a look at how multicollinearity can also mess up the interpretation of a commonly used machine learning algorithm.&lt;/p&gt;
&lt;h3 id=&#34;random-forest&#34;&gt;Random Forest&lt;/h3&gt;
&lt;p&gt;It is not uncommon to hear something like &amp;ldquo;random forest is insensitive to multicollinearity&amp;rdquo;. Actually, I cannot confirm nor deny that I have said that before. Anyway, it is kind of true if one is focused on prediction problmes. However, when the aim is interpreting predictor importance scores, then one has to be mindful about multicollinearity as well.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see an example. The code below fits two random forest models with our two sets of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#non-collinear predictors
rf_ab &amp;lt;- ranger::ranger(
  formula = y ~ a + b,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1 #for reproducibility
)

#collinear predictors
rf_abcd &amp;lt;- ranger::ranger(
  formula = y ~ a + b + c + d,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at the prediction error the two models on the out-of-bag data. While building each regression tree, Random Forest leaves a random subset of the data out. Then, each case gets a prediction from all trees that had it in the out-of-bag data, and the prediction error is averaged across all cases to get the numbers below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_ab$prediction.error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1026779
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abcd$prediction.error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1035678
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to these numbers, these two models are basically equivalent in their ability to predict our response &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But now, you noticed that I set the argument &lt;code&gt;importance&lt;/code&gt; to &amp;ldquo;permutation&amp;rdquo;. Permutation importance quantifies how the out-of-bag error increases when a predictor is permuted across all trees where the predictor is used. It is pretty robust importance metric that bears no resemblance whatsoever with the coefficients of a linear model. Think of it as a very different way to answer the question &amp;ldquo;what variables are important in this model?&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The permutation importance scores of the two random forest models are show below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_ab$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 1.0702 0.1322
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abcd$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.5019 0.0561 0.1662 0.0815
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is one interesting detail here. The predictor &lt;code&gt;a&lt;/code&gt; has a permutation error three times higher than &lt;code&gt;c&lt;/code&gt; in the second model, even though we could expect them to be similar due to their very high correlation. There are two reasons for this mismatch:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Random Forest is much more sensitive to the white noise in &lt;code&gt;c&lt;/code&gt; than linear models, especially in the deep parts of the regression trees, due to local (within-split data) decoupling with the response &lt;code&gt;y&lt;/code&gt;. In consequence, it does not get selected as often as &lt;code&gt;a&lt;/code&gt; in these deeper areas of the trees, and has less overall importance.&lt;/li&gt;
&lt;li&gt;The predictor &lt;code&gt;c&lt;/code&gt; competes with &lt;code&gt;d&lt;/code&gt;, that has around 50% of the information in &lt;code&gt;c&lt;/code&gt; (and &lt;code&gt;a&lt;/code&gt;). If we remove &lt;code&gt;d&lt;/code&gt; from the model, then the permutation importance of &lt;code&gt;c&lt;/code&gt; doubles up. Then, with &lt;code&gt;d&lt;/code&gt; in the model, we underestimate the real importance of &lt;code&gt;c&lt;/code&gt; due to multicollinearity alone.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abc &amp;lt;- ranger::ranger(
  formula = y ~ a + b + c,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1
)
rf_abc$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c 
## 0.5037 0.1234 0.3133
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With all that in mind, we can conclude that interpreting importance scores in Random Forest models is challenging when multicollinearity is high. But Random Forest is not the only machine learning affected by this issue. In the Appendix below I have left an example with Extreme Gradient Boosting so you can play with it.&lt;/p&gt;
&lt;p&gt;And that&amp;rsquo;s all for now, folks, I hope you found this post useful!&lt;/p&gt;
&lt;h1 id=&#34;appendix&#34;&gt;Appendix&lt;/h1&gt;
&lt;p&gt;This section shows several extra examples of linear and machine learning models you can play with.&lt;/p&gt;
&lt;h2 id=&#34;other-linear-models-using-qr-decomposition&#34;&gt;Other linear models using QR decomposition&lt;/h2&gt;
&lt;p&gt;As I commented above, many linear modeling functions use QR decomposition, and you will have to be careful interpreting model coefficients in the presence of strong multicollinearity in the predictors.&lt;/p&gt;
&lt;p&gt;Here I show several examples with &lt;code&gt;glm()&lt;/code&gt; (Generalized Linear Models), &lt;code&gt;nlme::gls()&lt;/code&gt; (Generalized Least Squares), and &lt;code&gt;glmnet::cv.glmnet()&lt;/code&gt; (Elastic Net Regularization). In all them, no matter how fancy, the interpretation of coefficients becomes tricky when multicollinearity is high.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generalized Linear Models with glm()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Generalized Linear Models
#non-collinear predictors
glm_ab &amp;lt;- glm(
  formula = y ~ a + b,
  data = df
  )

round(coefficients(glm_ab), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
glm_abcd &amp;lt;- glm(
  formula = y ~ a + b + c + d,
  data = df
  )

round(coefficients(glm_abcd), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Generalized Least Squares with nlme::gls()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(nlme)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;nlme&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:dplyr&#39;:
## 
##     collapse
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Generalized Least Squares
#non-collinear predictors
gls_ab &amp;lt;- nlme::gls(
  model = y ~ a + b,
  data = df
  )

round(coefficients(gls_ab), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
gls_abcd &amp;lt;- nlme::gls(
  model = y ~ a + b + c + d,
  data = df
  )

round(coefficients(gls_abcd), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Elastic Net Regularization and Lasso penalty with glmnet::glmnet()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(glmnet)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loaded glmnet 4.1-8
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Elastic net regularization with Lasso penalty
#non-collinear predictors
glmnet_ab &amp;lt;- glmnet::cv.glmnet(
  x = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]),
  y = df$y,
  alpha = 1 #lasso penalty
)

round(coef(glmnet_ab$glmnet.fit, s = glmnet_ab$lambda.min), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7438 0.2578
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
glmnet_abcd &amp;lt;- glmnet::cv.glmnet(
  x = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  y = df$y,
  alpha = 1 
)

#notice that the lasso regularization nuked the coefficients of predictors b and c
round(coef(glmnet_abcd$glmnet.fit, s = glmnet_abcd$lambda.min), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7101 0.2507 0.0267 0.0149
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;extreme-gradient-boosting-under-multicollinearity&#34;&gt;Extreme Gradient Boosting under multicollinearity&lt;/h2&gt;
&lt;p&gt;Gradient Boosting models trained with multicollinear predictors behave in a way similar to linear models with QR decomposition. When two variables are highly correlated, one of them is going to have an importance much higher than the other.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(xgboost)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;xgboost&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:dplyr&#39;:
## 
##     slice
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#without multicollinearity
gb_ab &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
  )

#with multicollinearity
gb_abcd &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xgb.importance(model = gb_ab)[, c(1:2)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature      Gain
## 1:       a 0.8463005
## 2:       b 0.1536995
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xgb.importance(model = gb_abcd)[, c(1:2)] |&amp;gt; 
  dplyr::arrange(Feature)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature       Gain
## 1:       a 0.78129661
## 2:       b 0.07386393
## 3:       c 0.03595619
## 4:       d 0.10888327
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But there is a twist too. When two variables are perfectly correlated, one of them is removed right away from the model!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#replace c with perfect copy of a
df$c &amp;lt;- df$a

#with multicollinearity
gb_abcd &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
)

xgb.importance(model = gb_abcd)[, c(1:2)] |&amp;gt; 
  dplyr::arrange(Feature)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature       Gain
## 1:       a 0.79469959
## 2:       b 0.07857141
## 3:       d 0.12672900
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
