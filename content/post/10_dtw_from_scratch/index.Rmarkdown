---
title: "Coding a Minimalistic Dynamic Time Warping Library with R"
author: ''
date: '2025-01-13'
slug: dynamic-time-warping-from-scratch
categories: []
tags: [Rstats]
subtitle: ''
summary: 'Tutorial on how to implement dynamic time warping in R'
authors: [admin]
lastmod: '2025-01-13T05:14:20+01:00'
featured: yes
draft: true
image:
  caption: Dynamic time warping represented as a landscape.
  focal_point: Smart
  margin: auto
projects: []
---

```{r, echo = FALSE, eval = FALSE}
install.packages("distantia")
install.packages("gridExtra")
options(max.print = 100)
knitr::opts_chunk$set(
  fig.width = 7, fig.height = 6
)
```


# Summary

This post walks you through the implementation of a minimalistic yet fully functional [Dynamic Time Warping](https://www.blasbenito.com/post/dynamic-time-warping/) (DTW) library in R, built entirely from scratch without dependencies or complex abstractions. While there are many [open-source DTW implementations](https://blasbenito.github.io/distantia/articles/dtw_applications.html) readily available, understanding the inner workings of the algorithm can be invaluable. Whether youâ€™re simply curious or need a deeper grasp of DTW for your projects, this step-by-step guide offers a hands-on approach to demystify the method.

# Design

## Example Data

Having good example data at hand is a must when developing new code. For this tutorial we use a subset of two multivariate time series of temperature, rainfall, and normalized vegetation index. To facilitate our development, the time series, named `zoo_spain` and `zoo_sweden`, are stored as objects of the class [zoo](https://CRAN.R-project.org/package=zoo), which is a very robust time series management library.

```{r, echo = FALSE, message=FALSE}
library(distantia, quietly = TRUE)
library(zoo, quietly = TRUE)

tsl <- tsl_initialize(
  x = fagus_dynamics,
  name_column = "name",
  time_column = "time"
) |> 
  tsl_subset(
    names = c("Spain", "Sweden"),
    time = c("2010-01-01", "2011-01-01")
  ) |> 
  tsl_colnames_set(
    names = c("evi", "rain", "temp")
  )

zoo_sweden <- tsl$Sweden
zoo_spain <- tsl$Spain
```

```{r, echo = FALSE, fig.height=3.5}
graphics::layout(matrix(1:2, ncol = 2))

plot(
  x = zoo_sweden, 
  col = "red4",
  mar = c(0.5, 5, 0, 5)
  )

plot(
  x = zoo_spain, 
  col = "red4",
  mar = c(0.5, 5, 0, 5)
  )

```

Each zoo object has a *core data* of the class `matrix` with one observation per row and one variable per column, and an *index*, which is a vector of dates, one per row in the core data.

```{r}
zoo::coredata(zoo_sweden)
zoo::index(zoo_sweden)
```


## Required Library Functions

The section *DTW Step by Step* from the previous article [A Gentle Intro to Dynamic Time Warping](https://www.blasbenito.com/post/dynamic-time-warping/) describes the computational steps required by the algorithm. Below, these steps are broken down into sub-steps that will correspond to specific functions in our library:

**Time Series Pre-processing** 

These steps help DTW work seamlessly with the input time series.

  - **Linear detrending**: forces time series to be stationary by removing any upwards or downwards trends. 
  - **Z-score normalization**: equalizes the range of the time series to ensure that the different variables contribute evenly to the distance computation.

**Dynamic Time Warping*

These steps perform dynamic time warping and evaluate the similarity between the time series.

  - **Multivariate distance**: compute distances between pairs of samples from each time series.
  - **Distance matrix**: organize the multivariate distances in a matrix in which each axis represents a time series.
  - **Cost matrix**: this matrix accumulates the distances in the distance matrix across time and represents all possible alignments between two time series.
  - **Least-cost path**: path in the cost matrix that minimizes the overall distance between two time series.
  - **Dissimilarity metric**: value to summarize the similarity/dissimilarity between time series.
  
**Main function**

Once all the steps above are implemented in their respective functions, we will wrap all them in a single function to streamline the DTW analysis.
  
# Implementation

In this section we will be developing the library function by function. Remember that the content of each code chunk should be added to the file `mini_dtw.R`.

## Time Series Pre-processing

In this section we create the function `ts_preprocessing()`, which will prepare the time series data for dynamic time warping. This function contains two functionalities: linear detrending, and z-score normalization.
    
### Linear Detrending

Applying linear detrending to a multivariate time series involves computing a linear model of each variable against time, and subtracting the the model prediction to the original data. This operation only requires two steps: 

First, the function `stats::lm()` can be applied to all variables in one of our time series at once

```{r}
model_sweden <- stats::lm(
  formula = zoo_sweden ~ stats::time(zoo_sweden)
  )

model_sweden
```

Second, the residuals of the linear model, which represent the differences between the prediction and the observed data, correspond exactly with the detrended time series. As a plus, these residuals are returned as a zoo object when the `zoo` library is loaded.

```{r}
stats::residuals(model_sweden)
```

```{r, echo = FALSE, fig.height=3.5}
plot(
  x = stats::residuals(model_sweden), 
  col = "red4",
  mar = c(0.5, 5, 0, 5),
  main = "Detrended zoo_sweden"
  )
```
Then, the first function of our library could be something like this:

```{r}
#' Linear Detrending
#' @param x (required, zoo object) time series to detrend.
#' @return zoo object
ts_preprocessing <- function(x){
  m <- stats::lm(formula = x ~ stats::time(x))
  y <- stats::residuals(object = m)
  y
}
```

This function could be more concise, but it is written to facilitate line-by-line debugging instead. I have also added minimal roxygen documentation. Future me usually appreciates this kind of extra effort.

This function should check that `x` is really a zoo object, and any other condition that would make it fail. However, to keep code simple, in this tutorial we won't do any error catching.

Ok, we can now test the new function:

```{r}
ts_preprocessing(x = zoo_spain)
```

```{r, echo = FALSE, fig.height=3.5}
plot(
  x = ts_preprocessing(x = zoo_spain), 
  col = "red4",
  mar = c(0.5, 5, 0, 5),
  main = "Detrended zoo_spain"
  )
```
The effect of our new function is not very noticeable because none of our time series have long-term trends, but let's try something different to check that our function actually detrends time series. The code below creates a mock-up time series with an ascending trend.

```{r}
x <- zoo::zoo(0:10)
```

```{r, fig.height=2.5, echo = FALSE}
plot(
  x = x, 
  col = "red4",
  mar = c(0.5, 1, 0, 1)
  )
```

If we apply `ts_preprocessing()` to this time series, the result shows a horizontal line, which is a perfect linear detrending. Now we can be sure our implementation works!

```{r, fig.height=3}
x_detrended <- ts_preprocessing(x = x)
```

```{r, fig.height=2.5, echo = FALSE}
plot(
  x = x_detrended, 
  col = "red4",
  ylim = range(x),
  mar = c(0.5, 5, 0, 5)
  )
```


### Z-score Normalization

Normalization consists of two operations: 

  - **Centering**: performed by subtracting the column mean to each case, and results in a column mean equal to zero. 
  - **Scaling**: divides each case by the standard deviation of the column, resulting in a column standard deviation equal to one.

The R base function `scale()` implements z-score normalization, so there's not much we have to do from scratch here. Also, when the library `zoo` is loaded, the method `zoo:::scale.zoo` (`:::` denotes methods and functions that are not exported) allows `scale()` to work seamlessly with zoo objects.

```{r}
scale(
  x = zoo_spain,
  center = TRUE,
  scale = TRUE
  )
```

Normalization can be easily added to `ts_preprocessing()`:

```{r}
#' Linear Detrending and Normalization
#' @param x (required, zoo object) time series to detrend.
#' @return zoo object
ts_preprocessing <- function(x){
  m <- stats::lm(formula = x ~ stats::time(x))
  y <- stats::residuals(object = m)
  z <- scale(y)
  z
}
```

If you like pipes, a slightly more concise version of the same function is shown below. Both produce exaclty the same results, so it is just a matter of preference here.

```{r, eval = FALSE}
ts_preprocessing <- function(x){
  y <- stats::lm(formula = x ~ stats::time(x)) |> 
    stats::residuals() |> 
    scale()
  y
}
```

You might find that using the intermediate object `y` is kinda silly, but being explicit about what the function should return might prevent a headache later on.

Anyway, we are ready to test `ts_preprocessing()` and move forward with our implementation

```{r}
ts_preprocessing(x = zoo_spain)
```

```{r, echo = FALSE, fig.height=3.5}
plot(
  x = ts_preprocessing(x = zoo_spain), 
  col = "red4",
  mar = c(0.5, 5, 0, 5)
  )
```

## Dynamic Time Warping Functions

This section describes the implementation of the DTW algorithm, which requires functions to compute a distance matrix, convert it to a cost matrix, find a least-cost path maximizing the alignment between the time series, and compute their similarity.

### Distance Matrix

In DTW, a distance matrix represents all the distances between all pairs of samples in two time series. Hence, each time series is represented in one axis of the matrix. But before getting there, we need a function to obtain the distance between arbitrary pairs of rows from two separate zoo objects.

#### Distance Function

Let's say we have two vectors, `x` with one row of `zoo_spain`, and `y` with one row of `zoo_sweden`. Then, the expression to compute the Euclidean distances `x` and `y` is `sqrt(sum((x-y)^2))`. From there, implementing the distance function seems pretty trivial.

```{r}
#' Euclidean Distance
#' @param x (required, numeric) row of a zoo object.  
#' @param y (required, numeric) row of a zoo object.
#' @return numeric
d_euclidean <- function(x, y){
  sqrt(sum((x - y)^2))
}
```

Notice that the function does not indicate the return explicitly. Since the function's body is a one-liner, one cannot be really worried about the function returning something unexpected. Also, implementing such a simple expression in a function might seem like too much, but it may facilitate the addition of new distance metrics to the library in the future. For example, we could create something like `d_manhattan()` with the Manhattan distance, and later switch between one or another depending on the user's needs.

The code below tests the function by computing the euclidean distance between the row 1 from `zoo_sweden` and the row 2 from `zoo_spain`.

```{r}
zoo_sweden[1, ]
zoo_spain[2, ]

d_euclidean(
  x = zoo_sweden[1, ],
  y = zoo_spain[2, ]
)
```

What? That doesn't seem right! For whatever reason, `zoo_sweden[1, ]` and `zoo_spain[2, ]` are not being interpreted as numeric vectors by `d_euclidean()`. Let's try something different:

```{r}
d_euclidean(
  x = as.numeric(zoo_sweden[1, ]),
  y = as.numeric(zoo_spain[2, ])
)
```
Ok, that makes more sense! Then, we just have to move these `as.numeric()` inside `d_euclidean()` to simplify the usage of the function:

```{r}
#' Euclidean Distance
#' @param x (required, numeric) row of a zoo object.  
#' @param y (required, numeric) row of a zoo object.
#' @return numeric
d_euclidean <- function(x, y){
  x <- as.numeric(x)
  y <- as.numeric(y)
  z <- sqrt(sum((x - y)^2))
  z
}
```

The new function should have no issues returning the right distance between these rows now:

```{r}
d_euclidean(
  x = zoo_sweden[1, ],
  y = zoo_spain[2, ]
)
```

That was kinda bumpy, but we can move on and go compute the distance matrix now.

#### Distance Matrix

To generate the distance matrix, the function `d_euclidean()` must be applied to all pairs of rows in the two zoo objects. A simple yet inefficient way to do this involves creating an empty matrix, and traversing it cell by cell to compute the euclidean distances between the corresponding pair of rows.

```{r}
#empty distance matrix
m_dist <- matrix(
  data = NA, 
  nrow = nrow(zoo_spain), 
  ncol = nrow(zoo_sweden)
)

#iterate over rows
for(row in 1:nrow(zoo_spain)){
  
  #iterate over columns
  for(col in 1:nrow(zoo_sweden)){
    
    #distance between time series rows
    m_dist[row, col] <- d_euclidean(
      x = zoo_spain[row, ],
      y = zoo_sweden[col, ]
    )
    
  }
}
```

This code generates a matrix with `zoo_spain` in the rows, from top to bottom, and `zoo_sweden` in the columns, from left to right. The first five rows and columns are shown below.

```{r}
m_dist[1:5, 1:5]
```

This matrix can be plotted with the function `graphics::image()`, but please be aware that it rotates the distance matrix 90 degrees counter clock-wise, which can be pretty confusing at first. Remember this: **in the matrix plot, the x axis represents the matrix rows**.

```{r}
graphics::image(
  x = seq_len(ncol(m_dist)),
  y = seq_len(nrow(m_dist)),
  z = m_dist,
  xlab = "zoo_spain",
  ylab = "zoo_sweden",
  main = "Euclidean Distance"
  )
```

Darker values in the plot above indicate larger distances between pairs of samples in each time series.

We can now wrap the code above (without the plot) in a new function named `distance_matrix()`.

```{r}
#' Distance Matrix Between Time Series
#' @param a (required, zoo object) time series.
#' @param b (required, zoo object) time series with same columns as `x`
#' @return matrix
distance_matrix <- function(a, b){
  
  m <- matrix(
    data = NA, 
    nrow = nrow(b), 
    ncol = nrow(a)
  )
  
  for(row in 1:nrow(b)){
    for(col in 1:nrow(a)){
      
      m[row, col] <- d_euclidean(
        x = a[row, ],
        y = b[col, ]
      )
      
    }
  }
  
  m
  
}
```

Let's run a little test before moving forward!

```{r}
m_dist <- distance_matrix(
  a = zoo_spain,
  b = zoo_sweden
)

m_dist[1:5, 1:5]
```

We are good to go! The next function will transform this distance matrix into a *cost matrix*.

### Cost Matrix

Now we are getting into the important parts of the DTW algorithm! 

A cost matrix is like a valley's landscape, with hills in regions where the time series are different, and ravines where they are more similar. Such landscape is built by accumulating the values of the distance matrix cell by cell, from `[1, 1]` at the bottom of the valley (upper left corner of the matrix, but lower left in the plot), to `[m, n]` at the top (lower right corner of the matrix, upper right in the plot).

Let's see how that works.

First, we use the dimensions of the distance matrix to create an empty cost matrix.

```{r, warning = FALSE}
m_cost <- matrix(
  data = NA, 
  nrow = nrow(m_dist), 
  ncol = ncol(m_dist)
  )
```

Second, to initialize the cost matrix we accumulate the values of the first row and the first column of the distance matrix using `cumsum()`. This step is very important for the second part of the algorithm, as it provides the starting values.

```{r}
m_cost[1, ] <- cumsum(m_dist[1, ])
m_cost[, 1] <- cumsum(m_dist[, 1])
```


```{r, echo = FALSE}
graphics::image(
  x = seq_len(ncol(m_cost)),
  y = seq_len(nrow(m_cost)),
  z = m_cost,
  xlab = "zoo_spain",
  ylab = "zoo_sweden",
  main = "Cost Matrix (work in progress)"
  )
```

Now, before going into the third step, let's focus for a moment on the next cell of the cost matrix we need to fill, with coordinates `[2, 2]` and value `NA`.

```{r}
m_cost[1:2, 1:2]
```

The new value of this cell results from the addition of:

  - Its value in the distance matrix `m_dist` (`r round(m_dist[2, 2], 2)`).
  - The minimum accumulated distance of its neighbors, which are:
      - Upper neighbor with coordinates `[1, 2]`.
      - Left neighbor with coordinates `[2, 1]`. 
      
NOTE: DTW can also consider diagonal neighborhood, but in this tutorial we only focus on orthogonal moves to keep the code as simple as possible.

The general expression to find the value of the empty cell is shown below. It uses `min()` to get the value of the *smallest* neighbor, and then adds it to the vaue of the target cell in the distance matrix.

```{r}
m_cost[2, 2] <- min(
  m_cost[1, 2], 
  m_cost[2, 1]
  ) + m_dist[2, 2]

m_cost[1:2, 1:2]
```

But there are many cells to fill yet!

```{r, echo = FALSE}
graphics::image(
  x = seq_len(ncol(m_cost)),
  y = seq_len(nrow(m_cost)),
  z = m_cost,
  xlab = "zoo_spain",
  ylab = "zoo_sweden",
  main = "Cost Matrix (work in progress)"
  )
```

The expression we used to fill the cell `m_cost[2, 2]` can be generalized to fill all remaining empty cells. We just have to wrap it in a nested loop that for each new empty cell identifies the smallest neighbor in the x and y axies, and adds its cumulative cost to the distance of new cell.

```{r}
#iterate over rows of the cost matrix
for(row.i in 2:nrow(m_dist)){
  
  #iterate over columns of the cost matrix
  for(col.j in 2:ncol(m_dist)){
    
    #get cost of neighbor with minimum accumulated cost
    min_cost <- min(
      m_cost[row.i - 1, col.j], 
      m_cost[row.i, col.j - 1]
      )
    
    #add it to the distance of the target cell
    new_value <- min_cost + m_dist[row.i, col.j]
    
    #fill the empty cell with the new value
    m_cost[row.i, col.j] <- new_value
    
  }
}
```

Running the code above results in a nicely filled cost matrix!

```{r, echo = FALSE}
graphics::image(
  x = seq_len(ncol(m_cost)),
  y = seq_len(nrow(m_cost)),
  z = m_cost,
  xlab = "zoo_spain",
  ylab = "zoo_sweden",
  main = "Cost Matrix"
  )
```

Still, there is one more step left! For a reason that will remain unclear until the next section, the terminal cell `[13, 13]` of the cost matrix must have a higher value than any pf its immediate neighbors. If we look at the lower left corner (upper left in the plot) of our cost matrix, this is not the case yet.

```{r}
m <- nrow(m_cost)
n <- ncol(m_cost)
m_cost[(m-1):m, (n-1):n]
```

To fix this issue, it is customary to sum the cost of the first cell of the cost matrix, `m_cost[1, 1]`, to the last one `m_cost[m, n]`.

```{r}
m_cost[m, n] <- m_cost[m, n] + m_cost[1, 1]

m_cost[(m-1):m, (n-1):n]
```

Now that we have all the pieces figured out, we can define our new function to compute the cost matrix. Notice that the code within the nested loops is slightly more concise than shown before.

```{r}
#' Cost Matrix from Distance Matrix
#' @param m (required, matrix) distance matrix.
#' @return matrix
cost_matrix <- function(m){
  
  m_cost <- matrix(
    data = NA, 
    nrow = nrow(m), 
    ncol = ncol(m)
  )
  
  m_cost[1, ] <- cumsum(m[1, ])
  m_cost[, 1] <- cumsum(m[, 1])
  
  for(row in 2:nrow(m)){
    for(col in 2:ncol(m)){
      
      m_cost[row, col] <- min(
        m_cost[row - 1, col], 
        m_cost[row, col - 1]
      ) + m[row, col]
      
    }
  }
  
  m_cost[row.i, col.j] <- m_cost[row.i, col.j] + m_cost[1, 1]
  
  m_cost
  
}
```

Let's test our new function using `m_dist` as input!

```{r}
m_cost <- cost_matrix(m = m_dist)
```

```{r}
graphics::image(
  x = seq_len(ncol(m_cost)),
  y = seq_len(nrow(m_cost)),
  z = m_cost,
  xlab = "zoo_spain",
  ylab = "zoo_sweden",
  main = "Cost Matrix"
  )
```

So far so good! We can now dive into the generation of the least-cost path.

### Least-Cost Path

If we describe the cost matrix as a valley with its hills and ravines, then the least-cost path is the river flowing all its way to the bottom following the line of maximum slope. Following the analogy, the least-cost path starts in the terminal cell of the cost matrix (`[13, 13]`), and ends in the first cell.

To find the least-cost path we first define a data frame with the coordinates of the terminal cell in the cost matrix.

```{r}
path <- data.frame(
  row = ncol(m_cost),
  col = nrow(m_cost)
)

path
```

This is the first step of the least cost path. Now, there are two alternative new steps to consider:

  - *one column left*: `[row, col - 1]`.
  - *one row up*: `[row - 1, col]`.
  
But before moving forward, notice that if we apply these steps indefinitely in our cost matrix, at some point a move up `row - 1` or left `col - 1` will go out of bounds and produce an error. That's why it's safer to define the next move as...

  - *one column left*: `[row, max(col - 1, 1)]`.
  - *one row up*: `[max(row - 1, 1), col]`


...which confines all steps within the first row and column of the cost matrix. 

With that out of the way, now we have to select the move towards a cell with a lower cost. There are many ways to accomplish this task! Let's look one of them.

First, we define the candidate moves using the first row of the least-cost path as reference.

```{r}
steps <- list(
  left = c(path$row, max(path$col - 1, 1)),
  up = c(max(path$row - 1, 1), path$col)
)

steps
```
Notice that the function returns a list with the coordinates of the candidate moves. The move `left` is one column to the left (second number is 12 instead of 13), and the move `up` is one row above (first number is 12 instead of 13).

Second, we need to extract the values of the cost matrix for the coordinates of these two steps.

```{r}
costs <- list(
  left = m_cost[steps$left[1], steps$left[2]],
  up = m_cost[steps$up[1], steps$up[2]]
)

costs
```

Finally, we choose the candidate step with the lower cost using `which.min()`, a function that returns the index of the smallest value in a vector or list. Notice that we use `[1]` in `which.min(costs)[1]` to resolve potential ties that may be returned by `which.min()` if the two costs are the same (unlikely, but possible).

```{r}
steps[[which.min(costs)[1]]]
```

Combining these pieces we can now build a function named `next_step()` that takes the cost matrix and the last row of a least cost path, and returns a new row with the coordinates of the next step.
  
```{r}
#' Identify Next Step of Least-Cost Path
#' @param m (required, matrix) cost matrix.
#' @param step one row data frame with columns "row" and "col"
#' @return one row data frame
next_step <- function(m, step){
  
  steps <- list(
    left = c(step$row, max(step$col - 1, 1)),
    up = c(max(step$row - 1, 1), step$col)
  )
  
  costs <- list(
    left = m[steps$left[1], steps$left[2]],
    up = m[steps$up[1], steps$up[2]]
  )
  
  coords <- steps[[which.min(costs)[1]]]
  
  #rewrite input with new values
  step[,] <- c(coords[1], coords[2])
  
  step
  
}
```

Notice that the function overwrites the input data frame `step` with the new values. Let's check how it works:

```{r}
next_step(
  m = m_cost, 
  step = path
  )
```

Good, it returned the move to the left. Now, if you think about the function for a bit, you'll see that it takes a step in the least-cost path, and returns a new one. From there, it seems we can feed it its own result again and again until it runs out of steps to find.

We can do that in a concise way using a `repeat{}` loop. Notice that it will keep running until both coordinates in the last row of the path are equal to 1.

```{r}
repeat{
  
  #find next step
  new.step <- next_step(
    m = m_cost, 
    step = tail(path, n = 1)
    )
  
  #join the new step with path
  path <- rbind(
    path, new.step,
    make.row.names = FALSE
    )
  
  #stop when coordinates are 1, 1
  if(all(tail(path, n = 1) == 1)){break}
  
}

path
```
  
The resulting least-cost path can be plotted on top of the cost matrix. Please, remember that the data is not pre-processed, and the plot below does not represent the real alignment (yet) between our target time series.

```{r}
graphics::image(
    x = seq_len(ncol(m_cost)),
    y = seq_len(nrow(m_cost)),
    z = m_cost,
    xlab = "zoo_spain",
    ylab = "zoo_sweden",
    main = "Cost Matrix and Least-Cost Path"
    )

graphics::lines(
  x = path$row, 
  y = path$col,
  lwd = 2
  )
```
At this point we have all the pieces required to write the function `least_cost_path()`. Notice that the `repeat{}` statement is slightly more concise than before, as `next_step()` is directly wrapped within `rbind()`

```{r}
#' Least-Cost Path from Cost Matrix
#' @param m (required, matrix) cost matrix.
#' @return data frame with least-cost path coordinates
least_cost_path <- function(m){
  
  #first step of the least cost path
  path <- data.frame(
    row = ncol(m),
    col = nrow(m)
  )
  
  #iterate until path is completed
  repeat{
    
    #merge path with result of next_step()
    path <- rbind(
      path, 
      #find next step
      next_step(
        m = m, 
        step = tail(path, n = 1)
      ),
      make.row.names = FALSE
    )
    
    #stop when coordinates are 1, 1
    if(all(tail(path, n = 1) == 1)){break}
    
  }
  
  path
  
}
```

We can give it a go to see that it works as expected.

```{r}
least_cost_path(m = m_cost)
```

Just a little detail about the least-cost path: every time one row index is linked to several column indices, it means that the samples belonging to these column indices are seeing their time *compressed* (or *warped*) to the time of the row index. So yes, this is where the *warping* in *dynamic time warping* is really happening.

Now it's time to use the least-cost path to quantify the similarity between the time series.

```{r, echo = FALSE, eval = FALSE}
distantia::distantia_dtw_plot(
  tsl = tsl,
  diagonal = FALSE
)
```


### Dissimilarity Metric

The objective of dynamic time warping is computing a metric of similarity between time series. This operation requires three steps:

  - Extract the values of the distance matrix corresponding with the least-cost path
  - Sum the values extracted in the first step.
  - Normalize the sum of distances by some number to help make results comparable across pairs of time series of different lengths.

Since the data frame has the `rows` columns first and `cols` second, it just have to be converted to matrix to extract the distance values from `m_dist`.

```{r}
path$distance <- m_dist[as.matrix(path)]

path
```

The second step to sum the distances is easy enough:

```{r}
d <- sum(path$distance)
d
```

And now, the normalizataion to make this number comparable across different time series. There are several options, but probably the simplest one, implemented in the R package [`dtw`](https://cran.r-project.org/package=dtw), divides the number above by the sum of the total number of cases in both time series.

```{r}
n <- nrow(zoo_sweden) + nrow(zoo_spain)
n
```

But we can also extract this number from the cost path to reduce the number of objects required by our new function:

```{r}
sum(path[1, c("row", "col")])
```
Now we just have to divide the sum of distances by this number to obtain the normalized similarity:

```{r}
d/n
```


## Library Source

Our library will *live* in a file named `mini_dtw.R`. We will be adding new R functions to this file as we progress with the implementation. Remember typing `source("mini_dtw.R")` everytime you add new code to this file to make the functions available in your R environment!
