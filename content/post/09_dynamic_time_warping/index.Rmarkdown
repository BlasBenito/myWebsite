---
title: "A Gentle Intro to Dynamic Time Warping"
author: ''
date: '2025-01-05'
slug: dynamic-time-warping
categories: []
tags: [Time Series Analysis, Dynamic Time Warping]
subtitle: ''
summary: 'Brief introduction to dynamic time warping and its role in the field of time series analysis.'
authors: [admin]
lastmod: '2025-01-05T05:14:20+01:00'
featured: no
draft: true
image:
  caption: Graph by Blas M. Benito
  focal_point: Smart
  margin: auto
projects: []
---

# Resources

  + [distantia: an open-source toolset to quantify dissimilarity between multivariate ecological time-series](https://nsojournals.onlinelibrary.wiley.com/doi/full/10.1111/ecog.04895)
  + [Dynamic Time Warping vs Lock-Step](https://blasbenito.github.io/distantia/articles/dynamic_time_warping_and_lock_step.html)
  + [Everything you know about Dynamic Time Warping is Wrong](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=598dcc162548d1deabc9ef8eaa2de7609b7c7682#page=53)

# Summary

This post explains what *dynamic time warping* is and how it works from a conceptual perspective alone, without code examples or math expressions.

```{r, include = FALSE}
knitr::opts_chunk$set(
  eval = TRUE,
  collapse = TRUE,
  comment = "#>",
  dpi = 150,
  fig.show = 'hold',
  fig.width = 5
)

options(scipen = 9999)
options(conflicts.policy = list(warn = FALSE))
```


```{r, eval = FALSE, echo = FALSE}
#required
install.packages("remotes")
install.packages("dplyr")
install.packages("dtw")
remotes::install_github(
  repo = "blasbenito/distantia", 
  ref = "main"
  )
```

```{r, echo = FALSE}
library(distantia, quietly = TRUE)
suppressPackageStartupMessages(library(dtw, quietly = TRUE))
suppressPackageStartupMessages(library(dplyr, quietly = TRUE)) 
```


# Comparing Time Series

Time series comparison is a critical task in many fields, such as environmental monitoring, finance, and healthcare. The goal is often to quantify similarities or differences between pairs of time series to gain insights in how the data is structured and identify meaningful patterns.

For example, the data below shows time series representing the same phenomena in three different places and time ranges: `a` and `b` have 30 synchronized observations, while `c` has 20 observations from a different year. 

```{r, echo = FALSE, fig.height=3.5}
#create two regular synchronous time series and one irregular
a <- zoo_simulate(
  name = "a", 
  cols = 1,
  rows = 30,
  seasons = 2, 
  time_range = c("2023-01-01", "2025-01-01"),
  seed = 3
  )

colnames(a) <- "x"

b <- zoo_permute(
  x = a,
  block_size = 3,
  seed = 2
)[[1]]

b <- zoo_smooth_window(
  x = b
)

b <- zoo_name_set(x = b, name = "b")

c <- zoo_simulate(
  name = "c", 
  cols = 1,
  rows = 20,
  seasons = 2, 
  irregular = TRUE,
  time_range = c("2022-01-01", "2023-01-01"),
  seed = 3
  )

colnames(c) <- "x"

tsl <- tsl_init(
  x = list(
    a = a,
    b = b,
    c = c
  )
) |> 
  tsl_transform(
    f = f_rescale_local
  )

tsl_plot(
  tsl, 
  ylim = "absolute", 
  xlim = c("2022-01-01", "2025-01-01"),
  guide = FALSE
  )
```


There are several options to compare `a` and `b` directly, such as assessing their correlation (`r round(cor(x = tsl$a, y = tsl$b)[1, 1], 3)`), or computing the sum of euclidean distances between their respective samples (`r round(sum(sqrt((tsl$a[, 1] - tsl$b[, 1])^2)), 3)`). 

This comparison approach is named *lock-step* (also known as *inelastic comparison*), and works best when the time series represent phenomena with relatively similar shapes and are aligned in time and frequency, as it is the case of `a` and `b`.

Comparing `c` with `a` or `b` is a completely different task though, exactly the one *Dynamic Time Warping* was designed for.

Now it'd make sense to explain right away what dynamic time warping is and how it works, but there's a bit of history to know about it first.

# A Useless Bit of History

Dynamic Time Warping (DTW) might sound like a modern high-tech buzzword, but its roots go way back—older than me (gen X guy here!). This powerful method was first developed in the pioneering days of speech recognition. The earliest reference I uncovered is from Shearme and Leach’s 1968 paper, [*Some experiments with a simple word recognition system*](https://doi.org/10.1109/TAU.1968.1161985), published by the Joint Speech Research Unit in the UK.

These foundational ideas were later expanded upon by Sakoe and Chiba in their seminal 1971 paper, [*A Dynamic Programming Approach to Continuous Speech Recognition*](https://api.semanticscholar.org/CorpusID:107516844), often regarded as the definitive starting point for modern DTW applications. 

From there, DTW found applications in diverse fields such as seismology, bioinformatics, and financial analysis. But one particular trans-disciplinary application of DTW brings me to write this post. In 1973, Gordon and Birks published the paper [*Numerical Methods in Quaternary Palaeoecology: II Comparison of Pollen Diagrams*](https://doi.org/10.1111/j.1469-8137.1974.tb04621.x), where DTW—renamed "sequence slotting"—was applied to combine pollen time series. This work inspired the development of the Fortran program [*SLOTSEQ*](https://doi.org/10.1016/0098-3004(80)90003-5) in 1980, which I somehow *inherited* 38 years later, resulting in version 1.0 of the R package [`distantia`](https://doi.org/10.1111/ecog.04895). Fast-forward to today, and my recent work on [version 2.0 of the same package](https://github.com/BlasBenito/distantia) is what brings us here.

Nowadays, Dynamic Time Warping is widely used across fields relying on time-dependent data, such as [medical sciences](https://doi.org/10.1016/j.bspc.2024.106677), [sports analytics](https://doi.org/10.1371/journal.pone.0272848), [astronomy](https://iopscience.iop.org/article/10.3847/1538-4357/ac4af6), [econometrics](https://doi.org/10.1016/j.eneco.2020.105036), [robotics](https://www.mdpi.com/2079-9292/8/11/1306), [epidemiology](https://doi.org/10.1111/exsy.13237), and many others.

Ok, let's stop wandering in time, and go back to the meat in this post.

# What is *Dynamic Time Warping*?

Dynamic Time Warping is a method to compare univariate or multivariate time series of different length, timing, and/or shape. To do so, DTW stretches or compresses parts of the time series (hence *warping*) until it finds the alignment that minimizes their overall differences. Think of it as a way to match the rhythm of two songs even if one plays faster than the other.

The figure below represents a dynamic time warping solution for the time series `c` and `a`. Notice how each sample in one time series matches one or several samples from the other. These matches are optimized to minimize the sum of distances between the samples they connect (3.285 in this case). Any other combination of matches would result in a higher sum of distances.

```{r, fig.height=3.5, echo = FALSE}
a <- as.vector(tsl$a$x)
c <- as.vector(tsl$c$x)

xy_dtw <- dtw::dtw(
  x = a,
  y = c,
  keep = TRUE
  )

dtw::dtwPlotTwoWay(d = xy_dtw, offset = 2, xlab = "", ylab = "", main = "", lwd = 1.5, lty = 1, col = c("red4", "black"))

par(mar = c(0, 0, 0, 0), xpd = TRUE)

legend("bottom",inset = c(0, -0.5), c("a","c"), lwd = 1.5, ncol = 2, bty = "n", col = c("red4", "black"))
```

In dynamic time warping, the actual *warping* happens when a sample in one time series is matched with two or more samples from the other, independently of their observation times. The figure below identifies one of these instances with blue bubbles. The sample 10 of `c` (upper blue bubble), with date `r names(tsl$c$x[10])`, is matched with the samples 14 to 16 of `a` (lower bubble), with dates `r names(tsl$a$x[14])` to `r names(tsl$a$x[16])`. This matching structure represents a time compression in `a` for the range of involved dates.

```{r, fig.height=3.5, echo = FALSE}
dtw::dtwPlotTwoWay(
  d = xy_dtw, 
  offset = 2, 
  xlab = "", 
  ylab = "", 
  main = "", 
  lwd = 1.5, 
  lty = 1, 
  col = c("red4", "black")
  )

points(x = 0.295, y = 0.74, cex = 3, col = "blue4")
points(x = 0.43, y = 0.16, cex = 6, col = "blue4")

par(mar = c(0, 0, 0, 0), xpd = TRUE)

legend(
  "bottom",
  inset = c(0, -0.5), 
  c("a","c"), 
  lwd = 1.5, 
  ncol = 2, 
  bty = "n", 
  col = c("red4", "black")
  )
```
This ability to warp time makes DTW incredibly useful for analyzing time series that are similar in shape but don't have the same length or are not fully synchronized. 

The next section explains how DTW is computed on a step-by-step fashion.

## DTW: Step by Step

Time series comparison via Dynamic Time Warping (DTW) involves several key steps:

- **Detrending and z-score normalization** of the time series.
- **Computation of the distance matrix** between all pairs of samples.
- **Computation of a cost matrix** from the distance matrix.
- **Finding the least-cost path** within the cost matrix.
- **Computation of a similarity metric** based on the least-cost path.

### Detrending and Z-score Normalization

DTW is highly sensitive to differences in trends and ranges between time series (see the *Pitfalls* section). To address this, [detrending](https://sherbold.github.io/intro-to-data-science/09_Time-Series-Analysis.html#Trend-and-Seasonal-Effects) and [z-score normalization](https://developers.google.com/machine-learning/crash-course/numerical-data/normalization#z-score_scaling) are important preprocessing steps.

In this example, the time series `a` and `c` already have matching ranges, so normalization is not strictly necessary. For demonstration purposes, however, the figure below shows them normalized using z-score scaling:

```{r, echo = FALSE, fig.height=2.5}
tsl <- tsl |> 
  tsl_transform(
    f = f_scale_local
  ) |> 
  tsl_subset(
    names = c("a", "c")
  )

tsl_plot(
  tsl, 
  ylim = "absolute", 
  xlim = c("2022-01-01", "2025-01-01"),
  guide = FALSE
)
```

### Distance Matrix

The next step involves computing the distance matrix, which contains pairwise distances between all samples in the two time series. 

```{r, fig.width=4, fig.height=3.5, echo = FALSE}
m.dist <- distantia::psi_distance_matrix(
  x = tsl$a,
  y = tsl$c,
  distance = "euclidean"
)

distantia::utils_matrix_plot(
  m = m.dist,
  diagonal_width = 0
)
```

Choosing an appropriate distance metric is crucial. While Euclidean distance works well in many cases, other metrics may be more suitable depending on the data.

### Cost Matrix

The cost matrix is derived from the distance matrix by accumulating distances dynamically from the starting corner (lower-left) to the ending corner (upper-right). Different rules for cell neighborhood determine how costs propagate:

  - **Orthogonal only**: Accumulation occurs in the *x* and *y* directions only, ignoring diagonals.
  - **Orthogonal and diagonal**: Diagonal movements are also considered, typically weighted by a factor of `√2` (1.414) to balance with orthogonal movements.

The figure below illustrates the cost matrix with both orthogonal and diagonal paths:

```{r, fig.width=4, fig.height=3.5, echo = FALSE}
m.cost <- distantia::psi_cost_matrix(
  dist_matrix = m.dist,
  diagonal = TRUE
)

distantia::utils_matrix_plot(
  m = m.cost,
  diagonal_width = 0,
  text_cex = 0.9
)
```

### Least-cost Path

This is where the actual time warping happens! The least-cost path minimizes the total cost from the start to the end of the cost matrix, aligning the time series optimally. 

The figure below shows the least-cost path (black line). Deviations from the diagonal represent adjustments made to align the time series.

```{r, fig.width=4, fig.height=3.5, echo = FALSE}
m.cost.path <- distantia::psi_cost_path(
  dist_matrix = m.dist,
  cost_matrix = m.cost,
  diagonal = TRUE
)

distantia::utils_matrix_plot(
  m = m.cost,
  path = m.cost.path,
  text_cex = 0.9
)
```

### Similarity Metric

Finally, DTW produces a similarity metric based on the least-cost path. The simplest approach is to sum the distances of all points along the path.

```{r, echo = FALSE}
cost.sum <- distantia::psi_cost_path_sum(
  path = m.cost.path
)
```

For this example, the total cost is `r round(cost.sum, 3)`. 
However, when comparing time series of varying lengths, normalization is often useful. Common options include:

  - **Sum of lengths**: Normalize by the combined lengths of the time series, e.g., `Normalized Cost = Total Cost / (Length(a) + Length(c))`. For `a` and `c`, this would be `r round(cost.sum / (30 + 20), 3)`.
  - **Auto-sum of distances**: Normalize by the sum of distances between adjacent samples in each series, as in `Normalized Cost = Total Cost / (Auto-sum(a) + Auto-sum(c))`. For `a` and `c`, this results in `r round(cost.sum / (11.044 + 10.125), 3)`.

These normalized metrics allow comparisons across datasets with varying characteristics.

# Pitfalls of Dynamic Time Warping

DTW is a powerful tool for time series comparison, but it comes with several challenges that users should be aware of when starting to work with it. Below is a summary of the most common pitfalls and how to mitigate them.

DTW is highly sensitive to differences in the scales or trends of the time series. A series with larger values or a strong trend might dominate the alignment process, skewing the results. This can result in misleading results called *pathological alignments*, especially when one of the time series has noise, spikes, or outliers.

The figure below shows an example of pahological alignment between temperature time series of two major cities. It results from

```{r, echo = FALSE, fig.height=4.5}
tsl <- tsl_initialize(
  x = cities_temperature,
  name_column = "name",
  time_column = "time"
) |> 
  tsl_subset(
    names = c("London", "Kinshasa"),
    time = c("2000-01-01", "2010-01-01")
  )

distantia_dtw_plot(tsl = tsl, text_cex = 0.9)
```

These long straight lines in the least-cost path are clear features of a pathological alignment. They result from the algorithm *getting stuck* in a particular sample of one of the time series, which is matched with many samples from the other one.

There are two complementary approaches to mitigate this issue:

The first one involves z-score normalization, detrending, and maybe smoothing. These pre-processing steps alone can transform a pathological alignnment into a very healthy one, as the figure below shows.

```{r, echo = FALSE, fig.height=4.5}
tsl_scaled <- tsl |> 
  tsl_transform(
    f = f_detrend_poly,
    degree = 10
  ) |> 
  tsl_smooth() |> 
  tsl_transform(
    f = f_scale_local
  )

distantia_dtw_plot(tsl = tsl_scaled, text_cex = 0.9)
```
The second approach, which can be combined with the previous one, involves applying *constrained DTW*, which limits the area of the cost matrix the least-cost path can use. For example, *Sakoe-Chiba bands*, shown in the figure below, and *Itakura parallelograms* define an area at both sides of the cost matrix diagonal outside of which the least-cost path cannot cross.

However, as seen below, these methods have a limited ability to fix extremely pathological alignments, which mostly happen when the proper data preprocessing steps are ignored or skipped.

```{r, echo = FALSE, fig.height=4.5}
distantia_dtw_plot(tsl = tsl, text_cex = 0.9, bandwidth = 0.1)
```


Understanding these pitfalls and implementing the suggested mitigations can greatly improve the reliability of DTW-based analyses.

# Computational considerations

The computational complexity of DTW is $(O(n \times m))$, where $n$ and $m$ are the lengths of the time series. This makes it computationally expensive for long time series.

> **Mitigation**: Consider down-sampling the time series to reduce their lengths. Parallel processing can also help when working with multiple time series.

# Closing Thoughts

TODO: small paragraph with take-home details.

Blas

