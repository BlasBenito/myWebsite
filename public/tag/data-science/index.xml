<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science | Blas M. Benito, PhD</title>
    <link>https://blasbenito.com/tag/data-science/</link>
      <atom:link href="https://blasbenito.com/tag/data-science/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Science</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2023 Blas M. Benito. All Rights Reserved.</copyright><lastBuildDate>Mon, 13 Jan 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://blasbenito.com/media/avatar.jpg</url>
      <title>Data Science</title>
      <link>https://blasbenito.com/tag/data-science/</link>
    </image>
    
    <item>
      <title>Coding a Minimalistic Dynamic Time Warping Library with R</title>
      <link>https://blasbenito.com/post/dynamic-time-warping-from-scratch/</link>
      <pubDate>Mon, 13 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/dynamic-time-warping-from-scratch/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;This post walks you through the implementation of a minimalistic yet fully functional 
&lt;a href=&#34;https://www.blasbenito.com/post/dynamic-time-warping/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Time Warping&lt;/a&gt; (DTW) library in R, built entirely from scratch without dependencies or complex abstractions. While there are many 
&lt;a href=&#34;https://blasbenito.github.io/distantia/articles/dtw_applications.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;open-source DTW implementations&lt;/a&gt; readily available, understanding the inner workings of the algorithm can be invaluable. Whether you’re simply curious or need a deeper grasp of DTW for your projects, this step-by-step guide offers a hands-on approach to demystify the method.&lt;/p&gt;
&lt;h1 id=&#34;design&#34;&gt;Design&lt;/h1&gt;
&lt;h2 id=&#34;example-data&#34;&gt;Example Data&lt;/h2&gt;
&lt;p&gt;Having good example data at hand is a must when developing new code. For this tutorial we use three multivariate time series of temperature, rainfall, and normalized vegetation index. These time series are named &lt;code&gt;zoo_germany&lt;/code&gt;, &lt;code&gt;zoo_sweden&lt;/code&gt;, and &lt;code&gt;zoo_spain&lt;/code&gt;, and are stored as objects of the class 
&lt;a href=&#34;https://CRAN.R-project.org/package=zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;zoo&lt;/a&gt;, which is a very robust time series management library.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Each zoo object has a &lt;em&gt;core data&lt;/em&gt; of the class &lt;code&gt;matrix&lt;/code&gt; with one observation per row and one variable per column, and an &lt;em&gt;index&lt;/em&gt;, which is a vector of dates, one per row in the time series.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zoo::coredata(zoo_sweden)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         evi rainfall temperature
## 1092 0.1259     32.0        -4.4
## 1132 0.1901     55.6        -2.8
## 1142 0.2664     38.8         1.8
## 1152 0.2785     20.3         7.0
## 1162 0.7068     59.4        10.1
## 1172 0.7085     69.5        14.3
## 1182 0.6580     85.2        19.2
## 1192 0.5831    150.2        16.8
## 1202 0.5036     74.9        12.7
## 1103 0.3587     74.9         7.8
## 1113 0.2213    114.6         2.5
## 1122 0.1475     52.1        -4.7
## 1213 0.2140     49.5        -0.8
## attr(,&amp;quot;name&amp;quot;)
## [1] &amp;quot;zoo_sweden&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zoo::index(zoo_sweden)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;2010-01-01&amp;quot; &amp;quot;2010-02-01&amp;quot; &amp;quot;2010-03-01&amp;quot; &amp;quot;2010-04-01&amp;quot; &amp;quot;2010-05-01&amp;quot;
##  [6] &amp;quot;2010-06-01&amp;quot; &amp;quot;2010-07-01&amp;quot; &amp;quot;2010-08-01&amp;quot; &amp;quot;2010-09-01&amp;quot; &amp;quot;2010-10-01&amp;quot;
## [11] &amp;quot;2010-11-01&amp;quot; &amp;quot;2010-12-01&amp;quot; &amp;quot;2011-01-01&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;required-library-functions&#34;&gt;Required Library Functions&lt;/h2&gt;
&lt;p&gt;The section &lt;em&gt;DTW Step by Step&lt;/em&gt; from the previous article 
&lt;a href=&#34;https://www.blasbenito.com/post/dynamic-time-warping/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Gentle Intro to Dynamic Time Warping&lt;/a&gt; describes the computational steps required by the algorithm. Below, these steps are broken down into sub-steps that will correspond to specific functions in our library:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time Series Pre-processing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These steps help DTW work seamlessly with the input time series.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear detrending&lt;/strong&gt;: forces time series to be stationary by removing any upwards or downwards trends.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Z-score normalization&lt;/strong&gt;: equalizes the range of the time series to ensure that the different variables contribute evenly to the distance computation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Time Warping&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These steps perform dynamic time warping and evaluate the dissimilarity between the time series.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multivariate distance&lt;/strong&gt;: compute distances between pairs of samples from each time series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distance matrix&lt;/strong&gt;: organize the multivariate distances in a matrix in which each axis represents a time series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cost matrix&lt;/strong&gt;: this matrix accumulates the distances in the distance matrix across time and represents all possible alignments between two time series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Least-cost path&lt;/strong&gt;: path in the cost matrix that minimizes the overall distance between two time series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dissimilarity metric&lt;/strong&gt;: value to summarize the dissimilarity between time series.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Main function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once all the steps above are implemented in their respective functions, we will wrap all them in a single function to streamline the DTW analysis.&lt;/p&gt;
&lt;h1 id=&#34;implementation&#34;&gt;Implementation&lt;/h1&gt;
&lt;p&gt;In this section we will be developing the library concept by concept and function by function.&lt;/p&gt;
&lt;h2 id=&#34;time-series-pre-processing&#34;&gt;Time Series Pre-processing&lt;/h2&gt;
&lt;p&gt;In this section we develop the function &lt;code&gt;ts_preprocessing()&lt;/code&gt;, which will prepare the time series data for dynamic time warping. This function contains two functionalities: &lt;strong&gt;linear detrending&lt;/strong&gt;, and &lt;strong&gt;z-score normalization&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;linear-detrending&#34;&gt;Linear Detrending&lt;/h3&gt;
&lt;p&gt;Applying linear detrending to a multivariate time series involves computing a linear model of each variable against time, and subtracting the the model prediction to the original data. This can be performed in two steps:&lt;/p&gt;
&lt;p&gt;First, the function &lt;code&gt;stats::lm()&lt;/code&gt; can be applied to all variables in one of our time series at once.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model_sweden &amp;lt;- stats::lm(
  formula = zoo_sweden ~ stats::time(zoo_sweden)
  )

model_sweden
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## stats::lm(formula = zoo_sweden ~ stats::time(zoo_sweden))
## 
## Coefficients:
##                          evi         rainfall    temperature
## (Intercept)               8.965e-01  -1.710e+03  -5.706e+01 
## stats::time(zoo_sweden)  -3.480e-05   1.202e-01   4.271e-03
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, the residuals of the linear model, which represent the differences between observations and predictions, correspond exactly with the detrended time series. As a plus, these residuals are returned as a zoo object when the &lt;code&gt;zoo&lt;/code&gt; library is loaded!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;stats::residuals(model_sweden)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    evi     rainfall temperature
## 2010-01-01 -0.26214896 -13.62153187   -9.739028
## 2010-02-01 -0.19687011   6.25374419   -8.271433
## 2010-03-01 -0.11959566 -13.91052259   -3.791024
## 2010-04-01 -0.10641680 -36.13524652    1.276572
## 2010-05-01  0.32292725  -0.63981807    4.248439
## 2010-06-01  0.32570610   5.73545800    8.316034
## 2010-07-01  0.27625015  17.83088645   13.087901
## 2010-08-01  0.20242901  79.10616252   10.555496
## 2010-09-01  0.12400786   0.08143858    6.323092
## 2010-10-01 -0.01984809  -3.52313297    1.294959
## 2010-11-01 -0.15616924  32.45214310   -4.137446
## 2010-12-01 -0.22892518 -33.65242845  -11.465579
## 2011-01-01 -0.16134633 -39.97715238   -7.697983
## attr(,&amp;quot;name&amp;quot;)
## [1] zoo_sweden
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;
Then, the pre-processing function of our library could be something like this:
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Linear Detrending
#&#39; @param x (required, zoo object) time series to detrend.
#&#39; @return zoo object
ts_preprocessing &amp;lt;- function(x){
  m &amp;lt;- stats::lm(formula = x ~ stats::time(x))
  y &amp;lt;- stats::residuals(object = m)
  y
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function could be more concise, but it is written to facilitate line-by-line debugging instead. I have also added minimal roxygen documentation. Future me usually appreciates this kind of extra effort. Any kind of effort actually.&lt;/p&gt;
&lt;p&gt;This function should check that &lt;code&gt;x&lt;/code&gt; is really a zoo object, and any other condition that would make it fail, but to keep code simple, in this tutorial we won&amp;rsquo;t do any error catching.&lt;/p&gt;
&lt;p&gt;We can use a mock-up time series with an ascending trend to really test the effect of our detrending function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- zoo::zoo(0:10)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;If we apply &lt;code&gt;ts_preprocessing()&lt;/code&gt; to this time series, the result shows a horizontal line, which is a perfect linear detrending. Now we can be sure our implementation works!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x_detrended &amp;lt;- ts_preprocessing(x = x)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h3 id=&#34;z-score-normalization&#34;&gt;Z-score Normalization&lt;/h3&gt;
&lt;p&gt;Normalization (also named &lt;em&gt;standardization&lt;/em&gt;) consists of two operations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Centering&lt;/strong&gt;: performed by subtracting the column mean to each case, resulting in a column mean equal to zero.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scaling&lt;/strong&gt;: divides each case by the standard deviation of the column, resulting in a standard deviation equal to one.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The R base function &lt;code&gt;scale()&lt;/code&gt; implements z-score normalization, so there&amp;rsquo;s not much we have to do from scratch here. Also, when the library &lt;code&gt;zoo&lt;/code&gt; is loaded, the method &lt;code&gt;zoo:::scale.zoo&lt;/code&gt; (&lt;code&gt;:::&lt;/code&gt; denotes methods and functions that are not exported by a package) allows &lt;code&gt;scale()&lt;/code&gt; to work seamlessly with zoo objects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;scale(
  x = zoo_germany,
  center = TRUE,
  scale = TRUE
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   evi    rainfall temperature
## 2010-01-01 -2.1072111 -0.77369778 -1.37937171
## 2010-02-01 -0.5757809 -0.44531242 -0.97212864
## 2010-03-01 -0.4963522 -0.87526026 -0.40724308
## 2010-04-01 -0.1500661 -1.49817681  0.26273747
## 2010-05-01  1.2590783  1.21354145  0.39410620
## 2010-06-01  1.3482213 -0.06614582  1.20859236
## 2010-07-01  0.9973637  0.53307282  1.61583543
## 2010-08-01  0.9990780  1.72812469  1.19545548
## 2010-09-01  0.3750773 -0.07630207  0.66998055
## 2010-10-01  0.2670772 -0.94973941  0.05254749
## 2010-11-01 -0.3477806  0.33671869 -0.38096933
## 2010-12-01 -0.7843525  1.44713515 -1.36623484
## 2011-01-01 -0.7843525 -0.57395823 -0.89330739
## attr(,&amp;quot;name&amp;quot;)
## [1] zoo_germany
## attr(,&amp;quot;scaled:center&amp;quot;)
##         evi    rainfall temperature 
##   0.4376615  60.8538462   8.8000000 
## attr(,&amp;quot;scaled:scale&amp;quot;)
##         evi    rainfall temperature 
##   0.1749998  29.5384669   7.6121613
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to this seamless integration with &lt;code&gt;zoo&lt;/code&gt; time series, z-score normalization can be easily added to our pre-processing function!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Linear Detrending and Normalization
#&#39; @param x (required, zoo object) time series to detrend.
#&#39; @return zoo object
ts_preprocessing &amp;lt;- function(x){
  m &amp;lt;- stats::lm(formula = x ~ stats::time(x))
  y &amp;lt;- stats::residuals(object = m)
  z &amp;lt;- scale(y) #new
  z
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now test &lt;code&gt;ts_preprocessing()&lt;/code&gt; and move forward with our implementation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ts_preprocessing(x = zoo_germany)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    evi     rainfall temperature
## 2010-01-01 -1.91389650 -0.254652344 -1.35356479
## 2010-02-01 -0.40410400 -0.001606348 -0.95069824
## 2010-03-01 -0.35682407 -0.548358009 -0.38973731
## 2010-04-01 -0.04363355 -1.310450989  0.27590451
## 2010-05-01  1.34386644  1.489002490  0.40300011
## 2010-06-01  1.39742780  0.026066230  1.21316833
## 2010-07-01  1.00791041  0.571260913  1.61617795
## 2010-08-01  0.97319785  1.749131031  1.19130241
## 2010-09-01  0.30672100 -0.273757334  0.66131677
## 2010-10-01  0.16240893 -1.300041138  0.03950285
## 2010-11-01 -0.49483667 -0.024630977 -0.39851146
## 2010-12-01 -0.97089711  1.066065435 -1.38821075
## 2011-01-01 -1.00734053 -1.188028960 -0.91965038
## attr(,&amp;quot;name&amp;quot;)
## [1] zoo_germany
## attr(,&amp;quot;scaled:center&amp;quot;)
##           evi      rainfall   temperature 
## -9.607699e-18  2.903660e-16  4.440892e-16 
## attr(,&amp;quot;scaled:scale&amp;quot;)
##         evi    rainfall temperature 
##   0.1733241  27.6809389   7.6110663
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;dynamic-time-warping-functions&#34;&gt;Dynamic Time Warping Functions&lt;/h2&gt;
&lt;p&gt;This section describes the implementation of the DTW algorithm, which requires functions to compute a distance matrix, convert it to a cost matrix, find a least-cost path maximizing the alignment between the time series, and compute their dissimilarity.&lt;/p&gt;
&lt;h3 id=&#34;distance-matrix&#34;&gt;Distance Matrix&lt;/h3&gt;
&lt;p&gt;In DTW, a distance matrix represents the distances between all pairs of samples in two time series. Hence, each time series is represented in one axis of the matrix. But before getting there, we need a function to obtain the distance between arbitrary pairs of rows from two separate zoo objects.&lt;/p&gt;
&lt;h4 id=&#34;distance-function&#34;&gt;Distance Function&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s say we have two vectors, &lt;code&gt;x&lt;/code&gt; with one row of &lt;code&gt;zoo_germany&lt;/code&gt;, and &lt;code&gt;y&lt;/code&gt; with one row of &lt;code&gt;zoo_sweden&lt;/code&gt;. Then, the expression to compute the Euclidean distances between &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; becomes &lt;code&gt;sqrt(sum((x-y)^2))&lt;/code&gt;. From there, implementing a distance function is kinda trivial.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Euclidean Distance
#&#39; @param x (required, numeric) row of a zoo object.  
#&#39; @param y (required, numeric) row of a zoo object.
#&#39; @return numeric
distance_euclidean &amp;lt;- function(x, y){
  sqrt(sum((x - y)^2))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the function does not indicate the return explicitly. Since the function&amp;rsquo;s body is a one-liner, one cannot be really worried about the function returning something unexpected. Also, implementing such a simple expression in a function might seem like too much, but it may facilitate the addition of new distance metrics to the library in the future. For example, we could create something like &lt;code&gt;distance_manhattan()&lt;/code&gt; with the Manhattan distance, and later switch between one or another depending on the user&amp;rsquo;s needs.&lt;/p&gt;
&lt;p&gt;The code below tests the function by computing the euclidean distance between the row 1 from &lt;code&gt;zoo_sweden&lt;/code&gt; and the row 2 from &lt;code&gt;zoo_germany&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zoo_sweden[1, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               evi rainfall temperature
## 2010-01-01 0.1259       32        -4.4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zoo_germany[2, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               evi rainfall temperature
## 2010-02-01 0.3369     47.7         1.4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;distance_euclidean(
  x = zoo_sweden[1, ],
  y = zoo_germany[2, ]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sorry, what? That doesn&amp;rsquo;t seem right!&lt;/p&gt;
&lt;p&gt;For whatever reason, &lt;code&gt;zoo_sweden[1, ]&lt;/code&gt; and &lt;code&gt;zoo_germany[2, ]&lt;/code&gt; are not being interpreted as numeric vectors by &lt;code&gt;distance_euclidean()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try something different:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;distance_euclidean(
  x = as.numeric(zoo_sweden[1, ]),
  y = as.numeric(zoo_germany[2, ])
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16.73841
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, that makes more sense!&lt;/p&gt;
&lt;p&gt;Then, we just have to move these &lt;code&gt;as.numeric()&lt;/code&gt; commands inside &lt;code&gt;distance_euclidean()&lt;/code&gt; to simplify the usage of the function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Euclidean Distance
#&#39; @param x (required, numeric) row of a zoo object.  
#&#39; @param y (required, numeric) row of a zoo object.
#&#39; @return numeric
distance_euclidean &amp;lt;- function(x, y){
  x &amp;lt;- as.numeric(x) #new
  y &amp;lt;- as.numeric(y) #new
  z &amp;lt;- sqrt(sum((x - y)^2))
  z
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new function should have no issues returning the right distance between these rows now:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;distance_euclidean(
  x = zoo_sweden[1, ],
  y = zoo_germany[2, ]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16.73841
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can go compute the distance matrix.&lt;/p&gt;
&lt;h4 id=&#34;distance-matrix-1&#34;&gt;Distance Matrix&lt;/h4&gt;
&lt;p&gt;To generate the distance matrix, the function &lt;code&gt;distance_euclidean()&lt;/code&gt; must be applied to all pairs of rows in the two time series.&lt;/p&gt;
&lt;p&gt;A simple yet inefficient way to do this involves creating an empty matrix, and traversing it cell by cell to compute the euclidean distances between the corresponding pair of rows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#empty distance matrix
m_dist &amp;lt;- matrix(
  data = NA, 
  nrow = nrow(zoo_germany), 
  ncol = nrow(zoo_sweden)
)

#iterate over rows
for(row in 1:nrow(zoo_germany)){
  
  #iterate over columns
  for(col in 1:nrow(zoo_sweden)){
    
    #distance between time series rows
    m_dist[row, col] &amp;lt;- distance_euclidean(
      x = zoo_germany[col, ],
      y = zoo_sweden[row, ]
    )
    
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code generates a matrix with &lt;code&gt;zoo_germany&lt;/code&gt; in the rows, from top to bottom, and &lt;code&gt;zoo_sweden&lt;/code&gt; in the columns, from left to right. The first five rows and columns are shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_dist[1:5, 1:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]      [,3]      [,4]     [,5]
## [1,]  6.579761 16.738415 10.538528 21.639813 66.69942
## [2,] 17.634758  8.948271 22.285328 41.303861 43.61868
## [3,]  3.595693  8.909263  5.445835 23.955397 58.75852
## [4,] 19.723690 27.966469 14.757548  5.305437 76.55158
## [5,] 24.446000 14.584815 24.796103 42.806743 37.33875
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This matrix can be plotted with the function &lt;code&gt;graphics::image()&lt;/code&gt;, but please be aware that it rotates the distance matrix 90 degrees counter clock-wise, which can be pretty confusing at first.&lt;/p&gt;
&lt;p&gt;Remember this: &lt;strong&gt;in the matrix plot, the x axis represents the matrix rows&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;graphics::image(
  x = seq_len(nrow(m_dist)),
  y = seq_len(ncol(m_dist)),
  z = m_dist,
  xlab = &amp;quot;zoo_germany&amp;quot;,
  ylab = &amp;quot;zoo_sweden&amp;quot;,
  main = &amp;quot;Euclidean Distance&amp;quot;
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Darker values indicate larger distances between pairs of samples in each time series.&lt;/p&gt;
&lt;p&gt;We can now wrap the code above (without the plot) in a new function named &lt;code&gt;distance_matrix()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Distance Matrix Between Time Series
#&#39; @param a (required, zoo object) time series.
#&#39; @param b (required, zoo object) time series with same columns as `x`
#&#39; @return matrix
distance_matrix &amp;lt;- function(a, b){
  
  m &amp;lt;- matrix(
    data = NA, 
    nrow = nrow(b), 
    ncol = nrow(a)
  )
  
  for (row in 1:nrow(b)) {
    for (col in 1:nrow(a)) {
      m[row, col] &amp;lt;- distance_euclidean(
        x = a[col, ],
        y = b[row, ] 
      )
    }
  }
  
  m
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s run a little test before moving forward!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_dist &amp;lt;- distance_matrix(
  a = zoo_germany,
  b = zoo_sweden
)

m_dist[1:5, 1:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]      [,3]      [,4]     [,5]
## [1,]  6.579761 16.738415 10.538528 21.639813 66.69942
## [2,] 17.634758  8.948271 22.285328 41.303861 43.61868
## [3,]  3.595693  8.909263  5.445835 23.955397 58.75852
## [4,] 19.723690 27.966469 14.757548  5.305437 76.55158
## [5,] 24.446000 14.584815 24.796103 42.806743 37.33875
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are good to go! The next function will transform this distance matrix into a &lt;em&gt;cost matrix&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;cost-matrix&#34;&gt;Cost Matrix&lt;/h3&gt;
&lt;p&gt;Now we are getting into the important parts of the DTW algorithm!&lt;/p&gt;
&lt;p&gt;A cost matrix is like a valley&amp;rsquo;s landscape, with hills in regions where the time series are different, and ravines where they are more similar. Such landscape is built by accumulating the values of the distance matrix cell by cell, from &lt;code&gt;[1, 1]&lt;/code&gt; at the bottom of the valley (upper left corner of the matrix, but lower left in the plot), to &lt;code&gt;[m, n]&lt;/code&gt; at the top (lower right corner of the matrix, upper right in the plot).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see how that works!&lt;/p&gt;
&lt;p&gt;First, we use the dimensions of the distance matrix to create an empty cost matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_cost &amp;lt;- matrix(
  data = NA, 
  nrow = nrow(m_dist), 
  ncol = ncol(m_dist)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, to initialize the cost matrix we accumulate the values of the first row and column of the distance matrix using &lt;code&gt;cumsum()&lt;/code&gt;. This step is very important for the second part of the algorithm, as it provides the starting values of the cost matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_cost[1, ] &amp;lt;- cumsum(m_dist[1, ])
m_cost[, 1] &amp;lt;- cumsum(m_dist[, 1])
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Now, before going into the third step, let&amp;rsquo;s focus for a moment on the first cell of the cost matrix we need to fill, with coordinates &lt;code&gt;[2, 2]&lt;/code&gt; and value &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_cost[1:2, 1:2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]     [,2]
## [1,]  6.579761 23.31818
## [2,] 24.214519       NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new value of this cell results from the addition of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Its value in the distance matrix &lt;code&gt;m_dist&lt;/code&gt; (8.95).&lt;/li&gt;
&lt;li&gt;The minimum accumulated distance of its neighbors, which are:
&lt;ul&gt;
&lt;li&gt;Upper neighbor with coordinates &lt;code&gt;[1, 2]&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Left neighbor with coordinates &lt;code&gt;[2, 1]&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The general expression to find the value of the empty cell is shown below. It uses &lt;code&gt;min()&lt;/code&gt; to get the value of the &lt;em&gt;smallest&lt;/em&gt; neighbor, and then adds it to the vaue of the target cell in the distance matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_cost[2, 2] &amp;lt;- min(
  m_cost[1, 2], 
  m_cost[2, 1]
  ) + m_dist[2, 2]

m_cost[1:2, 1:2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]     [,2]
## [1,]  6.579761 23.31818
## [2,] 24.214519 32.26645
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But there are many cells to fill yet!&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;The expression we used to fill the cell &lt;code&gt;m_cost[2, 2]&lt;/code&gt; can be generalized to fill all remaining empty cells. We just have to wrap it in a nested loop that for each new empty cell identifies the smallest neighbor in the x and y axies, and adds its cumulative cost to the distance of the new cell.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#iterate over rows of the cost matrix
for(row in 2:nrow(m_dist)){
  
  #iterate over columns of the cost matrix
  for(col in 2:ncol(m_dist)){
    
    #get cost of neighbor with minimum accumulated cost
    min_cost &amp;lt;- min(
      m_cost[row - 1, col], 
      m_cost[row, col - 1]
      )
    
    #add it to the distance of the target cell
    new_value &amp;lt;- min_cost + m_dist[row, col]
    
    #fill the empty cell with the new value
    m_cost[row, col] &amp;lt;- new_value
    
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running the code above results in a nicely filled cost matrix!&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Now that we have all the pieces figured out, we can define our new function to compute the cost matrix. Notice that the code within the nested loops is slightly more concise than shown before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Cost Matrix from Distance Matrix
#&#39; @param distance_matrix (required, matrix) distance matrix.
#&#39; @return matrix
cost_matrix &amp;lt;- function(distance_matrix){
  
  m &amp;lt;- matrix(
    data = NA, 
    nrow = nrow(distance_matrix), 
    ncol = ncol(distance_matrix)
  )
  
  m[1, ] &amp;lt;- cumsum(distance_matrix[1, ])
  m[, 1] &amp;lt;- cumsum(distance_matrix[, 1])
  
  for(row in 2:nrow(distance_matrix)){
    for(col in 2:ncol(distance_matrix)){
      
      m[row, col] &amp;lt;- min(
        m[row - 1, col], 
        m[row, col - 1]
      ) + distance_matrix[row, col]
      
    }
  }
  
  m
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s test our new function using &lt;code&gt;m_dist&lt;/code&gt; as input:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m_cost &amp;lt;- cost_matrix(distance_matrix = m_dist)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;So far so good! We can now dive into the generation of the least-cost path.&lt;/p&gt;
&lt;h3 id=&#34;least-cost-path&#34;&gt;Least-Cost Path&lt;/h3&gt;
&lt;p&gt;If we describe the cost matrix as a valley with its hills and ravines, then the least-cost path is the river following the line of maximum slope all the way to the bottom of the valley. Following the analogy, the least-cost path starts in the terminal cell of the cost matrix (&lt;code&gt;[13, 13]&lt;/code&gt;), and ends in the first cell.&lt;/p&gt;
&lt;p&gt;To find the least-cost path we first define a data frame with the coordinates of the terminal cell in the cost matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;path &amp;lt;- data.frame(
  row = ncol(m_cost),
  col = nrow(m_cost)
)

path
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   row col
## 1  13  13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the first step of the least cost path. From here, there are two candidate steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;one column left&lt;/em&gt;: &lt;code&gt;[row, col - 1]&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;one row up&lt;/em&gt;: &lt;code&gt;[row - 1, col]&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But before moving forward, notice that if we apply these steps indefinitely in our cost matrix, at some point a move up &lt;code&gt;row - 1&lt;/code&gt; or left &lt;code&gt;col - 1&lt;/code&gt; will go out of bounds and produce an error. That&amp;rsquo;s why it&amp;rsquo;s safer to define the next move as&amp;hellip;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;one column left&lt;/em&gt;: &lt;code&gt;[row, max(col - 1, 1)]&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;one row up&lt;/em&gt;: &lt;code&gt;[max(row - 1, 1), col]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;hellip;which confines all steps within the first row and column of the cost matrix.&lt;/p&gt;
&lt;p&gt;With that out of the way, now we have to select the move towards a cell with a lower cost. There are many ways to accomplish this task! Let&amp;rsquo;s look one of them.&lt;/p&gt;
&lt;p&gt;First, we define the candidate moves using the first row of the least-cost path as reference, and generate a list with the coordinates of the candidate steps.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;steps &amp;lt;- list(
  left = c(path$row, max(path$col - 1, 1)),
  up = c(max(path$row - 1, 1), path$col)
)

steps
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $left
## [1] 13 12
## 
## $up
## [1] 12 13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we extract the values of the cost matrix for the coordinates of these two steps.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;costs &amp;lt;- list(
  left = m_cost[steps$left[1], steps$left[2]],
  up = m_cost[steps$up[1], steps$up[2]]
)

costs
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $left
## [1] 495.481
## 
## $up
## [1] 457.7403
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we choose the candidate step with the lower cost using &lt;code&gt;which.min()&lt;/code&gt;, a function that returns the index of the smallest value in a vector or list. Notice that we use &lt;code&gt;[1]&lt;/code&gt; in &lt;code&gt;which.min(costs)[1]&lt;/code&gt; to resolve potential ties that may be returned by &lt;code&gt;which.min()&lt;/code&gt; if the two costs are the same (unlikely, but possible).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;steps[[which.min(costs)[1]]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12 13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Combining these pieces we can now build a function named &lt;code&gt;least_cost_step()&lt;/code&gt; that takes the cost matrix and the last row of a least-cost path, and returns a new row with the coordinates of the next step.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Identify Next Step of Least-Cost Path
#&#39; @param cost_matrix (required, matrix) cost matrix.
#&#39; @param last_step (required, data frame) one row data frame with columns &amp;quot;row&amp;quot; and &amp;quot;col&amp;quot; representing the last step of a least-cost path.
#&#39; @return one row data frame, new step in least-cost path
least_cost_step &amp;lt;- function(cost_matrix, last_step){
  
  #define candidate steps
  steps &amp;lt;- list(
    left = c(last_step$row, max(last_step$col - 1, 1)),
    up = c(max(last_step$row - 1, 1), last_step$col)
  )
  
  #obtain their costs
  costs &amp;lt;- list(
    left = cost_matrix[steps$left[1], steps$left[2]],
    up = cost_matrix[steps$up[1], steps$up[2]]
  )
  
  #select the one with a smaller cost
  coords &amp;lt;- steps[[which.min(costs)[1]]]
  
  #rewrite input with new values
  last_step[,] &amp;lt;- c(coords[1], coords[2])
  
  last_step
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the function overwrites the input data frame &lt;code&gt;step&lt;/code&gt; with the new values to avoid generating a new data frame, making the code a bit more concise.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s check how it works:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;least_cost_step(
  cost_matrix = m_cost, 
  last_step = path
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   row col
## 1  12  13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Good, it returned the move to the upper neighbor!&lt;/p&gt;
&lt;p&gt;Now, if you think about the function for a bit, you&amp;rsquo;ll see that it takes a step in the least-cost path, and returns a new one. From there, it seems we can feed it its own result again and again until it runs out of new steps to find.&lt;/p&gt;
&lt;p&gt;We can do that in a concise way using a &lt;code&gt;repeat{}&lt;/code&gt; loop. Notice that it will keep running until both coordinates in the last row of the path are equal to 1.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;repeat{
  
  #find next step
  new.step &amp;lt;- least_cost_step(
    cost_matrix = m_cost, 
    last_step = tail(path, n = 1)
    )
  
  #join the new step with path
  path &amp;lt;- rbind(
    path, new.step,
    make.row.names = FALSE
    )
  
  #stop when step coordinates are 1, 1
  if(all(tail(path, n = 1) == 1)){break}
  
}

path
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    row col
## 1   13  13
## 2   12  13
## 3   12  12
## 4   11  12
## 5   10  12
## 6   10  11
## 7    9  11
## 8    9  10
## 9    9   9
## 10   9   8
## 11   8   8
## 12   7   8
## 13   7   7
## 14   6   7
## 15   6   6
## 16   5   6
## 17   5   5
## 18   5   4
## 19   4   4
## 20   4   3
## 21   3   3
## 22   3   2
## 23   3   1
## 24   2   1
## 25   1   1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The resulting least-cost path can be plotted on top of the cost matrix. Please, remember that the data is not pre-processed, and the plot below does not represent the real alignment (yet) between our target time series.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;graphics::image(
    x = seq_len(nrow(m_cost)),
    y = seq_len(ncol(m_cost)),
    z = m_cost,
    xlab = &amp;quot;zoo_germany&amp;quot;,
    ylab = &amp;quot;zoo_sweden&amp;quot;,
    main = &amp;quot;Cost Matrix and Least-Cost Path&amp;quot;
    )

graphics::lines(
  x = path$row, 
  y = path$col,
  lwd = 2
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-46-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;At this point we have all the pieces required to write the function &lt;code&gt;least_cost_path()&lt;/code&gt;. Notice that the &lt;code&gt;repeat{}&lt;/code&gt; statement is slightly more concise than before, as &lt;code&gt;least_cost_step()&lt;/code&gt; is directly wrapped within &lt;code&gt;rbind()&lt;/code&gt;. However, using &lt;code&gt;rbind()&lt;/code&gt; in a loop to add rows to a data frame is not a computationally efficient operation, but it was used here anyway because it makes the code more concise.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Least-Cost Path from Cost Matrix
#&#39; @param cost_matrix (required, matrix) cost matrix.
#&#39; @return data frame with least-cost path coordinates
least_cost_path &amp;lt;- function(cost_matrix){
  
  #first step of the least cost path
  path &amp;lt;- data.frame(
    row = nrow(cost_matrix),
    col = ncol(cost_matrix)
  )
  
  #iterate until path is completed
  repeat{
    
    #merge path with result of least_cost_step()
    path &amp;lt;- rbind(
      path, 
      #find next step
      least_cost_step(
        cost_matrix = cost_matrix, 
        last_step = tail(path, n = 1)
      ),
      make.row.names = FALSE
    )
    
    #stop when coordinates are 1, 1
    if(all(tail(path, n = 1) == 1)){break}
    
  }
  
  path
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can give it a go now to see that it works as expected.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;least_cost_path(cost_matrix = m_cost)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    row col
## 1   13  13
## 2   12  13
## 3   12  12
## 4   11  12
## 5   10  12
## 6   10  11
## 7    9  11
## 8    9  10
## 9    9   9
## 10   9   8
## 11   8   8
## 12   7   8
## 13   7   7
## 14   6   7
## 15   6   6
## 16   5   6
## 17   5   5
## 18   5   4
## 19   4   4
## 20   4   3
## 21   3   3
## 22   3   2
## 23   3   1
## 24   2   1
## 25   1   1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice, it worked, and now we have a nice least-cost path.&lt;/p&gt;
&lt;p&gt;But before continuing, there is just a little detail to notice about the least-cost path. Every time the same row index (such as &lt;code&gt;9&lt;/code&gt;) is linked to different column indices (&lt;code&gt;8&lt;/code&gt; to &lt;code&gt;11&lt;/code&gt;), it means that the samples of the time series identified by these column indices are having their time &lt;em&gt;compressed&lt;/em&gt; (or &lt;em&gt;warped&lt;/em&gt;) to the time of the row index. Hence, according to the least-cost path, the samples &lt;code&gt;8&lt;/code&gt; to &lt;code&gt;11&lt;/code&gt; of &lt;code&gt;zoo_sweden&lt;/code&gt; would be aligned in time with the sample &lt;code&gt;9&lt;/code&gt; of &lt;code&gt;zoo_germany&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now it&amp;rsquo;s time to use the least-cost path to quantify the similarity between the time series.&lt;/p&gt;
&lt;h3 id=&#34;dissimilarity-metric&#34;&gt;Dissimilarity Metric&lt;/h3&gt;
&lt;p&gt;The objective of dynamic time warping is to compute a metric of &lt;em&gt;dissimilarity&lt;/em&gt; (or &lt;em&gt;similarity&lt;/em&gt;, it just depends on the side from where you are looking at the issue) between time series. This operation requires two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Obtain the accumulated distance of the least cost path.&lt;/li&gt;
&lt;li&gt;Normalize the sum of distances by some number to help make results comparable across pairs of time series of different lengths.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, the method designed to build the cost matrix accumulates the distance of the least-cost path in the terminal cell, so we just have to extract it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;distance &amp;lt;- m_cost[nrow(m_cost), ncol(m_cost)]
distance
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 464.0019
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we need to find a number to normalize this distance value to make it comparable across different time series. There are several options, such as dividing &lt;code&gt;distance&lt;/code&gt; by the sum of lengths of the two time series, or by the length of the least-cost path (&lt;code&gt;nrow(path)&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;distance/sum(dim(m_cost))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 17.84623
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;distance/nrow(path)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18.56008
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another elegant normalization option requires computing the sum of distances between consecutive samples on each time series. This operation, named &lt;em&gt;auto-sum&lt;/em&gt;, requires applying &lt;code&gt;distance_euclidean()&lt;/code&gt; between the samples 1 and 2 of the given time series, then between the samples 2 and the 3, and so on until all consecutive sample pairs are processed, to finally sum all computed distances.&lt;/p&gt;
&lt;p&gt;To apply this operation to a time series we iterate between pairs of consecutive samples and save their distance in a vector (named &lt;code&gt;autodistance&lt;/code&gt; in the code below). Once the loop is done, then the auto-sum of the time series is the sum of this vector.&lt;/p&gt;
&lt;p&gt;For example, for &lt;code&gt;zoo_germany&lt;/code&gt; we would have:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#vector to store auto-distances
autodistance &amp;lt;- vector(
  mode = &amp;quot;numeric&amp;quot;, 
  length = nrow(zoo_germany) - 1
  )

#compute of row-to-row distance
for (row in 2:nrow(zoo_germany)) {
  autodistance[row - 1] &amp;lt;- distance_euclidean(
    x = zoo_germany[row, ],
    y = zoo_germany[row - 1, ]
  )
}

#compute autosum
sum(autodistance)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 425.7877
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we&amp;rsquo;ll need to apply this operation to many time series, it&amp;rsquo;s better to formalize this logic as a function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Time Series Autosum
#&#39; @param x (required, zoo object) time series
#&#39; @return numeric
auto_sum &amp;lt;- function(x){
  
  autodistance &amp;lt;- vector(
    mode = &amp;quot;numeric&amp;quot;, 
    length = nrow(x) - 1
  )
  
  for (row in 2:nrow(x)) {
    autodistance[row - 1] &amp;lt;- distance_euclidean(
      x = x[row, ],
      y = x[row - 1, ]
    )
  }
  
  sum(autodistance)
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now apply it to our two time series:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zoo_germany_autosum &amp;lt;- auto_sum(
  x = zoo_germany
)

zoo_germany_autosum
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 425.7877
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;zoo_sweden_autosum &amp;lt;- auto_sum(
  x = zoo_sweden
)

zoo_sweden_autosum
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 379.9118
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the auto-sum of both time series, we just have to add them together to obtain our normalization value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;normalizer &amp;lt;- zoo_germany_autosum + zoo_sweden_autosum
normalizer
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 805.6995
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we obtained &lt;code&gt;distance&lt;/code&gt; from the cost matrix and the &lt;code&gt;normalizer&lt;/code&gt; from the auto-sum of the two time series, we can compute our dissimilarity score, which follows the expression below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;((2 * distance) / normalizer) - 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1517989
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why this particular formula? Because it returns zero when comparing a time series with itself!&lt;/p&gt;
&lt;p&gt;When the two time series are the same, &lt;code&gt;2 * distance&lt;/code&gt; equals &lt;code&gt;normalizer&lt;/code&gt; because DTW and auto-sum are equivalent (sample-to-sample distances!), and dividing them returns one. We then subtract one to it, and get zero, which represents a perfect similarity score.&lt;/p&gt;
&lt;p&gt;The same affect cannot be achieved when using other normalization values, such as the sum of lengths of the time series, or the length of the least cost path.&lt;/p&gt;
&lt;p&gt;We can integrate these pieces into the function &lt;code&gt;dissimilarity_score()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Similarity Metric from Least Cost Path and Cost Matrix
#&#39; @param a (required, zoo object) time series.
#&#39; @param b (required, zoo object) time series with same columns as `x`
#&#39; @param cost_path (required, data frame) least cost path with the columns &amp;quot;row&amp;quot; and &amp;quot;col&amp;quot;.
#&#39; @return numeric, similarity metric
dissimilarity_score &amp;lt;- function(a, b, cost_matrix){
  
  #distance of the least cost path
  distance &amp;lt;- cost_matrix[nrow(cost_matrix), ncol(cost_matrix)]
  
  #compute normalization factor from autosum
  autosum_a &amp;lt;- auto_sum(x = a)
    
  autosum_b &amp;lt;- auto_sum(x = b)
  
  normalizer &amp;lt;- autosum_a + autosum_b
  
  #compute dissimilarity
  psi &amp;lt;- ((2 * distance) / normalizer) - 1
  
  psi
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can give it a test run now:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dissimilarity_score(
  a = zoo_germany,
  b = zoo_sweden,
  cost_matrix = m_cost
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1517989
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the &lt;code&gt;dissimilarity_score()&lt;/code&gt; function ready, it is time to go write our main function!&lt;/p&gt;
&lt;h2 id=&#34;main-function&#34;&gt;Main Function&lt;/h2&gt;
&lt;p&gt;To recapitulate before moving forward, we have the following functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ts_preprocessing()&lt;/code&gt; applies linear detrending and z-score normalization to a time series.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;distance_matrix()&lt;/code&gt; and &lt;code&gt;distance_euclidean()&lt;/code&gt; work together to compute a distance matrix.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cost_matrix()&lt;/code&gt; transforms the distance matrix into a cost matrix.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;least_cost_path()&lt;/code&gt; applies &lt;code&gt;least_cost_step()&lt;/code&gt; recursively to build a least-cost path.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dissimilarity_score()&lt;/code&gt;, which calls &lt;code&gt;auto_sum()&lt;/code&gt; and quantifies time series dissimilarity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can wrap together all these functions into a new one with the unimaginative name &lt;code&gt;dynamic_time_warping()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Similarity Between Time Series
#&#39; @param a (required, zoo object) time series.
#&#39; @param b (required, zoo object) time series with same columns as `x`
#&#39; @param plot (optional, logical) if `TRUE`, a dynamic time warping plot is produced.
#&#39; @return psi score
dynamic_time_warping &amp;lt;- function(a, b, plot = FALSE){
  
  #linear detrending and z-score normalization
  a_ &amp;lt;- ts_preprocessing(x = a)
  b_ &amp;lt;- ts_preprocessing(x = b)
  
  #distance matrix
  m_dist &amp;lt;- distance_matrix(
    a = a_,
    b = b_
  )
  
  #cost matrix
  m_cost &amp;lt;- cost_matrix(
    distance_matrix = m_dist
  )
  
  #least-cost path
  cost_path &amp;lt;- least_cost_path(
    cost_matrix = m_cost
  )
  
  #similarity metric
  score &amp;lt;- dissimilarity_score(
    a = a_,
    b = b_,
    cost_matrix = m_cost
  )
  
  #plot
  if(plot == TRUE){
    
    graphics::image(
      x = seq_len(nrow(m_cost)),
      y = seq_len(ncol(m_cost)),
      z = m_cost,
      xlab = &amp;quot;a&amp;quot;,
      ylab = &amp;quot;b&amp;quot;,
      main = paste0(&amp;quot;Similarity score = &amp;quot;, round(score, 3))
    )
    
    graphics::lines(
      x = cost_path$row, 
      y = cost_path$col,
      lwd = 2
    )
    
  }
  
  score
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before I said that one advantage of the presented dissimilarity formula is that it returns 0 when comparing a time series with itself. Let&amp;rsquo;s see if that&amp;rsquo;s true by comparing &lt;code&gt;zoo_germany&lt;/code&gt; with itself:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dynamic_time_warping(
  a = zoo_germany,
  b = zoo_germany
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That&amp;rsquo;s good, right? Perfect similarity happens at 0, which is a lovely number to start with. If we now compare &lt;code&gt;zoo_germany&lt;/code&gt; and &lt;code&gt;zoo_sweden&lt;/code&gt;, we should expect a larger number:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dynamic_time_warping(
  a = zoo_germany,
  b = zoo_sweden
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2366642
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, when comparing &lt;code&gt;zoo_sweden&lt;/code&gt; with &lt;code&gt;zoo_spain&lt;/code&gt; we should expect an even higher dissimilarity score, given that they are quite far apart. As a bonus, we let the function plot their alignment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dynamic_time_warping(
  a = zoo_sweden,
  b = zoo_spain,
  plot = TRUE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping-from-scratch/index_files/figure-html/unnamed-chunk-63-1.png&#34; width=&#34;672&#34; /&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.509285
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, that was freaking long, but these are such nice results, aren&amp;rsquo;t they? I hope it was worth it! One minute more, and we are done.&lt;/p&gt;
&lt;h2 id=&#34;library-source&#34;&gt;Library Source&lt;/h2&gt;
&lt;p&gt;Once we have all our DTW functions written and tested, the easiest way to make them usable without any extra hassle is to write them to a source file. Having them all in a single file allows loading them at once via the &lt;code&gt;source()&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;For example, if our library file is 
&lt;a href=&#34;https://www.dropbox.com/scl/fi/z2n9hnenxwuxwol5i0zcn/dtw.R?rlkey=bhsyew12r1glnqihtnocxwri6&amp;amp;dl=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;dtw.R&lt;/code&gt;&lt;/a&gt;, running &lt;code&gt;source(&amp;quot;dtw.R&amp;quot;)&lt;/code&gt; will load all functions into your R environment and they will be readily available for your DTW analysis.&lt;/p&gt;
&lt;h2 id=&#34;closing-thoughts&#34;&gt;Closing Thoughts&lt;/h2&gt;
&lt;p&gt;I hope you found this tutorial useful in one way or another. Writing a methodological library from scratch is hard work. There are many moving parts to consider, many concepts that need to be mapped and then translated into code, that making mistakes becomes exceedingly easy. Never worry about that and take your time until things start clicking.&lt;/p&gt;
&lt;p&gt;But above everything, enjoy the learning journey!&lt;/p&gt;
&lt;h2 id=&#34;coming-next&#34;&gt;Coming Next&lt;/h2&gt;
&lt;p&gt;In my TODO list there is a post focused on identifying computational bottlenecks in the DTW library we just wrote, and optimize the parts worth optimizing. There&amp;rsquo;s no timeline yet though, so stay tuned!&lt;/p&gt;
&lt;p&gt;Blas&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R package collinear</title>
      <link>https://blasbenito.com/project/collinear/</link>
      <pubDate>Sun, 12 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/project/collinear/</guid>
      <description>&lt;!-- badges: start --&gt;
&lt;p&gt;
&lt;a href=&#34;https://doi.org/10.5281/zenodo.10039489&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.10039489.svg&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://cran.r-project.org/package=collinear&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version/collinear&#34; alt=&#34;CRAN status&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://CRAN.R-project.org/package=collinear&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://cranlogs.r-pkg.org/badges/grand-total/collinear&#34; alt=&#34;CRAN\_Download\_Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;!-- badges: end --&gt;
&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.blasbenito.com/post/multicollinearity-model-interpretability/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multicollinearity hinders the interpretability&lt;/a&gt; of linear and machine learning models.&lt;/p&gt;
&lt;p&gt;The R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;, 
&lt;a href=&#34;https://CRAN.R-project.org/package=collinear&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available on CRAN&lt;/a&gt;, combines four methods for easy management of multicollinearity in modelling data frames with numeric and categorical variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Target Encoding&lt;/strong&gt;: Transforms categorical predictors to numeric using a numeric response as reference.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Preference Order&lt;/strong&gt;: Ranks predictors by their association with a response variable to preserve important ones in multicollinearity filtering.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pairwise Correlation Filtering&lt;/strong&gt;: Automated multicollinearity filtering of numeric and categorical predictors based&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;main-improvements-in-version-200&#34;&gt;Main Improvements in Version 2.0.0&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Expanded Functionality&lt;/strong&gt;: Functions &lt;code&gt;collinear()&lt;/code&gt; and &lt;code&gt;preference_order()&lt;/code&gt; support both categorical and numeric responses and predictors, and can handle several responses at once.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Robust Selection Algorithms&lt;/strong&gt;: Enhanced selection in &lt;code&gt;vif_select()&lt;/code&gt; and &lt;code&gt;cor_select()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enhanced Functionality to Rank Predictors&lt;/strong&gt;: New functions to compute association between response and predictors covering most use-cases, and automated function selection depending on data features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simplified Target Encoding&lt;/strong&gt;: Streamlined and parallelized for better efficiency, and new default is &amp;ldquo;loo&amp;rdquo; (leave-one-out).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallelization and Progress Bars&lt;/strong&gt;: Utilizes &lt;code&gt;future&lt;/code&gt; and &lt;code&gt;progressr&lt;/code&gt; for enhanced performance and user experience.on pairwise correlations.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Variance Inflation Factor Filtering&lt;/strong&gt;: Automated multicollinearity filtering of numeric predictors based on Variance Inflation Factors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The article 
&lt;a href=&#34;https://blasbenito.github.io/collinear/articles/how_it_works.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How It Works&lt;/a&gt; explains how the package works in detail.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you find this package useful, please cite it as:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Blas M. Benito (2024). collinear: R Package for Seamless Multicollinearity Management. Version 2.0.0. doi: 10.5281/zenodo.10039489&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R package distantia</title>
      <link>https://blasbenito.com/project/distantia/</link>
      <pubDate>Sun, 12 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/project/distantia/</guid>
      <description>&lt;!-- badges: start --&gt;
&lt;p&gt;
&lt;a href=&#34;https://zenodo.org/badge/latestdoi/187805264&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/187805264.svg&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://CRAN.R-project.org/package=distantia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version-ago/distantia&#34; alt=&#34;CRAN\_Release\_Badge&#34;&gt;&lt;/a&gt;

&lt;a href=&#34;https://CRAN.R-project.org/package=distantia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://cranlogs.r-pkg.org/badges/distantia&#34; alt=&#34;CRAN\_Download\_Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;!-- badges: end --&gt;
&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;The R package 
&lt;a href=&#34;https://blasbenito.github.io/distantia/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;&lt;code&gt;distantia&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;, 
&lt;a href=&#34;https://CRAN.R-project.org/package=distantia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available on CRAN&lt;/a&gt;, offers an efficient, feature-rich toolkit for managing, comparing, and analyzing time series data. It is designed to handle a wide range of scenarios, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multivariate and univariate time series.&lt;/li&gt;
&lt;li&gt;Regular and irregular sampling.&lt;/li&gt;
&lt;li&gt;Time series of different lengths.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;h3 id=&#34;comprehensive-analytical-tools&#34;&gt;Comprehensive Analytical Tools&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;10 distance metrics: see &lt;code&gt;distantia::distances&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The normalized dissimilarity metric &lt;code&gt;psi&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Free and Restricted Dynamic Time Warping (DTW) for shape-based comparison.&lt;/li&gt;
&lt;li&gt;A Lock-Step method for sample-to-sample comparison&lt;/li&gt;
&lt;li&gt;Restricted permutation tests for robust inferential support.&lt;/li&gt;
&lt;li&gt;Analysis of contribution to dissimilarity of individual variables in multivariate time series.&lt;/li&gt;
&lt;li&gt;Hierarchical and K-means clustering of time series based on dissimilarity matrices.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;computational-efficiency&#34;&gt;Computational Efficiency&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;C++ back-end&lt;/strong&gt; powered by 
&lt;a href=&#34;https://www.rcpp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rcpp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel processing&lt;/strong&gt; managed through the 
&lt;a href=&#34;https://future.futureverse.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;future&lt;/a&gt; package.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficient data handling&lt;/strong&gt; via 
&lt;a href=&#34;https://CRAN.R-project.org/package=zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;zoo&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;time-series-management-tools&#34;&gt;Time Series Management Tools&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Introduces &lt;strong&gt;time series lists&lt;/strong&gt; (TSL), a versatile format for handling collections of time series stored as lists of &lt;code&gt;zoo&lt;/code&gt; objects.&lt;/li&gt;
&lt;li&gt;Includes a suite of &lt;code&gt;tsl_...()&lt;/code&gt; functions for generating, resampling, transforming, analyzing, and visualizing univariate and multivariate time series.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you find this package useful, please cite it as:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Blas M. Benito, H. John B. Birks (2020). distantia: an open-source toolset to quantify dissimilarity between multivariate ecological time-series. Ecography, 43(5), 660-667. doi: 
&lt;a href=&#34;https://nsojournals.onlinelibrary.wiley.com/doi/10.1111/ecog.04895&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.1111/ecog.04895&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Blas M. Benito (2024). distantia: A Toolset for Time Series Dissimilarity Analysis. R package version 2.0.0. url:  
&lt;a href=&#34;https://blasbenito.github.io/distantia/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://blasbenito.github.io/distantia/&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Gentle Intro to Dynamic Time Warping</title>
      <link>https://blasbenito.com/post/dynamic-time-warping/</link>
      <pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/dynamic-time-warping/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;This post provides a gentle conceptual introduction to &lt;em&gt;Dynamic Time Warping&lt;/em&gt; (DTW), a method to compare time series of different lengths that has found its way into our daily lives. It starts with a very general introduction to the comparison of time series, follows with a bit of history about its development and a step-by-step breakdown, to finalize with a summary of its real-world applications.&lt;/p&gt;
&lt;h1 id=&#34;comparing-time-series&#34;&gt;Comparing Time Series&lt;/h1&gt;
&lt;p&gt;Time series comparison is a critical task in many fields, such as environmental monitoring, finance, and healthcare. The goal is often to quantify similarities or differences between pairs of time series to gain insights into how the data is structured and identify meaningful patterns.&lt;/p&gt;
&lt;p&gt;For example, the data below shows time series representing the same phenomenon in three different places and time ranges: &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; have 30 synchronized observations, while &lt;code&gt;c&lt;/code&gt; has 20 observations from a different year.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;750&#34; /&gt;
&lt;p&gt;There are several options to compare &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; directly, such as assessing their correlation (0.955), or computing the sum of Euclidean distances between their respective samples (2.021).&lt;/p&gt;
&lt;p&gt;This comparison approach is named &lt;em&gt;lock-step&lt;/em&gt; (also known as &lt;em&gt;inelastic comparison&lt;/em&gt;), and works best when the time series represent phenomena with relatively similar shapes and are aligned in time and frequency, as it is the case with &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Comparing &lt;code&gt;c&lt;/code&gt; with &lt;code&gt;a&lt;/code&gt; and/or &lt;code&gt;b&lt;/code&gt; is a completely different task though, exactly the one 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Dynamic_time_warping&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic Time Warping&lt;/a&gt; was designed to address.&lt;/p&gt;
&lt;p&gt;Now it would make sense to explain right away what dynamic time warping is and how it works, but there&amp;rsquo;s a bit of history to explore first.&lt;/p&gt;
&lt;h1 id=&#34;a-bit-of-history&#34;&gt;A Bit of History&lt;/h1&gt;
&lt;p&gt;Dynamic Time Warping (DTW) might sound like a modern high-tech buzzword, but its roots go way back—older than me (gen X guy here!). This powerful method was first developed in the pioneering days of speech recognition. The earliest reference I uncovered is from Shearme and Leach’s 1968 paper, 
&lt;a href=&#34;https://doi.org/10.1109/TAU.1968.1161985&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Some experiments with a simple word recognition system&lt;/em&gt;&lt;/a&gt;, published by the Joint Speech Research Unit in the UK.&lt;/p&gt;
&lt;p&gt;These foundational ideas were later expanded upon by Sakoe and Chiba in their seminal 1971 paper, 
&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:107516844&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;A Dynamic Programming Approach to Continuous Speech Recognition&lt;/em&gt;&lt;/a&gt;, often regarded as the definitive starting point for modern DTW applications.&lt;/p&gt;
&lt;p&gt;From there, DTW has found applications in diverse fields relying on time-dependent data, such as 
&lt;a href=&#34;https://doi.org/10.1016/j.bspc.2024.106677&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;medical sciences&lt;/a&gt;, 
&lt;a href=&#34;https://doi.org/10.1371/journal.pone.0272848&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sports analytics&lt;/a&gt;, 
&lt;a href=&#34;https://iopscience.iop.org/article/10.3847/1538-4357/ac4af6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;astronomy&lt;/a&gt;, 
&lt;a href=&#34;https://doi.org/10.1016/j.eneco.2020.105036&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;econometrics&lt;/a&gt;, 
&lt;a href=&#34;https://www.mdpi.com/2079-9292/8/11/1306&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;robotics&lt;/a&gt;, 
&lt;a href=&#34;https://doi.org/10.1111/exsy.13237&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;epidemiology&lt;/a&gt;, and many others.&lt;/p&gt;
&lt;p&gt;Ok, let&amp;rsquo;s stop wandering in time, and go back to the meat in this post.&lt;/p&gt;
&lt;h1 id=&#34;what-is-dynamic-time-warping&#34;&gt;What is &lt;em&gt;Dynamic Time Warping&lt;/em&gt;?&lt;/h1&gt;
&lt;p&gt;Dynamic Time Warping is a method to compare univariate or multivariate time series of different length, timing, and/or shape. To do so, DTW stretches or compresses parts of the time series (hence &lt;em&gt;warping&lt;/em&gt;) until it finds the alignment that minimizes their overall differences. Think of it as a way to match the rhythm of two songs even if one plays faster than the other.&lt;/p&gt;
&lt;p&gt;The figure below represents a dynamic time warping solution for the time series &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;a&lt;/code&gt;. Notice how each sample in one time series matches one or several samples from the other. These matches are optimized to minimize the sum of distances between the samples they connect (3.285 in this case). Any other combination of matches would result in a higher sum of distances.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;750&#34; /&gt;
&lt;p&gt;In dynamic time warping, the actual &lt;em&gt;warping&lt;/em&gt; happens when a sample in one time series is matched with two or more samples from the other, independently of their observation times. The figure below identifies one of these instances with blue bubbles. The sample 10 of &lt;code&gt;c&lt;/code&gt; (upper blue bubble), with date 2022-07-16, is matched with the samples 14 to 16 of &lt;code&gt;a&lt;/code&gt; (lower bubble), with dates 2023-12-13 to 2024-03-09. This matching structure represents a time compression in &lt;code&gt;a&lt;/code&gt; for the range of involved dates.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;750&#34; /&gt;
&lt;p&gt;This ability to warp time makes DTW incredibly useful for analyzing time series that are similar in shape but don&amp;rsquo;t have the same length or are not fully synchronized.&lt;/p&gt;
&lt;p&gt;The next section delves into the computational steps of DTW.&lt;/p&gt;
&lt;h1 id=&#34;dtw-step-by-step&#34;&gt;DTW Step by Step&lt;/h1&gt;
&lt;p&gt;Time series comparison via Dynamic Time Warping (DTW) involves several key steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Detrending and z-score normalization&lt;/strong&gt; of the time series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computation of the distance matrix&lt;/strong&gt; between all pairs of samples.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computation of a cost matrix&lt;/strong&gt; from the distance matrix.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finding the least-cost path&lt;/strong&gt; within the cost matrix.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computation of a similarity metric&lt;/strong&gt; based on the least-cost path.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;detrending-and-z-score-normalization&#34;&gt;Detrending and Z-score Normalization&lt;/h2&gt;
&lt;p&gt;DTW is highly sensitive to differences in trends and ranges between time series (see the &lt;em&gt;Pitfalls&lt;/em&gt; section). To address this, 
&lt;a href=&#34;https://sherbold.github.io/intro-to-data-science/09_Time-Series-Analysis.html#Trend-and-Seasonal-Effects&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;detrending&lt;/a&gt; and 
&lt;a href=&#34;https://developers.google.com/machine-learning/crash-course/numerical-data/normalization#z-score_scaling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;z-score normalization&lt;/a&gt; are important preprocessing steps. The former removes any upwards or downwards trend in the time series, while the later scales the time series values to a mean of zero and a standard deviation of one.&lt;/p&gt;
&lt;p&gt;In this example, the time series &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; already have matching ranges, so normalization is not strictly necessary. For demonstration purposes, however, the figure below shows them normalized using z-score scaling:&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;750&#34; /&gt;
&lt;h2 id=&#34;distance-matrix&#34;&gt;Distance Matrix&lt;/h2&gt;
&lt;p&gt;This step involves computing the distance matrix, which contains pairwise distances between all combinations of samples in the two time series.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;600&#34; /&gt;
&lt;p&gt;Choosing an appropriate distance metric is crucial. While Euclidean distance works well in many cases, other metrics may be more suitable depending on the data.&lt;/p&gt;
&lt;h2 id=&#34;cost-matrix&#34;&gt;Cost Matrix&lt;/h2&gt;
&lt;p&gt;The cost matrix is derived from the distance matrix by accumulating distances recursively, neighbor to neighbor, from the starting corner (lower-left) to the ending one (upper-right).&lt;/p&gt;
&lt;p&gt;Different rules for cell neighborhood determine how these costs propagate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Orthogonal only&lt;/strong&gt;: Accumulation occurs in the &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; directions only, ignoring diagonals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orthogonal and diagonal&lt;/strong&gt;: Diagonal movements are also considered, typically weighted by a factor of &lt;code&gt;√2&lt;/code&gt; (1.414) to balance with orthogonal movements.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The figure below illustrates the cost matrix with both orthogonal and diagonal paths.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;600&#34; /&gt;
&lt;p&gt;The result of the cost matrix is similar to the topographic map of a valley, in which the value of each cell represents the slope we have to overcome to walk through it.&lt;/p&gt;
&lt;p&gt;Now that we have a valley, let&amp;rsquo;s go create a river!&lt;/p&gt;
&lt;h2 id=&#34;least-cost-path&#34;&gt;Least-cost Path&lt;/h2&gt;
&lt;p&gt;This is the step where the actual time warping happens!&lt;/p&gt;
&lt;p&gt;The least-cost path minimizes the total cost from the start to the end of the cost matrix, aligning the time series optimally.&lt;/p&gt;
&lt;p&gt;The algorithm building the least-cost path starts on the upper right corner of the cost matrix, and recursively selects the next neighbor with the lowest cumulative cost to build the least-cost path step by step. This process is similar to letting a river find its shortest path way down a valley.&lt;/p&gt;
&lt;p&gt;The figure below shows the least-cost path (black line). Deviations from the diagonal represent adjustments made to align the time series.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/dynamic-time-warping/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;600&#34; /&gt;
&lt;h2 id=&#34;similarity-metric&#34;&gt;Similarity Metric&lt;/h2&gt;
&lt;p&gt;Finally, DTW produces a similarity metric based on the least-cost path. The simplest approach is to sum the distances of all points along the path.&lt;/p&gt;
&lt;p&gt;For this example, the total cost is 7.588.
However, when comparing time series of varying lengths, normalization is often useful. Common options include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sum of lengths&lt;/strong&gt;: Normalize by the combined lengths of the time series, e.g., &lt;code&gt;Normalized Cost = Total Cost / (Length(a) + Length(c))&lt;/code&gt;. For &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;, this would be 0.152.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Auto-sum of distances&lt;/strong&gt;: Normalize by the sum of distances between adjacent samples in each series, as in &lt;code&gt;Normalized Cost = Total Cost / (Auto-sum(a) + Auto-sum(c))&lt;/code&gt;. For &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;, this results in 0.358.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These normalized metrics allow comparisons across datasets with varying characteristics.&lt;/p&gt;
&lt;h1 id=&#34;real-world-applications&#34;&gt;Real World Applications&lt;/h1&gt;
&lt;p&gt;Dynamic Time Warping is a well-studied topic in academic community, with more than 86k research articles listed in 
&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=%22dynamic&amp;#43;time&amp;#43;warping%22&amp;amp;btnG=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Scholar&lt;/a&gt;. However, the real-world impact of an academic concept often differs from its academic popularity. Examining DTW-related patents provides a clearer view of its practical applications.&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://www.uspto.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;United States Patent and Trademark Office&lt;/a&gt; includes the class 
&lt;a href=&#34;https://patents.justia.com/patents-by-us-classification/704/241&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;704/241&lt;/a&gt; specifically for &amp;ldquo;Dynamic Time Warping Patents&amp;rdquo;. Similarly, the 
&lt;a href=&#34;https://www.cooperativepatentclassification.org/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cooperative Patent Classification&lt;/a&gt; includes the classification 
&lt;a href=&#34;https://www.uspto.gov/web/patents/classification/cpc/pdf/cpc-scheme-G10L.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(G10L 15/12)&lt;/a&gt; with the title &amp;ldquo;Speech recognition using dynamic programming techniques, e.g. dynamic time warping (DTW)&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Using the 
&lt;a href=&#34;https://www.epo.org/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;European Patent Office&lt;/a&gt; search tool 
&lt;a href=&#34;https://worldwide.espacenet.com/patent/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spacenet&lt;/a&gt; for &amp;ldquo;Dynamic Time Warping&amp;rdquo; returns approximately 
&lt;a href=&#34;https://worldwide.espacenet.com/patent/search/family/007654522/publication/US2002049591A1?q=%22dynamic%20time%20warping%22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;12.000 results&lt;/a&gt;. 
&lt;a href=&#34;https://patents.google.com/?q=%28%22dynamic&amp;#43;time&amp;#43;warping%22%29&amp;amp;oq=%22dynamic&amp;#43;time&amp;#43;warping%22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Patents&lt;/a&gt; reports over 35.000 results.&lt;/p&gt;
&lt;p&gt;While patents illustrate the technical implementation of DTW, uncovering its application in company blogs, wikis, or manuals is more challenging. Nonetheless, a few compelling examples demonstrate DTW&amp;rsquo;s real-world utility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Media and Entertainment&lt;/strong&gt;*&lt;/p&gt;
&lt;p&gt;Closed caption alignment is perhaps the most pervasive yet invisible application of DTW. Companies like 
&lt;a href=&#34;https://netflixtechblog.com/detecting-scene-changes-in-audiovisual-content-77a61d3eaad6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Netflix&lt;/a&gt; and 
&lt;a href=&#34;https://patents.google.com/patent/US20150271442A1/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft&lt;/a&gt; use DTW to synchronize subtitles with soundtracks in movies, TV shows, and video games, ensuring an accurate match regardless of pacing or timing inconsistencies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Wearables and Fitness Devices&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Many wearables employ DTW to classify user activities by aligning accelerometer and gyroscope data with predefined templates. For example, 
&lt;a href=&#34;https://patents.google.com/patent/US11517789B2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goertek&amp;rsquo;s&lt;/a&gt; &lt;em&gt;Comma 2&lt;/em&gt; smart ring 
&lt;a href=&#34;https://sleepreviewmag.com/sleep-diagnostics/consumer-sleep-tracking/wearable-sleep-trackers/goertek-reveals-smart-ring-reference-designs-voice-gesture-controls/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;presented in January 2025&lt;/a&gt; uses DTW to recognize user movements. Another creative example is the 
&lt;a href=&#34;https://genkiinstruments.com/products/wave&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wave&lt;/a&gt; MIDI controller ring, 
&lt;a href=&#34;https://patents.google.com/patent/US20220085841A1/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;patented by Genki&lt;/a&gt;. This device applies DTW with a nearest-neighbor classifier to analyze hand movements and trigger musical effects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Biomechanics&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In biomechanics, DTW helps analyze movement patterns and detect anomalies. For instance, the software 
&lt;a href=&#34;https://wiki.has-motion.com/doku.php?id=sift:sift_overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sift&lt;/a&gt; by 
&lt;a href=&#34;https://www.has-motion.ca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HAS Motion&lt;/a&gt; 
&lt;a href=&#34;https://wiki.has-motion.com/doku.php?id=sift:dynamic_time_warping:dynamic_time_warping&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;uses DTW&lt;/a&gt; to compare large datasets of movement traces and identify deviations. Similarly, the 
&lt;a href=&#34;https://orthoload.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OrthoLoad&lt;/a&gt; processes load measurements on joint implants 
&lt;a href=&#34;https://orthoload.com/software/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;using DTW&lt;/a&gt; to analyze patterns and identify irregularities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Industrial Applications&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DTW is also used in manufacturing to monitor machinery health.  Toshiba&amp;rsquo;s 
&lt;a href=&#34;https://www.global.toshiba/ww/technology/corporate/rdc/rd/topics/20/2006-01.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LAMTSS&lt;/a&gt; technology applies DTW to noise and motion data from manufacturing equipment, helping detect and predict operational failures before they occur.&lt;/p&gt;
&lt;p&gt;These examples highlight the versatility and practical relevance of DTW, spanning industries from entertainment to biomechanics and industrial maintenance. Its ability to adapt to diverse time series challenges underscores its value in real-world problem-solving.&lt;/p&gt;
&lt;h1 id=&#34;closing-thoughts&#34;&gt;Closing Thoughts&lt;/h1&gt;
&lt;p&gt;Dynamic Time Warping exemplifies how a sophisticated algorithm, initially developed for niche applications, has evolved into a versatile tool with real-world significance. From aligning movie subtitles to monitoring machinery health, DTW bridges the gap between academic theory and practical innovation. Its ability to adapt to various industries highlights the importance of robust time series analysis techniques, and further cements its place in both research and applied fields.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Reading List: Data Science</title>
      <link>https://blasbenito.com/post/my-reading-list-data-science/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/my-reading-list-data-science/</guid>
      <description>&lt;p&gt;This is a live post listing links to Data Science related posts and videos I consider to be interesting, high-quality, or even essential to better understand particular topics within such a wide field.&lt;/p&gt;
&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://towardsdatascience.com/extending-target-encoding-443aa9414cae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Extending Target Encoding&lt;/strong&gt;&lt;/a&gt;: post by 
&lt;a href=&#34;https://www.aitimejournal.com/interview-with-daniele-micci-barreca-product-analytics-lead-data-science-google/30110/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniele Micci-Barreca&lt;/a&gt; explaining how he came up with the idea of target encoding, and its possible extensions.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://maxhalford.github.io/blog/target-encoding/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Target encoding done the right way&lt;/strong&gt;&lt;/a&gt;: post by 
&lt;a href=&#34;https://maxhalford.github.io/bio/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Max Halford&lt;/a&gt;, Head of Data at 
&lt;a href=&#34;https://www.carbonfact.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Carbonfact&lt;/a&gt;, explaining in detail how to combine additive smoothing and target encoding.&lt;/p&gt;
&lt;h2 id=&#34;handling-and-management&#34;&gt;Handling and Management&lt;/h2&gt;
&lt;h3 id=&#34;apache-parquet&#34;&gt;Apache Parquet&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://airbyte.com/data-engineering-resources/parquet-data-format&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A Deep Dive into Parquet: The Data Format Engineers Need to Know&lt;/strong&gt;&lt;/a&gt;: This by Aditi Prakash, published in the 
&lt;a href=&#34;https://airbyte.com/blog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Airbyte Blog&lt;/a&gt; offers a complete guide about the 
&lt;a href=&#34;https://parquet.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apache Parquet&lt;/a&gt; file format.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.influxdata.com/blog/querying-parquet-millisecond-latency/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Querying Parquet with Millisecond Latency&lt;/strong&gt;&lt;/a&gt; this post from by Raphael Taylor-Davies and Andrew Lamb explains in deep the optimization methods used in 
&lt;a href=&#34;https://parquet.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apache Parquet&lt;/a&gt; files. Warning, this is a very technical read!&lt;/p&gt;
&lt;h3 id=&#34;duckdb&#34;&gt;DuckDB&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://duckdb.org/2024/01/26/multi-database-support-in-duckdb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Multi-Database Support in DuckDB&lt;/strong&gt;&lt;/a&gt; This post by Mark Raasveldt published in the DuckDB blog explains how to query together data from different databases at once.&lt;/p&gt;
&lt;h1 id=&#34;analysis-and-modeling&#34;&gt;Analysis and Modeling&lt;/h1&gt;
&lt;h2 id=&#34;modeling-methods&#34;&gt;Modeling Methods&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://loreley.one/2024-09-pca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Unraveling Principal Component Analysis&lt;/strong&gt;&lt;/a&gt;: This book, reviewed 
&lt;a href=&#34;https://loreley.one/2024-09-pca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, is a &lt;em&gt;tour of linear algebra&lt;/em&gt; focused on intuitive explanations rather than mathematical demonstrations.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://m-clark.github.io/posts/2019-10-20-big-mixed-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Mixed Models for Big Data&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://m-clark.github.io/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Clark&lt;/a&gt; (see entry below by the same author) reviews several mixed modelling approach for large data in R.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://m-clark.github.io/generalized-additive-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Generalized Additive Models&lt;/strong&gt;&lt;/a&gt;: A good online book on Generalized Additive Models by 
&lt;a href=&#34;https://m-clark.github.io/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Clark&lt;/a&gt;, Senior Machine Learning Scientist at 
&lt;a href=&#34;https://www.strong.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Strong Analytics&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;model-explainability&#34;&gt;Model Explainability&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://towardsdatascience.com/a-simple-model-independent-score-explanation-method-c17002d66da7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Model-Independent Score Explanation&lt;/strong&gt;&lt;/a&gt;: Post by 
&lt;a href=&#34;https://www.aitimejournal.com/interview-with-daniele-micci-barreca-product-analytics-lead-data-science-google/30110/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniele Micci-Barreca&lt;/a&gt; on model explainability. It also explains a very clever method to better understand any model just from it&amp;rsquo;s predictions.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AI Explanations whitepaper&lt;/strong&gt;&lt;/a&gt;: White paper of Google&amp;rsquo;s &amp;ldquo;AI Explanations&amp;rdquo; product with a pretty good overall view of the state of the art of model explainability.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1702.08608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Towards A Rigorous Science of Interpretable Machine Learning&lt;/strong&gt;&lt;/a&gt;: Pre-print by Finale Doshi-Velez and Been Kim offering a rigorous definition and evaluation of model interpretability.&lt;/p&gt;
&lt;h2 id=&#34;spatial-analysis&#34;&gt;Spatial Analysis&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://duckdb.org/2023/04/28/spatial.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;PostGEESE? Introducing The DuckDB Spatial Extension&lt;/strong&gt;&lt;/a&gt;: In this post, the authors of 
&lt;a href=&#34;https://duckdb.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DuckDB&lt;/a&gt; present the new PostGIS-like &lt;em&gt;spatial&lt;/em&gt; extension for this popular in-process data base engine.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://py.geocompx.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Geocomputation with Python&lt;/strong&gt;&lt;/a&gt;: A very nice book on geographic data analysis with Python.&lt;/p&gt;
&lt;h1 id=&#34;coding&#34;&gt;Coding&lt;/h1&gt;
&lt;h2 id=&#34;general-concepts&#34;&gt;General Concepts&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://archive.org/details/a-philosophy-of-software-design/mode/2up&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A Philosophy Of Software Design&lt;/strong&gt;&lt;/a&gt;: This book by 
&lt;a href=&#34;https://web.stanford.edu/~ouster/cgi-bin/home.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Ousterhout&lt;/a&gt; is full of high-level concepts and tips to help tackle software complexity. It&amp;rsquo;s so good I had to buy a hard copy that now lives in my desk. 
&lt;a href=&#34;https://blog.pragmaticengineer.com/a-philosophy-of-software-design-review/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This post&lt;/a&gt; by 
&lt;a href=&#34;https://blog.pragmaticengineer.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gergely Orosz&lt;/a&gt; offers a balanced review of the book.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://youtu.be/CFRhGnuXG-4?si=7Xr3E9L7GFvoRJqA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Why You Shouldn&amp;rsquo;t Nest Your Code&lt;/strong&gt;&lt;/a&gt;: In this wonderful video, 
&lt;a href=&#34;https://www.youtube.com/@CodeAesthetic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CodeAesthetic&lt;/a&gt; explains in detail (and beautiful graphics!) a couple of methods to reduce the level of nesting in our code to improve readability and maintainability. This video has truly changed how I code in R!&lt;/p&gt;
&lt;h2 id=&#34;r&#34;&gt;R&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://ropensci.org/blog/2024/02/22/beautiful-code/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Beautiful Code, Because We’re Worth It!&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://mastodon.social/@maelle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maëlle Salmon&lt;/a&gt; (research software engineer), and 
&lt;a href=&#34;https://fosstodon.org/@yabellini&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yanina Bellini Saibene&lt;/a&gt; (rOpenSci Community Manager) provides simple tips to help write more visually pleasant R code.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://journal.r-project.org/articles/RJ-2023-071/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Coloring in R’s Blind Spot&lt;/strong&gt;&lt;/a&gt;: This article published in 
&lt;a href=&#34;https://journal.r-project.org/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The R Journal&lt;/a&gt; by 
&lt;a href=&#34;https://www.zeileis.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Achim Zeileis&lt;/a&gt; (he has a 
&lt;a href=&#34;https://www.zeileis.org/blog/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;great analytics blog&lt;/a&gt; too!) and 
&lt;a href=&#34;https://www.stat.auckland.ac.nz/~paul/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paul Murrel&lt;/a&gt; offers a great overview of the base R color functions, and offers specific advice on what color palettes work better in different scenarios.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://peerj.com/preprints/26605v1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Taking R to its limits: 70+ tips&lt;/strong&gt;&lt;/a&gt;: This pre-print (not peer-reviewed AFAIK) by Tsagris and Papadakis offers a long list of tips to speed-up computation with the R language. I think a few of these tips lack enough context or are poorly explained, but it&amp;rsquo;s still a good resource to help optimize our R code.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.njtierney.com/post/2023/12/06/long-errors-smell/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Code Smell: Error Handling Eclipse&lt;/strong&gt; &lt;/a&gt;: This post by 
&lt;a href=&#34;https://fosstodon.org/@njtierney@aus.social&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nick Tierney&lt;/a&gt; explains how to address these situations when &lt;em&gt;error checking code totally eclipses the intent of the code&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.emilyriederer.com/post/team-of-packages/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Building a team of internal R packages&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://www.emilyriederer.com/about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emily Riederer&lt;/a&gt; delves into the particularities of building a team of R packages to do jobs helping a organization answer impactful questions.&lt;/p&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://archive.org/details/francois-chollet-deep-learning-with-python-manning-2018&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Deep Learning With Python&lt;/strong&gt;&lt;/a&gt;: This book by 
&lt;a href=&#34;https://fchollet.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;François Chollet&lt;/a&gt;, Software Engineer at Google and creator of the Keras library, seems to me like the best resource out there for those wanting to understand and build deep learning models from scratch. I have a hard copy on my desk, and I am finding it pretty easy to follow. Also, the code examples are clearly explained, and they ramp up in a very consistent manner.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.emilyriederer.com/post/py-rgo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Python Rgonomics&lt;/strong&gt;&lt;/a&gt;: In this post, 
&lt;a href=&#34;https://www.emilyriederer.com/about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emily Riederer&lt;/a&gt; offers a list of Python libraries with an &amp;ldquo;R feeling&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;coding-workflow&#34;&gt;Coding Workflow&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://graphite.dev/blog/stacked-prs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;How to use stacked PRs to unblock your entire team&lt;/strong&gt;&lt;/a&gt;: This post in 
&lt;a href=&#34;https://graphite.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graphite&lt;/a&gt;&amp;rsquo;s blog explains how to split large coding changes into small managed PRs (aka &amp;ldquo;stacked PRs&amp;rdquo;) to avoid blocks when PR reviews are hard to come by.&lt;/p&gt;
&lt;h1 id=&#34;other-fancy-things&#34;&gt;Other Fancy Things&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://vickiboykis.com/2024/01/15/whats-new-with-ml-in-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;What&amp;rsquo;s new with ML in production&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://vickiboykis.com/about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vicki Boykis&lt;/a&gt;, machine learning engineer at Mozilla.ai, goes deep into the differences and similarities between classical Machine Learning approaches and Large Language Models. I learned a lot from this read!&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://youtu.be/T-D1OfcDW1M?si=sAZO-5NGD8yF2WYe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;What is Retrieval-Augmented Generation (RAG)?&lt;/strong&gt;&lt;/a&gt;: In this video, Marina Danilevsky, Senior Data Scientist at IBM, offers a pretty good explanation on how the 
&lt;a href=&#34;https://research.ibm.com/blog/retrieval-augmented-generation-RAG?utm_id=YT-101-What-is-RAG&amp;amp;_gl=1*p6ef17*_ga*MTQwMzQ5NjMwMi4xNjkxNDE2MDc0*_ga_FYECCCS21D*MTY5MjcyMjgyNy40My4xLjE2OTI3MjMyMTcuMC4wLjA.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Retrieval-Augmented Generation&lt;/a&gt; method can improve the credibility of large language models.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.nature.com/articles/s41598-020-79148-7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A novel framework for spatio-temporal prediction of environmental data using deep learning&lt;/strong&gt;&lt;/a&gt;: This paper by 
&lt;a href=&#34;https://www.linkedin.com/in/federico-amato-66208637&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Federico Amato&lt;/a&gt; and collaborators describes an intriguing regression method combining a feedforward neural network with empirical orthogonal functions for spatio-temporal interpolation. Regrettably, the paper offers no code or data at all, but it&amp;rsquo;s still an interesting read.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2310.10196.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Large Models for Time Series and
Spatio-Temporal Data A Survey and Outlook&lt;/strong&gt;&lt;/a&gt;: This pre-print by Weng and collaborators reviews the current state of the art in spatio-temporal modelling with Large Language Models and Pre-Trained Foundation Models.&lt;/p&gt;
&lt;h1 id=&#34;management-and-leadership&#34;&gt;Management and Leadership&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://zaidesanton.substack.com/p/using-fake-deadlines-without-driving?publication_id=1804629&amp;amp;post_id=152010688&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Using fake deadlines without driving your engineers crazy&lt;/strong&gt;&lt;/a&gt;: In this post, 
&lt;a href=&#34;https://substack.com/@jstanier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;James Stanier&lt;/a&gt; explains how fake deadlines can help push projects forward in healthy work environments.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://zaidesanton.substack.com/p/when-engineering-managers-become&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;You are hurting your team without even noticing&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://substack.com/@zaidesanton&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anton Zaides&lt;/a&gt; (Development Team Leader), and 
&lt;a href=&#34;https://substack.com/@crushingtecheducation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eugene Shulga&lt;/a&gt;, (Software Engineer) offers insight on the harmful effects of a manager&amp;rsquo;s ego in their team dynamics.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://the.managers.guide/p/teamwork-habits-for-leaders&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Teamwork Habits for Leaders&lt;/strong&gt;&lt;/a&gt;: This post by 
&lt;a href=&#34;https://substack.com/@ochronus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Csaba Okrona&lt;/a&gt; focuses on how shifting from talker to listener in team meetings offers a good insight to better address the team&amp;rsquo;s needs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Everything You Don&#39;t Need to Know About Variance Inflation Factors</title>
      <link>https://blasbenito.com/post/variance-inflation-factor/</link>
      <pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/variance-inflation-factor/</guid>
      <description>&lt;h1 id=&#34;resources&#34;&gt;Resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/BlasBenito/notebooks/blob/main/variance_inflation_factors.Rmd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rmarkdown notebook used in this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://blasbenito.com/post/multicollinearity-model-interpretability/&#34;&gt;Multicollinearity Hinders Model Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R package &lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;This post focuses on Variance Inflation Factors (VIF) and their crucial role in identifying multicollinearity within linear models.&lt;/p&gt;
&lt;p&gt;The post covers the following main points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VIF meaning and interpretation&lt;/strong&gt;: Through practical examples, I demonstrate how to compute VIF values and their significance in model design. Particularly, I try to shed light on their influence on coefficient estimates and their confidence intervals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Impact of High VIF&lt;/strong&gt;: I use a small simulation to show how having a model design with a high VIF hinders the identification of predictors with moderate effects, particularly in situations with limited data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Effective VIF Management&lt;/strong&gt;: I introduce how to use the &lt;code&gt;collinear&lt;/code&gt; package and its &lt;code&gt;vif_select()&lt;/code&gt; function. to aid in the selection of predictors with low VIF, thereby enhancing model stability and interpretability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ultimately, this post serves as a comprehensive resource for understanding, interpreting, and managing VIF in the context of linear modeling. It caters to those with a strong command of R and a keen interest in statistical modeling.&lt;/p&gt;
&lt;h1 id=&#34;r-packages&#34;&gt;R packages&lt;/h1&gt;
&lt;p&gt;This tutorial requires the development version (&amp;gt;= 1.0.3) of the newly released R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;, and a few more.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#required
install.packages(&amp;quot;remotes&amp;quot;)
remotes::install_github(
  repo = &amp;quot;blasbenito/collinear&amp;quot;, 
  ref = &amp;quot;development&amp;quot;
  )
install.packages(&amp;quot;ranger&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)
install.packages(&amp;quot;ggplot2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;example-data&#34;&gt;Example data&lt;/h1&gt;
&lt;p&gt;This post uses the &lt;code&gt;toy&lt;/code&gt; data set shipped with the version &amp;gt;= 1.0.3 of the R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;. It is a data frame of centered and scaled variables representing a model design of the form &lt;code&gt;y ~ a + b + c + d&lt;/code&gt;, where the predictors show varying degrees of relatedness. Let&amp;rsquo;s load and check it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
library(ggplot2)
library(collinear)

toy |&amp;gt; 
  round(3) |&amp;gt; 
  head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        y      a      b      c      d
## 1  0.655  0.342 -0.158  0.254  0.502
## 2  0.610  0.219  1.814  0.450  1.373
## 3  0.316  1.078 -0.643  0.580  0.673
## 4  0.202  0.956 -0.815  1.168 -0.147
## 5 -0.509 -0.149 -0.356 -0.456  0.187
## 6  0.675  0.465  1.292 -0.020  0.983
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The columns in &lt;code&gt;toy&lt;/code&gt; are related as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: response generated from &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; using the expression &lt;code&gt;y = a * 0.75 + b * 0.25 + noise&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt;: predictor of &lt;code&gt;y&lt;/code&gt; uncorrelated with &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt;: predictor of &lt;code&gt;y&lt;/code&gt; uncorrelated with &lt;code&gt;a&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: predictor generated as &lt;code&gt;c = a + noise&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d&lt;/code&gt;: predictor generated as &lt;code&gt;d = (a + b)/2 + noise&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The pairwise correlations between all predictors in &lt;code&gt;toy&lt;/code&gt; are shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::cor_df(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x y correlation
## 1 c a  0.96154984
## 2 d b  0.63903887
## 3 d a  0.63575882
## 4 d c  0.61480312
## 5 b a -0.04740881
## 6 c b -0.04218308
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep these pairwise correlations in mind for what comes next!&lt;/p&gt;
&lt;h1 id=&#34;the-meaning-of-variance-inflation-factors&#34;&gt;The Meaning of Variance Inflation Factors&lt;/h1&gt;
&lt;p&gt;There are two general cases of multicollinearity in model designs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When there are pairs of predictors highly correlated.&lt;/li&gt;
&lt;li&gt;When there are &lt;strong&gt;predictors that are linear combinations of other predictors&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The focus of this post is on the second one.&lt;/p&gt;
&lt;p&gt;We can say a predictor is a linear combination of other predictors when it can be reasonably predicted from a multiple regression model against all other predictors.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say we focus on &lt;code&gt;a&lt;/code&gt; and fit the multiple regression model &lt;code&gt;a ~ b + c + d&lt;/code&gt;. The higher the R-squared of this model, the more confident we are to say that &lt;code&gt;a&lt;/code&gt; is a linear combination of &lt;code&gt;b + c + d&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#model of a against all other predictors
abcd_model &amp;lt;- lm(
  formula = a ~ b + c + d,
  data = toy
)

#r-squared of the a_model
abcd_R2 &amp;lt;- summary(abcd_model)$r.squared
abcd_R2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9381214
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the R-squared of &lt;code&gt;a&lt;/code&gt; against all other predictors is pretty high, it definitely seems that &lt;code&gt;a&lt;/code&gt; is a linear combination of the other predictors, and we can conclude that there is multicollinearity in the model design.&lt;/p&gt;
&lt;p&gt;However, as informative as this R-squared is, it tells us nothing about the consequences of having multicollinearity in our model design. And this is where &lt;strong&gt;Variance Inflation Factors&lt;/strong&gt;, or &lt;strong&gt;VIF&lt;/strong&gt; for short, come into play.&lt;/p&gt;
&lt;h2 id=&#34;what-are-variance-inflation-factors&#34;&gt;What are Variance Inflation Factors?&lt;/h2&gt;
&lt;p&gt;The Variance Inflation Factor (VIF) of a predictor is computed as &lt;code&gt;\(1/(1 - R^2)\)&lt;/code&gt;, where &lt;code&gt;\(R^²\)&lt;/code&gt; is the R-squared of the multiple linear regression of the predictor against all other predictors.&lt;/p&gt;
&lt;p&gt;In the case of &lt;code&gt;a&lt;/code&gt;, we just have to apply the VIF expression to the R-squared of the regression model against all other predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;abcd_vif &amp;lt;- 1/(1-abcd_R2)
abcd_vif
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16.16067
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This VIF score is relative to the other predictors in the model design. If we change the model design, so does the VIF of all predictors! For example, if we remove &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt; from the model design, we are left with this VIF for &lt;code&gt;a&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ab_model &amp;lt;- lm(
  formula = a ~ b,
  data = toy
)

ab_vif &amp;lt;- 1/(1 - summary(ab_model)$r.squared)
ab_vif
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.002253
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An almost perfect VIF score!&lt;/p&gt;
&lt;p&gt;We can simplify the VIF computation using &lt;code&gt;collinear::vif_df()&lt;/code&gt;, which returns the VIF of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; at once.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::vif_df(
  df = toy[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   predictor    vif
## 1         a 1.0023
## 2         b 1.0023
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In plot below, the worst and best VIF scores of &lt;code&gt;a&lt;/code&gt; are shown in the context of the relationship between R-squared and VIF, and three VIF thresholds commonly mentioned in the literature. These thresholds are represented as vertical dashed lines at VIF 2.5, 5, and 10, and are used as criteria to control multicollinearity in model designs. I will revisit this topic later in the post.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/variance-inflation-factor/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;When the R-squared of the linear regression model is 0, then the VIF expression becomes &lt;code&gt;\(1/(1 - 0) = 1\)&lt;/code&gt; and returns the minimum possible VIF. On the other end, when R-squared is 1, then we get &lt;code&gt;\(1/(1 - 1) = Inf\)&lt;/code&gt;, the maximum VIF.&lt;/p&gt;
&lt;p&gt;So far, we have learned that to assess whether the predictor &lt;code&gt;a&lt;/code&gt; induces multicollinearity in the model design &lt;code&gt;y ~ a + b + c + d&lt;/code&gt; we can compute it&amp;rsquo;s Variance Inflation Factor from the R-squared of the model &lt;code&gt;a ~ b + c + d&lt;/code&gt;. We have also learned that if the model design changes, so does the VIF of &lt;code&gt;a&lt;/code&gt;. We also know that there are some magic numbers (the VIF thresholds) we can use as reference.&lt;/p&gt;
&lt;p&gt;But still, we have no indication of what these VIF values actually mean! I will try to fix that in the next section.&lt;/p&gt;
&lt;h2 id=&#34;but-really-what-are-variance-inflation-factors&#34;&gt;But really, what are Variance Inflation Factors?&lt;/h2&gt;
&lt;p&gt;Variance Inflation Factors are inherently linked to these fundamental linear modeling concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Coefficient Estimate&lt;/strong&gt; (&lt;code&gt;\(\hat{\beta}\)&lt;/code&gt;): The estimated slope of the relationship between a predictor and the response in a linear model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Standard Error&lt;/strong&gt; (&lt;code&gt;\(\text{SE}\)&lt;/code&gt;): Represents the uncertainty around the estimation of the coefficient due to data variability.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Significance level&lt;/strong&gt; (&lt;code&gt;\(1.96\)&lt;/code&gt;): The acceptable level of error when determining the significance of the &lt;em&gt;coefficient estimate&lt;/em&gt;. Here it is simplified to 1.96, the 97.5th percentile of a normal distribution, to approximate a significance level of 0.05.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Confidence Interval&lt;/strong&gt; (&lt;code&gt;\(CI\)&lt;/code&gt;): The range of values containing the true value of the &lt;em&gt;coefficient estimate&lt;/em&gt; withing a certain &lt;em&gt;significance level&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These terms are related by the expression to compute the confidence interval of the coefficient estimate:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$\text{CI} = \beta \pm 1.96 \cdot \text{SE}$$&lt;/code&gt;
Let me convert this equation into a small function to compute confidence intervals of coefficient estimates named &lt;code&gt;ci()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ci &amp;lt;- function(b, se){
  x &amp;lt;- se * 1.96
  as.numeric(c(b-x, b+x))
}
#note: stats::confint() which uses t-critical values to compute more precise confidence intervals. 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are going to look at the coefficient estimate and standard error of &lt;code&gt;a&lt;/code&gt; in the model &lt;code&gt;y ~ a + b&lt;/code&gt;. We know that &lt;code&gt;a&lt;/code&gt; in this model has a vif of 1.0022527.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yab_model &amp;lt;- lm(
  formula = y ~ a + b,
  data = toy
) |&amp;gt; 
  summary()

#coefficient estimate and standard error of a
a_coef &amp;lt;- yab_model$coefficients[2, 1:2]
a_coef
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Estimate  Std. Error 
## 0.747689326 0.006636511
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we plug them into our little function to compute the confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;a_ci &amp;lt;- ci(
  b = a_coef[1], 
  se = a_coef[2]
  )
a_ci
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7346818 0.7606969
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And, finally, we compute the width of the confidence interval for &lt;code&gt;a&lt;/code&gt; as the difference between the extremes of the confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;old_width &amp;lt;- diff(a_ci)
old_width
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02601512
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep this number in mind, it&amp;rsquo;s important.&lt;/p&gt;
&lt;p&gt;Now, let me tell you something weird: &lt;strong&gt;The confidence interval of a predictor is widened by a factor equal to the square root of its Variance Inflation Factor&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So, if the VIF of a predictor is, let&amp;rsquo;s say, 16, then this means that, in a linear model, multicollinearity is inflating the width of its confidence interval by a factor of 4.&lt;/p&gt;
&lt;p&gt;In case you don&amp;rsquo;t want to take my word for it, here goes a demonstration. Now we fit the model &lt;code&gt;y ~ a + b + c + d&lt;/code&gt;, where &lt;code&gt;a&lt;/code&gt; has a vif of 16.1606674. If we follow the definition above, we could now expect an inflation of the confidence interval for &lt;code&gt;a&lt;/code&gt; of about 4.0200333. Let&amp;rsquo;s find out if that&amp;rsquo;s the case!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#model y against all predictors and get summary
yabcd_model &amp;lt;- lm(
  formula = y ~ a + b + c + d,
  data = toy
) |&amp;gt; 
  summary()

#compute confidence interval of a
a_ci &amp;lt;- ci(
  b = yabcd_model$coefficients[&amp;quot;a&amp;quot;, &amp;quot;Estimate&amp;quot;], 
  se = yabcd_model$coefficients[&amp;quot;a&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )

#compute width of confidence interval of a
new_width &amp;lt;- diff(a_ci)
new_width
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1044793
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to find out the inflation factor of this new confidence interval, we divide it by the width of the old one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;new_width/old_width
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.016101
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the result is VERY CLOSE to the square root of the VIF of &lt;code&gt;a&lt;/code&gt; (4.0200333) in this model. &lt;strong&gt;Notice that this works because in the model &lt;code&gt;y ~ a + b&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt; has a perfect VIF of 1.0022527. This demonstration needs a model with a quasi-perfect VIF as reference.&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now we can confirm our experiment about the meaning of VIF by repeating the exercise with &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;First we compute the VIF of &lt;code&gt;b&lt;/code&gt; against &lt;code&gt;a&lt;/code&gt; alone, and against &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;, and the expected level of inflation of the confidence interval as the square root of the second VIF.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#vif of b vs a
ba_vif &amp;lt;- collinear::vif_df(
  df = toy[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]
) |&amp;gt; 
  dplyr::filter(predictor == &amp;quot;b&amp;quot;)

#vif of b vs a c d
bacd_vif &amp;lt;- collinear::vif_df(
  df = toy[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]
) |&amp;gt; 
  dplyr::filter(predictor == &amp;quot;b&amp;quot;)

#expeced inflation of the confidence interval
sqrt(bacd_vif$vif)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.015515
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, since &lt;code&gt;b&lt;/code&gt; is already in the models &lt;code&gt;y ~ a + b&lt;/code&gt; and &lt;code&gt;y ~ a + b + c + d&lt;/code&gt;, we just need to extract its coefficients, compute their confidence intervals, and divide one by the other to obtain the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#compute confidence interval of b in y ~ a + b
b_ci_old &amp;lt;- ci(
  b = yab_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;], 
  se = yab_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )

#compute confidence interval of b in y ~ a + b + c + d
b_ci_new &amp;lt;- ci(
  b = yabcd_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;], 
  se = yabcd_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )

#compute inflation
diff(b_ci_new)/diff(b_ci_old)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.013543
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, the square root of the VIF of &lt;code&gt;b&lt;/code&gt; in &lt;code&gt;y ~ a + b + c + d&lt;/code&gt; is a great indicator of how much the confidence interval of &lt;code&gt;b&lt;/code&gt; is inflated by multicollinearity in the model.&lt;/p&gt;
&lt;p&gt;And that, folks, is the meaning of VIF.&lt;/p&gt;
&lt;h1 id=&#34;when-the-vif-hurts&#34;&gt;When the VIF Hurts&lt;/h1&gt;
&lt;p&gt;In the previous sections we acquired an intuition of how Variance Inflation Factors measure the effect of multicollinearity in the precision of the coefficient estimates in a linear model. But there is more to that!&lt;/p&gt;
&lt;p&gt;A coefficient estimate divided by its standard error results in the &lt;strong&gt;T statistic&lt;/strong&gt;. This number is named &amp;ldquo;t value&amp;rdquo; in the table of coefficients shown below, and represents the distance (in number of standard errors) between the estimate and zero.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yabcd_model$coefficients[-1, ] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Estimate Std. Error t value Pr(&amp;gt;|t|)
## a   0.7184     0.0267 26.9552   0.0000
## b   0.2596     0.0134 19.4253   0.0000
## c   0.0273     0.0232  1.1757   0.2398
## d   0.0039     0.0230  0.1693   0.8656
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;p-value&lt;/strong&gt;, named &amp;ldquo;Pr(&amp;gt;|t|)&amp;rdquo; above, is the probability of getting the T statistic when there is &lt;em&gt;no effect of the predictor over the response&lt;/em&gt;. The part in italics is named the &lt;em&gt;null hypothesis&lt;/em&gt; (H0), and happens when the confidence interval of the estimate intersects with zero, as in &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ci(
  b = yabcd_model$coefficients[&amp;quot;c&amp;quot;, &amp;quot;Estimate&amp;quot;],
  se = yabcd_model$coefficients[&amp;quot;c&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.01819994  0.07276692
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ci(
  b = yabcd_model$coefficients[&amp;quot;d&amp;quot;, &amp;quot;Estimate&amp;quot;],
  se = yabcd_model$coefficients[&amp;quot;d&amp;quot;, &amp;quot;Std. Error&amp;quot;]
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.04111146  0.04888457
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value of any predictor in the coefficients table above is computed as:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#predictor
predictor &amp;lt;- &amp;quot;d&amp;quot;

#number of cases
n &amp;lt;- nrow(toy)

#number of model terms
p &amp;lt;- nrow(yabcd_model$coefficients)

#one-tailed p-value
#q = absolute t-value
#df = degrees of freedom
p_value_one_tailed &amp;lt;- stats::pt(
  q = abs(yabcd_model$coefficients[predictor, &amp;quot;t value&amp;quot;]), 
  df = n - p #degrees of freedom
  )

#two-tailed p-value
2 * (1 - p_value_one_tailed)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8655869
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This p-value is then compared to a &lt;strong&gt;significance level&lt;/strong&gt; (for example, 0.05 for a 95% confidence), which is just the lowest p-value acceptable as strong evidence to make a claim:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;p-value &amp;gt; significance&lt;/strong&gt;: Evidence to claim that the predictor has no effect on the response. If the claim is wrong (we&amp;rsquo;ll see whey we could be wrong), we fall into a &lt;em&gt;false negative&lt;/em&gt; (also &lt;em&gt;Type II Error&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;p-value &amp;lt;= significance&lt;/strong&gt;: Evidence to claim that the predictor has an effect on the response. If the claim is wrong, we fall into a &lt;em&gt;false positive&lt;/em&gt; (also &lt;em&gt;Type I Error&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, how does all this matter when talking about the Variance Inflation Factor? Because a high VIF triggers a cascade of effects that increases p-values that can mess up your claims about the importance of the predictors!&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    ↑ VIF ► ↑ Std. Error ► ↓ T statistic  ► ↑ p-value  ► ↑  false negatives (Type II Error)
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;This cascade becomes a problem when the predictor has a small effect on the response, and the number of cases is small.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see how this works with &lt;code&gt;b&lt;/code&gt;. This predictor has a solid effect on the response &lt;code&gt;y&lt;/code&gt; (nonetheless, &lt;code&gt;y&lt;/code&gt; was created as &lt;code&gt;a * 0.75 + b * 0.25 + noise&lt;/code&gt;). It has a coefficient around 0.25, and a p-value of 0, so there is little to no risk of falling into a false negative when claiming that it is important to explain &lt;code&gt;y&lt;/code&gt;, even when its confidence interval is inflated by a factor of two in the full model.&lt;/p&gt;
&lt;p&gt;But let&amp;rsquo;s try a little experiment. We are going to create many small versions of &lt;code&gt;toy&lt;/code&gt;, using only 30 cases selected by chance over a number of iterations, we are going to fit models in which &lt;code&gt;b&lt;/code&gt; has a lower and a higher VIF, to monitor its p-values and estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#number of repetitions
repetitions &amp;lt;- 1000

#number of cases to subset in toy
sample_size &amp;lt;- 30

#vectors to store results
lowvif_p_value &amp;lt;- 
  highvif_p_value &amp;lt;- 
  lowvif_estimate &amp;lt;-
  highvif_estimate &amp;lt;- 
  vector(length = repetitions)

#repetitions
for(i in 1:repetitions){
  
  #seed to make randomization reproducible
  set.seed(i)
  
  #toy subset
  toy.i &amp;lt;- toy[sample(x = 1:nrow(toy), size = sample_size), ]
  
  #high vif model
  highvif_model &amp;lt;- lm(
    formula =  y ~ a + b + c + d,
    data = toy.i
  ) |&amp;gt; 
    summary()
  
  #gather results of high vif model
  highvif_p_value[i] &amp;lt;- highvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
  highvif_estimate[i] &amp;lt;- highvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;]
  
  #low_vif_model
  lowvif_model &amp;lt;- lm(
    formula =  y ~ a + b,
    data = toy.i
  ) |&amp;gt; 
    summary()
  
  #gather results of lowvif
  lowvif_p_value[i] &amp;lt;- lowvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Pr(&amp;gt;|t|)&amp;quot;]
  lowvif_estimate[i] &amp;lt;- lowvif_model$coefficients[&amp;quot;b&amp;quot;, &amp;quot;Estimate&amp;quot;]
  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below shows all p-values of the predictor &lt;code&gt;b&lt;/code&gt; for the high and low VIF models across the experiment repetitions.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/variance-inflation-factor/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;At a significance level of 0.05, the &lt;strong&gt;high VIF&lt;/strong&gt; model rejects &lt;code&gt;b&lt;/code&gt; as an important predictor of &lt;code&gt;y&lt;/code&gt; on 53.5% of the model repetitions, while the &lt;strong&gt;low VIf model&lt;/strong&gt; does the same on 2.2% of repetitions. This is a clear case of increase in Type II Error (false negatives) under multicollinearity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Under multicollinearity, the probability of overlooking predictors with moderate effects increases dramatically!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The plot below shifts the focus towards the coefficient estimates for &lt;code&gt;b&lt;/code&gt; across repetitions.&lt;/p&gt;
&lt;img src=&#34;https://blasbenito.com/post/variance-inflation-factor/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;480&#34; /&gt;
&lt;p&gt;The gray vertical line represents the real value of the slope of &lt;code&gt;b&lt;/code&gt;, and each dot represents a model repetition. The coefficients of the &lt;strong&gt;high VIF&lt;/strong&gt; model are all over the place when compared to the &lt;strong&gt;low VIF&lt;/strong&gt; one. Probably you have read somewhere that &amp;ldquo;multicollinearity induces model instability&amp;rdquo;, or something similar, and that is exactly what we are seeing here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finding the true effect of a predictor with a moderate effect becomes harder under multicollinearity.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;managing-vif-in-a-model-design&#34;&gt;Managing VIF in a Model Design&lt;/h1&gt;
&lt;p&gt;The second most common form of modeling self-sabotage is &lt;em&gt;having high VIF predictors in a model design&lt;/em&gt;, just right after &lt;em&gt;throwing deep learning at tabular problems to see what sticks&lt;/em&gt;. I don&amp;rsquo;t have solutions for the deep learning issue, but I have some pointers for the VIFs one: &lt;strong&gt;letting things go!&lt;/strong&gt;. And with &lt;em&gt;things&lt;/em&gt; I mean &lt;em&gt;predictors&lt;/em&gt;, not the pictures of your old love. There is no rule &lt;em&gt;the more predictors the better&lt;/em&gt; rule written anywhere relevant, and letting your model shed some fat is the best way to go here.&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt; package has something to help here. The function 
&lt;a href=&#34;https://blasbenito.github.io/collinear/reference/vif_select.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear::vif_select()&lt;/code&gt;&lt;/a&gt; is specifically designed to help reduce VIF in a model design. And it can do it in two ways: either using domain knowledge to guide the process, or applying quantitative criteria instead.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s follow the domain knowledge route first. Imagine you know a lot about &lt;code&gt;y&lt;/code&gt;, you have read that &lt;code&gt;a&lt;/code&gt; is very important to explain it, and you need to discuss this predictor in your results. But you are on the fence about the other predictors, so you don&amp;rsquo;t really care about what others are in the design. You can express such an idea using the argument &lt;code&gt;preference_order&lt;/code&gt;, as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = &amp;quot;a&amp;quot;,
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you have it, your new model design with a VIF below 2.5 is now &lt;code&gt;y ~ a + b&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;But what if you get new information and it turns out that &lt;code&gt;d&lt;/code&gt; is also a variable of interest? Then you should just modify &lt;code&gt;preference_order&lt;/code&gt; to include this new information.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = c(&amp;quot;a&amp;quot;, &amp;quot;d&amp;quot;),
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;d&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that if your favorite variables are highly correlated, some of them are going to be removed anyway. For example, if &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; are your faves, since they are highly correlated, &lt;code&gt;c&lt;/code&gt; is removed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = c(&amp;quot;a&amp;quot;, &amp;quot;c&amp;quot;),
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In either case, you can now build your model while being sure that the coefficients of these predictors are going to be stable and precise.&lt;/p&gt;
&lt;p&gt;Now, what if &lt;code&gt;y&lt;/code&gt; is totally new for you, and you have no idea about what to use? In this case, the function 
&lt;a href=&#34;https://blasbenito.github.io/collinear/reference/preference_order.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear::preference_order()&lt;/code&gt;&lt;/a&gt; helps you rank the predictors following a quantiative criteria, and after that, &lt;code&gt;collinear::vif_select()&lt;/code&gt; can use it to reduce your VIFs.&lt;/p&gt;
&lt;p&gt;By default, &lt;code&gt;collinear::preference_order()&lt;/code&gt; calls 
&lt;a href=&#34;https://blasbenito.github.io/collinear/reference/f_rsquared.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear::f_rsquared()&lt;/code&gt;&lt;/a&gt; to compute the R-squared between each predictor and the response variable (that&amp;rsquo;s why the argument &lt;code&gt;response&lt;/code&gt; is required here), to return a data frame with the variables ranked from &amp;ldquo;better&amp;rdquo; to &amp;ldquo;worse&amp;rdquo;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;preference &amp;lt;- collinear::preference_order(
  df = toy,
  response = &amp;quot;y&amp;quot;,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  f = collinear::f_r2_pearson,
  quiet = TRUE
)

preference
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   response predictor                       f preference
## 1        y         a collinear::f_r2_pearson 0.77600503
## 2        y         c collinear::f_r2_pearson 0.72364944
## 3        y         d collinear::f_r2_pearson 0.59345954
## 4        y         b collinear::f_r2_pearson 0.07343563
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can use this data frame as input for the argument &lt;code&gt;preference_order&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;selected_predictors &amp;lt;- collinear::vif_select(
  df = toy,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;),
  preference_order = preference,
  max_vif = 2.5,
  quiet = TRUE
)
selected_predictors
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;d&amp;quot;
## attr(,&amp;quot;validated&amp;quot;)
## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now at least you can be sure that the predictors in your model design have low VIF, and were selected taking their correlation with the response as criteria.&lt;/p&gt;
&lt;p&gt;Well, I think that&amp;rsquo;s enough for today. I hope you found this post helpful. Have a great time!&lt;/p&gt;
&lt;p&gt;Blas&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multicollinearity Hinders Model Interpretability</title>
      <link>https://blasbenito.com/post/multicollinearity-model-interpretability/</link>
      <pubDate>Sun, 29 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://blasbenito.com/post/multicollinearity-model-interpretability/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This post is written for beginner to intermediate R users wishing to learn what multicollinearity is and how it can turn model interpretation into a challenge.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post, I delve into the intricacies of model interpretation under the influence of multicollinearity, and use R and a toy data set to demonstrate how this phenomenon impacts both linear and machine learning models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The section &lt;em&gt;Multicollinearity Explained&lt;/em&gt; explains the origin of the word and the nature of the problem.&lt;/li&gt;
&lt;li&gt;The section &lt;em&gt;Model Interpretation Challenges&lt;/em&gt; describes how to create the toy data set, and applies it to &lt;em&gt;Linear Models&lt;/em&gt; and &lt;em&gt;Random Forest&lt;/em&gt; to explain how multicollinearity can make model interpretation a challenge.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;Appendix&lt;/em&gt; shows extra examples of linear and machine learning models affected by multicollinearity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I hope you&amp;rsquo;ll enjoy it!&lt;/p&gt;
&lt;h1 id=&#34;r-packages&#34;&gt;R packages&lt;/h1&gt;
&lt;p&gt;This tutorial requires the newly released R package 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt;, and a few more listed below. The optional ones are used only in the &lt;em&gt;Appendix&lt;/em&gt; at the end of the post.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#required
install.packages(&amp;quot;collinear&amp;quot;)
install.packages(&amp;quot;ranger&amp;quot;)
install.packages(&amp;quot;dplyr&amp;quot;)

#optional
install.packages(&amp;quot;nlme&amp;quot;)
install.packages(&amp;quot;glmnet&amp;quot;)
install.packages(&amp;quot;xgboost&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;multicollinearity-explained&#34;&gt;Multicollinearity Explained&lt;/h1&gt;
&lt;p&gt;This cute word comes from the amalgamation of these three Latin terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;multus&lt;/em&gt;: adjective meaning &lt;em&gt;many&lt;/em&gt; or &lt;em&gt;multiple&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;con&lt;/em&gt;: preposition often converted to &lt;em&gt;co-&lt;/em&gt; (as in &lt;em&gt;co-worker&lt;/em&gt;) meaning &lt;em&gt;together&lt;/em&gt; or &lt;em&gt;mutually&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;linealis&lt;/em&gt; (later converted to &lt;em&gt;linearis&lt;/em&gt;): from &lt;em&gt;linea&lt;/em&gt; (line), adjective meaning &amp;ldquo;resembling a line&amp;rdquo; or &amp;ldquo;belonging to a line&amp;rdquo;, among others.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After looking at these serious words, we can come up with a (VERY) liberal translation: &amp;ldquo;several things together in the same line&amp;rdquo;. From here, we just have to replace the word &amp;ldquo;things&amp;rdquo; with &amp;ldquo;predictors&amp;rdquo; (or &amp;ldquo;features&amp;rdquo;, or &amp;ldquo;independent variables&amp;rdquo;, whatever rocks your boat) to build an intuition of the whole meaning of the word in the context of statistical and machine learning modeling.&lt;/p&gt;
&lt;p&gt;If I lost you there, we can move forward with this idea instead: &lt;strong&gt;multicollinearity happens when there are redundant predictors in a modeling dataset&lt;/strong&gt;. A predictor can be redundant because it shows a high pairwise correlation with other predictors, or because it is a linear combination of other predictors. For example, in a data frame with the columns &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt;, if the correlation between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; is high, we can say that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are mutually redundant and there is multicollinearity. But also, if &lt;code&gt;c&lt;/code&gt; is the result of a linear operation between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, like &lt;code&gt;c &amp;lt;- a + b&lt;/code&gt;, or &lt;code&gt;c &amp;lt;- a * 1 + b * 0.5&lt;/code&gt;, then we can also say that there is multicollinearity between &lt;code&gt;c&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt;, and &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Multicollinearity is a fact of life that lurks in most data sets. For example, in climate data, variables like temperature, humidity and air pressure are closely intertwined, leading to multicollinearity. That&amp;rsquo;s the case as well in medical research, where parameters like blood pressure, heart rate, and body mass index frequently display common patterns. Economic analysis is another good example, as variables such as Gross Domestic Product (GDP), unemployment rate, and consumer spending often exhibit multicollinearity.&lt;/p&gt;
&lt;h1 id=&#34;model-interpretation-challenges&#34;&gt;Model Interpretation Challenges&lt;/h1&gt;
&lt;p&gt;Multicollinearity isn&amp;rsquo;t inherently problematic, but it can be a real buzz kill when the goal is interpreting predictor importance in explanatory models. In the presence of highly correlated predictors, most modelling methods, from the veteran linear models to the fancy gradient boosting, attribute a large part of the importance to only one of the predictors and not the others. In such cases, neglecting multicollinearity will certainly lead to underestimate the relevance of certain predictors.&lt;/p&gt;
&lt;p&gt;Let me go ahead and develop a toy data set to showcase this issue. But let&amp;rsquo;s load the required libraries first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#load the collinear package and its example data
library(collinear)
data(vi)

#other required libraries
library(ranger)
library(dplyr)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;vi&lt;/code&gt; data frame shipped with the 
&lt;a href=&#34;https://blasbenito.github.io/collinear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;collinear&lt;/code&gt;&lt;/a&gt; package, the variables &amp;ldquo;soil_clay&amp;rdquo; and &amp;ldquo;humidity_range&amp;rdquo; are not correlated at all (Pearson correlation = -0.06).&lt;/p&gt;
&lt;p&gt;In the code block below, the &lt;code&gt;dplyr::transmute()&lt;/code&gt; command selects and renames them as &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. After that, the two variables are scaled and centered, and &lt;code&gt;dplyr::mutate()&lt;/code&gt; generates a few new columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: response variable resulting from a linear model where &lt;code&gt;a&lt;/code&gt; has a slope of 0.75, &lt;code&gt;b&lt;/code&gt; has a slope of 0.25, plus a bit of white noise generated with &lt;code&gt;runif()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: a new predictor highly correlated with &lt;code&gt;a&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d&lt;/code&gt;: a new predictor resulting from a linear combination of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(1)
df &amp;lt;- vi |&amp;gt;
  dplyr::slice_sample(n = 2000) |&amp;gt;
  dplyr::transmute(
    a = soil_clay,
    b = humidity_range
  ) |&amp;gt;
  scale() |&amp;gt;
  as.data.frame() |&amp;gt; 
  dplyr::mutate(
    y = a * 0.75 + b * 0.25 + runif(n = dplyr::n(), min = -0.5, max = 0.5),
    c = a + runif(n = dplyr::n(), min = -0.5, max = 0.5),
    d = (a + b)/2 + runif(n = dplyr::n(), min = -0.5, max = 0.5)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Pearson correlation between all pairs of these predictors is shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collinear::cor_df(
  df = df,
  predictors = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x y correlation
## 1 c a  0.96154984
## 2 d b  0.63903887
## 3 d a  0.63575882
## 4 d c  0.61480312
## 5 b a -0.04740881
## 6 c b -0.04218308
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, we have are two groups of predictors useful to understand how multicollinearity muddles model interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictors with &lt;strong&gt;no&lt;/strong&gt; multicollinearity: &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Predictors with multicollinearity: &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the next two sections and the &lt;em&gt;Appendix&lt;/em&gt;, I show how and why model interpretation becomes challenging when multicollinearity is high. Let&amp;rsquo;s start with linear models.&lt;/p&gt;
&lt;h3 id=&#34;linear-models&#34;&gt;Linear Models&lt;/h3&gt;
&lt;p&gt;The code below fits &lt;em&gt;multiple linear regression models&lt;/em&gt; for both groups of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#non-collinear predictors
lm_ab &amp;lt;- lm(
  formula = y ~ a + b,
  data = df
  )

#collinear predictors
lm_abcd &amp;lt;- lm(
  formula = y ~ a + b + c + d,
  data = df
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I would like you to pay attention to the estimates of the predictors &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; for both models. The estimates are the slopes in the linear model, a direct indication of the effect of a predictor over the response.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coefficients(lm_ab)[2:3] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coefficients(lm_abcd)[2:5] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On one hand, the model with no multicollinearity (&lt;code&gt;lm_ab&lt;/code&gt;) achieved a pretty good solution for the coefficients of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. Remember that we created &lt;code&gt;y&lt;/code&gt; as &lt;code&gt;a * 0.75 + b * 0.25&lt;/code&gt; plus some noise, and that&amp;rsquo;s exactly what the model is telling us here, so the interpretation is pretty straightforward.&lt;/p&gt;
&lt;p&gt;On the other hand, the model with multicollinearity (&lt;code&gt;lm_abcd&lt;/code&gt;) did well with &lt;code&gt;b&lt;/code&gt;, but there are a few things in there that make the interpretation harder.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The coefficient of &lt;code&gt;a&lt;/code&gt; (0.7165) is slightly smaller than the true one (0.75), which could lead us to downplay its relationship with &lt;code&gt;y&lt;/code&gt; by a tiny bit. This is kinda OK though, as long as one is not using the model&amp;rsquo;s results to build nukes in the basement.&lt;/li&gt;
&lt;li&gt;The coefficient of &lt;code&gt;c&lt;/code&gt; is so small that it could led us to believe that this predictor not important at all to explain &lt;code&gt;y&lt;/code&gt;. But we know that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; are almost identical copies, so model interpretation here is being definitely muddled by multicollinearity.&lt;/li&gt;
&lt;li&gt;The coefficient of &lt;code&gt;d&lt;/code&gt; is tiny. Since &lt;code&gt;d&lt;/code&gt; results from the sum of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, we could expect this predictor to be important in explaining &lt;code&gt;y&lt;/code&gt;, but it got the shorter end of the stick in this case.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is not that the model it&amp;rsquo;s wrong though. This behavior of the linear model results from the &lt;em&gt;QR decomposition&lt;/em&gt; (also &lt;em&gt;QR factorization&lt;/em&gt;) applied by functions like &lt;code&gt;lm()&lt;/code&gt;, &lt;code&gt;glm()&lt;/code&gt;, &lt;code&gt;glmnet::glmnet()&lt;/code&gt;, and &lt;code&gt;nlme::gls()&lt;/code&gt; to improve numerical stability and computational efficiency, and to&amp;hellip; address multicollinearity in the model predictors.&lt;/p&gt;
&lt;p&gt;The QR decomposition transforms the original predictors into a set of orthogonal predictors with no multicollinearity. This is the &lt;em&gt;Q matrix&lt;/em&gt;, created in a fashion that resembles the way in which a Principal Components Analysis generates uncorrelated components from a set of correlated variables.&lt;/p&gt;
&lt;p&gt;The code below applies QR decomposition to our multicollinear predictors, extracts the Q matrix, and shows the correlation between the new versions of &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#predictors names
predictors &amp;lt;- c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)

#QR decomposition of predictors
df.qr &amp;lt;- qr(df[, predictors])

#extract Q matrix
df.q &amp;lt;- qr.Q(df.qr)
colnames(df.q) &amp;lt;- predictors

#correlation between transformed predictors
collinear::cor_df(df = df.q)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x y   correlation
## 1 d c  4.823037e-04
## 2 d a  1.585298e-16
## 3 c b  1.708728e-17
## 4 c a -3.036108e-18
## 5 d b  2.385256e-18
## 6 b a -1.431486e-18
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new set of predictors we are left with after the QR decomposition have exactly zero correlation! And now they are not our original predictors anymore, and have a different interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt; is now &amp;ldquo;the part of &lt;code&gt;a&lt;/code&gt; not in &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt; is now &amp;ldquo;the part of &lt;code&gt;b&lt;/code&gt; not in &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&amp;hellip;and so on&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The result of the QR decomposition can be plugged into the &lt;code&gt;solve()&lt;/code&gt; function along with the response vector to estimate the coefficients of the linear model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solve(a = df.qr, b = df$y) |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7189 0.2595 0.0268 0.0040
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are almost exactly the ones we got for our model with multicollinearity. In the end, the coefficients resulting from a linear model are not those of the original predictors, but the ones of their uncorrelated versions generated by the QR decomposition.&lt;/p&gt;
&lt;p&gt;But this is not the only issue of model interpretability under multicollinearity. Let&amp;rsquo;s take a look at the standard errors of the estimates. These are a measure of the coefficient estimation uncertainty, and are used to compute the p-values of the estimates. As such, they are directly linked with the &amp;ldquo;statistical significance&amp;rdquo; (whatever that means) of the predictors within the model.&lt;/p&gt;
&lt;p&gt;The code below shows the standard errors of the model without and with multicollinearity.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(lm_ab)$coefficients[, &amp;quot;Std. Error&amp;quot;][2:3] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.0066 0.0066
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(lm_abcd)$coefficients[, &amp;quot;Std. Error&amp;quot;][2:5] |&amp;gt; 
  round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.0267 0.0134 0.0232 0.0230
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These standard errors of the model with multicollinearity are an order of magnitude higher than the ones of the model without multicollinearity.&lt;/p&gt;
&lt;p&gt;Since our toy dataset is relatively large (2000 cases) and the relationship between the response and a few of the predictors pretty robust, there are no real issues arising, as these differences in estimation precision are not enough to change the p-values of the estimates. However, in a small data set with high multicollinearity and a weaker relationship between the response and the predictors, standard errors of the estimate become wide, which increases p-values and reduces &amp;ldquo;significance&amp;rdquo;. Such a situation might lead us to believe that a predictor does not explain the response, when in fact it does. And this, again, is a model interpretability issue caused by multicollinearity.&lt;/p&gt;
&lt;p&gt;At the end of this post there is an appendix with code examples of other types of linear models that use QR decomposition and become challenging to interpret in the presence of multicollinearity. Play with them as you please!&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s take a look at how multicollinearity can also mess up the interpretation of a commonly used machine learning algorithm.&lt;/p&gt;
&lt;h3 id=&#34;random-forest&#34;&gt;Random Forest&lt;/h3&gt;
&lt;p&gt;It is not uncommon to hear something like &amp;ldquo;random forest is insensitive to multicollinearity&amp;rdquo;. Actually, I cannot confirm nor deny that I have said that before. Anyway, it is kind of true if one is focused on prediction problmes. However, when the aim is interpreting predictor importance scores, then one has to be mindful about multicollinearity as well.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see an example. The code below fits two random forest models with our two sets of predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#non-collinear predictors
rf_ab &amp;lt;- ranger::ranger(
  formula = y ~ a + b,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1 #for reproducibility
)

#collinear predictors
rf_abcd &amp;lt;- ranger::ranger(
  formula = y ~ a + b + c + d,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at the prediction error the two models on the out-of-bag data. While building each regression tree, Random Forest leaves a random subset of the data out. Then, each case gets a prediction from all trees that had it in the out-of-bag data, and the prediction error is averaged across all cases to get the numbers below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_ab$prediction.error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1026779
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abcd$prediction.error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1035678
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to these numbers, these two models are basically equivalent in their ability to predict our response &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But now, you noticed that I set the argument &lt;code&gt;importance&lt;/code&gt; to &amp;ldquo;permutation&amp;rdquo;. Permutation importance quantifies how the out-of-bag error increases when a predictor is permuted across all trees where the predictor is used. It is pretty robust importance metric that bears no resemblance whatsoever with the coefficients of a linear model. Think of it as a very different way to answer the question &amp;ldquo;what variables are important in this model?&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The permutation importance scores of the two random forest models are show below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_ab$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 1.0702 0.1322
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abcd$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.5019 0.0561 0.1662 0.0815
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is one interesting detail here. The predictor &lt;code&gt;a&lt;/code&gt; has a permutation error three times higher than &lt;code&gt;c&lt;/code&gt; in the second model, even though we could expect them to be similar due to their very high correlation. There are two reasons for this mismatch:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Random Forest is much more sensitive to the white noise in &lt;code&gt;c&lt;/code&gt; than linear models, especially in the deep parts of the regression trees, due to local (within-split data) decoupling with the response &lt;code&gt;y&lt;/code&gt;. In consequence, it does not get selected as often as &lt;code&gt;a&lt;/code&gt; in these deeper areas of the trees, and has less overall importance.&lt;/li&gt;
&lt;li&gt;The predictor &lt;code&gt;c&lt;/code&gt; competes with &lt;code&gt;d&lt;/code&gt;, that has around 50% of the information in &lt;code&gt;c&lt;/code&gt; (and &lt;code&gt;a&lt;/code&gt;). If we remove &lt;code&gt;d&lt;/code&gt; from the model, then the permutation importance of &lt;code&gt;c&lt;/code&gt; doubles up. Then, with &lt;code&gt;d&lt;/code&gt; in the model, we underestimate the real importance of &lt;code&gt;c&lt;/code&gt; due to multicollinearity alone.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_abc &amp;lt;- ranger::ranger(
  formula = y ~ a + b + c,
  data = df,
  importance = &amp;quot;permutation&amp;quot;,
  seed = 1
)
rf_abc$variable.importance |&amp;gt; round(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c 
## 0.5037 0.1234 0.3133
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With all that in mind, we can conclude that interpreting importance scores in Random Forest models is challenging when multicollinearity is high. But Random Forest is not the only machine learning affected by this issue. In the Appendix below I have left an example with Extreme Gradient Boosting so you can play with it.&lt;/p&gt;
&lt;p&gt;And that&amp;rsquo;s all for now, folks, I hope you found this post useful!&lt;/p&gt;
&lt;h1 id=&#34;appendix&#34;&gt;Appendix&lt;/h1&gt;
&lt;p&gt;This section shows several extra examples of linear and machine learning models you can play with.&lt;/p&gt;
&lt;h2 id=&#34;other-linear-models-using-qr-decomposition&#34;&gt;Other linear models using QR decomposition&lt;/h2&gt;
&lt;p&gt;As I commented above, many linear modeling functions use QR decomposition, and you will have to be careful interpreting model coefficients in the presence of strong multicollinearity in the predictors.&lt;/p&gt;
&lt;p&gt;Here I show several examples with &lt;code&gt;glm()&lt;/code&gt; (Generalized Linear Models), &lt;code&gt;nlme::gls()&lt;/code&gt; (Generalized Least Squares), and &lt;code&gt;glmnet::cv.glmnet()&lt;/code&gt; (Elastic Net Regularization). In all them, no matter how fancy, the interpretation of coefficients becomes tricky when multicollinearity is high.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generalized Linear Models with glm()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Generalized Linear Models
#non-collinear predictors
glm_ab &amp;lt;- glm(
  formula = y ~ a + b,
  data = df
  )

round(coefficients(glm_ab), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
glm_abcd &amp;lt;- glm(
  formula = y ~ a + b + c + d,
  data = df
  )

round(coefficients(glm_abcd), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Generalized Least Squares with nlme::gls()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(nlme)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;nlme&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:dplyr&#39;:
## 
##     collapse
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Generalized Least Squares
#non-collinear predictors
gls_ab &amp;lt;- nlme::gls(
  model = y ~ a + b,
  data = df
  )

round(coefficients(gls_ab), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b 
## 0.7477 0.2616
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
gls_abcd &amp;lt;- nlme::gls(
  model = y ~ a + b + c + d,
  data = df
  )

round(coefficients(gls_abcd), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      a      b      c      d 
## 0.7184 0.2596 0.0273 0.0039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Elastic Net Regularization and Lasso penalty with glmnet::glmnet()&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(glmnet)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loaded glmnet 4.1-8
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Elastic net regularization with Lasso penalty
#non-collinear predictors
glmnet_ab &amp;lt;- glmnet::cv.glmnet(
  x = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]),
  y = df$y,
  alpha = 1 #lasso penalty
)

round(coef(glmnet_ab$glmnet.fit, s = glmnet_ab$lambda.min), 4)[2:3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7438 0.2578
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#collinear predictors
glmnet_abcd &amp;lt;- glmnet::cv.glmnet(
  x = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  y = df$y,
  alpha = 1 
)

#notice that the lasso regularization nuked the coefficients of predictors b and c
round(coef(glmnet_abcd$glmnet.fit, s = glmnet_abcd$lambda.min), 4)[2:5]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7101 0.2507 0.0267 0.0149
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;extreme-gradient-boosting-under-multicollinearity&#34;&gt;Extreme Gradient Boosting under multicollinearity&lt;/h2&gt;
&lt;p&gt;Gradient Boosting models trained with multicollinear predictors behave in a way similar to linear models with QR decomposition. When two variables are highly correlated, one of them is going to have an importance much higher than the other.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(xgboost)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;xgboost&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:dplyr&#39;:
## 
##     slice
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#without multicollinearity
gb_ab &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
  )

#with multicollinearity
gb_abcd &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xgb.importance(model = gb_ab)[, c(1:2)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature      Gain
##     &amp;lt;char&amp;gt;     &amp;lt;num&amp;gt;
## 1:       a 0.8463005
## 2:       b 0.1536995
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xgb.importance(model = gb_abcd)[, c(1:2)] |&amp;gt; 
  dplyr::arrange(Feature)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature       Gain
##     &amp;lt;char&amp;gt;      &amp;lt;num&amp;gt;
## 1:       a 0.78129661
## 2:       b 0.07386393
## 3:       c 0.03595619
## 4:       d 0.10888327
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But there is a twist too. When two variables are perfectly correlated, one of them is removed right away from the model!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#replace c with perfect copy of a
df$c &amp;lt;- df$a

#with multicollinearity
gb_abcd &amp;lt;- xgboost::xgboost(
  data = as.matrix(df[, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)]),
  label = df$y,
  objective = &amp;quot;reg:squarederror&amp;quot;,
  nrounds = 100,
  verbose = FALSE
)

xgb.importance(model = gb_abcd)[, c(1:2)] |&amp;gt; 
  dplyr::arrange(Feature)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Feature       Gain
##     &amp;lt;char&amp;gt;      &amp;lt;num&amp;gt;
## 1:       a 0.79469959
## 2:       b 0.07857141
## 3:       d 0.12672900
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
